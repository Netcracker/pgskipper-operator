diff --git a/.github/workflows/backup-daemon/build.yaml b/.github/workflows/backup-daemon/build.yaml
new file mode 100644
index 0000000..b79a359
--- /dev/null
+++ b/.github/workflows/backup-daemon/build.yaml
@@ -0,0 +1,68 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-backup-daemon"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-backup-daemon
+            file: Dockerfile
+            context: ""
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}  
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/backup-daemon/cla.yaml b/.github/workflows/backup-daemon/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/backup-daemon/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/backup-daemon/clean.yaml b/.github/workflows/backup-daemon/clean.yaml
new file mode 100644
index 0000000..a889e94
--- /dev/null
+++ b/.github/workflows/backup-daemon/clean.yaml
@@ -0,0 +1,35 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-backup-daemon
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{ secrets.GITHUB_TOKEN }}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/backup-daemon/license.yaml b/.github/workflows/backup-daemon/license.yaml
new file mode 100644
index 0000000..82c392c
--- /dev/null
+++ b/.github/workflows/backup-daemon/license.yaml
@@ -0,0 +1,22 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/dbaas-adapter/build.yaml b/.github/workflows/dbaas-adapter/build.yaml
new file mode 100644
index 0000000..afc627b
--- /dev/null
+++ b/.github/workflows/dbaas-adapter/build.yaml
@@ -0,0 +1,95 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-dbaas-adapter"
+        type: boolean
+        default: false
+        required: false
+
+concurrency:
+  group: build-and-push-${{ github.ref }}
+  cancel-in-progress: true
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  tests:
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Setup Go
+        uses: actions/setup-go@v5
+        with:
+          go-version: '>=1.21.0'
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Run Tests
+        run: make test
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-dbaas-adapter
+            file: build/Dockerfile
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Validate
+        run: |
+          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == refs/tags* ]]; then
+            echo -e "\033[91mManual workflow run on tags is not allowed!\033[0m"
+            exit 1
+          fi
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/dbaas-adapter/cla.yaml b/.github/workflows/dbaas-adapter/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/dbaas-adapter/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/dbaas-adapter/clean.yaml b/.github/workflows/dbaas-adapter/clean.yaml
new file mode 100644
index 0000000..9fa01dc
--- /dev/null
+++ b/.github/workflows/dbaas-adapter/clean.yaml
@@ -0,0 +1,33 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-dbaas-adapter
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: pgskipper-dbaas-adapter
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/dbaas-adapter/license.yaml b/.github/workflows/dbaas-adapter/license.yaml
new file mode 100644
index 0000000..82c392c
--- /dev/null
+++ b/.github/workflows/dbaas-adapter/license.yaml
@@ -0,0 +1,22 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/monitoring-agent/build.yaml b/.github/workflows/monitoring-agent/build.yaml
new file mode 100644
index 0000000..a4520a7
--- /dev/null
+++ b/.github/workflows/monitoring-agent/build.yaml
@@ -0,0 +1,74 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-operator"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-monitoring-agent
+            file: Dockerfile
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Validate
+        run: |
+          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == refs/tags* ]]; then
+            echo -e "\033[91mManual workflow run on tags is not allowed!\033[0m"
+            exit 1
+          fi
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/monitoring-agent/cla.yaml b/.github/workflows/monitoring-agent/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/monitoring-agent/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/monitoring-agent/clean.yaml b/.github/workflows/monitoring-agent/clean.yaml
new file mode 100644
index 0000000..611f6c5
--- /dev/null
+++ b/.github/workflows/monitoring-agent/clean.yaml
@@ -0,0 +1,33 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-monitoring-agent
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
\ No newline at end of file
diff --git a/.github/workflows/monitoring-agent/license.yaml b/.github/workflows/monitoring-agent/license.yaml
new file mode 100644
index 0000000..82c392c
--- /dev/null
+++ b/.github/workflows/monitoring-agent/license.yaml
@@ -0,0 +1,22 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/patroni/build.yaml b/.github/workflows/patroni/build.yaml
new file mode 100644
index 0000000..127c99e
--- /dev/null
+++ b/.github/workflows/patroni/build.yaml
@@ -0,0 +1,84 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-patroni"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-patroni-17
+            file: Dockerfile
+            context: ""
+            pg_version: "17"
+          - name: pgskipper-patroni-16
+            file: Dockerfile
+            context: ""
+            pg_version: "16"
+          - name: pgskipper-patroni-15
+            file: Dockerfile
+            context: ""
+            pg_version: "15"
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Validate
+        run: |
+          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == refs/tags* ]]; then
+            echo -e "\033[91mManual workflow run on tags is not allowed!\033[0m"
+            exit 1
+          fi
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}} 
+        if: ${{ env.PUSH }}
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          build-args: PG_VERSION=${{ matrix.component.pg_version }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/patroni/cla.yaml b/.github/workflows/patroni/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/patroni/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/patroni/clean.yaml b/.github/workflows/patroni/clean.yaml
new file mode 100644
index 0000000..457ecce
--- /dev/null
+++ b/.github/workflows/patroni/clean.yaml
@@ -0,0 +1,43 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-patroni
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-patroni-17
+            context: ""
+          - name: pgskipper-patroni-16
+            context: ""
+          - name: pgskipper-patroni-15
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/patroni/license.yaml b/.github/workflows/patroni/license.yaml
new file mode 100644
index 0000000..6609d79
--- /dev/null
+++ b/.github/workflows/patroni/license.yaml
@@ -0,0 +1,24 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          token: ${{ secrets.GH_ACCESS_TOKEN }}
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/patroni/security-scan.yml b/.github/workflows/patroni/security-scan.yml
new file mode 100644
index 0000000..2ac4e13
--- /dev/null
+++ b/.github/workflows/patroni/security-scan.yml
@@ -0,0 +1,52 @@
+name: Docker Security Scan
+on:
+  workflow_dispatch:
+    inputs:
+      target:
+        description: "Scan part"
+        required: true
+        default: "docker"
+        type: choice
+        options:
+          - docker
+          - source
+      image:
+        description: "Docker image (for 'docker' target). By default ghcr.io/<owner>/<repo>:latest"
+        required: false
+        default: ""
+      only-high-critical:
+        description: "Scan only HIGH + CRITICAL"
+        required: false
+        default: true
+        type: boolean
+      trivy-scan:
+        description: "Run Trivy scan"
+        required: false
+        default: true
+        type: boolean
+      grype-scan:
+        description: "Run Grype scan"
+        required: false
+        default: true
+        type: boolean
+      continue-on-error:
+        description: "Continue on error"
+        required: false
+        default: true
+        type: boolean
+
+permissions:
+  contents: read
+  security-events: write
+  actions: read
+
+jobs:
+  security-scan:
+    uses: netcracker/qubership-workflow-hub/.github/workflows/re-security-scan.yml@main
+    with:
+      target: ${{ github.event.inputs.target || 'source' }}
+      image: ${{ github.event.inputs.image || '' }}
+      only-high-critical: ${{ inputs.only-high-critical}}
+      trivy-scan: ${{ inputs.trivy-scan }}
+      grype-scan: ${{ inputs.grype-scan }}
+      continue-on-error: ${{ inputs.continue-on-error }}
\ No newline at end of file
diff --git a/.github/workflows/pgbckrest-sidecar/build.yaml b/.github/workflows/pgbckrest-sidecar/build.yaml
new file mode 100644
index 0000000..d1d6d76
--- /dev/null
+++ b/.github/workflows/pgbckrest-sidecar/build.yaml
@@ -0,0 +1,74 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-pgbackrest-sidecar"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ github.event_name != 'workflow_dispatch' || inputs.publish_docker }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-pgbackrest-sidecar
+            file: build/Dockerfile
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Validate
+        run: |
+          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == refs/tags* ]]; then
+            echo -e "\033[91mManual workflow run on tags is not allowed!\033[0m"
+            exit 1
+          fi
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}  
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/pgbckrest-sidecar/cla.yaml b/.github/workflows/pgbckrest-sidecar/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/pgbckrest-sidecar/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/pgbckrest-sidecar/clean.yaml b/.github/workflows/pgbckrest-sidecar/clean.yaml
new file mode 100644
index 0000000..0c1ea93
--- /dev/null
+++ b/.github/workflows/pgbckrest-sidecar/clean.yaml
@@ -0,0 +1,33 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-pgbackrest-sidecar
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/pgbckrest-sidecar/license.yaml b/.github/workflows/pgbckrest-sidecar/license.yaml
new file mode 100644
index 0000000..82c392c
--- /dev/null
+++ b/.github/workflows/pgbckrest-sidecar/license.yaml
@@ -0,0 +1,22 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/query-exporter/automatic-pr-labeler.yaml b/.github/workflows/query-exporter/automatic-pr-labeler.yaml
new file mode 100644
index 0000000..b807867
--- /dev/null
+++ b/.github/workflows/query-exporter/automatic-pr-labeler.yaml
@@ -0,0 +1,45 @@
+---
+
+# The workflow template for automatic PR labeler.
+# It requires to have a configuration file with labels and conditions to apply them.
+# The configuration file should be placed in the .github folder and named auto-labeler-config.yaml.
+# Example file can be found there:
+# https://github.com/Netcracker/.github/blob/main/config/examples/auto-labeler-config.yaml
+
+name: Automatic PR Labeler
+
+on:
+  pull_request:
+    branches: [main]
+    types:
+      [opened, reopened, synchronize]
+
+permissions:
+  pull-requests: write
+  contents: read
+  issues: write
+
+jobs:
+  assign-labels:
+    if: (github.event.pull_request.merged == false) && (github.event.pull_request.user.login != 'dependabot[bot]') && (github.event.pull_request.user.login != 'github-actions[bot]')
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: "Execute assign labels"
+        id: action-assign-labels
+        uses: mauroalderete/action-assign-labels@v1
+        with:
+          pull-request-number: ${{ github.event.pull_request.number }}
+          github-token: ${{ github.token }}
+          conventional-commits: "./.github/auto-labeler-config.yaml"
+          maintain-labels-not-matched: true
+          apply-changes: ${{ github.event.pull_request.base.repo.id == github.event.pull_request.head.repo.id }}
+      - name: "Drop warning if PR from fork"
+        if: ${{ github.event.pull_request.base.repo.id != github.event.pull_request.head.repo.id }}
+        run: |
+          {
+            echo "⚠️ Pull request from fork! ⚠️";
+            echo "Labels will not be applied to PR. Assign them manually please.";
+            echo "Labels to assign: ${{ steps.action-assign-labels.outputs.labels-next }}";
+          } >> $GITHUB_STEP_SUMMARY
diff --git a/.github/workflows/query-exporter/build.yaml b/.github/workflows/query-exporter/build.yaml
new file mode 100644
index 0000000..08b9f18
--- /dev/null
+++ b/.github/workflows/query-exporter/build.yaml
@@ -0,0 +1,74 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/qubership-query-exporter"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: qubership-query-exporter
+            file: build/Dockerfile
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Validate
+        run: |
+          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.ref }}" == refs/tags* ]]; then
+            echo -e "\033[91mManual workflow run on tags is not allowed!\033[0m"
+            exit 1
+          fi
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64 #,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/query-exporter/cla.yaml b/.github/workflows/query-exporter/cla.yaml
new file mode 100644
index 0000000..f0d0505
--- /dev/null
+++ b/.github/workflows/query-exporter/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: read
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/query-exporter/clean.yaml b/.github/workflows/query-exporter/clean.yaml
new file mode 100644
index 0000000..1a3b455
--- /dev/null
+++ b/.github/workflows/query-exporter/clean.yaml
@@ -0,0 +1,33 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: qubership-query-exporter
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token:  ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/query-exporter/license.yaml b/.github/workflows/query-exporter/license.yaml
new file mode 100644
index 0000000..6609d79
--- /dev/null
+++ b/.github/workflows/query-exporter/license.yaml
@@ -0,0 +1,24 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          token: ${{ secrets.GH_ACCESS_TOKEN }}
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/query-exporter/link-checker.yaml b/.github/workflows/query-exporter/link-checker.yaml
new file mode 100644
index 0000000..c41cfda
--- /dev/null
+++ b/.github/workflows/query-exporter/link-checker.yaml
@@ -0,0 +1,47 @@
+---
+name: Link Checker
+
+on:
+  push: null
+  repository_dispatch: null
+  workflow_dispatch: null
+  pull_request:
+    branches: [main]
+    types:
+      [opened, reopened, synchronize]
+permissions:
+  contents: read
+jobs:
+  linkChecker:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - name: Restore lychee cache
+        uses: actions/cache@v4
+        id: restore-cache
+        with:
+          path: .lycheecache
+          key: cache-lychee-${{ github.sha }}
+          restore-keys: cache-lychee-
+
+      - name: Link Checker
+        id: lychee
+        uses: lycheeverse/lychee-action@v2
+        with:
+          args: >-
+            './**/*.md'
+            --verbose
+            --no-progress
+            --user-agent 'Mozilla/5.0 (X11; Linux x86_64) Chrome/134.0.0.0'
+            --retry-wait-time 60
+            --max-retries 8
+            --accept 100..=103,200..=299,429
+            --cookie-jar cookies.json
+            --exclude-all-private
+            --max-concurrency 4
+            --cache
+            --cache-exclude-status '429, 500..502'
+            --max-cache-age 1d
+          format: markdown
+          fail: true
diff --git a/.github/workflows/query-exporter/pr-conventional-commits.yaml b/.github/workflows/query-exporter/pr-conventional-commits.yaml
new file mode 100644
index 0000000..1177d01
--- /dev/null
+++ b/.github/workflows/query-exporter/pr-conventional-commits.yaml
@@ -0,0 +1,21 @@
+---
+
+name: Conventional Commits PR Check
+
+on:
+  pull_request:
+    types:
+      - opened
+      - edited
+      - synchronize
+
+permissions:
+  pull-requests: read
+jobs:
+  build:
+    name: Conventional Commits
+    runs-on: ubuntu-latest
+    steps:
+      - uses: actions/checkout@v4
+
+      - uses: webiny/action-conventional-commits@v1.3.0
diff --git a/.github/workflows/query-exporter/pr-lint-title.yaml b/.github/workflows/query-exporter/pr-lint-title.yaml
new file mode 100644
index 0000000..78bec8b
--- /dev/null
+++ b/.github/workflows/query-exporter/pr-lint-title.yaml
@@ -0,0 +1,23 @@
+---
+
+name: "Lint PR Title"
+
+on:
+  pull_request:
+    types:
+      - opened
+      - edited
+      - synchronize
+      - reopened
+
+permissions:
+  pull-requests: read
+
+jobs:
+  main:
+    name: Validate PR title
+    runs-on: ubuntu-latest
+    steps:
+      - uses: amannn/action-semantic-pull-request@v5
+        env:
+          GITHUB_TOKEN: ${{ github.token }}
diff --git a/.github/workflows/query-exporter/profanity-filter.yaml b/.github/workflows/query-exporter/profanity-filter.yaml
new file mode 100644
index 0000000..74926d3
--- /dev/null
+++ b/.github/workflows/query-exporter/profanity-filter.yaml
@@ -0,0 +1,29 @@
+---
+name: Profanity filter
+
+on:
+  issue_comment:
+    types: [created, edited]
+  issues:
+    types: [opened, edited, reopened]
+  pull_request:
+    types: [opened, edited, reopened]
+
+permissions:
+  issues: write
+  pull-requests: write
+
+jobs:
+  apply-filter:
+    runs-on: ubuntu-latest
+    steps:
+    - name: Scan issue or pull request for profanity
+      # Conditionally run the step if the actor isn't a bot
+      if: ${{ github.actor != 'dependabot[bot]' && github.actor != 'github-actions[bot]' }}
+      uses: IEvangelist/profanity-filter@9.07
+      id: profanity-filter
+      with:
+        token: ${{ secrets.GITHUB_TOKEN }}
+        # See https://bit.ly/potty-mouth-replacement-strategies
+        replacement-strategy: middle-asterisk # See Replacement strategy
+        custom-profane-words-url: https://github.com/Hesham-Elbadawi/list-of-banned-words/raw/refs/heads/master/ru
diff --git a/.github/workflows/query-exporter/super-linter.yaml b/.github/workflows/query-exporter/super-linter.yaml
new file mode 100644
index 0000000..526dc15
--- /dev/null
+++ b/.github/workflows/query-exporter/super-linter.yaml
@@ -0,0 +1,94 @@
+---
+# This workflow executes several linters on changed files based on languages used in your code base whenever
+# you push a code or open a pull request.
+#
+# You can adjust the behavior by modifying this file.
+# For more information, see:
+# https://github.com/super-linter/super-linter
+# Configuration file for super-linter example:
+# .github/super-linter.env
+# Configuration files for individual linters should be placed in .github/linters
+
+name: Lint Code Base
+
+on:
+  push:
+    branches:
+      - '**'
+  pull_request:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      full_scan:
+        type: boolean
+        default: false
+        required: false
+        description: "Lint all codebase"
+permissions:
+  contents: read
+
+jobs:
+  prepare-configs:
+    runs-on: ubuntu-latest
+    steps:
+    - name: "Get the common linters configuration"
+      uses: actions/checkout@v4
+      with:
+        ref: main # fix/superlinter-config
+        repository: netcracker/.github
+        sparse-checkout: |
+          config/linters
+    - name: "Upload the common linters configsuration"
+      uses: actions/upload-artifact@v4
+      with:
+        name: linter-config
+        path: "${{ github.workspace }}/config"
+        include-hidden-files: true
+  run-lint:
+    needs: [prepare-configs]
+    runs-on: ubuntu-latest
+    permissions:
+      contents: read
+      packages: read
+      # To report GitHub Actions status checks
+      statuses: write
+    steps:
+    - name: Checkout code
+      uses: actions/checkout@v4
+      with:
+        # Full git history is needed to get a proper list of changed files within `super-linter`
+        fetch-depth: 0
+    - name: "Get the common linters configuration"
+      uses: actions/download-artifact@v4
+      id: download
+      with:
+        name: linter-config
+        path: /tmp/linter-config
+    - name: "Apply the common linters configuration"
+      if: ${{ steps.download.outputs.download-path != '' }}
+      run: |
+        mkdir -p ./.github/linters
+        cp --update=none -vR /tmp/linter-config/linters/* ./.github/linters
+
+    - name: "Load super-linter environment file"
+      shell: bash
+      run: |
+        # shellcheck disable=2086
+        if [ -f "${GITHUB_WORKSPACE}/.github/super-linter.env" ]; then
+          echo "Applying local linter environment:"
+          grep "\S" ${GITHUB_WORKSPACE}/.github/super-linter.env | grep -v "^#"
+          grep "\S" ${GITHUB_WORKSPACE}/.github/super-linter.env | grep -v "^#" >> $GITHUB_ENV
+        elif [ -f "/tmp/linter-config/linters/super-linter.env" ]; then
+          echo "::warning:: Local linter environment file .github/super-linter.env is not found"
+          echo "Applying common linter environment:"
+          grep "\S" /tmp/linter-config/linters/super-linter.env | grep -v "^#"
+          grep "\S" /tmp/linter-config/linters/super-linter.env | grep -v "^#" >> $GITHUB_ENV
+        fi
+
+    - name: Lint Code Base
+      uses: super-linter/super-linter/slim@v8.1.0
+      env:
+        VALIDATE_ALL_CODEBASE: ${{ inputs.full_scan || false }}
+        # To report GitHub Actions status checks
+        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
diff --git a/.github/workflows/replication-controller/build.yaml b/.github/workflows/replication-controller/build.yaml
new file mode 100644
index 0000000..11752df
--- /dev/null
+++ b/.github/workflows/replication-controller/build.yaml
@@ -0,0 +1,69 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-replication-controller"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  GH_ACCESS_TOKEN: ${{ secrets.GH_ACCESS_TOKEN }}
+  PUSH: ${{ (github.event_name != 'workflow_dispatch' || inputs.publish_docker) && github.actor != 'dependabot[bot]' }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-replication-controller
+            file: build/Dockerfile
+            context: ""
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{ secrets.GH_ACCESS_TOKEN }} 
+        if: ${{ env.PUSH }} 
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }} 
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/replication-controller/cla.yaml b/.github/workflows/replication-controller/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/replication-controller/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/replication-controller/clean.yaml b/.github/workflows/replication-controller/clean.yaml
new file mode 100644
index 0000000..feab04f
--- /dev/null
+++ b/.github/workflows/replication-controller/clean.yaml
@@ -0,0 +1,36 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-replication-controller
+  TAG_NAME: ${{ github.event.ref }}
+  GH_ACCESS_TOKEN: ${{ secrets.GH_ACCESS_TOKEN }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-24.04
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{ secrets.GITHUB_TOKEN }} 
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
diff --git a/.github/workflows/replication-controller/license.yaml b/.github/workflows/replication-controller/license.yaml
new file mode 100644
index 0000000..02908a5
--- /dev/null
+++ b/.github/workflows/replication-controller/license.yaml
@@ -0,0 +1,24 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          token: ${{ secrets.GH_ACCESS_TOKEN }}
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/.github/workflows/upgrade/build.yaml b/.github/workflows/upgrade/build.yaml
new file mode 100644
index 0000000..2e427e5
--- /dev/null
+++ b/.github/workflows/upgrade/build.yaml
@@ -0,0 +1,68 @@
+name: Build Artifacts
+on:
+  release:
+    types: [created]
+  push:
+    branches:
+      - '**'
+  workflow_dispatch:
+    inputs:
+      publish_docker:
+        description: "Publish image to ghcr.io/netcracker/pgskipper-upgrade"
+        type: boolean
+        default: false
+        required: false
+
+env:
+  TAG_NAME: ${{ github.event.release.tag_name || github.ref }}
+  PUSH: ${{ github.event_name != 'workflow_dispatch' || inputs.publish_docker }}
+
+jobs:
+  multiplatform_build:
+    strategy:
+      fail-fast: false
+      matrix:
+        component:
+          - name: pgskipper-upgrade
+            file: Dockerfile
+            context: ""
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Set up QEMU
+        uses: docker/setup-qemu-action@v3
+      - name: Set up Docker Buildx
+        uses: docker/setup-buildx-action@v3
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/tags/@@;s@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ matrix.component.name }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+        if: ${{ env.PUSH }}
+      - name: Build and push
+        uses: docker/build-push-action@v6
+        with:
+          no-cache: true
+          context: ${{ matrix.component.context }}
+          file: ${{ matrix.component.file }}
+          platforms: linux/amd64,linux/arm64
+          push: ${{ env.PUSH }}
+          tags: ghcr.io/netcracker/${{ matrix.component.name }}:${{ env.TAG_NAME }}
+          provenance: false
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ matrix.component.name }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/upgrade/cla.yaml b/.github/workflows/upgrade/cla.yaml
new file mode 100644
index 0000000..4db5591
--- /dev/null
+++ b/.github/workflows/upgrade/cla.yaml
@@ -0,0 +1,32 @@
+---
+name: CLA Assistant
+on:
+  issue_comment:
+    types: [created]
+  pull_request_target:
+    types: [opened, closed, synchronize]
+
+permissions:
+  actions: write
+  contents: write
+  pull-requests: write
+  statuses: write
+
+jobs:
+  CLAAssistant:
+    runs-on: ubuntu-latest
+    steps:
+      - name: "CLA Assistant"
+        if: (github.event.comment.body == 'recheck' || github.event.comment.body == 'I have read the CLA Document and I hereby sign the CLA') || github.event_name == 'pull_request_target'
+        uses: contributor-assistant/github-action@v2.6.1
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          PERSONAL_ACCESS_TOKEN: ${{ secrets.CLA_ACCESS_TOKEN }}
+        with:
+          path-to-signatures: 'signatures/version1/cla.json'
+          path-to-document: 'https://github.com/Netcracker/qubership-github-workflows/blob/main/CLA/cla.md'
+          # branch should not be protected
+          branch: 'main'
+          allowlist: NetcrackerCLPLCI,web-flow,bot*
+          remote-repository-name: cla-storage
+          remote-organization-name: Netcracker
diff --git a/.github/workflows/upgrade/clean.yaml b/.github/workflows/upgrade/clean.yaml
new file mode 100644
index 0000000..96c763a
--- /dev/null
+++ b/.github/workflows/upgrade/clean.yaml
@@ -0,0 +1,35 @@
+name: Branch Deleted
+on: delete
+
+env:
+  COMPONENT_NAME: pgskipper-upgrade
+  TAG_NAME: ${{ github.event.ref }}
+
+jobs:
+  delete:
+    if: github.event.ref_type == 'branch'
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout
+        uses: actions/checkout@v4
+      - name: Prepare Tag
+        run: echo "TAG_NAME=$(echo ${TAG_NAME} | sed 's@refs/heads/@@;s@/@_@g')" >> $GITHUB_ENV
+      - name: Login to Docker Hub
+        uses: docker/login-action@v3
+        with:
+          registry: ghcr.io
+          username: ${GITHUB_ACTOR}
+          password: ${{secrets.GITHUB_TOKEN}}
+      - name: Get package IDs for delete 
+        id: get-ids-for-delete
+        uses: Netcracker/get-package-ids@v0.0.1
+        with:
+          component-name: ${{ env.COMPONENT_NAME }}
+          component-tag: ${{ env.TAG_NAME }}
+          access-token: ${{secrets.GITHUB_TOKEN}}
+      - uses: actions/delete-package-versions@v5
+        with: 
+          package-name: ${{ env.COMPONENT_NAME }}
+          package-type: 'container'
+          package-version-ids: ${{ steps.get-ids-for-delete.outputs.ids-for-delete }}
+        if: ${{ steps.get-ids-for-delete.outputs.ids-for-delete != '' }}
diff --git a/.github/workflows/upgrade/license.yaml b/.github/workflows/upgrade/license.yaml
new file mode 100644
index 0000000..6609d79
--- /dev/null
+++ b/.github/workflows/upgrade/license.yaml
@@ -0,0 +1,24 @@
+name: Add License Header
+on:
+  push:
+    branches:
+      - 'main'
+env:
+   COPYRIGHT_COMPANY: 'NetCracker Technology Corporation'
+   COPYRIGHT_YEAR: '2024-2025'
+jobs:
+  license:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@v4
+        with:
+          token: ${{ secrets.GH_ACCESS_TOKEN }}
+      - run: docker run -v "${PWD}:/src" -i ghcr.io/google/addlicense -v -c "${{ env.COPYRIGHT_COMPANY }}" -y "${{ env.COPYRIGHT_YEAR }}" $(find . -type f -name "*.go" -o -type f -name "*.sh" -o -type f -name "*.py" | xargs echo)
+      - name: Create Pull Request
+        uses: peter-evans/create-pull-request@v7
+        with:
+          commit-message: Auto-update license header
+          branch: license-update
+          title: Add License Header
+          body: Automated license header update
+          delete-branch: true
\ No newline at end of file
diff --git a/docker-backup-daemon/.gitignore b/docker-backup-daemon/.gitignore
new file mode 100644
index 0000000..750dba1
--- /dev/null
+++ b/docker-backup-daemon/.gitignore
@@ -0,0 +1,4 @@
+# Build output.
+target
+
+.idea/*
\ No newline at end of file
diff --git a/docker-backup-daemon/CODE-OF-CONDUCT.md b/docker-backup-daemon/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-backup-daemon/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-backup-daemon/CONTRIBUTING.md b/docker-backup-daemon/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-backup-daemon/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-backup-daemon/Dockerfile b/docker-backup-daemon/Dockerfile
new file mode 100644
index 0000000..19d24d7
--- /dev/null
+++ b/docker-backup-daemon/Dockerfile
@@ -0,0 +1,93 @@
+FROM --platform=$BUILDPLATFORM golang:1.25.3-alpine3.22 AS builder
+
+ENV GO111MODULE=on
+
+# Copy the go source
+COPY go /workspace
+
+WORKDIR /workspace
+
+RUN go mod tidy
+
+# Build
+RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o ./_output/bin/azure_restore ./cmd/main.go
+
+FROM ubuntu:22.04
+
+ENV PGPASSWORD=password PG_CLUSTER_NAME=common STORAGE_ROOT=/backup-storage EXTERNAL_STORAGE_ROOT=/external \
+    LC_ALL=en_US.UTF-8 \
+    LANG=en_US.UTF-8
+
+COPY docker/pip.conf /root/.pip/pip.conf
+COPY docker/requirements.txt /root/requirements.txt
+
+RUN echo "deb [trusted=yes] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" >> /etc/apt/sources.list.d/pgdg.list
+RUN cat /etc/apt/sources.list
+RUN ls -la /etc/apt/
+RUN apt-get -y update
+RUN apt-get -o DPkg::Options::="--force-confnew" -y dist-upgrade
+RUN apt-get update && \
+    apt-get install -y --allow-downgrades gcc-12 cpp-12 gcc-12-base libgcc-12-dev libstdc++6 libgcc-s1 libnsl2
+RUN apt-get --no-install-recommends install -y python3.11 python3-pip python3-dev libpq-dev cython3
+
+RUN ls -ls /usr/bin/
+ARG DEBIAN_FRONTEND=noninteractive
+RUN apt-get --no-install-recommends install -y comerr-dev \
+                       unzip \
+                       build-essential \
+                       manpages-dev \
+                       libkrb5-dev \
+                       libsasl2-dev libldap2-dev libssl-dev \
+                       postgresql-13 postgresql-14 postgresql-15 postgresql-16 postgresql-17 \
+                       jq \
+                       openssl curl \
+                       vim \
+                       locales
+RUN python3 -m pip install -U setuptools==78.1.1 wheel==0.38.0
+RUN python3 -m pip install --no-cache-dir -r /root/requirements.txt \
+      && python3 -m pip install --upgrade pip \
+      && python3 -m pip install grpcio \
+      && python3 -m pip install opentelemetry-distro opentelemetry-exporter-otlp opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-flask \
+      && opentelemetry-bootstrap -a install \
+      && pip3 uninstall -y pip \
+      && apt-get remove -y --purge gcc-12 \
+      && apt-get remove -y --purge python3-dev \
+      && apt-get remove -y --purge libpq-dev \
+      && apt-get remove -y --purge cython3 \
+      && locale-gen en_US.UTF-8 \
+      && apt-get clean
+
+RUN ln -s /usr/bin/python3 /usr/bin/python
+
+COPY --from=builder /workspace/_output/bin/azure_restore /opt/backup/
+COPY docker/postgres/ docker/health.sh /opt/backup/
+COPY docker/granular /opt/backup/granular
+COPY docker/postgres/encryption.py /opt/backup/granular/encryption.py
+COPY docker/external_scripts/azure_restore.sh /opt/backup/
+COPY maintenance /maintenance
+
+RUN mkdir -p /backup-storage/ && \
+    mkdir -p /external/ && \
+    chmod -R +x /opt/backup/ && \
+    chmod -R 777 /opt/backup/ && \
+    chmod -R 777 /backup-storage/ && \
+    chmod -R 777 /external/ && \
+    chmod -R g+w /maintenance/recovery/ && \
+    chmod +x /maintenance/recovery/*.sh && \
+    chgrp -R 0 /backup-storage/ && \
+    chgrp -R 0 /external/ && \
+    chmod g+w /etc && \
+    chgrp 0 /etc/passwd &&  \
+    chmod g+w /etc/passwd
+
+
+#VOLUME /backup-storage
+#VOLUME /external
+# Volumes are defined to support read-only root file system
+VOLUME /etc
+VOLUME /opt/backup
+VOLUME /tmp
+
+EXPOSE 8080 8081 8082 9000
+
+CMD ["bash", "/opt/backup/start_backup_daemon.sh"]
\ No newline at end of file
diff --git a/docker-backup-daemon/LICENSE b/docker-backup-daemon/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-backup-daemon/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-backup-daemon/README.md b/docker-backup-daemon/README.md
new file mode 100644
index 0000000..38dd9d4
--- /dev/null
+++ b/docker-backup-daemon/README.md
@@ -0,0 +1,11 @@
+# pgskipper-backup-daemon
+
+## Repository structure
+
+* `./docker` - directory with backup daemon source code.
+* `./go` - directory with backup daemon source code for azure part.
+* `./docs` - directory with actual documentation for the service.
+
+# Overview
+
+postgres-backup-daemon allows to run backups periodically and recovery database from this backups.
diff --git a/docker-backup-daemon/SECURITY.md b/docker-backup-daemon/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-backup-daemon/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-backup-daemon/docker/external_scripts/azure_restore.sh b/docker-backup-daemon/docker/external_scripts/azure_restore.sh
new file mode 100644
index 0000000..b04f9ca
--- /dev/null
+++ b/docker-backup-daemon/docker/external_scripts/azure_restore.sh
@@ -0,0 +1,147 @@
+#!/usr/bin/bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+#set -x
+#set -e
+function log {
+    echo -e "[$(date +'%Y-%d-%m-%T.%N')][az_restore] $2$1\e[m"
+}
+
+[[ "${DEBUG}" == 'true' ]] && set -x
+
+timestamp=$1
+
+#default values
+restoreAsSeparate='false'
+geoRestore='false'
+mirrorRestore='false'
+
+usage(){
+    echo "$0 < timestamp > [ --mirror-restore ] [ --restore-as-separate ] [ --geo-restore ]"
+    echo '  timestamp               - format yyyy-mm-ddThh:mm:ssZ'
+    echo '  --mirror-restore        - restore to another cloud, src instance does not stop'
+    echo '  --restore-as-separate   - restore to another ns/cloud, but src instance does not stop and src svc does not changed'
+    echo '  --geo-restore           - invoke geo-restore during restoration of Azure PG'
+    echo "  Examples:"
+    echo "    $0 2023-07-14T07:23:57Z"
+    echo "    $0 2023-01-11T01:11:11Z --restore-as-separate"
+    echo "    $0 2023-01-11T01:11:11Z --restore-as-separate --geo-restore"
+}
+
+log "azure restore backup invoked"
+
+if [[ -z "$timestamp" ]]; then
+    echo 'Timestamp should be set'
+    exit 1
+fi
+
+isCorrectTimestamp=$(echo "$timestamp" | grep '^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]Z*$' -c)
+if [[ "$isCorrectTimestamp" -ne 1 ]]; then
+    log 'Timestamp is incorrect!'
+    log 'Timestamp should be the following format yyyy-mm-ddThh:mm:ssZ'
+    exit 1
+fi
+
+
+COUNT_ARGS=$#
+i=2
+while [ $i -le $COUNT_ARGS ]; do
+    argument="${!i}"
+    let i=i+1
+
+    # find tabs and space
+    echo "$argument" | grep -P ' |\t' > /dev/null
+    EXIT_CODE=$?
+    if [ "$EXIT_CODE" -eq 0 ]; then
+        echo ''
+        echo '|'"$argument"'| - have space ot tab! Exit!'
+        echo ''
+        usage
+        exit 2
+    fi
+    if [[ $argument == --restore-as-separate ]];       then restoreAsSeparate='true'; continue; fi
+    if [[ $argument == --mirror-restore ]];            then mirrorRestore='true'; continue; fi
+    if [[ $argument == --geo-restore ]];               then geoRestore='true'; continue; fi
+
+    log 'ERROR! Wrong parameters!'; usage; exit 1;
+done
+
+echo
+echo "Set parameters: $@"
+echo
+
+log 'Waiting for API server till 120...'
+
+for i in {1..120}
+do
+   curl --fail --connect-timeout 1 --max-time 2 -o /dev/null -s localhost:8080/v2/health && break
+   sleep 1
+   log "$i"
+done
+
+echo
+log 'Check API code for restore method'
+checkApiCode=$(curl -XOPTION --max-time 30 -o /dev/null -s -w "%{http_code}\n" localhost:8080/external/restore)
+if [[ "$checkApiCode" == '404' ]]; then
+    echo "ERROR! API to restore external DB is off"
+    exit 1
+fi
+
+if [[ "$checkApiCode" == '000' ]]; then
+    echo "ERROR! API to restore external DB is not ready"
+    exit 1
+fi
+
+request="{\"restore_time\":\"$timestamp\", \"restore_as_separate\":\"$restoreAsSeparate\", \"subnet\":\"$mirrorRestore\", \"geo_restore\":\"$geoRestore\"}"
+
+log "request to backup daemon:"
+log "$request"
+
+rawResponse=$(curl --max-time 30 -v -XPOST -H "Content-Type: application/json" localhost:8080/external/restore -d "$request")
+
+log "response from backup daemon:"
+log "$rawResponse"
+
+restoreId=$(echo "$rawResponse" | grep '^restore-20[0-9T]*$' || echo 'error')
+
+if [[ "$restoreId" == 'error' ]]; then
+    log "Error. Expected restore id but found '$rawResponse'"
+    exit 1
+fi
+
+log "restoreId is: $restoreId"
+
+operationStatus=''
+counter=0
+
+
+for i in {1..1500}
+do
+    if [[ "$operationStatus" == 'Successful' ]]; then
+        break
+    fi
+    operationStatus=$(curl -XGET --max-time 3 localhost:8080/external/restore/$restoreId | jq -r '.status')
+    if [[ "$operationStatus" == 'Failed' ]]; then
+        log 'Restore operation is in Failed status'
+        exit 1
+    fi
+    sleep 10
+    log "waiting Successful status, current status: $operationStatus"
+done
+
+log 'Successful status'
+
+exit 0
diff --git a/docker-backup-daemon/docker/granular/backups.py b/docker-backup-daemon/docker/granular/backups.py
new file mode 100644
index 0000000..e20778a
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/backups.py
@@ -0,0 +1,538 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import json
+import logging
+import shutil
+import os
+import re
+import time
+
+import storage_s3
+import configs
+import utils
+from datetime import timezone
+from itertools import groupby
+
+
+class BackupNotFoundException(Exception):
+
+    def __init__(self, backup_id, namespace, database=None):
+        super(Exception, self).__init__("Backup of database '%s' is not found in backup '%s' in namespace '%s'."
+                                        % (database, backup_id, namespace))
+
+
+class BackupBadStatusException(Exception):
+
+    def __init__(self, backup_id, status=None, database=None):
+        super(Exception, self).__init__("Backup of database '%s' in backup '%s' has bad status for restore: %s"
+                                        % (database, backup_id, status))
+
+
+class BackupFailedException(Exception):
+
+    def __init__(self, database, reason=None):
+        super(Exception, self).__init__("Backup of database '%s' has failed: %s." % (database, reason))
+
+
+class RestoreFailedException(Exception):
+
+    def __init__(self, database, reason=None):
+        super(Exception, self).__init__("Restore of database '%s' has failed: %s" % (database, reason))
+
+
+class BackupStatus:
+    SUCCESSFUL = 'Successful'
+    FAILED = 'Failed'
+    IN_PROGRESS = 'In progress'
+    PLANNED = 'Planned'
+    UNKNOWN = 'Unknown'
+    CANCELED = 'Canceled'
+
+
+def get_backup_status_id(status):
+    statuses = {
+        BackupStatus.SUCCESSFUL: 0,
+        BackupStatus.CANCELED: 7,
+        BackupStatus.FAILED: 6,
+        BackupStatus.IN_PROGRESS: 5,
+        BackupStatus.PLANNED: 4,
+        BackupStatus.UNKNOWN: -1
+    }
+    return statuses[status]
+
+def is_valid_namespace(namespace):
+    return re.match("^[a-zA-z0-9_]+$", namespace) is not None
+
+
+def backup_exists(backup_id, namespace=configs.default_namespace(), external_backup_storage=None):
+    return os.path.exists(build_backup_path(backup_id, namespace, external_backup_storage))
+
+
+def database_backup_exists(backup_id, database, namespace=configs.default_namespace(), external_backup_storage=None):
+    return os.path.exists(build_database_backup_path(backup_id, database, namespace, external_backup_storage))
+
+
+def build_namespace_path(namespace=configs.default_namespace()):
+    return '%s/%s' % (configs.backups_storage(), namespace)
+
+
+def build_database_backup_path(backup_id, database, 
+                               namespace=configs.default_namespace(), external_backup_storage=None, blob_path=None):
+    ext = '_enc.dump' if configs.get_encryption() else '.dump'
+    return '%s/%s%s' % (
+                build_backup_path(backup_id, namespace, external_backup_storage, blob_path), database, ext)
+
+def build_roles_backup_path(backup_id, database,
+                            namespace=configs.default_namespace(), external_backup_storage=None, blob_path=None):
+    ext = 'roles_enc.sql' if configs.get_encryption() else 'roles.sql'
+    return "%s/%s.%s" % (
+        build_backup_path(backup_id, namespace, external_backup_storage, blob_path), database, ext)
+
+def build_database_backup_full_path(backup_id, database, storage_root,
+                                        namespace=configs.default_namespace(),
+                                        blob_path=None):
+    ext = '_enc.dump' if configs.get_encryption() else '.dump'
+    if blob_path is not None:
+        return '%s/%s/%s%s' % (blob_path, backup_id, database, ext)
+    else:
+        return '%s/%s/%s/%s%s' % (storage_root, namespace, backup_id, database, ext)
+
+
+def build_database_restore_report_path(backup_id, database, restore_tracking_id, namespace=configs.default_namespace()):
+    return '%s/%s.%s.report' % (build_backup_path(backup_id, namespace), database, restore_tracking_id)
+
+
+def build_backup_path(backup_id, namespace=configs.default_namespace(), external_backup_storage=None, blob_path=None):
+    if blob_path is not None:
+        return '%s/%s' % (blob_path, backup_id)
+    else:
+        return '%s/%s/%s' % (configs.backups_storage() if external_backup_storage is None else external_backup_storage,
+                             namespace, backup_id)
+
+
+def build_external_backup_root(external_backup_path):
+    return '%s/%s' % (os.getenv("EXTERNAL_STORAGE_ROOT"), external_backup_path)
+
+
+def build_backup_status_file_path(backup_id, namespace=configs.default_namespace(), external_backup_storage=None, blob_path=None):
+    return '%s/status.json' % build_backup_path(backup_id, namespace, external_backup_storage, blob_path)
+
+
+def build_restore_status_file_path(backup_id, tracking_id, namespace=configs.default_namespace(),
+                                   external_backup_storage=None, blob_path=None):
+    return '%s/%s.json' % (build_backup_path(backup_id, namespace, external_backup_storage, blob_path), tracking_id)
+
+
+def get_key_name_by_backup_id(backup_id, namespace, external_backup_storage=None, blob_path=None):
+    status_path = build_backup_status_file_path(backup_id, namespace, external_backup_storage, blob_path)
+    with open(status_path) as f:
+        data = json.load(f)
+        return data.get("key_name")
+
+def generate_short_id():
+    return datetime.datetime.now().strftime("%Y%m%dT%H%M")
+
+def generate_id():
+    return datetime.datetime.now().strftime("%Y%m%dT%H%M%S%f")
+
+
+def generate_backup_id():
+    return 'backup-%s' % generate_id()
+
+
+def generate_restore_id(backup_id, namespace=configs.default_namespace()):
+    m = re.match("^backup-(?P<backupId>[a-zA-Z0-9]+)$", backup_id)
+    backup_id = m.group('backupId')
+    return 'restore-%s-%s-%s' % (namespace, backup_id, generate_id())
+
+
+def extract_backup_id_from_tracking_id(tracking_id):
+    m = re.match("^restore-(?P<namespace>[a-zA-Z0-9_]+)-(?P<backupId>[a-zA-Z0-9]+)-[a-zA-Z0-9]+$", tracking_id)
+    return 'backup-%s' % m.group('backupId'), m.group('namespace')
+
+
+# Kindly offered by https://stackoverflow.com/a/1094933/6519476
+def sizeof_fmt(num, suffix='B'):
+    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:
+        if abs(num) < 1024.0:
+            return "%3.1f%s%s" % (num, unit, suffix)
+        num /= 1024.0
+    return "%.1f%s%s" % (num, 'Yi', suffix)
+
+
+def calculate_expiration_timestamp(start_timestamp, period):
+    return start_timestamp + get_seconds(period)
+
+
+def is_backup_completed(status):
+    return status == BackupStatus.SUCCESSFUL or status == BackupStatus.FAILED
+
+
+def get_seconds(delta):
+    time_unit = delta.split()
+
+    if len(time_unit) != 2 or not time_unit[0].isdigit():
+        raise Exception("Malformed expiration period: %s." % delta)
+
+    value = int(time_unit[0])
+    unit = time_unit[1].lower()
+
+    if unit == 'week' or unit == 'weeks':
+        td = datetime.timedelta(weeks=value)
+    elif unit == 'day' or unit == 'days':
+        td = datetime.timedelta(days=value)
+    elif unit == 'hour' or unit == 'hours':
+        td = datetime.timedelta(hours=value)
+    elif unit == 'minute' or unit == 'minutes':
+        td = datetime.timedelta(minutes=value)
+    elif unit == 'second' or unit == 'seconds':
+        td = datetime.timedelta(seconds=value)
+    else:
+        raise Exception("Time unit '%s' is not supported" % unit)
+
+    return int(td.total_seconds())
+
+
+def is_database_protected(database):
+    return database in configs.protected_databases() or database in configs.protected_greenplum_databases()
+
+
+def get_backup_create_date(backup_id):
+    return int(datetime.datetime.strptime(backup_id, "backup-%Y%m%dT%H%M%S%f").strftime('%s'))
+
+
+def backup_expired(backup, expire_date):
+    return 1 if get_backup_create_date(os.path.basename(backup)) < expire_date else 0
+
+
+def get_s3_client():
+    return storage_s3.AwsS3Vault()
+
+
+def sweep_manager():
+    """ Sweep procedure manager """
+    if os.getenv("USE_EVICTION_POLICY_FIRST") is None or os.getenv("USE_EVICTION_POLICY_FIRST").lower() == 'false':
+        sweep_by_keep()
+    else:
+        sweep_by_policy()
+
+
+def sweep_by_keep():
+    log = logging.getLogger("Sweeper")
+    log.info("Start backups sweeping by keep.")
+    current_time = time.time()
+    storage = configs.backups_storage()
+    s3 = None
+    if os.environ['STORAGE_TYPE'] == "s3":
+        s3 = get_s3_client()
+        namespaces = s3.get_granular_namespaces(storage)
+
+    for namespace in os.listdir(storage) if not s3 else namespaces:
+        log.info("Sweeping namespace: %s." % namespace)
+        expired_backups = []
+        failed_expired_backups = []
+
+        healthy_backups = 0
+        if s3:
+            backup_ids = s3.get_backup_ids(storage, namespace)
+
+        for backup_id in os.listdir(build_namespace_path(namespace)) if not s3 else backup_ids:
+            status_file = build_backup_status_file_path(backup_id, namespace)
+            if s3:
+                try:
+                    if s3.is_file_exists(status_file):
+                        status_file = s3.read_object(status_file)
+                        backup_details = json.loads(status_file)
+                    else:
+                        log.error("Cannot find status file in bucket with backup id {}".format(backup_id))
+                        failed_expired_backups.append(backup_id)
+                        continue
+                except ValueError:
+                    log.exception("Cannot read status file")
+
+            elif not os.path.isfile(status_file):
+                failed_expired_backups.append(backup_id)
+                continue
+            else:
+                backup_details = utils.get_json_by_path(status_file)
+
+            expires = backup_details.get('expires')
+            backup_status = backup_details.get('status')
+            timestamp = backup_details.get('timestamp')
+
+            if backup_status == BackupStatus.SUCCESSFUL:
+                # We may get unicode string here so check `basestring`, but not `str`
+                if isinstance(expires, str) and expires.lower() == 'never':
+                    continue
+                else:
+                    healthy_backups += 1
+
+            if expires < current_time:
+                if backup_status == BackupStatus.SUCCESSFUL:
+                    expired_backups.append((backup_id, timestamp))  # Keep timestamp to sort later.
+                else:
+                    # We delete expired PLANNED and IN_PROGRESS backups as well
+                    # since they probably hang if they have already expired, not not even finished.
+                    failed_expired_backups.append(backup_id)
+
+        if expired_backups and len(expired_backups) == healthy_backups:
+            # If all healthy are expired then keep the freshest.
+            # Sort by timestamp and take ID of the latest.
+            expired_backups.sort(key=lambda backup: backup[1])
+            saved_backup = expired_backups.pop()[0]
+            log.info("All successful backups are expired. Keep backup '%s/%s' as last healthy."
+                     % (saved_backup, namespace))
+
+        for i in failed_expired_backups:
+            log.info("Sweep out failed expired backup status of '%s/%s'." % (namespace, i))
+            shutil.rmtree(build_backup_path(i, namespace)) if not s3 else s3.delete_objects(build_backup_path(i, namespace))
+
+        for i in expired_backups:
+            log.info("Sweep out expired backup '%s/%s'." % (namespace, i[0]))
+            shutil.rmtree(build_backup_path(i[0], namespace)) if not s3 else s3.delete_objects(
+                build_backup_path(i[0], namespace))
+
+    log.info("Backups sweeping finished.")
+
+
+def sweep_by_policy():
+    """ remove expired backups according EVICTION_POLICY """
+
+    log = logging.getLogger("Sweeper (policy)")
+    log.info("Start backups sweeping according eviction policy")
+    current_time = time.time()
+    storage = configs.backups_storage()
+    s3 = None
+    if os.environ['STORAGE_TYPE'] == "s3":
+        s3 = get_s3_client()
+        namespaces = s3.get_granular_namespaces(storage)
+
+    start_point_time = time.time()
+    for namespace in os.listdir(storage) if not s3 else namespaces:
+        log.info("Sweeping namespace: %s." % namespace)
+        expired_backups = []
+        failed_expired_backups = []
+        success_backups = []
+        log.debug("namespace: {}".format(namespace))
+        current_policy = os.getenv("EVICTION_POLICY_GRANULAR_" + namespace) or os.getenv("EVICTION_POLICY")
+        log.debug("Current_policy: {}".format(current_policy))
+        log.debug("policy for current namespace")
+
+        for i in utils.parse(current_policy):
+            log.debug('{}/{}'.format(i.start, i.interval))
+
+        if s3:
+            backup_ids = s3.get_backup_ids(storage, namespace)
+
+        for backup_id in os.listdir(build_namespace_path(namespace)) if not s3 else backup_ids:
+            status_file = build_backup_status_file_path(backup_id, namespace)
+            if s3:
+                try:
+                    if s3.is_file_exists(status_file):
+                        status_file = s3.read_object(status_file)
+                        backup_details = json.loads(status_file)
+                    else:
+                        log.error("Cannot find status file in bucket with backup id {}".format(backup_id))
+                        failed_expired_backups.append(os.path.join(build_backup_path(backup_id, namespace)))
+                        continue
+                except ValueError:
+                    log.exception("Cannot read status file")
+            elif not os.path.isfile(status_file):
+                failed_expired_backups.append(os.path.join(build_backup_path(backup_id, namespace)))
+                continue
+            else:
+                backup_details = utils.get_json_by_path(status_file)
+
+            expires = backup_details.get('expires')
+            backup_status = backup_details.get('status')
+
+            if backup_status == BackupStatus.SUCCESSFUL or backup_status == BackupStatus.IN_PROGRESS:
+                if isinstance(expires, str) and expires.lower() == 'never':
+                    log.debug("Skip backup {} as marked Never delete".format(backup_id))
+                    continue
+                success_backups.append(os.path.join(build_backup_path(backup_id, namespace)))
+            else:
+                failed_expired_backups.append(os.path.join(build_backup_path(backup_id, namespace)))
+
+        log.debug("success backups:")
+        log.debug("\n".join(success_backups))
+        log.debug("")
+
+        log.debug("fail backups:")
+        log.debug("\n".join(failed_expired_backups))
+        log.debug("")
+
+        for rule in utils.parse(current_policy):
+            log.debug("current rule: {}/{}".format(rule.start, rule.interval))
+            operateVersions = [t for t in success_backups if backup_expired(t, start_point_time - rule.start)]
+            log.debug("stage1 selected:")
+            log.debug("\n".join(operateVersions))
+            if rule.interval == "delete":
+                # all versions should be evicted catched by this interval
+                expired_backups.extend(operateVersions)
+            else:
+                # group by interval and leave only first on each
+                thursday = 3 * 24 * 60 * 60
+                for _, versionsIt in groupby(operateVersions, lambda t: int(
+                        (get_backup_create_date(os.path.basename(t)) - thursday) / rule.interval)):
+                    grouped = sorted(list(versionsIt), key=lambda t: get_backup_create_date(os.path.basename(t)))
+                    expired_backups.extend(grouped[:-1])
+            log.debug("stage2 expired:")
+            log.debug("\n".join(expired_backups))
+
+        expired_backups = list(set(expired_backups))
+        log.debug("stage3 expired unique backups:")
+        log.debug("\n".join(expired_backups))
+
+        log.debug("Remove expired backups:")
+        for dir in expired_backups:
+            log.info("remove backup: {}".format(dir))
+            shutil.rmtree(dir) if not s3 else s3.delete_objects(dir)
+
+        log.debug("Remove failed backups:")
+        for dir in failed_expired_backups:
+            log.info("remove backup: {}".format(dir))
+            shutil.rmtree(dir) if not s3 else s3.delete_objects(dir)
+
+    log.info("Backups sweeping finished.")
+
+
+def _as_iso8601(v):
+    if v is None or v == "":
+        return ""
+    if isinstance(v, (int, float)):
+        if v > 1e12:  
+            v = v / 1000.0
+        return datetime.datetime.fromtimestamp(v, tz=timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')
+    
+    # Handle string timestamps that are already in ISO format but missing timezone
+    if isinstance(v, str):
+        iso_pattern = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?$'
+        if re.match(iso_pattern, v):
+            try:
+                dt = datetime.datetime.fromisoformat(v)
+                return dt.replace(tzinfo=timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')
+            except ValueError:
+                pass
+    
+    return str(v)
+
+def _parse_bytes(v):
+    if v is None:
+        return None
+    if isinstance(v, (int, float)):
+        return int(v)
+    s = str(v).strip().replace(',', '').lower()
+
+    # plain number
+    try:
+        return int(float(s))
+    except ValueError:
+        pass
+
+    if s.endswith('bytes'):
+        s = s[:-5].strip()
+
+    import re
+    m = re.match(r'^([0-9]+(?:\.[0-9]+)?)\s*([a-z]+)$', s)
+    if not m:
+        return None
+    num, unit = m.groups()
+    num = float(num)
+
+    dec = {
+        'kb': 1000, 'mb': 1000**2, 'gb': 1000**3, 'tb': 1000**4,
+        'b': 1
+    }
+    bin_ = {
+        'kib': 1024, 'mib': 1024**2, 'gib': 1024**3, 'tib': 1024**4,
+    }
+
+    if unit in bin_:
+        return int(num * bin_[unit])
+    if unit in dec:
+        return int(num * dec[unit])
+
+    short = {'k': 1000, 'm': 1000**2, 'g': 1000**3, 't': 1000**4}
+    if unit in short:
+        return int(num * short[unit])
+
+    return None
+
+def transform_backup_status_v1(raw: dict) -> dict:
+    raw = raw or {}
+    dbs = raw.get("databases") or {}
+
+    storage_name = raw.get("storageName") or raw.get("storage") or raw.get("storage_name") or ""
+    blob_path    = raw.get("blobPath") or raw.get("externalBackupPath") or raw.get("blob_path") or ""
+
+    creation  = _as_iso8601(raw.get("created") or raw.get("creationTime") or raw.get("creation") or raw.get("timestamp") or raw.get("startTime"))
+    completion= _as_iso8601(raw.get("completed") or raw.get("completionTime") or raw.get("completedTime"))
+
+    return {
+        "status":       raw.get("status"),
+        "errorMessage": raw.get("errorMessage") or raw.get("error"),
+        "backupId":     raw.get("backupId"),
+        "storageName":  storage_name,
+        "blobPath":     blob_path,
+        "creationTime": creation,
+        "completionTime": completion,
+        "databases": [
+            {
+                "databaseName": name,
+                "status":       (info or {}).get("status"),
+                "size":         _parse_bytes((info or {}).get("size") or (info or {}).get("sizeBytes")),
+                "duration":     (info or {}).get("duration"),
+                "path":         (info or {}).get("path"),
+                "errorMessage": (info or {}).get("errorMessage") or (info or {}).get("error"),
+                "creationTime": _as_iso8601((info or {}).get("created") or (info or {}).get("creationTime") or creation),
+            }
+            for name, info in dbs.items()
+        ],
+    }
+
+def transform_restore_status_v1(raw: dict) -> dict:
+    raw = raw or {}
+    dbs = raw.get("databases") or {}
+
+    storage_name = raw.get("storageName") or raw.get("storage") or raw.get("storage_name") or ""
+    blob_path    = raw.get("blobPath") or raw.get("externalBackupPath") or raw.get("blob_path") or ""
+
+    creation  = _as_iso8601(raw.get("created") or raw.get("creationTime") or raw.get("creation") or raw.get("timestamp") or raw.get("startTime"))
+    completion= _as_iso8601(raw.get("completed") or raw.get("completionTime") or raw.get("completedTime"))
+
+    out = {
+        "status":         raw.get("status"),
+        "errorMessage":   raw.get("errorMessage") or raw.get("error"),
+        "restoreId":      raw.get("trackingId") or raw.get("restoreId"),
+        "storageName":    storage_name,
+        "blobPath":       blob_path,
+        "creationTime":   creation,
+        "completionTime": completion,
+        "databases": [],
+    }
+    for prev_name, info in dbs.items():
+        info = info or {}
+        out["databases"].append({
+            "previousDatabaseName": prev_name,
+            "databaseName":   info.get("newDatabaseName") or info.get("databaseName") or info.get("restoredAs") or prev_name,
+            "status":         info.get("status"),
+            "duration":       info.get("duration"),
+            "path":           info.get("path"),
+            "errorMessage":   info.get("errorMessage") or info.get("error"),
+            "creationTime":   _as_iso8601(info.get("created") or info.get("creationTime") or creation),
+        })
+    return out
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/granular/configs.py b/docker-backup-daemon/docker/granular/configs.py
new file mode 100644
index 0000000..61a23e3
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/configs.py
@@ -0,0 +1,182 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import utils
+import logging
+from utils import get_postgres_version_by_path
+
+
+_PROTECTED_DATABASES = ['template0', 'template1', 'postgres',
+                        'rdsadmin',  # aws rds
+                        'cloudsqladmin',  # cloudsql
+                        'azure_maintenance', 'azure_sys', # azure postgresql
+                        'powa'] # powa
+_PROTECTED_GREENPLUM_DATABASES = ['gpadmin', 'gpperfmon']
+_PROTECTED_ROLES = ['replicator', 'postgresadmin', 'psqladmin', 'azuresu', 'azure_pg_admin', 'replication']
+
+log = logging.getLogger("configs")
+
+def backups_storage(version=None):
+    if is_external_pg():
+        return '/backup-storage/external/granular'
+
+    storageRoot = '/backup-storage'
+    if not version:
+        try:
+            version = utils.get_version_of_pgsql_server()
+        except Exception as e:
+            version = get_postgres_version_by_path(storageRoot)
+            log.info(f"version returned from exception block {version}")
+            log.exception(e)
+    storage_path = '/backup-storage/granular'
+    # checking if version of pgsql server if above 10 => saving backups
+    # in different folder
+    if [10, 0] <= version < [11, 0]:
+        storage_path = '/backup-storage/pg10/granular'
+    elif [11, 0] <= version < [12, 0]:
+        storage_path = '/backup-storage/pg11/granular'
+    elif [12, 0] <= version < [13, 0]:
+        storage_path = '/backup-storage/pg12/granular'
+    elif [13, 0] <= version < [14, 0]:
+        storage_path = '/backup-storage/pg13/granular'
+    elif [14, 0] <= version < [15, 0]:
+        storage_path = '/backup-storage/pg14/granular'
+    elif [15, 0] <= version < [16, 0]:
+        storage_path = '/backup-storage/pg15/granular'
+    elif [16, 0] <= version < [17, 0]:
+        storage_path = '/backup-storage/pg16/granular'
+    elif version >= [17, 0]:
+        storage_path = '/backup-storage/pg17/granular'
+    return storage_path
+
+
+def default_namespace():
+    return 'default'
+
+
+def default_backup_type():
+    return 'all'
+
+
+def default_backup_expiration_period():
+    return '2 weeks'
+
+
+def postgresql_user():
+    return os.getenv('POSTGRES_USER') or 'postgres'
+
+
+def postgresql_host():
+    return os.getenv('POSTGRES_HOST') or 'localhost'
+
+
+def postgresql_port():
+    return os.getenv('POSTGRES_PORT') or '5432'
+
+
+def postgres_password():
+    return os.getenv('POSTGRES_PASSWORD')
+
+
+def postgresql_no_role_password_flag():
+    if is_external_pg():
+        return "--no-role-passwords"
+    return ""
+
+
+def protected_databases():
+    return _PROTECTED_DATABASES
+
+
+def protected_greenplum_databases():
+    return _PROTECTED_GREENPLUM_DATABASES
+
+
+def protected_roles():
+    postgres_admin_user = os.getenv("POSTGRES_USER", "postgres")
+    return _PROTECTED_ROLES + [postgres_admin_user]
+
+
+def eviction_interval():
+    interval = os.getenv("GRANULAR_EVICTION", "3600")
+    if interval:
+        interval = int(interval)
+    return interval or 3600
+
+
+def granular_cron_pattern():
+    return os.getenv("GRANULAR_BACKUP_SCHEDULE", "none")
+
+def diff_cron_pattern():
+    return os.getenv("DIFF_SCHEDULE", "none")
+
+def incr_cron_pattern():
+    return os.getenv("INCR_SCHEDULE", "none")
+
+
+def get_parallel_jobs():
+    return os.getenv("JOB_FLAG" , "1")
+
+def dbs_to_granular_backup():
+    databases = os.getenv("DATABASES_TO_SCHEDULE")
+    if databases:
+        databases = databases.split(',')
+    return databases or []
+
+def connection_properties(username = postgresql_user(), password = postgres_password(), database = 'postgres'):
+    return {
+        'host': postgresql_host(),
+        'port': postgresql_port(),
+        'user': username,
+        'password': password,
+        'database': database,
+        'connect_timeout': int(os.getenv("CONNECT_TIMEOUT", "5"))
+    }
+
+
+def get_encryption():
+    encrypt_backups = os.getenv("KEY_SOURCE", 'false').lower()
+    return encrypt_backups != 'false'
+
+
+def get_pgsql_bin_path(version):
+    major_version = version[0]
+    minor_version = version[1]
+    if major_version == 9:
+        if minor_version != 4:
+            return "/usr/pgsql-9.{}/bin".format(version[1])
+        else:
+            # GPDB uses Postgresql 9.4
+            return "/usr/local/greenplum-db/bin/"
+    elif major_version == 10:
+        return "/usr/lib/postgresql/10/bin"
+    elif major_version == 11:
+        return "/usr/lib/postgresql/11/bin"
+    elif major_version == 12:
+        return "/usr/lib/postgresql/12/bin"
+    elif major_version == 13:
+        return "/usr/lib/postgresql/13/bin"
+    elif major_version == 14:
+        return "/usr/lib/postgresql/14/bin"
+    elif major_version == 15:
+        return "/usr/lib/postgresql/15/bin"
+    elif major_version == 16:
+        return "/usr/lib/postgresql/16/bin"
+    elif major_version == 17:
+        return "/usr/lib/postgresql/17/bin"
+
+
+def is_external_pg():
+    return os.getenv("EXTERNAL_POSTGRESQL", "") != ""
diff --git a/docker-backup-daemon/docker/granular/granular.py b/docker-backup-daemon/docker/granular/granular.py
new file mode 100644
index 0000000..a57e4c8
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/granular.py
@@ -0,0 +1,1688 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import http.client
+import json
+import logging
+import os
+import io
+
+import flask
+import flask_restful
+from flask import Flask, Response
+from apscheduler.schedulers.background import BackgroundScheduler
+
+import requests
+import backups
+import configs
+import pg_backup
+import pg_restore
+import utils
+import glob
+import storage_s3
+import psycopg2
+import threading
+from functools import wraps
+
+import shutil
+from backups import build_backup_path, build_namespace_path, is_valid_namespace, build_backup_status_file_path
+from flask_httpauth import HTTPBasicAuth
+
+from flask import request, abort, Response, stream_with_context
+
+from opentelemetry.instrumentation.flask import FlaskInstrumentor
+from opentelemetry.sdk.trace import TracerProvider
+from opentelemetry.sdk.trace.export import BatchSpanProcessor
+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
+from opentelemetry.sdk.resources import SERVICE_NAME, Resource
+
+
+auth = HTTPBasicAuth()
+
+
+def superuser_authorization(func_to_decorate):
+    @wraps(func_to_decorate)
+    def wrap(self, *args, **kwargs):
+        if utils.is_auth_needed():
+            if request.authorization.username == configs.postgresql_user():
+                return func_to_decorate(self, *args, **kwargs)
+            else:
+                abort(403, 'You are not authorized to perform such action')
+        else:
+            return func_to_decorate(self, *args, **kwargs)
+
+    return wrap
+
+
+@auth.verify_password
+def authenticate_user(username, password):
+    if utils.is_auth_needed():
+
+        connection_properties = configs.connection_properties(username=username, password=password)
+        connect = None
+        try:
+            connect = psycopg2.connect(**connection_properties)
+            connect.cursor()
+            return True
+        except psycopg2.Error:
+            return False
+        finally:
+            if connect:
+                connect.close()
+    else:
+        return True
+
+
+def common_authorization(func_to_decorate):
+    @wraps(func_to_decorate)
+    def wrap(self, *args, **kwargs):
+        if utils.is_auth_needed():
+            content_type = request.headers.get('Content-Type')
+
+            if content_type and content_type.split(";")[0] != 'application/json' \
+                    and request.headers.get('Content-Length'):
+                return "Invalid request body: Content Type is not json", http.client.BAD_REQUEST
+
+            backup_request = request.get_json() or {}
+
+            for k in list(backup_request.keys()):
+                if k not in self.allowed_fields:
+                    return "Unknown field: %s" % k.encode('utf-8'), http.client.BAD_REQUEST
+
+            databases = backup_request.get('databases') or []
+
+            cred = request.authorization
+            if not cred:
+                abort(401, 'Credentials should be provided for this endpoint')
+
+            databases_count = len(databases)
+            if databases_count == 1:
+                dbname = databases[0]
+                connection_properties = \
+                    configs.connection_properties(username=cred.username, password=cred.password, database='postgres')
+                connect = None
+                try:
+                    connect = psycopg2.connect(**connection_properties)
+                    with connect.cursor() as cur:
+                        cur.execute("""
+                            SELECT pg_catalog.pg_get_userbyid(d.datdba) as Owner
+                            FROM pg_catalog.pg_database d WHERE d.datname = %s
+                            ORDER BY 1;
+                            """, (dbname,))
+                        database_owner = cur.fetchone()[0]
+                        if database_owner == cred.username:
+                            return func_to_decorate(self, *args, **kwargs)
+                        else:
+                            abort(403, 'You are not authorized to perform such action')
+                finally:
+                    if connect:
+                        connect.close()
+            elif not cred.username == configs.postgresql_user():
+                abort(403, 'You are not authorized to perform such action')
+            else:
+                return func_to_decorate(self, *args, **kwargs)
+        else:
+            return func_to_decorate(self, *args, **kwargs)
+
+    return wrap
+
+
+if os.getenv("DEBUG") and os.getenv("DEBUG").lower() == 'true':
+    logging.getLogger().setLevel(logging.DEBUG)
+
+
+def schedule_granular_backup(scheduler):
+    cron_pattern = configs.granular_cron_pattern()
+    if cron_pattern.lower() != 'none' and os.getenv("GRANULAR_BACKUP_SCHEDULE") != "":
+        if utils.is_mirror_env():
+            logging.info('It is a mirror env')
+            return
+        logging.info('Start schedule granular backup')
+        databases = configs.dbs_to_granular_backup()
+        backup_request = {'databases': databases, 'namespace': 'schedule'}
+        items = cron_pattern.split(' ', 5)
+        minute, hour, day, month, day_of_week = items[0], items[1], items[2], items[3], items[4]
+
+        granular_backup_request = GranularBackupRequestEndpoint()
+
+        return scheduler.add_job(
+            granular_backup_request.perform_granular_backup,
+            'cron',
+            [backup_request],
+            minute=minute,
+            hour=hour,
+            day=day,
+            month=month,
+            day_of_week=day_of_week)
+
+
+def schedule_diff_backup(scheduler):
+    cron_pattern = configs.diff_cron_pattern()
+    logging.info(f'DIFF SHEDULE {os.getenv("DIFF_SCHEDULE")}')
+    if cron_pattern.lower() != 'none' and os.getenv("DIFF_SCHEDULE") is not None:
+        logging.info('Start schedule diff backup')
+        items = cron_pattern.split(' ', 5)
+        logging.info(f"{items} cron items")
+        minute, hour, day, month, day_of_week = items[0], items[1], items[2], items[3], items[4]
+
+        diff_backup_request = DiffBackupRequestEndpoint()
+
+        return scheduler.add_job(
+            diff_backup_request.perform_diff_backup,
+            'cron',
+            minute=minute,
+            hour=hour,
+            day=day,
+            month=month,
+            day_of_week=day_of_week)
+
+def schedule_incr_backup(scheduler):
+    cron_pattern = configs.incr_cron_pattern()
+    logging.info(f'INCR SHEDULE {os.getenv("INCR_SCHEDULE")}')
+    if cron_pattern.lower() != 'none' and os.getenv("INCR_SCHEDULE") is not None:
+        logging.info('Start schedule incr backup')
+        items = cron_pattern.split(' ', 5)
+        logging.info(f"{items} cron items")
+        minute, hour, day, month, day_of_week = items[0], items[1], items[2], items[3], items[4]
+
+        incr_backup_request = IncrBackupRequestEndpoint()
+
+        return scheduler.add_job(
+            incr_backup_request.perform_incr_backup,
+            'cron',
+            minute=minute,
+            hour=hour,
+            day=day,
+            month=month,
+            day_of_week=day_of_week)
+
+class GranularBackupsListEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupsListEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    def get(self):
+        # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Getting GKE backup list')
+        #     client = utils.GkeBackupApiCaller()
+        #     response = client.backup_list()
+        #     return response
+        status = {}
+        storage = configs.backups_storage()
+
+        if self.s3:
+            if not self.s3.is_s3_storage_path_exist(storage):
+                return "Backups in s3 storage does not exist.", http.client.NOT_FOUND
+            namespaces = self.s3.get_granular_namespaces(storage)
+        elif not os.path.exists(storage):
+            return "Backups storage does not exist.", http.client.NOT_FOUND
+
+        for namespace in os.listdir(storage) if not self.s3 else namespaces:
+            if not backups.is_valid_namespace(namespace):
+                continue
+
+            status[namespace] = {}
+            if self.s3:
+                backup_ids = self.s3.get_backup_ids(storage, namespace)
+            for backup in os.listdir(backups.build_namespace_path(namespace)) if not self.s3 else backup_ids:
+                status_file = backups.build_backup_status_file_path(backup, namespace)
+                if self.s3:
+                    try:
+                        if self.s3.is_file_exists(status_file):
+                            status_file = self.s3.read_object(status_file)
+                            backup_status = json.loads(status_file)
+                            status[namespace][backup] = {
+                                'status': backup_status.get('status'),
+                                'created': backup_status.get('created'),
+                                'expirationDate': backup_status.get('expirationDate')
+                            }
+                        else:
+                            self.log.error("Cannot find status file in bucket with backup id {}".format(backup))
+                            status[namespace][backup] = {'status': 'Unknown'}
+
+                    except ValueError:
+                        self.log.exception("Cannot read status file")
+                        status[namespace][backup] = {'status': 'Unknown'}
+
+                elif os.path.isfile(status_file):
+                    with open(status_file, 'r') as f:
+                        try:
+                            backup_status = json.load(f)
+                            status[namespace][backup] = {
+                                'status': backup_status.get('status'),
+                                'created': backup_status.get('created'),
+                                'expirationDate': backup_status.get('expirationDate')
+                            }
+                        except ValueError:
+                            self.log.exception("Cannot read status file")
+                            status[namespace][backup] = {'status': 'Unknown'}
+                else:
+                    status[namespace][backup] = {'status': 'Unknown'}
+
+        return status, http.client.OK
+
+
+class GranularBackupRequestEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.allowed_fields = ['backupId',
+                               'namespace',
+                               'databases',
+                               'keep',
+                               'compressionLevel',
+                               'externalBackupPath',
+                               'storageName',
+                               'blobPath']
+
+    def perform_granular_backup(self, backup_request):
+        # # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Perform GKE backup')
+        #     client = utils.GkeBackupApiCaller()
+        #     backup_id = client.perform_backup()
+        #     if "error" not in backup_id:
+        #         return {
+        #             'backupId': backup_id
+        #         }, http.client.ACCEPTED
+        #     else:
+        #         return backup_id, http.client.BAD_REQUEST
+
+        self.log.info('Perform granular backup')
+
+        for k in list(backup_request.keys()):
+            if k not in self.allowed_fields:
+                self.log.exception("Unknown field: %s" % k.encode('utf-8'))
+                return "Unknown field: %s" % k.encode('utf-8'), http.client.BAD_REQUEST
+
+        databases = backup_request.get('databases') or []
+        namespace = backup_request.get('namespace') or configs.default_namespace()
+        if databases:
+            baselist = utils.get_database_list(databases)
+
+        if not isinstance(databases, list) and not isinstance(databases, tuple):
+            self.log.exception("Field 'database' must be an array.")
+            return "Field 'database' must be an array.", http.client.BAD_REQUEST
+
+        if not backups.is_valid_namespace(namespace):
+            self.log.exception("Invalid namespace name: %s." % namespace.encode('utf-8'))
+            return "Invalid namespace name: %s." % namespace.encode('utf-8'), http.client.BAD_REQUEST
+
+        for database in databases:
+            if backups.is_database_protected(database):
+                self.log.exception("Database '%s' is not suitable for backup/restore." % database)
+                return "Database '%s' is not suitable for backup/restore." % database, http.client.FORBIDDEN
+
+            if database not in baselist:
+                self.log.exception("Database '%s' does not exist" % database)
+                return "Database '%s' does not exist" % database, http.client.BAD_REQUEST
+
+        backup_id = backups.generate_backup_id()
+        backup_request['backupId'] = backup_id
+
+        worker = pg_backup.PostgreSQLDumpWorker(databases, backup_request, backup_request.get('blobPath'))
+
+        worker.start()
+
+        return {
+                   'backupId': backup_id
+               }, http.client.ACCEPTED
+
+    @auth.login_required
+    @common_authorization
+    def post(self):
+        content_type = request.headers.get('Content-Type')
+
+        if content_type and content_type.split(";")[0] != 'application/json' \
+                and request.headers.get('Content-Length'):
+            return "Invalid request body: Content Type is not json", http.client.BAD_REQUEST
+
+        backup_request = request.get_json() or {}
+
+        return self.perform_granular_backup(backup_request)
+
+
+class GranularBackupStatusEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    def get(self, backup_id):
+        if not backup_id:
+            return "Backup ID is not specified.", http.client.BAD_REQUEST
+        # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Getting GKE backup status')
+        #     client = utils.GkeBackupApiCaller()
+        #     response = client.backup_status(backup_id)
+        #     return response
+
+        namespace = flask.request.args.get('namespace') or configs.default_namespace()
+
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode('utf-8'), http.client.BAD_REQUEST
+
+        external_backup_path = flask.request.args.get('externalBackupPath') or None
+        external_backup_root = None
+        if external_backup_path is not None:
+            external_backup_root = backups.build_external_backup_root(external_backup_path)
+        backup_status_file = backups.build_backup_status_file_path(backup_id, namespace, external_backup_root)
+        if self.s3:
+            try:
+                status = self.s3.read_object(backup_status_file)
+                logging.info(status)
+
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+            return json.loads(status), http.client.OK
+        else:
+            if not os.path.isfile(backup_status_file):
+                return "Backup is not found.", http.client.NOT_FOUND
+
+            return utils.get_json_by_path(backup_status_file), http.client.OK
+
+
+class GranularBackupStatusJSONEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.allowed_fields = ['backupId', 'namespace', 'externalBackupPath']
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    def post(self):
+        backup_request = flask.request.get_json() or {}
+
+        for k in list(backup_request.keys()):
+            if k not in self.allowed_fields:
+                return "Unknown field: %s" % k.encode('utf-8'), http.client.BAD_REQUEST
+
+        backup_id = backup_request.get('backupId')
+        namespace = backup_request.get('namespace') or configs.default_namespace()
+
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode('utf-8'), http.client.BAD_REQUEST
+
+        if not backup_request:
+            return "Request body is empty.", http.client.BAD_REQUEST
+
+        if not backup_id:
+            return "Backup ID is not specified.", http.client.BAD_REQUEST
+
+        external_backup_path = backup_request.get('externalBackupPath')
+        external_backup_root = None
+        if external_backup_path is not None:
+            external_backup_root = backups.build_external_backup_root(external_backup_path)
+        status_path = backups.build_backup_status_file_path(backup_id, namespace, external_backup_root)
+
+        if self.s3:
+            try:
+                status = self.s3.read_object(status_path)
+                logging.info(status)
+
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+            return json.loads(status), http.client.OK
+        else:
+            if not os.path.isfile(status_path):
+                return "Backup is not found.", http.client.NOT_FOUND
+
+            with open(status_path) as f:
+                return json.load(f), http.client.OK
+
+
+class GranularRestoreRequestEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.allowed_fields = ['backupId', 'namespace', 'databases', 'force', 'restoreRoles', 'databasesMapping',
+                               'externalBackupPath', 'singleTransaction', "dbaasClone"]
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    @superuser_authorization
+    def post(self):
+        restore_request = flask.request.get_json() or {}
+        # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Perform GKE restore')
+        #     client = utils.GkeBackupApiCaller()
+        #     response = client.restore(restore_request)
+        #     return response
+
+        for k in list(restore_request.keys()):
+            if k not in self.allowed_fields:
+                return "Unknown field: %s" % k.encode('utf-8'), http.client.BAD_REQUEST
+
+        databases = restore_request.get('databases') or []
+        databases_mapping = restore_request.get('databasesMapping') or {}
+
+        if not isinstance(databases, list) and not isinstance(databases, tuple):
+            return "Field 'database' must be an array.", http.client.BAD_REQUEST
+
+        if not isinstance(databases_mapping, dict):
+            return "Field 'database_mapping' must be a dictionary.", http.client.BAD_REQUEST
+
+        backup_id = restore_request.get('backupId')
+        if not backup_id:
+            return "Backup ID is not specified.", http.client.BAD_REQUEST
+
+        namespace = restore_request.get('namespace') or configs.default_namespace()
+
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode('utf-8'), http.client.BAD_REQUEST
+
+        external_backup_path = restore_request.get('externalBackupPath')
+        external_backup_root = None
+        if external_backup_path is not None:
+            external_backup_root = backups.build_external_backup_root(external_backup_path)
+        backup_details_file = backups.build_backup_status_file_path(backup_id, namespace, external_backup_root)
+
+        if self.s3:
+            try:
+                status = self.s3.read_object(backup_details_file)
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+            backup_details = json.loads(status)
+
+        else:
+            if not os.path.isfile(backup_details_file):
+                return "Backup is not found.", http.client.NOT_FOUND
+
+            with open(backup_details_file, 'r') as f:
+                backup_details = json.load(f)
+
+        backup_status = backup_details['status']
+
+        if backup_status != backups.BackupStatus.SUCCESSFUL:
+            return "Backup status '%s' is unsuitable status for restore." % backup_status, http.client.FORBIDDEN
+        if self.s3:
+            databases = list(backup_details.get('databases', {}).keys())
+            for database in databases:
+                if not self.s3.is_file_exists(backups.build_database_backup_path(backup_id, database,
+                                                                                 namespace, external_backup_root)):
+                    return "Backup in bucket is not found.", http.client.NOT_FOUND
+        elif not backups.backup_exists(backup_id, namespace, external_backup_root):
+            return "Backup is not found.", http.client.NOT_FOUND
+
+        ghost_databases = []
+        uncompleted_backups = []
+
+        databases = restore_request.get('databases') or list(backup_details.get('databases', {}).keys())
+
+        # dict of owners {"db": "db_owner", ..}
+        owners_mapping = {}
+
+        for database in databases:
+            database_details = backup_details['databases'].get(database)
+            if not database_details:
+                ghost_databases.append(database)
+                continue
+            if database_details['status'] != backups.BackupStatus.SUCCESSFUL:
+                uncompleted_backups.append((database, database_details['status']))
+                continue
+
+            owners_mapping[database] = database_details.get('owner', 'postgres')
+
+        if ghost_databases:
+            return "Databases are not found: %s." % ', '.join([db.encode('utf-8') for db in ghost_databases]), \
+                   http.client.NOT_FOUND
+
+        if uncompleted_backups:
+            return "Database backup is in unsuitable status for restore: %s." \
+                   % ', '.join(['%s: %s' % (i[0].encode('utf-8'), i[1]) for i in uncompleted_backups]), \
+                   http.client.FORBIDDEN
+
+        tracking_id = backups.generate_restore_id(backup_id, namespace)
+        restore_request['trackingId'] = tracking_id
+
+        # force is false by default
+        force = False
+        force_param = restore_request.get('force')
+
+        if force_param:
+            if isinstance(force_param, str):
+                force = force_param == 'true'
+            elif type(force_param) is bool:
+                force = force_param
+
+
+        # restore_roles is true by default
+        restore_roles = True
+        restore_roles_param = restore_request.get('restoreRoles', True)
+
+        if restore_roles_param:
+            if isinstance(restore_roles_param, str):
+                restore_roles = restore_roles_param == 'true'
+            elif type(restore_roles_param) is bool:
+                restore_roles = restore_roles_param
+        single_transaction = False
+        single_transaction_param = restore_request.get('singleTransaction', True)
+        if single_transaction_param:
+            if isinstance(single_transaction_param, str):
+                single_transaction = single_transaction_param == 'true'
+        elif type(single_transaction_param) is bool:
+            single_transaction = single_transaction_param
+
+        is_dbaas_clone= restore_request.get('dbaasClone')
+        worker = pg_restore.PostgreSQLRestoreWorker(databases, force, restore_request, databases_mapping,
+                                                    owners_mapping, restore_roles,single_transaction, is_dbaas_clone)
+
+        worker.start()
+
+        return {
+                   'trackingId': tracking_id
+               }, http.client.ACCEPTED
+
+
+class TerminateBackupEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger("TerminateBackupEndpoint")
+
+    @auth.login_required
+    def post(self, backup_id):
+        self.log.info("Terminate request accepted for backup {}".format(backup_id))
+        cancelled = False
+
+        try:
+            for thread in threading.enumerate():
+                if thread.name == str(backup_id):
+                    thread.cancel()
+                    cancelled = thread.is_cancelled()
+            if cancelled:
+                self.log.info("Backup {} terminated successfully".format(thread.name))
+                return Response("Backup %s terminated successfully\n" % backup_id, status=200)
+            else:
+                self.log.info("There is no active backup with id {}".format(backup_id))
+                return Response("There is no active backup with id: %s\n" % backup_id, status=404)
+        except Exception as e:
+            self.log.exception("Backup {0} termination failed. \n {1}".format(backup_id, str(e)))
+            return Response("Backup {} termination failed".format(backup_id), status=500)
+
+
+class TerminateRestoreEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger("TerminateRestoreEndpoint")
+
+    @auth.login_required
+    def post(self, tracking_id):
+        self.log.info("Terminate request accepted for id {}".format(tracking_id))
+        cancelled = False
+
+        try:
+            for thread in threading.enumerate():
+                if thread.name == str(tracking_id):
+                    thread.cancel()
+                    cancelled = thread.is_cancelled()
+            if cancelled:
+                self.log.info("Restore {} terminated successfully".format(thread.name))
+                return Response("Restore %s terminated successfully\n" % tracking_id, status=200)
+            else:
+                self.log.info("There is no active restore with id {}".format(tracking_id))
+                return Response("There is no active backup with id: %s\n" % tracking_id, status=404)
+        except Exception as e:
+            self.log.exception("Restore {0} termination failed. \n {1}".format(tracking_id, str(e)))
+            return Response("Restore {} termination failed".format(tracking_id), status=500)
+
+class GranularRestoreStatusEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    @superuser_authorization
+    def get(self, tracking_id):
+        if not tracking_id:
+            return http.client.BAD_REQUEST, "Restore tracking ID is not specified."
+
+        # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Getting GKE restore status')
+        #     client = utils.GkeBackupApiCaller()
+        #     response = client.restore_status(tracking_id)
+        #     return response
+
+        try:
+            backup_id, namespace = backups.extract_backup_id_from_tracking_id(tracking_id)
+        except Exception as e:
+            self.log.exception(e)
+            return 'Malformed restore tracking ID.', http.client.BAD_REQUEST
+
+        external_backup_path = flask.request.args.get('externalBackupPath') or None
+        external_backup_root = None
+        if external_backup_path is not None:
+            external_backup_root = backups.build_external_backup_root(external_backup_path)
+        restore_status_file = backups.build_restore_status_file_path(backup_id, tracking_id, namespace,
+                                                                     external_backup_root)
+
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode('utf-8'), http.client.BAD_REQUEST
+        if self.s3:
+            try:
+                status = self.s3.read_object(restore_status_file)
+                logging.info(status)
+
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+            return json.loads(status), http.client.OK
+        else:
+            if not os.path.isfile(restore_status_file):
+                return "Restore is not found.", http.client.NOT_FOUND
+
+            return utils.get_json_by_path(restore_status_file), http.client.OK
+
+
+class GranularRestoreStatusJSONEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.allowed_fields = ['trackingId', 'externalBackupPath']
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    @superuser_authorization
+    def post(self):
+        tracking_request = flask.request.get_json() or {}
+
+        for k in list(tracking_request.keys()):
+            if k not in self.allowed_fields:
+                return "Unknown field: %s" % k.encode('utf-8'), http.client.BAD_REQUEST
+
+        if not tracking_request:
+            return "Restore tracking request has empty body.", http.client.BAD_REQUEST
+
+        tracking_id = tracking_request.get('trackingId')
+
+        if not tracking_id:
+            return "Restore tracking ID is not specified.", http.client.BAD_REQUEST
+
+        try:
+            backup_id, namespace = backups.extract_backup_id_from_tracking_id(tracking_id)
+        except Exception as e:
+            self.log.exception(e)
+            return 'Malformed restore tracking ID.', http.client.BAD_REQUEST
+
+        external_backup_path = tracking_request.get('externalBackupPath')
+        external_backup_root = None
+        if external_backup_path is not None:
+            external_backup_root = backups.build_external_backup_root(external_backup_path)
+        restore_status_file = backups.build_restore_status_file_path(backup_id, tracking_id, namespace,
+                                                                     external_backup_root)
+        if self.s3:
+            try:
+                status = self.s3.read_object(restore_status_file)
+                logging.info(status)
+
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+            return json.loads(status), http.client.OK
+        else:
+            if not os.path.isfile(restore_status_file):
+                return "Restore is not found.", http.client.NOT_FOUND
+
+            with open(restore_status_file) as f:
+                return json.load(f), http.client.OK
+
+
+class GranularBackupDeleteEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('GranularBackupDeleteEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    @superuser_authorization
+    def get(self, backup_id):
+        return self.process_delete(backup_id)
+
+    @auth.login_required
+    @superuser_authorization
+    def post(self, backup_id):
+        return self.process_delete(backup_id)
+
+    def process_delete(self, backup_id):
+        self.log.info("Request to delete backup %s" % backup_id)
+        if not backup_id:
+            return self.response(backup_id,
+                                 "Backup ID is not specified.",
+                                 backups.BackupStatus.FAILED,
+                                 http.client.BAD_REQUEST)
+
+        # for gke full backup
+        # if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
+        #     self.log.info('Perform GKE backup delete')
+        #     client = utils.GkeBackupApiCaller()
+        #     response = client.delete_backup(backup_id)
+        #     return response
+
+        namespace = flask.request.args.get('namespace') or configs.default_namespace()
+
+        if not is_valid_namespace(namespace):
+            return self.response(backup_id,
+                                 "Invalid namespace name: %s." % namespace.encode('utf-8'),
+                                 backups.BackupStatus.FAILED,
+                                 http.client.BAD_REQUEST)
+
+        backup_status_file = build_backup_status_file_path(backup_id, namespace)
+        if self.s3:
+            try:
+                self.s3.read_object(backup_status_file)
+            except:
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+
+        elif not os.path.isfile(backup_status_file):
+            return self.response(backup_id,
+                                 "Backup is not found.",
+                                 backups.BackupStatus.FAILED,
+                                 http.client.NOT_FOUND)
+
+        try:
+            dir = build_backup_path(backup_id, namespace)
+            if self.s3:
+                self.s3.delete_objects(dir)
+            else:
+                terminate = TerminateBackupEndpoint()
+                terminate.post(backup_id)
+                shutil.rmtree(dir)
+
+                # remove namespace dir if no more backups in namespace
+                backup_list = os.listdir(build_namespace_path(namespace))
+                if len(backup_list) == 0 and namespace != 'default':
+                    shutil.rmtree(build_namespace_path(namespace))
+
+        except Exception as e:
+            self.log.exception(e)
+            return self.response(backup_id,
+                                 'An error occurred while deleting backup {} : {}.'.format(backup_id, e),
+                                 backups.BackupStatus.FAILED,
+                                 http.client.INTERNAL_SERVER_ERROR)
+
+        return self.response(backup_id, "Backup deleted successfully.", backups.BackupStatus.SUCCESSFUL, http.client.OK)
+
+    def response(self, backup_id, message, status, code):
+        return {
+                   'backupId': backup_id,
+                   'message': message,
+                   'status': status
+               }, code
+
+
+class GranularBackupHealthEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('GranularBackupHealthEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    def get(self):
+
+        status = {}
+        namespace = "schedule"
+        status[namespace] = {}
+
+        namespace_path = backups.build_namespace_path(namespace)
+        if not os.path.exists(namespace_path):
+            return status, http.client.OK
+
+        sorted_backups = sorted(os.listdir(namespace_path), reverse=True)
+        dump_count = len(sorted_backups)
+        space = os.statvfs(namespace_path)
+        free_space, total_space = space.f_bfree * space.f_bsize, space.f_blocks * space.f_bsize
+        status[namespace]['dump_count'] = dump_count
+        status[namespace]['total_space'] = total_space
+        status[namespace]['free_space'] = free_space
+
+        if len(sorted_backups) > 0:
+            status[namespace]['backup'] = {
+                'count': len(sorted_backups)
+            }
+            last_backup = sorted_backups[-1]
+            status_file = backups.build_backup_status_file_path(last_backup, namespace)
+            if os.path.isfile(status_file):
+                with open(status_file, 'r') as f:
+                    try:
+                        backup_status = json.load(f)
+                        status[namespace]['last'] = {
+                            'id': last_backup,
+                            'status': backup_status.get('status'),
+                            'status_id': backups.get_backup_status_id(backup_status.get('status')),
+                            'created': backup_status.get('created'),
+                            'expires': backup_status.get('expires'),
+                            'expirationDate': backup_status.get('expirationDate')
+                        }
+                        return status, http.client.OK
+                    except ValueError:
+                        self.log.exception("Cannot read status file")
+                        status[namespace]['last'] = {
+                            'id': last_backup,
+                            'status': backups.BackupStatus.UNKNOWN,
+                            'status_id': backups.get_backup_status_id(backups.BackupStatus.UNKNOWN)
+                        }
+            else:
+                status[namespace]['last'] = {
+                    'id': last_backup,
+                    'status': backups.BackupStatus.UNKNOWN,
+                    'status_id': backups.get_backup_status_id(backups.BackupStatus.UNKNOWN)
+                }
+
+        return status, http.client.OK
+
+
+class GranularBackupDownloadEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger("GranularBackupDownloadEndpoint")
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "Zs3" else None
+
+    @auth.login_required
+    def get(self, backup_id):
+        self.log.info("Download request accepted ")
+
+        def generate(stream_path):
+            stream = io.FileIO(stream_path, "r", closefd=True)
+            with stream as f:
+                chunk_size = 4096
+                while True:
+                    data = f.read(chunk_size)
+                    if len(data) == 0:
+                        f.close()
+                        os.remove(stream_path)
+                        self.log.info("Download ends ")
+                        return
+                    yield data
+
+        namespace = flask.request.args.get('namespace') or configs.default_namespace()
+        path_for_streaming = utils.get_backup_tar_file_path(backup_id, namespace)
+        if path_for_streaming:
+            return Response(stream_with_context(
+                generate(path_for_streaming)),
+                mimetype='application/octet-stream',
+                headers=[
+                    ('Content-Type', 'application/octet-stream'),
+                    ('Content-Disposition',
+                     "pg_granular_backup_{}.tar.gz".format(
+                         backup_id))
+                ])
+        else:
+            return Response("Cannot find backup ", status=404)
+
+
+
+class DiffBackupRequestEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('DifferentialBackup')
+        self.allowed_fields = ['timestamp']
+
+    def perform_diff_backup(self):
+
+        self.log.info('Perform diff backup')
+
+        backup_id = backups.generate_short_id()
+        payload = {'timestamp':backup_id}
+
+        try:
+            pgbackrest_service = get_pgbackrest_service()
+            self.log.info(f"Using pgbackrest service: {pgbackrest_service}")
+        except Exception as e:
+            self.log.error(f"Failed to get pgbackrest service: {str(e)}")
+            return http.client.INTERNAL_SERVER_ERROR
+
+        r = requests.post(f"http://{pgbackrest_service}:3000/backup/diff", payload)
+        if r.status_code == 200:
+            return {
+                'backupId': backup_id
+            }, http.client.ACCEPTED
+        else:
+            return r.status_code
+
+
+    def post(self):
+        content_type = request.headers.get('Content-Type')
+
+        # if content_type and content_type.split(";")[0] != 'application/json' \
+        #         and request.headers.get('Content-Length'):
+        #     return "Invalid request body: Content Type is not json", http.client.BAD_REQUEST
+
+
+        return self.perform_diff_backup()
+
+class IncrBackupRequestEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('IncrementalBackup')
+        self.allowed_fields = ['timestamp']
+
+    def perform_incr_backup(self):
+
+        self.log.info('Perform incremental backup')
+
+        backup_id = backups.generate_short_id()
+        payload = {'timestamp':backup_id}
+
+        try:
+            pgbackrest_service = get_pgbackrest_service()
+            self.log.info(f"Using pgbackrest service: {pgbackrest_service}")
+        except Exception as e:
+            self.log.error(f"Failed to get pgbackrest service: {str(e)}")
+            return http.client.INTERNAL_SERVER_ERROR
+
+        r = requests.post(f"http://{pgbackrest_service}:3000/backup/incr", payload)
+        if r.status_code == 200:
+            return {
+                'backupId': backup_id
+            }, http.client.ACCEPTED
+        else:
+            return r.status_code
+
+
+    def post(self):
+        return self.perform_incr_backup()
+
+class GranularBackupStatusInfoEndpoint(flask_restful.Resource):
+
+    def __init__(self):
+        self.log = logging.getLogger('GranularBackupStatusMetricEndpoint')
+        self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+    @auth.login_required
+    def get(self):
+        self.log.info("Backups metric gathering")
+        storage = configs.backups_storage()
+        s3 = None
+        if os.environ['STORAGE_TYPE'] == "s3":
+            s3 = backups.get_s3_client()
+            namespaces = s3.get_granular_namespaces(storage)
+
+        all_backups = []
+
+        for namespace in os.listdir(storage) if not s3 else namespaces:
+            if s3:
+                backup_ids = s3.get_backup_ids(storage, namespace)
+
+            for backup_id in os.listdir(build_namespace_path(namespace)) if not s3 else backup_ids:
+                status_file = build_backup_status_file_path(backup_id, namespace)
+                if s3:
+                    try:
+                        if s3.is_file_exists(status_file):
+                            status_file = s3.read_object(status_file)
+                            backup_details = json.loads(status_file)
+                            all_backups.append(self.build_backup_info(backup_details))
+                            continue
+                        else:
+                            self.log.error("Cannot find status file in bucket with backup id {}".format(backup_id))
+                            failed_backup = {"backupId": backup_id, "namespace": namespace,  "status": backups.BackupStatus.FAILED}
+                            all_backups.append(failed_backup)
+                            continue
+                    except ValueError:
+                        self.log.exception("Cannot read status file")
+
+                if not os.path.isfile(status_file):
+                    failed_backup = {"backupId": backup_id, "namespace": namespace, "status": backups.BackupStatus.FAILED}
+                    all_backups.append(failed_backup)
+                    continue
+                else:
+                    backup_details = utils.get_json_by_path(status_file)
+                    all_backups.append(self.build_backup_info(backup_details))
+        response = {"granular": all_backups}
+
+        return response, http.client.OK
+
+    def build_backup_info(self, backup):
+
+        backupInfo = {
+            "backupId": backup.get("backupId", "UNDEFINED"),
+            "namespace": backup.get("namespace", "UNDEFINED"),
+            "status": backup.get("status", "UNDEFINED"),
+            "expirationDate": backup.get("expirationDate", "UNDEFINED"),
+            "created": backup.get("created", "UNDEFINED"),
+        }
+
+        return backupInfo
+
+class NewBackup(flask_restful.Resource):
+    __endpoints = ["/api/v1/backup"]
+
+    def __init__(self):
+        self.log = logging.getLogger("NewBackup")
+        self.allowed_fields = ["storageName", "blobPath", "databases"]
+        self.s3 = storage_s3.AwsS3Vault(prefix="")
+
+    @staticmethod
+    def get_endpoints():
+        return NewBackup.__endpoints
+
+    @auth.login_required
+    def post(self):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        body = request.get_json(silent=True) or {}
+        storage_name = body.get("storageName")
+        blob_path = body.get("blobPath")
+        databases = body.get("databases") or []
+
+        if not blob_path:
+            return {"message": "blobPath is required"}, http.client.BAD_REQUEST
+        if databases and not isinstance(databases, (list, tuple)):
+            return {"message": "databases must be an array"}, http.client.BAD_REQUEST
+
+        blob_path = normalize_blobPath(blob_path)
+
+        # Reuse old logic directly
+        backup_request = {
+            "databases": list(databases),
+            "blobPath": blob_path,
+            "storageName": storage_name
+        }
+        resp = GranularBackupRequestEndpoint().perform_granular_backup(backup_request)
+        body = None
+        code = None
+        if isinstance(resp, tuple) and len(resp) >= 2:
+            body, code = resp[0], resp[1]
+        elif isinstance(resp, dict):
+            body, code = resp, http.client.ACCEPTED
+        else:
+            if code == http.client.BAD_REQUEST:
+                return resp, http.client.NOT_FOUND
+            if code == http.client.BAD_REQUEST:
+                return resp, http.client.NOT_FOUND
+            return resp
+
+        try:
+            import datetime
+            created_iso = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
+        except Exception:
+            created_iso = ""
+
+        dbs_out = []
+        for d in (databases or []):
+            if isinstance(d, dict):
+                name = d.get("databaseName") or d.get("name") or ""
+            else:
+                name = str(d or "")
+            dbs_out.append({
+                "databaseName": name,
+                "status": "notStarted" if name else "",
+                "creationTime": created_iso
+            })
+
+        enriched = {
+            "status": "notStarted",
+            "backupId": body.get("backupId") if isinstance(body, dict) else None,
+            "creationTime": created_iso,
+            "storageName": storage_name or "",
+            "blobPath": blob_path or "",
+            "databases": dbs_out
+        }
+
+        return enriched, code
+
+
+class NewBackupStatus(flask_restful.Resource):
+    __endpoints = ["/api/v1/backup/<backup_id>"]
+
+    def __init__(self):
+        self.log = logging.getLogger("NewBackupStatus")
+        self.s3 = storage_s3.AwsS3Vault(prefix="")
+
+    @staticmethod
+    def get_endpoints():
+        return NewBackupStatus.__endpoints
+
+    @auth.login_required
+    def get(self, backup_id):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        if not backup_id:
+            return "Backup ID is not specified.", http.client.BAD_REQUEST
+
+        namespace = request.args.get("namespace") or configs.default_namespace()
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode("utf-8"), http.client.BAD_REQUEST
+
+        blob_path = normalize_blobPath(request.args.get("blobPath"))
+        status_path = backups.build_backup_status_file_path(backup_id, blob_path=blob_path)
+
+        
+        try:
+            raw = json.loads(self.s3.read_object(status_path))
+        except Exception:
+            return "Backup in bucket is not found.", http.client.NOT_FOUND
+
+        if blob_path and not raw.get("blobPath"):
+            raw["blobPath"] = blob_path
+
+        return backups.transform_backup_status_v1(raw), http.client.OK
+    
+    @auth.login_required
+    @superuser_authorization
+    def delete(self, backup_id):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        if not backup_id:
+            return {"backupId": backup_id, "message": "Backup ID is not specified", "status": "Failed"}, http.client.BAD_REQUEST
+
+        req_ns = request.args.get("namespace")
+        blob_path = request.args.get("blobPath")
+        if not blob_path:
+            return {"backupId": backup_id,
+                    "message": "blobPath query parameter is required (e.g. ?blobPath=tmp/a/b/c).",
+                    "status": "Failed"}, http.client.BAD_REQUEST
+        blob_path = normalize_blobPath(blob_path)
+
+        def _exists(p: str) -> bool:            
+            return self.s3.is_file_exists(p)
+
+        namespace = req_ns
+        if not namespace:
+            candidates = []
+            try:
+                candidates.append(configs.default_namespace())
+            except Exception:
+                pass
+            if "default" not in candidates:
+                candidates.append("default")
+            for cand in candidates:
+                status_try = backups.build_backup_status_file_path(backup_id, cand, blob_path=blob_path)
+                if _exists(status_try):
+                    namespace = cand
+                    break
+        if not namespace:
+            namespace = configs.default_namespace()
+
+        status_path = backups.build_backup_status_file_path(backup_id, namespace, blob_path=blob_path)
+        existed_before = _exists(status_path)
+
+        resp = TerminateBackupEndpoint().post(backup_id)
+        term_body, term_code = None, None
+        try:
+            if isinstance(resp, Response):
+                term_body = resp.get_data(as_text=True)
+                term_code = getattr(resp, "status_code", None)
+            elif isinstance(resp, tuple) and len(resp) >= 2:
+                term_body = resp[0]
+                try:
+                    term_code = int(resp[1])
+                except Exception:
+                    term_code = None
+            elif isinstance(resp, dict):
+                term_body = json.dumps(resp)
+                term_code = http.client.OK
+            elif isinstance(resp, (bytes, bytearray)):
+                term_body = resp.decode("utf-8", "replace")
+            elif isinstance(resp, str):
+                term_body = resp
+            else:
+                term_body = repr(resp)
+        except Exception:
+            term_body = repr(resp)
+
+        self.log.info("Terminate response for %s: code=%s body=%s", backup_id, term_code, term_body)
+
+        try:
+            target_dir = backups.build_backup_path(backup_id, blob_path=blob_path)
+            self.s3.delete_objects(target_dir)
+        except Exception as e:
+            if not existed_before and term_code == http.client.NOT_FOUND:
+                return {"backupId": backup_id, "message": "Backup is not found.", "status": "Failed"}, http.client.NOT_FOUND
+            self.log.exception("Delete failed for %s: %s", backup_id, e)
+            return {"backupId": backup_id,
+                    "message": f"An error occurred while deleting backup: {e}",
+                    "status": "Failed"}, http.client.INTERNAL_SERVER_ERROR
+
+        if not existed_before and term_code == http.client.NOT_FOUND:
+            return {"backupId": backup_id, "message": "Backup is not found.", "status": "Failed"}, http.client.NOT_FOUND
+
+        if term_code and 200 <= term_code < 300:
+            msg = "Backup terminated successfully. Cleanup completed."
+        elif term_code == http.client.NOT_FOUND:
+            msg = "No active backup. Cleanup completed."
+        else:
+            msg = "Termination attempted. Cleanup completed."
+
+        return {
+            "backupId": backup_id,
+            "message": msg,
+            "status": "Successful",
+            "termination": {"code": term_code, "body": term_body}
+        }, http.client.OK
+
+class NewRestore(flask_restful.Resource):
+    __endpoints = ["/api/v1/restore/<backup_id>"]
+
+    def __init__(self):
+        self.log = logging.getLogger("NewRestore")
+        self.s3 = storage_s3.AwsS3Vault(prefix="")
+
+    @staticmethod
+    def get_endpoints():
+        return NewRestore.__endpoints
+
+    @auth.login_required
+    @superuser_authorization
+    def post(self, backup_id):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        body = request.get_json(silent=True) or {}
+        blob_path = body.get("blobPath")
+        pairs = body.get("databases") or []
+        
+        dry_run = body.get("dryRun")
+        if dry_run:
+            self.log.info(f"Dry run requested for restore with backup ID: {backup_id}")
+
+        if not blob_path:
+            return {"message": "blobPath is required"}, http.client.BAD_REQUEST
+        if not isinstance(pairs, (list, tuple)):
+            return {"message": "databases must be an array of objects"}, http.client.BAD_REQUEST
+        blob_path = normalize_blobPath(blob_path)
+
+        databases = []
+        databases_mapping = {}
+        for item in pairs:
+            item = item or {}
+            prev_name = item.get("previousDatabaseName")
+            curr_name = item.get("databaseName")
+            if not prev_name or not curr_name:
+                return {"message": "each databases item must have previousDatabaseName and databaseName"}, http.client.BAD_REQUEST
+            databases.append(prev_name)
+            databases_mapping[prev_name] = curr_name
+
+        namespace = configs.default_namespace()
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode("utf-8"), http.client.BAD_REQUEST
+
+        backup_details_file = backups.build_backup_status_file_path(backup_id, blob_path=blob_path)
+
+        try:
+            status = self.s3.read_object(backup_details_file)
+            backup_details = json.loads(status)
+        except Exception:
+            return "Backup in bucket is not found.", http.client.NOT_FOUND
+
+        backup_status = backup_details["status"]
+        if backup_status != backups.BackupStatus.SUCCESSFUL:
+            return "Backup status '%s' is unsuitable status for restore." % backup_status, http.client.FORBIDDEN
+
+        for database in list(backup_details.get("databases", {}).keys()):
+            if not self.s3.is_file_exists(backups.build_database_backup_path(backup_id, database, blob_path=blob_path)):
+                return "Backup in bucket is not found.", http.client.NOT_FOUND
+
+        ghost_databases = []
+        uncompleted_backups = []
+        requested = databases or list(backup_details.get("databases", {}).keys())
+        owners_mapping = {}
+
+        for database in requested:
+            database_details = backup_details["databases"].get(database)
+            if not database_details:
+                ghost_databases.append(database)
+                continue
+            if database_details["status"] != backups.BackupStatus.SUCCESSFUL:
+                uncompleted_backups.append((database, database_details["status"]))
+                continue
+            owners_mapping[database] = database_details.get("owner", "postgres")
+
+        if ghost_databases:
+            return "Databases are not found: %s." % ", ".join(ghost_databases), http.client.NOT_FOUND
+        if uncompleted_backups:
+            return (
+                "Database backup is in unsuitable status for restore: %s."
+                % ", ".join(["%s: %s" % (i[0], i[1]) for i in uncompleted_backups]),
+                http.client.BAD_REQUEST,
+            )
+
+        tracking_id = backups.generate_restore_id(backup_id, namespace)
+
+        # Defaults preserved from old endpoint
+        force = False
+        restore_roles = True
+        single_transaction = True
+
+        # Start worker (same as old)
+        if not dry_run:
+            worker = pg_restore.PostgreSQLRestoreWorker(
+                requested, force,
+                {"backupId": backup_id, "namespace": namespace, "trackingId": tracking_id},
+                databases_mapping, owners_mapping, restore_roles, single_transaction, body.get("dbaasClone"), blob_path
+            )
+            worker.start()
+
+        try:
+            import datetime
+            created_iso = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
+        except Exception:
+            created_iso = ""
+
+        storage_name = body.get("storageName") or ""
+
+        dbs_out = []
+        for prev in (requested or []):
+            prev_name = prev or ""
+            restored_as = databases_mapping.get(prev_name) if isinstance(databases_mapping, dict) else None
+            status = "notStarted" if prev_name else ""
+            if dry_run:
+                status = "Successful"
+            dbs_out.append({
+                "previousDatabaseName": prev_name,
+                "databaseName": restored_as or prev_name,
+                "status": status,
+                "creationTime": created_iso
+            })
+
+        enriched = {
+            "status": "notStarted",
+            "restoreId": tracking_id,
+            "creationTime": created_iso,
+            "storageName": storage_name,
+            "blobPath": blob_path,
+            "databases": dbs_out
+        }
+
+        try:
+            restore_status = {
+                "trackingId": tracking_id,
+                "restoreId": tracking_id,
+                "status": "notStarted",
+                "errorMessage": None,
+                "created": created_iso,
+                "creationTime": created_iso,
+                "completionTime": None,
+                "databases": { prev: {"newDatabaseName": (databases_mapping.get(prev) if isinstance(databases_mapping, dict) else prev) or prev,
+                                      "status": "notStarted",
+                                      "creationTime": created_iso} for prev in (requested or []) },
+                "storageName": storage_name,
+                "blobPath": blob_path,
+                "sourceBackupId": backup_id
+            }
+
+            if dry_run:
+                enriched["status"] = "Successful"
+                enriched, http.client.OK
+
+            status_path = backups.build_restore_status_file_path(backup_id, tracking_id, namespace)
+
+            try:
+                if hasattr(utils, "write_in_json"):
+                    utils.write_in_json(status_path, restore_status)
+                else:
+                    os.makedirs(os.path.dirname(status_path), exist_ok=True)
+                    with open(status_path, "w") as sf:
+                        json.dump(restore_status, sf)
+            except Exception:
+                self.log.exception("Failed to persist initial restore status for %s", tracking_id)
+        except Exception:
+            self.log.debug("Skipping initial restore status persist for %s", tracking_id)
+
+        return enriched, http.client.OK
+
+
+class NewRestoreStatus(flask_restful.Resource):
+    __endpoints = ["/api/v1/restore/<restore_id>"]
+
+    def __init__(self):
+        self.log = logging.getLogger("NewRestoreStatus")
+        self.s3 = storage_s3.AwsS3Vault(prefix="")
+
+    @staticmethod
+    def get_endpoints():
+        return NewRestoreStatus.__endpoints
+
+    @auth.login_required
+    @superuser_authorization
+    def get(self, restore_id):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        if not restore_id:
+            return "Restore tracking ID is not specified.", http.client.BAD_REQUEST
+
+        try:
+            backup_id, namespace = backups.extract_backup_id_from_tracking_id(restore_id)
+        except Exception as e:
+            self.log.exception(e)
+            return "Malformed restore tracking ID.", http.client.BAD_REQUEST
+
+        if not backups.is_valid_namespace(namespace):
+            return "Invalid namespace name: %s." % namespace.encode("utf-8"), http.client.BAD_REQUEST
+
+        blob_path = normalize_blobPath(request.args.get("blobPath"))
+        storage_name = request.args.get("storageName") or os.environ.get("STORAGE_NAME")
+        status_path = backups.build_restore_status_file_path(backup_id, restore_id, blob_path=blob_path)
+
+        try:
+            raw = json.loads(self.s3.read_object(status_path))
+        except Exception:
+            return "Backup in bucket is not found.", http.client.NOT_FOUND
+
+        if blob_path and not raw.get("blobPath"):
+            raw["blobPath"] = blob_path
+        if storage_name and not raw.get("storageName"):
+            raw["storageName"] = storage_name
+
+        return backups.transform_restore_status_v1(raw), http.client.OK
+    
+    @auth.login_required
+    @superuser_authorization
+    def delete(self, restore_id):
+        if not self.s3:
+            return "S3 is not configured for backup daemon", http.client.FORBIDDEN
+
+        if not restore_id:
+            return {"restoreId": restore_id, "message": "Restore ID is not specified", "status": "Failed"}, http.client.BAD_REQUEST
+
+        try:
+            backup_id, namespace = backups.extract_backup_id_from_tracking_id(restore_id)
+        except Exception as e:
+            self.log.exception(e)
+            resp = TerminateRestoreEndpoint().post(restore_id)
+            term_code, term_body = None, None
+            try:
+                if isinstance(resp, Response):
+                    term_body = resp.get_data(as_text=True)
+                    term_code = getattr(resp, "status_code", None)
+                elif isinstance(resp, tuple) and len(resp) >= 2:
+                    term_body = resp[0]
+                    try: term_code = int(resp[1])
+                    except Exception: term_code = None
+                elif isinstance(resp, dict):
+                    term_body = json.dumps(resp)
+                    term_code = http.client.OK
+                elif isinstance(resp, str):
+                    term_body = resp
+                else:
+                    term_body = repr(resp)
+            except Exception:
+                term_body = repr(resp)
+
+            return {
+                "restoreId": restore_id,
+                "message": "Malformed restore ID; termination attempted but cleanup skipped.",
+                "status": "Successful",
+                "termination": {"code": term_code, "body": term_body}
+            }, http.client.OK
+
+        blob_path = request.args.get("blobPath")
+        if not blob_path:
+            return {
+                "restoreId": restore_id,
+                "message": "blobPath query parameter is required for cleanup (e.g. ?blobPath=tmp/a/b/c).",
+                "status": "Failed"
+            }, http.client.BAD_REQUEST
+        blob_path = normalize_blobPath(blob_path)
+
+        status_path = backups.build_restore_status_file_path(backup_id, restore_id, blob_path=blob_path)
+        backup_base = backups.build_backup_path(backup_id, blob_path=blob_path)
+        pattern_name = f"{restore_id}"
+        pattern_glob = os.path.join(backup_base, pattern_name + "*")
+
+        def _exists_file(p: str) -> bool:
+            try:
+                return self.s3.is_file_exists(p)
+            except Exception:
+                return False
+
+        def _prefix_exists() -> bool:
+            if hasattr(self.s3, "is_prefix_exists"):
+                try:
+                    return self.s3.is_prefix_exists(os.path.join(backup_base, pattern_name))
+                except Exception:
+                    return False
+            return False
+
+        existed_status = _exists_file(status_path)
+        existed_prefix = _prefix_exists()
+        resource_exists = existed_status or existed_prefix
+
+        if not resource_exists:
+            try:
+                TerminateRestoreEndpoint().post(restore_id)
+            except Exception:
+                pass
+            return {"restoreId": restore_id, "message": "Restore is not found.", "status": "Failed"}, http.client.NOT_FOUND
+
+        resp = TerminateRestoreEndpoint().post(restore_id)
+        term_body, term_code = None, None
+        try:
+            if isinstance(resp, Response):
+                term_body = resp.get_data(as_text=True)
+                term_code = getattr(resp, "status_code", None)
+            elif isinstance(resp, tuple) and len(resp) >= 2:
+                term_body = resp[0]
+                try:
+                    term_code = int(resp[1])
+                except Exception:
+                    term_code = None
+            elif isinstance(resp, dict):
+                term_body = json.dumps(resp)   
+                term_code = http.client.OK
+            elif isinstance(resp, (bytes, bytearray)):
+                term_body = resp.decode("utf-8", "replace")
+            elif isinstance(resp, str):
+                term_body = resp
+            else:
+                term_body = repr(resp)
+        except Exception:
+            term_body = repr(resp)
+
+        self.log.info("Terminate response for restore %s: code=%s body=%s", restore_id, term_code, term_body)
+
+        try:
+            prefix = os.path.join(backup_base, pattern_name).rstrip("/")
+            self.s3.delete_objects(prefix if prefix.endswith("/") else prefix)
+        except Exception as e:
+            self.log.exception("Restore cleanup failed for %s: %s", restore_id, e)
+            return {
+                "restoreId": restore_id,
+                "message": f"Termination attempted; cleanup encountered an error: {e}",
+                "status": "Failed",
+                "termination": {"code": term_code, "body": term_body}
+            }, http.client.INTERNAL_SERVER_ERROR
+
+        if term_code and 200 <= term_code < 300:
+            msg = "Restore terminated successfully. Cleanup completed."
+        else:
+            msg = "Termination attempted. Cleanup completed."
+
+        return {"restoreId": restore_id, "message": msg, "status": "Successful",
+                "termination": {"code": term_code, "body": term_body}}, http.client.OK
+
+def normalize_blobPath(blob_path):
+    # Normalize blob_path by removing a single leading and trailing slash
+    if isinstance(blob_path, str):
+        if blob_path.startswith("/"):
+            if not os.getenv("AWS_S3_PREFIX", ""):
+                blob_path = blob_path[1:]
+        else:
+            if os.getenv("AWS_S3_PREFIX", ""):
+                blob_path = "/" + blob_path
+        if blob_path.endswith("/"):
+            blob_path = blob_path[:-1]
+    return blob_path
+
+def get_pgbackrest_service():
+    if os.getenv("BACKUP_FROM_STANDBY") == "true":
+        try:
+            # Query Patroni API
+            patroni_response = requests.get("http://pg-patroni:8008/cluster").json()
+            
+            # Look for healthy streaming replicas
+            streaming_replicas = [member for member in patroni_response.get('members', []) 
+                                if member.get('role') == 'replica' and member.get('state') == 'streaming']
+            
+            if streaming_replicas:
+                logging.info("Found healthy streaming replica(s), using pgbackrest-standby")
+                return "backrest-standby"
+            else:
+                logging.info("No healthy streaming replicas found, using leader")
+        except Exception as e:
+            logging.error(f"Failed to query Patroni API: {str(e)}")
+            raise e
+    
+    return "backrest"
+
+app = Flask("GranularREST")
+collector_endpoint = os.getenv("OTEL_EXPORTER_OTLP_TRACES_ENDPOINT", "")
+if collector_endpoint != "":
+    collector_endpoint = "http://" + collector_endpoint
+    NAMESPACE_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
+    ns = open(NAMESPACE_PATH).read()
+    resource = Resource(attributes={
+        SERVICE_NAME: "postgresql-backup-daemon-" + ns
+    })
+    provider = TracerProvider(resource=resource)
+    processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=collector_endpoint, insecure=True))
+    provider.add_span_processor(processor)
+    FlaskInstrumentor().instrument_app(app=app, tracer_provider=provider, excluded_urls="health,/health,v2/health,/v2/health")
+api = flask_restful.Api(app)
+
+api.add_resource(GranularBackupsListEndpoint, '/backups')
+api.add_resource(GranularBackupRequestEndpoint, '/backup/request')
+api.add_resource(GranularBackupStatusEndpoint, '/backup/status/<backup_id>')
+api.add_resource(GranularBackupStatusJSONEndpoint, '/backup/status')
+api.add_resource(GranularRestoreRequestEndpoint, '/restore/request')
+api.add_resource(TerminateBackupEndpoint, '/terminate/<backup_id>')
+api.add_resource(TerminateRestoreEndpoint, '/restore/terminate/<tracking_id>')
+api.add_resource(GranularRestoreStatusEndpoint, '/restore/status/<tracking_id>')
+api.add_resource(GranularRestoreStatusJSONEndpoint, '/restore/status')
+api.add_resource(GranularBackupDeleteEndpoint, '/delete/<backup_id>')
+api.add_resource(GranularBackupHealthEndpoint, '/health')
+api.add_resource(GranularBackupDownloadEndpoint, '/backup/download/<backup_id>')
+api.add_resource(DiffBackupRequestEndpoint, '/backup/diff')
+api.add_resource(IncrBackupRequestEndpoint, '/backup/incr')
+api.add_resource(GranularBackupStatusInfoEndpoint, '/backup/info')
+api.add_resource(NewBackup, *NewBackup.get_endpoints())
+api.add_resource(NewBackupStatus, *NewBackupStatus.get_endpoints())
+api.add_resource(NewRestore, *NewRestore.get_endpoints())
+api.add_resource(NewRestoreStatus, *NewRestoreStatus.get_endpoints())
+
+scheduler = BackgroundScheduler()
+scheduler.start()
+if os.environ['STORAGE_TYPE'] == "pgbackrest":
+    schedule_diff_backup(scheduler)
+    schedule_incr_backup(scheduler)
+else:
+    scheduler.add_job(backups.sweep_manager, 'interval', seconds=configs.eviction_interval())
+    schedule_granular_backup(scheduler)
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/granular/kube_utils.py b/docker-backup-daemon/docker/granular/kube_utils.py
new file mode 100644
index 0000000..95ec364
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/kube_utils.py
@@ -0,0 +1,36 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import logging
+from kubernetes import client, config
+from kubernetes.client.rest import ApiException
+
+NS_PATH = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+
+log = logging.getLogger("KubernetesAPI")
+namespace = open(NS_PATH).read()
+
+config.load_incluster_config()
+api_instance = client.CoreV1Api()
+
+def get_configmap(name):
+    try:
+        api_response = api_instance.read_namespaced_config_map(name, namespace)
+        return api_response
+    except ApiException as e:
+        if e.status != 404:
+            log.error(f'cannot get cm {namespace}/{name}')
+            raise e 
+        
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/granular/pg_backup.py b/docker-backup-daemon/docker/granular/pg_backup.py
new file mode 100644
index 0000000..d51900b
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/pg_backup.py
@@ -0,0 +1,588 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import http.client
+import datetime
+import glob
+import logging
+import os
+import subprocess
+import time
+from threading import Thread, Event
+from subprocess import Popen, PIPE
+import psycopg2
+
+import utils
+import backups
+import configs
+import encryption
+import storage_s3
+
+
+class PostgreSQLDumpWorker(Thread):
+
+    def __init__(self, databases, backup_request, blob_path=None):
+        Thread.__init__(self)
+
+        self.log = logging.getLogger("PostgreSQLDumpWorker")
+
+        self.backup_id = backup_request.get('backupId') or backups.generate_backup_id()
+        self.name = self.backup_id
+        self.namespace = backup_request.get('namespace') or configs.default_namespace()
+        self.compression_level = backup_request.get('compressionLevel', 9)
+        self.keep = backup_request.get('keep') or configs.default_backup_expiration_period()
+        self.postgres_version = utils.get_version_of_pgsql_server()
+        self.is_standard_storage = True if backup_request.get('externalBackupPath') is None else False
+        self.location = configs.backups_storage(self.postgres_version) if self.is_standard_storage \
+            else backups.build_external_backup_root(backup_request.get('externalBackupPath'))
+        self.external_backup_root = None if self.is_standard_storage else self.location
+        self.bin_path = configs.get_pgsql_bin_path(self.postgres_version)
+        self.parallel_jobs = configs.get_parallel_jobs()
+        self.databases = databases if databases else []
+        self.blob_path = blob_path
+        self.backup_dir = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+        self.create_backup_dir()
+        if blob_path:
+            self.s3 = storage_s3.AwsS3Vault(prefix="")
+        else:  
+            self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+
+        self._cancel_event = Event()
+        if configs.get_encryption():
+            self.encryption = True
+            self.key = encryption.KeyManagement.get_object().get_password()
+            self.key_name = encryption.KeyManagement.get_key_name()
+            self.key_source = encryption.KeyManagement.get_key_source()
+        else:
+            self.encryption = False
+            
+        self.storage_name = backup_request.get('storageName') or ""
+        self.status = {
+            'backupId': self.backup_id,
+            'namespace': self.namespace,
+            'status': backups.BackupStatus.PLANNED,
+            'storageName': self.storage_name,
+        }
+        self.pg_dump_proc = None
+
+        self.flush_status(self.external_backup_root)
+
+    def cancel(self):
+        self.kill_processes()
+        self._cancel_event.set()
+        self.log.info(self.log_msg("Worker stopped"))
+
+    def is_cancelled(self):
+        return self._cancel_event.is_set()
+
+    def log_msg(self, msg):
+        return "[backupId={}] {}".format(self.backup_id, msg)
+
+    def kill_processes(self):
+        if self.pg_dump_proc:
+            self.log.info("kill backup process with pid: {}".format(self.pg_dump_proc.pid))
+            self.pg_dump_proc.kill()
+
+    def update_status(self, key, value, database=None, flush=False):
+        if database:
+            databases_section = self.status.get('databases')
+
+            if not databases_section:
+                databases_section = {}
+                self.status['databases'] = databases_section
+
+            database_details = databases_section.get(database)
+            if not database_details:
+                database_details = {}
+                databases_section[database] = database_details
+
+            database_details[key] = value
+            databases_section[database] = database_details
+            self.status['databases'] = databases_section
+        else:
+            self.status[key] = value
+        if flush or self.s3:
+            self.flush_status(self.external_backup_root)
+
+    def flush_status(self, external_backup_storage=None):
+        path = backups.build_backup_status_file_path(self.backup_id, self.namespace, external_backup_storage)
+        utils.write_in_json(path, self.status)
+        if self.s3:
+            try:
+                # upload dumpfile
+                self.s3.upload_file(path, self.blob_path, self.backup_id)
+            except Exception as e:
+                raise e
+
+    def stderr_file(self, database):
+        return '{}/{}.error'.format(self.backup_dir, database)
+
+    def populate_databases_list(self):
+        connection_properties = configs.connection_properties()
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            with conn.cursor() as cur:
+                cur.execute("SELECT datname "
+                            "FROM pg_database "
+                            "WHERE datallowconn = 't' and "
+                            "      datistemplate = 'f' and "
+                            "      datname not in ({0})".format(",".join("'{0}'".format(x) for x in
+                                                                         configs.protected_databases())))
+                self.databases = [db[0] for db in cur]
+        finally:
+            if conn:
+                conn.close()
+
+    def get_included_extensions(self, database):
+        connection_properties = configs.connection_properties()
+        connection_properties['database'] = database
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            with conn.cursor() as cur:
+                excluded_env = os.getenv("EXCLUDED_EXTENSIONS", "")
+                excluded_extensions = [e.strip() for e in excluded_env.split(',') if e.strip()]
+
+                # Always exclude pg_hint_plan
+                if "pg_hint_plan" not in excluded_extensions:
+                    excluded_extensions.append("pg_hint_plan")
+
+                self.log.info(self.log_msg(f"Excluded extensions: {excluded_extensions}"))
+
+                if not excluded_extensions:
+                    self.log.warning(self.log_msg("No excluded extensions configured; all extensions will be included."))
+
+                placeholders = ','.join(['%s'] * len(excluded_extensions))
+                query = f"SELECT extname FROM pg_extension WHERE extname NOT IN ({placeholders})"
+                cur.execute(query, excluded_extensions)
+
+                included_extensions = [row[0] for row in cur]
+                self.log.info(self.log_msg(f"Fetched included extensions for '{database}': {included_extensions}"))
+                return included_extensions
+        except Exception as e:
+            raise backups.BackupFailedException(database, f"Failed to fetch included extensions: {e}")
+        finally:
+            if conn:
+                conn.close()
+
+    def backup_single_database(self, database):
+        self.log.info(self.log_msg("Start processing database '{}'.".format(database)))
+        self.log.info(self.log_msg("Will use binaries: '{}' for backup.".format(self.bin_path)))
+
+        if database == 'postgres':
+            raise backups.BackupFailedException(
+                database, "Database 'postgres' is not suitable for "
+                          "backup/restore since Patroni always keeps "
+                          "connection to the database.")
+
+        self.update_status('status', backups.BackupStatus.IN_PROGRESS, database)
+        self.update_status('timestamp', int(time.time()), database)
+        iso_date = datetime.datetime.fromtimestamp(self.status['timestamp']).isoformat()
+        self.update_status('created', str(iso_date), database, flush=True)
+
+        pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+
+        # Some databases may contain special symbols like '=',
+        # '!' and others, so use this WA.
+        os.environ['PGDATABASE'] = database
+        if configs.postgres_password():
+         os.environ['PGPASSWORD'] = configs.postgres_password()
+
+        #fix to exclude pg_hint_plan for azure pg
+        #for pg17 use --exclude-extension with pg_dump
+        extension_flags = []
+        if configs.is_external_pg():
+            included_exts = self.get_included_extensions(database)
+            if included_exts:
+            # Add each extension as a separate --extension flag
+                for ext in included_exts:
+                    extension_flags.extend(['--extension', ext])
+
+        if int(self.parallel_jobs) > 1:
+            command = ['{}/pg_dump'.format(self.bin_path),
+                    '--format=directory',
+                    '--file', os.path.join(pg_dump_backup_path, database),
+                    '--user', configs.postgresql_user(),
+                    '--host', configs.postgresql_host(),
+                    '--port', configs.postgresql_port(),
+                    # '--clean',
+                    # '--create',
+                    # '--if-exists',
+                    '--blobs']
+
+            command.extend(['-j', self.parallel_jobs])
+
+            if configs.is_external_pg():
+                command.extend(extension_flags)
+                if self.postgres_version[0] < 15:
+                    command.extend(['-T','cron.*'])
+
+            # Zero is corner-case in Python :(
+            if self.compression_level or self.compression_level == 0:
+                command.extend(['-Z', str(self.compression_level)])
+
+
+            with open(self.stderr_file(database), "w+") as stderr:
+                start = time.time()
+                if self.encryption:
+                    pg_dump_proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=stderr)
+                    openssl_proc = subprocess.Popen(
+                        "openssl enc -aes-256-cbc -nosalt -pass pass:%s" % self.key,
+                        stdin=pg_dump_proc.stdout, shell=True, stderr=stderr)
+                    self.pg_dump_proc = openssl_proc
+                    exit_code = openssl_proc.wait()
+                else:
+                    pg_dump_proc = subprocess.Popen(command, stderr=stderr)
+                    self.pg_dump_proc = pg_dump_proc
+                    exit_code = pg_dump_proc.wait()
+
+                if exit_code != 0:
+                    with open(self.stderr_file(database)) as f:
+                        raise backups.BackupFailedException(database, '\n'.join(f.readlines()))
+
+                self.pg_dump_proc = None
+
+
+        else:
+            command = ['{}/pg_dump'.format(self.bin_path),
+                '--format=custom',
+                '--user', configs.postgresql_user(),
+                '--host', configs.postgresql_host(),
+                '--port', configs.postgresql_port(),
+                # '--clean',
+                # '--create',
+                # '--if-exists',
+                '--blobs']
+
+            if self.compression_level or self.compression_level == 0:
+                command.extend(['-Z', str(self.compression_level)])
+
+            if configs.is_external_pg():
+                command.extend(extension_flags)
+                if self.postgres_version[0] < 15:
+                    command.extend(['-T','cron.*'])
+
+            database_backup_path = backups.build_database_backup_path(self.backup_id, database,
+                                                                  self.namespace, self.external_backup_root)
+
+            with open(database_backup_path, 'w+') as dump, \
+                    open(self.stderr_file(database), "w+") as stderr:
+                start = time.time()
+                # in case of encryption lets redirect output of pg_dump to openssl
+                if self.encryption:
+                    pg_dump_proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=stderr)
+                    openssl_proc = subprocess.Popen(
+                        "openssl enc -aes-256-cbc -nosalt -pass pass:%s" % self.key,
+                        stdin=pg_dump_proc.stdout, stdout=dump, shell=True, stderr=stderr)
+                    self.pg_dump_proc = openssl_proc
+                    exit_code = openssl_proc.wait()
+                else:
+                    pg_dump_proc = subprocess.Popen(command, stdout=dump, stderr=stderr)
+                    self.pg_dump_proc = pg_dump_proc
+                    exit_code = pg_dump_proc.wait()
+
+                if exit_code != 0:
+                    with open(self.stderr_file(database)) as f:
+                        raise backups.BackupFailedException(database, '\n'.join(f.readlines()))
+
+                self.pg_dump_proc = None
+        if self.s3:
+            try:
+                # upload dumpfile
+                self.s3.upload_file(database_backup_path, self.blob_path, self.backup_id)
+                # upload errorfile
+                self.s3.upload_file(self.stderr_file(database), self.blob_path, self.backup_id)
+            except Exception as e:
+                raise e
+
+        self.update_status('duration', (int(time.time() - start)), database)
+        owner = utils.get_owner_of_db(database)
+        self.log.info(self.log_msg("owner of database '%s'." % owner))
+        self.update_status('owner', owner, database)
+
+        roles = self.fetch_roles(database)
+        self.dump_roles_for_db(roles, database)
+        self.update_status('duration', (int(time.time() - start)), database)
+        self.log.info(self.log_msg("Finished processing of database '%s'." % database))
+        if self.s3:
+            os.remove(database_backup_path)
+
+    def fetch_roles(self, database):
+        connection_properties = configs.connection_properties()
+        rolesList = []
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            conn.autocommit = True
+            with conn.cursor() as cur:
+                # check if there are any active connections to pgsql
+                cur.execute("SELECT pid "
+                            "FROM pg_stat_activity "
+                            "WHERE pid <> pg_backend_pid() "
+                            "      AND datname = %s LIMIT 1", (database,))
+                pids = [p[0] for p in cur.fetchall()]
+
+                cur.execute("""
+                SELECT r2.rolname grantee
+                FROM
+                  (SELECT datname AS objname,
+                          datallowconn, (aclexplode(datacl)).grantor AS grantorI,
+                          (aclexplode(datacl)).grantee AS granteeI,
+                          (aclexplode(datacl)).privilege_type
+                   FROM pg_database) AS db
+                JOIN pg_roles r1 ON db.grantorI = r1.oid
+                JOIN pg_roles r2 ON db.granteeI = r2.oid
+                WHERE db.objname = %s
+                    AND db.privilege_type = 'CONNECT' AND r2.rolname not in ('postgresadmin');
+                """, (database,))
+                rolesList = [p[0] for p in cur.fetchall()]
+        finally:
+            if conn:
+                conn.close()
+
+        self.log.debug(self.log_msg("Roles {} have been fetched for backup ".format(rolesList)))
+
+        roles_backup_path = backups.build_roles_backup_path(self.backup_id, database,
+                                                            self.namespace, self.external_backup_root)
+        database_backup_path = backups.build_database_backup_path(self.backup_id, database,
+                                                                  self.namespace, self.external_backup_root)
+
+        pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+        path_for_parallel_flag_backup = os.path.join(pg_dump_backup_path, database)
+
+        self.log.debug("Will try to fetch users to %s" % roles_backup_path)
+        with open(self.stderr_file(database), "w+") as stderr:
+            fetch_command = \
+                "| grep -P 'ALTER TABLE.*OWNER TO.*' | " \
+                "awk 'NF>1{print  substr($NF, 1, length($NF)-1)}' | uniq"
+            if self.encryption:
+                encrypt = "openssl enc -aes-256-cbc -nosalt -d -pass " \
+                    "pass:'%s' < '%s' | %s/pg_restore " % \
+                          (self.key, database_backup_path, self.bin_path)
+                fetch_command = encrypt + fetch_command
+            else:
+                if int(self.parallel_jobs) > 1:
+                    dump_version = self.get_pg_version_from_dump(path_for_parallel_flag_backup)
+                else:    
+                    dump_version = self.get_pg_version_from_dump(database_backup_path)
+
+                pg_restore_options = "-f - "
+                if dump_version[0] == 9 and dump_version[1] == 4:
+                    pg_restore_options = ""
+                
+                if int(self.parallel_jobs) > 1:
+                    pg_restore_options += " -j {} ".format(self.parallel_jobs)
+                    fetch_command = ("%s/pg_restore '%s' %s" + fetch_command) % \
+                                    (self.bin_path, path_for_parallel_flag_backup, pg_restore_options)
+                else:
+                    fetch_command = ("%s/pg_restore '%s' %s" + fetch_command) % \
+                                    (self.bin_path, database_backup_path, pg_restore_options)
+            self.log.debug("Roles fetch command: %s." % fetch_command)
+            p = Popen(fetch_command, shell=True, stdout=PIPE, stderr=stderr)
+            output, err = p.communicate()
+            exit_code = p.returncode
+            self.log.info("Roles search result: {} type: {} . Exit code: {}".format(output, type(output), exit_code))
+            if exit_code != 0:
+                raise backups.BackupFailedException(database, '\n'.join(
+                    stderr.readlines()))
+            rolesFromRestore = [x for x in output.decode().split("\n") if x.strip()]
+            rolesList = list(set(rolesList + rolesFromRestore))
+            roles = "|".join(list(
+                [x for x in rolesList if x not in configs.protected_roles()]))
+            self.log.debug("Selected roles template: %s " % roles)
+            return roles
+
+    def dump_roles_for_db(self, roles, database):
+        roles_backup_path = backups.build_roles_backup_path(self.backup_id, database,
+                                                            self.namespace, self.external_backup_root)
+
+        with open(roles_backup_path, 'w+') as dump, \
+                open(self.stderr_file(database), "w+") as stderr:
+            if roles:
+                cmd = "{}/pg_dumpall --roles-only -U {} --host {} --port {} {}" \
+                      "| grep -P '{}' ".format(self.bin_path,
+                                           configs.postgresql_user(),
+                                           configs.postgresql_host(),
+                                           configs.postgresql_port(),
+                                           configs.postgresql_no_role_password_flag(),
+                                           roles
+                                           )
+                                                           
+                if self.encryption:
+                    encrypt_cmd = \
+                        "| openssl enc -aes-256-cbc -nosalt -pass" \
+                        " pass:'{}'".format(self.key)
+                    cmd = cmd + encrypt_cmd
+                p = Popen(cmd, shell=True, stdout=dump, stderr=stderr)
+                output, err = p.communicate()
+                self.log.debug("Fetch roles command: {}".format(cmd))
+                exit_code = p.returncode
+                if exit_code != 0:
+                    # stderr.seek(0)
+                    raise backups.BackupFailedException(
+                        database, '\n'.join(stderr.readlines()))
+            else:
+                self.log.info("No roles to fetch")
+            if self.s3:
+                try:
+                    logging.info("Streaming {} roles to AWS".format(database))
+                    self.s3.upload_file(roles_backup_path, self.blob_path, self.backup_id)
+                except Exception as e:
+                    raise e
+                finally:
+                    os.remove(roles_backup_path)
+
+    def cleanup(self, database):
+        if self.s3:
+            self.s3.delete_file(self.stderr_file(database))
+
+        os.remove(self.stderr_file(database))
+
+    def on_success(self, database):
+        database_backup_path = backups.build_database_backup_path(self.backup_id, database, self.namespace, self.external_backup_root, self.blob_path)
+
+        pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root, self.blob_path)
+        path_for_parallel_flag_backup = os.path.join(pg_dump_backup_path, database)
+        
+        if self.s3:
+            if int(self.parallel_jobs) > 1:
+                size_bytes = self.s3.get_file_size(path_for_parallel_flag_backup)
+            else:
+                size_bytes = self.s3.get_file_size(database_backup_path)
+        else:
+            if int(self.parallel_jobs) > 1:
+                size_bytes = os.path.getsize(path_for_parallel_flag_backup)
+            else:
+                size_bytes = os.path.getsize(database_backup_path)
+        self.update_status('path', backups.
+                           build_database_backup_full_path(
+            self.backup_id, database, self.location, self.namespace, blob_path=self.blob_path), database)
+        self.update_status('sizeBytes', size_bytes, database)
+        self.update_status('size', backups.sizeof_fmt(size_bytes), database)
+        if self.encryption:
+            self.update_status('key_name', self.key_name)
+            self.update_status('key_source', self.key_source)
+        self.update_status(
+            'status', backups.BackupStatus.SUCCESSFUL, database, flush=True
+        )
+
+    def on_failure(self, database, e):
+        self.log.exception("Failed to backup database {0} {1}.".format(database, str(e)))
+        self.update_status('details', str(e), database)
+        self.update_status('status', backups.BackupStatus.FAILED, database)
+
+        for f in glob.glob(self.backup_dir + '/*.dump'):
+            os.remove(f)
+
+    def on_cancel(self, database=None):
+        if database:
+            self.log.exception("Backup got canceled for database {0}".format(database))
+            self.update_status('details', "Backup got canceled for database {0}".format(database), database)
+            self.update_status('status', backups.BackupStatus.CANCELED, database)
+        self.update_status('details', "Backup got canceled for database")
+        self.update_status("status", backups.BackupStatus.CANCELED, flush=True)
+
+    def expire(self, start_timestamp=None, keep=configs.default_backup_expiration_period()):
+
+        if not start_timestamp:
+            start_timestamp = int(time.time())
+
+        if keep.lower() == 'forever':
+            self.update_status('expires', 'Never')
+            self.update_status('expirationDate', 'Never', flush=True)
+        else:
+            expiration_timestamp = backups.calculate_expiration_timestamp(start_timestamp, keep)
+            self.update_status('expires', expiration_timestamp)
+            self.update_status('expirationDate', str(datetime.datetime.fromtimestamp(expiration_timestamp).isoformat()), flush=True)
+
+    def process_backup_request(self):
+        self.update_status('status', backups.BackupStatus.IN_PROGRESS, flush=True)
+        self.log.info(self.log_msg("Backup request processing has been started. Databases to backup: '{}'.".format(self.databases)))
+
+        start_timestamp = int(time.time())
+        self.expire(keep=self.keep)
+        self.update_status('timestamp', start_timestamp)
+        self.update_status('created', str(datetime.datetime.fromtimestamp(start_timestamp).isoformat()))
+
+        for database in self.databases:
+            if backups.is_database_protected(database):
+                return "Database '{}' is not suitable for backup/restore.".format(database, http.client.FORBIDDEN)
+            #if self.should_be_skipped(database):
+            #    self.log.info("Skipping Logical Database: {}, because it's not suitable for the backup.".format(database))
+            try:
+                self.backup_single_database(database)
+                self.on_success(database)
+            except Exception as e:  # Call on_failure here to mark database backup failed on any exception.
+                self.on_failure(database, e)
+                raise e
+            finally:
+                self.cleanup(database)
+
+    def should_be_skipped(self, database):
+        # if not external, shouldn't skip
+        if not configs.is_external_pg():
+            return False
+
+        # if external check for _DBAAS_METADATA table presence
+        # establish connect to logical db and check if metadata presented
+        connection_properties = configs.connection_properties(database=database)
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            with conn.cursor() as cur:
+                cur.execute("select 1 from pg_tables where upper(tablename) = '_DBAAS_METADATA'")
+                return cur.fetchone() == None
+        finally:
+            if conn:
+                conn.close()
+
+    def create_backup_dir(self):
+        if not os.path.exists(self.backup_dir):
+            os.makedirs(self.backup_dir)
+
+    def on_failure(self, database, e):
+        msg = f'Backup of "{database}" failed: {str(e)}'
+        self.update_status('errorMessage', msg, database)
+        self.update_status('status', backups.BackupStatus.FAILED, database, flush=True)
+
+    def _mark_done(self, final_status):
+        now_iso = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')
+        self.update_status('status', final_status)
+        self.update_status('completionTime', now_iso, flush=True)
+
+    def run(self):
+        try:
+            if not self.databases:
+                self.log.info(self.log_msg("No databases specified for backup. "
+                                           "According to the contract, all databases will be backup'ed."))
+                self.populate_databases_list()
+
+            self.process_backup_request()
+            self._mark_done(backups.BackupStatus.SUCCESSFUL)
+            self.log.info(self.log_msg("Backup request processing has been completed."))
+        except Exception as e:
+            self.log.exception(self.log_msg("Backup request processing has failed."))
+            self.update_status('errorMessage', f'Backup failed: {e}')
+            self._mark_done(backups.BackupStatus.FAILED)
+            self.expire()
+            raise e
+        finally:
+            if self.is_cancelled():
+                self.on_cancel()
+
+    def get_pg_version_from_dump(self, database_backup_path):
+        return utils.get_pg_version_from_dump(database_backup_path, self.key_name if self.encryption else None, self.bin_path)
+    
+
diff --git a/docker-backup-daemon/docker/granular/pg_restore.py b/docker-backup-daemon/docker/granular/pg_restore.py
new file mode 100644
index 0000000..c3a2a4f
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/pg_restore.py
@@ -0,0 +1,715 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import json
+import shutil
+
+from psycopg2.extensions import AsIs
+import logging
+from threading import Thread, Event
+import os
+import os.path
+import subprocess
+import time
+
+import psycopg2
+
+import utils
+import backups
+import configs
+import encryption
+import storage_s3
+
+
+class PostgreSQLRestoreWorker(Thread):
+
+    def __init__(self, databases, force, restore_request, databases_mapping, owners_mapping, restore_roles=True, single_transaction=False, dbaas_clone=False, blobPath=None):
+        Thread.__init__(self)
+
+        self.log = logging.getLogger("PostgreSQLRestoreWorker")
+        self.backup_id = restore_request.get('backupId')
+
+        if not self.backup_id:
+            raise Exception("Backup ID is not specified.")
+
+        self.databases = databases or []
+        self.force = force
+        self.single_transaction = single_transaction
+        self.namespace = restore_request.get(
+            'namespace') or configs.default_namespace()
+        self.tracking_id = restore_request.get(
+            'trackingId') or backups.generate_restore_id(self.backup_id,
+                                                         self.namespace)
+        self.name = self.tracking_id
+        self.is_standard_storage = True if restore_request.get('externalBackupPath') is None else False
+        self.restore_roles = restore_roles
+        self.postgres_version = utils.get_version_of_pgsql_server()
+        self.location = configs.backups_storage(self.postgres_version) if self.is_standard_storage \
+            else backups.build_external_backup_root(restore_request.get('externalBackupPath')) if restore_request.get('externalBackupPath') is not None else blobPath
+        self.external_backup_root = None if self.is_standard_storage else self.location
+        self.databases_mapping = databases_mapping
+        self.owners_mapping = owners_mapping
+        self.bin_path = configs.get_pgsql_bin_path(self.postgres_version)
+        self.parallel_jobs = configs.get_parallel_jobs()
+        if blobPath:
+            self.s3 = storage_s3.AwsS3Vault(prefix="")
+        else:  
+            self.s3 = storage_s3.AwsS3Vault() if os.environ['STORAGE_TYPE'] == "s3" else None
+        self.blob_path = blobPath
+        self.backup_dir = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+        self.create_backup_dir(self.backup_dir)
+        if configs.get_encryption():
+            self.encryption = True
+            self.key_name = backups.get_key_name_by_backup_id(self.backup_id,
+                                                              self.namespace, self.external_backup_root)
+        else:
+            self.encryption = False
+        if databases_mapping:
+            self.databases = list(databases_mapping.keys())
+        self.status = {
+            'trackingId': self.tracking_id,
+            'namespace': self.namespace,
+            'backupId': self.backup_id,
+            'status': backups.BackupStatus.PLANNED
+        }
+        self._cancel_event = Event()
+        self.pg_restore_proc = None
+        self.flush_status(self.external_backup_root)
+        self.dbaas_clone=dbaas_clone
+
+    def create_backup_dir(self, backup_dir):
+        if not os.path.exists(backup_dir):
+            os.makedirs(backup_dir)
+
+    def log_msg(self, msg):
+        return "[trackingId=%s] %s" % (self.tracking_id, msg)
+
+    def flush_status(self, external_backup_storage=None):
+        path = backups.build_restore_status_file_path(self.backup_id, self.tracking_id, self.namespace,
+                                                          external_backup_storage)
+        utils.write_in_json(path, self.status)
+        if self.s3:
+            try:
+                # upload status file
+                self.s3.upload_file(path, self.blob_path, self.backup_id)
+            except Exception as e:
+                raise e
+
+    def update_status(self, key, value, database=None, flush=False):
+        if database:
+            databases_section = self.status.get('databases')
+
+            if not databases_section:
+                databases_section = {}
+                self.status['databases'] = databases_section
+
+            database_details = databases_section.get(database)
+            if not database_details:
+                database_details = {}
+                if self.databases_mapping and self.databases_mapping[database]:
+                    database_details['newDatabaseName'] = self.databases_mapping[database]
+
+            database_details[key] = value
+            databases_section[database] = database_details
+            self.status['databases'] = databases_section
+        else:
+            self.status[key] = value
+
+        if flush or self.s3:
+            self.flush_status(self.external_backup_root)
+
+    @staticmethod
+    def db_exists(database):
+        connection_properties = configs.connection_properties()
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            with conn.cursor() as cur:
+                cur.execute("select 1 from pg_database where datname = %s", (database,))
+                return bool(cur.fetchone())
+        finally:
+            if conn:
+                conn.close()
+
+    def kill_pids_and_revoke_rights(self, database, cur, roles):
+        if not self.db_exists(database):
+            self.log.info(self.log_msg("skipping revoke, for not existing '%s' db" % database))
+            return
+        for role in roles:
+            self.log.info(self.log_msg("Revoking grants from: {} on: {}.".format(role, database)))
+            cur.execute("REVOKE CONNECT ON DATABASE \"%(database)s\" from \"%(role)s\";", {"database": AsIs(database),
+                                                                                           "role": AsIs(role)})
+        # also, revoking connect rights from public
+        cur.execute("REVOKE CONNECT ON DATABASE \"%(database)s\" from PUBLIC;", {"database": AsIs(database)})
+        # selecting pids to kill after revoking rights
+        cur.execute("SELECT pg_terminate_backend(pid) "
+                    "FROM pg_stat_activity "
+                    "WHERE pid <> pg_backend_pid() "
+                    "      AND datname = %s ", (database,))
+
+    def get_pg_version_from_dump(self, dump_path):
+        return utils.get_pg_version_from_dump(dump_path, self.key_name if self.encryption else None, self.bin_path)
+
+    def restore_single_database(self, database):
+        self.log.info(self.log_msg("Start restoring database '%s'."
+                                   % database))
+
+        db_start = int(time.time())
+        self.update_status('creationTime',
+                       datetime.datetime.fromtimestamp(db_start).isoformat(),
+                       database=database)
+        self.update_status('status', backups.BackupStatus.IN_PROGRESS, database)
+        self.update_status('source', backups.build_database_backup_full_path(
+            self.backup_id, database, self.location,
+            self.namespace, blob_path=self.blob_path), database, flush=True)
+
+        if int(self.parallel_jobs) > 1:
+            pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+            dump_path = os.path.join(pg_dump_backup_path, database)
+        else:
+            dump_path = backups.build_database_backup_path(self.backup_id, database,
+                                                           self.namespace, self.external_backup_root)
+        roles_backup_path = backups.build_roles_backup_path(self.backup_id, database,
+                                                            self.namespace, self.external_backup_root)
+        stderr_path = backups.build_database_backup_path(self.backup_id, database,
+                                                         self.namespace, self.external_backup_root)
+        stderr_path = stderr_path + '.stderr'
+        stdout_path = backups.build_database_backup_path(self.backup_id, database,
+                                                         self.namespace, self.external_backup_root)
+        sql_script_path = stdout_path + '.sql'
+        stdout_path = stdout_path + '.stdout'
+        if self.s3:
+            try:
+                self.s3.download_file(dump_path, self.backup_id, self.blob_path)
+                if self.restore_roles:
+                    self.s3.download_file(roles_backup_path, self.backup_id, self.blob_path)
+            except Exception as e:
+                raise e
+        self.update_status('path',
+                       backups.build_database_backup_full_path(
+                           self.backup_id, database, self.location, self.namespace, blob_path=self.blob_path),
+                       database=database,
+                       flush=True)
+        new_bd_name = self.databases_mapping.get(database) or database
+        db_owner = self.owners_mapping.get(database, 'postgres')
+        dump_version = self.get_pg_version_from_dump(dump_path)
+        bin_path = configs.get_pgsql_bin_path(dump_version)
+        self.log.info(self.log_msg("Will use binaries: '%s' for restore."
+                                   % bin_path))
+        os.environ['PGPASSWORD'] = configs.postgres_password()
+
+        connection_properties = configs.connection_properties()
+        # Restoring the pg_restore command with the -C flag to save the role rights. If we restore to the old database and the same version of PG and backup
+        restore_without_psql = database == new_bd_name and self.postgres_version[0]==dump_version[0] and not configs.is_external_pg() and (not self.single_transaction)
+        roles = []
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            conn.autocommit = True
+            with conn.cursor() as cur:
+                # check if there are any active connections to pgsql
+                cur.execute("SELECT pid "
+                            "FROM pg_stat_activity "
+                            "WHERE pid <> pg_backend_pid() "
+                            "      AND datname = %s LIMIT 1", (new_bd_name,))
+                pids = [p[0] for p in cur.fetchall()]
+
+                # get roles to revoke from
+                pg_user = configs.postgresql_user()
+                cur.execute("""
+                SELECT r2.rolname grantee
+                FROM
+                  (SELECT datname AS objname,
+                          datallowconn, (aclexplode(datacl)).grantor AS grantorI,
+                          (aclexplode(datacl)).grantee AS granteeI,
+                          (aclexplode(datacl)).privilege_type
+                   FROM pg_database) AS db
+                JOIN pg_roles r1 ON db.grantorI = r1.oid
+                JOIN pg_roles r2 ON db.granteeI = r2.oid
+                WHERE db.objname = %s
+                    AND db.privilege_type = 'CONNECT' AND r2.rolname not in ('postgresadmin', %s);
+                """, (new_bd_name, pg_user,))
+
+                roles = [p[0] for p in cur.fetchall()]
+
+                if pids and not self.force:
+                    with open(stderr_path, 'a+') as f:
+                        raise backups.RestoreFailedException("Not able to restore database {} with running connection".
+                                                             format(new_bd_name), '\n'.join(f.readlines()))
+
+                if pids and self.force:
+                    # revoke grants for connection, to prevent new connection
+                    self.kill_pids_and_revoke_rights(new_bd_name, cur, roles)
+                if (not self.single_transaction):
+                    self.log.debug(self.log_msg("DROP DROP DATABASE IF EXISTS".format(new_bd_name)))
+                    cur.execute('DROP DATABASE IF EXISTS \"%(database)s\"', {"database": AsIs(new_bd_name)})
+
+                if (not restore_without_psql) and (not self.single_transaction):
+                    cur.execute('CREATE DATABASE \"%(database)s\"', {"database": AsIs(new_bd_name)})
+                    self.drop_lookup_func_for_db(new_bd_name)
+
+                if self.restore_roles and os.path.isfile(roles_backup_path):
+                    self.log.info(self.log_msg("Will try to restore roles"))
+                    command = "psql --dbname=postgres --username {} --host {}" \
+                              " --port {} --echo-all --file {}" \
+                        .format(configs.postgresql_user(), configs.postgresql_host(),
+                                configs.postgresql_port(), roles_backup_path)
+                    if self.encryption:
+                        command = \
+                            "openssl enc -aes-256-cbc -nosalt -d -pass " \
+                            "pass:'%s' < '%s' | psql" \
+                            " --username '%s' --dbname=postgres --host '%s' " \
+                            "--port '%s' --echo-all" % (
+                                encryption.KeyManagement.get_object().get_password_by_name(
+                                    self.key_name),
+                                roles_backup_path,
+                                configs.postgresql_user(),
+                                configs.postgresql_host(),
+                                configs.postgresql_port())
+
+                    with open(stderr_path, 'a') as stderr:
+                        with open(stdout_path, 'a') as stdout:
+                            p = subprocess.Popen(command, shell=True,
+                                                 stdout=stdout, stderr=stderr)
+                            self.pg_restore_proc = p
+                            exit_code = p.wait()
+
+                    if exit_code != 0:
+                        with open(stderr_path, 'r') as f:
+                            raise backups.RestoreFailedException(database, '\n'.join(f.readlines()))
+                    else:
+                        self.log.info(self.log_msg("Roles has been successfully restored"))
+                    self.pg_restore_proc = None
+
+
+                if self.single_transaction:
+                    self.log.info(self.log_msg("Attempt to restore the {} database to a single transaction".format(new_bd_name)))
+                    try:
+                        os.remove(sql_script_path)
+                    except OSError:
+                        pass
+                    con_properties2 = configs.connection_properties(database=new_bd_name)
+                    connDb = None
+                    try:
+                        connDb = psycopg2.connect(**con_properties2)
+                        connDb.autocommit = True
+                        with connDb.cursor() as curDb:
+                            with open(sql_script_path, 'w') as file:
+                                curDb.execute("select schemaname from pg_catalog.pg_tables where schemaname !~ '^pg_' AND schemaname <> 'information_schema' AND schemaname <>'public';")
+                                schemas = [r[0] for r in curDb.fetchall()]
+                                for schema in schemas:
+                                    self.log.debug(self.log_msg("Adding to the list for deletion: SCHEMA IF EXISTS {} CASCADE\n".format(schema)))
+                                    file.write(("DROP SCHEMA IF EXISTS {} CASCADE;\n".format(schema)))
+                                curDb.execute("SELECT tablename FROM pg_catalog.pg_tables where schemaname !~ '^pg_' AND schemaname <> 'information_schema';")
+                                tablenames = [r[0] for r in curDb.fetchall()]
+                                for table in tablenames:
+                                    self.log.debug(self.log_msg("Adding to the list for deletion: TABLE IF EXISTS {} CASCADE".format(table)))
+                                    file.write(("DROP TABLE IF EXISTS {} CASCADE;\n".format(table)))
+                    except psycopg2.OperationalError as err:
+                        self.log.error(self.log_msg(err))
+                        if ('database "{}" does not exist'.format(new_bd_name)) in str(err):
+                            self.log.info(self.log_msg("Try create database"))
+                            cur.execute('CREATE DATABASE \"%(database)s\"', {"database": AsIs(new_bd_name)})
+                            self.drop_lookup_func_for_db(new_bd_name)
+                    except psycopg2.Error as err:
+                        self.log.error(self.log_msg(err))
+                        with open(stderr_path, 'r') as f:
+                            raise backups.RestoreFailedException(database, '\n'.join(f.readlines()))
+                    except Exception as err:
+                        self.log.error(self.log_msg(err))
+                        with open(stderr_path, 'r') as f:
+                            raise backups.RestoreFailedException(database, '\n'.join(f.readlines()))
+                    finally:
+                        if connDb:
+                            connDb.close()
+
+                    pg_restore_options = "-f - "
+
+                    if self.dbaas_clone:
+                        pg_restore_options = pg_restore_options + "--no-owner  --no-acl"
+
+                    if int(self.parallel_jobs) > 1:
+                        commandWriteToFile = "{}/pg_restore {} {} -j {} >> {}".format(bin_path, dump_path, pg_restore_options, self.parallel_jobs, sql_script_path)
+                    else:
+                        commandWriteToFile = "{}/pg_restore {} {} >> {}".format(bin_path, dump_path, pg_restore_options, sql_script_path)
+                    commandRest = ' (echo "BEGIN;"; cat {}; echo "COMMIT;") | psql -v --single-transaction ON_ERROR_STOP=1 --echo-errors --dbname={} ' \
+                                  '--user {} --host {} --port {}' .format(sql_script_path,new_bd_name,
+                                                                          configs.postgresql_user(),configs.postgresql_host(),
+                                                                          configs.postgresql_port())
+                    self.log.debug(self.log_msg("command WriteToFile: {} ".format(commandWriteToFile)))
+                    self.log.debug(self.log_msg("command Restore: {} ".format( commandRest)))
+                    with open(stderr_path, 'a') as stderr:
+                        with open(stdout_path, 'a+') as stdout:
+                            p = subprocess.Popen(commandWriteToFile, shell=True,
+                                                 stdout=stdout, stderr=stderr)
+                            self.pg_restore_proc = p
+                            exit_code = p.wait()
+                            self.log.debug(self.log_msg("exit_code: {}".format(exit_code)))
+                            if exit_code != 0:
+                                raise backups.RestoreFailedException(database, '\n'.join(
+                                    stderr.readlines()))
+                                return()
+                            else:
+                                self.log.debug(self.log_msg("Restore file {} is recorded. Starting the restore".format(sql_script_path)))
+                                p2 = subprocess.Popen(commandRest, shell=True,
+                                                      stdout=stdout, stderr=stderr)
+                                self.pg_restore_proc = p2
+                                exit_code2 = p2.wait()
+                                if exit_code2 != 0:
+                                    raise backups.RestoreFailedException(database, '\n'.join(
+                                        stderr.readlines()))
+                                else:
+                                    # when resetting to a single transaction, ON_ERROR_STOP=1 is sometimes ignored and
+                                    # exit_code==0, so we read the last 9 characters from stdout if there is a ROLLBACK,
+                                    # then we generate an exception
+                                    stdout.seek(stdout.tell()-9)
+                                    line = stdout.read(9)
+                                    self.log.debug(self.log_msg("the last 9 characters from stdout:{}".format(line)))
+                                    if "ROLLBACK" in line:
+                                        self.log.error("ROLLBACK")
+                                        raise backups.RestoreFailedException(database, "ROLLBACK")
+                                    self.update_status('duration', int(time.time() - db_start), database=database, flush=True)  
+                                    self.log.info(self.log_msg("Successful restored"))
+                            self.pg_restore_proc = None
+                            try:
+                                os.remove(sql_script_path)
+                            except OSError:
+                                pass
+                    return()
+                # setting owner back
+                if self.restore_roles and not restore_without_psql:
+                    cur.execute('ALTER DATABASE \"%(database)s\" OWNER TO \"%(owner)s\"',
+                                {"database": AsIs(new_bd_name), "owner": AsIs(db_owner)})
+
+                if pids and self.force:
+                    self.kill_pids_and_revoke_rights(new_bd_name, cur, roles)
+                self.log.info(self.log_msg("Target database {} created".format(new_bd_name)))
+        finally:
+            if conn:
+                conn.close()
+        # restoring database entities
+        # add restore options "-f -" for all pg versions except greenplum
+        pg_restore_options = "-f - "
+        if dump_version[0] == 9 and dump_version[1] == 4:
+            pg_restore_options = ""
+        if configs.is_external_pg() or self.dbaas_clone:
+            pg_restore_options = pg_restore_options + "--no-owner  --no-acl"
+
+        if int(self.parallel_jobs) > 1:
+            pg_restore_options += "-j {} ".format(self.parallel_jobs)
+
+        command = "{}/pg_restore {} {}| psql -v ON_ERROR_STOP=1 --dbname={} " \
+                  "--user {} --host {} --port {}" \
+            .format(bin_path, dump_path,
+                    pg_restore_options,
+                    new_bd_name,
+                    configs.postgresql_user(),
+                    configs.postgresql_host(),
+                    configs.postgresql_port()
+                    )
+
+        self.log.info(self.log_msg("DB version: {} dump version: {}, database: {}, new DB name: {} ".
+                                   format(self.postgres_version[0], dump_version[0], database,new_bd_name)))
+        if restore_without_psql:
+            command = "{}/pg_restore -C {} " \
+                      "--user {} --host {} --port {} --dbname=postgres --no-password" \
+                .format(bin_path, dump_path,configs.postgresql_user(),
+                        configs.postgresql_host(), configs.postgresql_port())
+
+            # Add support for the -j flag
+            if int(self.parallel_jobs) > 1:
+                command += " -j {} ".format(self.parallel_jobs)
+
+        if self.encryption:
+            command = \
+                "openssl enc -aes-256-cbc -d -nosalt -pass pass:{} < {}" \
+                "| {}/pg_restore {}| psql -v ON_ERROR_STOP=1 --dbname={} " \
+                "--user {} --host {} " \
+                "--port {}".format(
+                    encryption.KeyManagement.get_object().get_password(),
+                    dump_path,
+                    bin_path,
+                    pg_restore_options,
+                    new_bd_name, configs.postgresql_user(),
+                    configs.postgresql_host(),
+                    configs.postgresql_port())
+
+            # Add support for the -j flag
+            if int(self.parallel_jobs) > 1:
+                command += " -j {} ".format(self.parallel_jobs)
+
+        self.log.debug(self.log_msg("Restore Command: {}".format(command)))
+        if database != new_bd_name:
+            self.log.info(self.log_msg("New database name: {} specified for database: {}".
+                                       format(new_bd_name, database)))
+
+        with open(stderr_path, 'a') as stderr:
+            with open(stdout_path, 'a') as stdout:
+                pg_restore_proc = subprocess.Popen(command,shell=True, stdout=stdout, stderr=stderr)
+                self.pg_restore_proc = pg_restore_proc
+                exit_code = pg_restore_proc.wait()
+                self.pg_restore_proc = None
+
+        if pids and self.force:
+            conn = None
+            try:
+                conn = psycopg2.connect(**connection_properties)
+                conn.autocommit = True
+                with conn.cursor() as cur:
+                    for role in roles:
+                        cur.execute("GRANT CONNECT ON DATABASE \"%(database)s\" to \"%(role)s\";",
+                                    {"database": AsIs(new_bd_name), "role": AsIs(role)})
+                        self.log.info(self.log_msg(
+                            "Rights for connection granted for db {} to role {}".format(new_bd_name, role)))
+            finally:
+                if conn:
+                    conn.close()
+
+        if self.restore_roles:
+            self.grant_connect_to_db_for_role(roles_backup_path, new_bd_name)
+
+        if self.restore_roles and configs.is_external_pg():
+            self.grant_connect_for_external_pg(roles_backup_path, new_bd_name)
+
+        if exit_code != 0:
+            with open(stderr_path, 'r') as f:
+                raise backups.RestoreFailedException(database, '\n'.join(f.readlines()))
+        else:
+            self.update_status('duration', int(time.time() - db_start), database=database, flush=True)
+
+        if database != new_bd_name:
+            self.log.info(self.log_msg("Database '%s' has been successfully restored with new name '%s'." %
+                                       (database, new_bd_name)))
+        else:
+            self.log.info(self.log_msg("Database '%s' has been successfully restored." % database))
+
+    def run(self):
+        try:
+            self.process_restore_request()
+            self.update_status('completionTime',
+                           datetime.datetime.utcnow().isoformat(),
+                           flush=True)
+            self.update_status('status', backups.BackupStatus.SUCCESSFUL, flush=True)
+            self.log.info(self.log_msg("Backup has been successfully restored."))
+            if self.s3:
+                shutil.rmtree(self.backup_dir)
+        except Exception as e:
+            self.log.exception(self.log_msg("Restore request processing has failed."))
+            self.update_status('details', str(e))
+            self.update_status('completionTime',
+                           datetime.datetime.utcnow().isoformat(),
+                           flush=True)
+            self.update_status('status', backups.BackupStatus.FAILED, flush=True)
+            if self.s3:
+                shutil.rmtree(self.backup_dir)
+        finally:
+            if self.is_cancelled():
+                self.on_cancel()
+
+    def process_restore_request(self):
+        self.log.info(self.log_msg("Start restore procedure."))
+        start_timestamp = int(time.time())
+        self.update_status('status', backups.BackupStatus.IN_PROGRESS)
+        self.update_status('timestamp', start_timestamp)
+        self.update_status('started', str(datetime.datetime.fromtimestamp(start_timestamp).isoformat()), flush=True)
+
+        backup_status_file = backups.build_backup_status_file_path(self.backup_id,
+                                                                   self.namespace, self.external_backup_root, self.blob_path)
+        if self.s3:
+            try:
+                status = self.s3.read_object(backup_status_file)
+            except Exception as e:
+                raise e
+            backup_details = json.loads(status)
+
+        else:
+            if not os.path.isfile(backup_status_file):
+                raise backups.BackupNotFoundException(self.backup_id, self.namespace)
+
+            backup_details = utils.get_json_by_path(backup_status_file)
+
+        backup_status = backup_details['status']
+
+        if backup_status != backups.BackupStatus.SUCCESSFUL:
+            raise backups.BackupBadStatusException(self.backup_id, backup_status)
+
+        if not self.databases:
+            self.log.info(self.log_msg("Databases not specified -> restore all databases from backup."))
+            self.databases = list(backup_details.get('databases', {}).keys())
+
+        self.log.info(self.log_msg("Databases to restore: %s" % (self.databases,)))
+
+        # Check physical dump files existence.
+        for database in self.databases:
+            if self.s3:
+                is_backup_exist = self.s3.is_file_exists(backups.build_database_backup_path(self.backup_id, database,
+                                                                                            self.namespace, self.external_backup_root, self.blob_path))
+            else:
+                if int(self.parallel_jobs) > 1:
+                    pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root, self.blob_path)
+                    path_for_parallel_flag_backup = os.path.join(pg_dump_backup_path, database)
+                    is_backup_exist = os.path.exists(path_for_parallel_flag_backup)
+                else:
+                    is_backup_exist = backups.database_backup_exists(self.backup_id, database,
+                                                                     self.namespace, self.external_backup_root)
+            if not is_backup_exist:
+                raise backups.BackupNotFoundException(self.backup_id, database, self.namespace)
+
+        for database in self.databases:
+            try:
+                self.restore_single_database(database)
+                self.update_status('status', backups.BackupStatus.SUCCESSFUL, database)
+                self.update_status('completed', str(datetime.datetime.fromtimestamp(int(time.time())).isoformat()),
+                                   flush=True)
+            except Exception as e:
+                self.log.exception(self.log_msg("Restore has failed: %s " % str(e)))
+                self.update_status('details', str(e), database)
+                self.update_status('status', backups.BackupStatus.FAILED, database, flush=True)
+                raise e
+            finally:
+                try:
+                    if int(self.parallel_jobs) > 1:
+                        pg_dump_backup_path = backups.build_backup_path(self.backup_id, self.namespace, self.external_backup_root)
+                        backup_path = os.path.join(pg_dump_backup_path, database)
+                    else:
+                        backup_path = backups.build_database_backup_path(self.backup_id, database,
+                                                                         self.namespace, self.external_backup_root)
+                        os.remove(backup_path + '.stderr')
+                except OSError as ex:
+                    self.log.exception(self.log_msg("Unable to remove stderr log due to: %s " % str(ex)))
+
+    def grant_connect_to_db_for_role(self, roles_backup_path, db_name):
+        connection_properties = configs.connection_properties()
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            conn.autocommit = True
+            with conn.cursor() as cur:
+                with open(roles_backup_path, "r") as role_file:
+                    for line in role_file:
+                        if not line.startswith("CREATE ROLE"):
+                            continue
+                        try:
+                            role_name = line.split('CREATE ROLE ')[1].split(';')[0]
+                            cur.execute(
+                                "GRANT CONNECT ON DATABASE \"%(database)s\" TO \"%(role)s\";",
+                                {"database": AsIs(db_name), "role": AsIs(role_name)}
+                            )
+                            self.log.info(self.log_msg(
+                                "Rights for connection granted for db {} to role {}".format(db_name, role_name)))
+                        except Exception as e:
+                            self.log.error(self.log_msg("error grant connect on database {} to role {}"
+                                                        .format(db_name, role_name)), e)
+        finally:
+            if conn:
+                conn.close()
+
+    def cancel(self):
+        self.kill_processes()
+        self._cancel_event.set()
+        self.log.info(self.log_msg("Worker stopped"))
+
+    def is_cancelled(self):
+        return self._cancel_event.is_set()
+
+    def on_cancel(self, database=None):
+        if database:
+            self.log.exception("Restore got canceled for database {0}".format(database))
+            self.update_status('details', "Restore got canceled for database {0}".format(database), database)
+            self.update_status('status', backups.BackupStatus.CANCELED, database)
+        self.update_status('details', "Backup got canceled for database")
+        self.update_status("status", backups.BackupStatus.CANCELED, flush=True)
+
+    def kill_processes(self):
+        if self.pg_restore_proc:
+            self.log.info("kill restore process with pid: {}".format(self.pg_restore_proc.pid))
+            self.pg_restore_proc.kill()
+
+    def grant_connect_for_external_pg(self, roles_backup_path, db_name):
+        with open(roles_backup_path, "r") as role_file:
+            for line in role_file:
+                if not line.startswith("CREATE ROLE"):
+                    continue
+                role_name = line.split('CREATE ROLE ')[1].split(';')[0]
+                self.log.debug(self.log_msg("Try restore TABLE, SEQUENCE, VIEW owners to '%s' without superuser"
+                                            " privileges." % role_name))
+                con_properties = configs.connection_properties(database=db_name)
+                conn = None
+                try:
+                    conn = psycopg2.connect(**con_properties)
+                    conn.autocommit = True
+                    with conn.cursor() as cur:
+                        cur.execute(
+                            "SELECT 'ALTER TABLE '||schemaname||'.'||tablename||' OWNER TO \"%(role)s\";' FROM pg_tables "
+                            "WHERE NOT schemaname IN ('pg_catalog', 'information_schema')", {"role": AsIs(role_name)})
+                        alter_roles = [r[0] for r in cur.fetchall()]
+                        for alter_role in alter_roles:
+                            self.log.debug(self.log_msg("Try execute command: %s" % alter_role))
+                            try:
+                                cur.execute(alter_role)
+                            except Exception as e:
+                                self.log.info(self.log_msg("ERROR ALTER TABLE. Command: {} "
+                                                           "ERROR: {}".format(alter_role, e)))
+
+                        cur.execute("SELECT 'ALTER SEQUENCE '||sequence_schema||'.'||sequence_name||' OWNER TO "
+                                    "\"%(role)s\";' FROM information_schema.sequences WHERE NOT sequence_schema "
+                                    "IN ('pg_catalog', 'information_schema')", {"role": AsIs(role_name)})
+                        alter_sequences = [r[0] for r in cur.fetchall()]
+                        for alter_sequence in alter_sequences:
+                            self.log.debug(self.log_msg("Try execute command: %s" % alter_sequence))
+                            try:
+                                cur.execute(alter_sequence)
+                            except Exception as e:
+                                self.log.info(self.log_msg("ERROR ALTER SEQUENCE. Command: {} "
+                                                           "ERROR: {}".format(alter_sequence, e)))
+
+                        cur.execute("SELECT 'ALTER VIEW '||table_schema||'.'||table_name ||' OWNER TO \"%(role)s\";' "
+                                    "FROM information_schema.views WHERE NOT table_schema "
+                                    "IN ('pg_catalog', 'information_schema')", {"role": AsIs(role_name)})
+                        alter_views = [r[0] for r in cur.fetchall()]
+                        for alter_view in alter_views:
+                            self.log.debug(self.log_msg("Try execute command: %s" % alter_view))
+                            try:
+                                cur.execute(alter_view)
+                            except Exception as e:
+                                self.log.info(self.log_msg("ERROR ALTER VIEW. Command: {} "
+                                                           "ERROR: {}".format(alter_view, e)))
+                        cur.execute("GRANT \"%(role)s\" TO \"%(admin)s\";", {"role": AsIs(role_name),
+                                                                             "admin": AsIs(configs.postgresql_user())})
+                        try:
+                            cur.execute(
+                                "ALTER ROLE \"%(role)s\" WITH LOGIN;", {"role": AsIs(role_name)})
+                        except Exception as e:
+                            self.log.info(self.log_msg("ERROR ALTER ROLE {} WITH LOGIN "
+                                                       "ERROR: {}".format(role_name, e)))
+                finally:
+                    if conn:
+                        conn.close()
+            else:
+                self.log.info(self.log_msg("Username not found in the file %s. "
+                                           "ALTER tables OWNER TO  was not executed" % roles_backup_path))
+
+    @staticmethod
+    def drop_lookup_func_for_db(db_name):
+        connection_properties = configs.connection_properties(database=db_name)
+        conn = None
+        try:
+            conn = psycopg2.connect(**connection_properties)
+            conn.autocommit = True
+            with conn.cursor() as cur:
+                cur.execute("drop function if exists lookup(name);")
+        finally:
+            if conn:
+                conn.close()
diff --git a/docker-backup-daemon/docker/granular/storage_s3.py b/docker-backup-daemon/docker/granular/storage_s3.py
new file mode 100644
index 0000000..4876949
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/storage_s3.py
@@ -0,0 +1,166 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import boto3
+
+import botocore
+import botocore.exceptions
+import urllib3
+import os
+import logging
+import configs
+from retrying import retry
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+bucket = os.getenv("CONTAINER") or os.getenv("AWS_S3_BUCKET") or os.getenv("S3_BUCKET")
+CONTAINER_SEG = "{}_segments".format(bucket) if bucket else None
+PG_CLUSTER_NAME = os.getenv("PG_CLUSTER_NAME")
+
+RETRY_COUNT = 10
+RETRY_WAIT = 1000
+
+urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
+class AwsS3Vault:
+    __log = logging.getLogger("AwsS3Granular")
+
+    def __init__(self, cluster_name=None, cache_enabled=False,
+                 aws_s3_bucket_listing=None, prefix=None):
+
+        self.bucket = bucket or os.getenv("CONTAINER") or os.getenv("AWS_S3_BUCKET") or os.getenv("S3_BUCKET")
+        self.console = None
+        self.cluster_name = cluster_name
+        self.cache_enabled = cache_enabled
+        self.cached_state = {}
+        self.aws_s3_bucket_listing = aws_s3_bucket_listing
+        if prefix is not None:
+            self.aws_prefix = prefix
+        else:
+            self.aws_prefix = os.getenv("AWS_S3_PREFIX", "")
+
+        if not self.bucket or not isinstance(self.bucket, str) or not self.bucket.strip():
+            raise ValueError("S3 bucket is not configured. Set one of CONTAINER, AWS_S3_BUCKET, or S3_BUCKET.")
+
+    def get_s3_client(self):
+        return boto3.client("s3",
+                            region_name=os.getenv("AWS_DEFAULT_REGION") if os.getenv("AWS_DEFAULT_REGION") else None,
+                            endpoint_url=os.getenv("AWS_S3_ENDPOINT_URL"),
+                            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
+                            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
+                            verify=(False if os.getenv("AWS_S3_UNTRUSTED_CERT", "false").lower() == "true" else None))
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def upload_file(self, file_path, blob_path=None, backup_id=None):
+        if blob_path:
+            file_name = file_path.rsplit('/',1)[1]
+            s3FilePath = self.get_prefixed_path(f'{blob_path}/{backup_id}/{file_name}')
+            return self.get_s3_client().upload_file(file_path, self.bucket, s3FilePath)
+        else:
+            return self.get_s3_client().upload_file(file_path, self.bucket, self.get_prefixed_path(file_path))
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def delete_file(self, filename):
+        return self.get_s3_client().delete_object(Bucket=self.bucket, Key=self.get_prefixed_path(filename))
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def delete_objects(self, filename):
+        objects_to_delete = self.get_s3_client().list_objects(Bucket=self.bucket, Prefix=self.get_prefixed_path(filename))
+        for obj in objects_to_delete.get('Contents', []):
+            self.get_s3_client().delete_object(Bucket=self.bucket, Key=obj['Key'])
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def read_object(self, file_path):
+        self.__log.info("Reading object %s" % self.get_prefixed_path(file_path))
+        obj = self.get_s3_client().get_object(Bucket=self.bucket, Key=self.get_prefixed_path(file_path))
+        # self.__log.info(obj['Body'].read().decode('utf8'))
+        return obj['Body'].read().decode('utf8')
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def get_file_size(self, file_path):
+        obj = self.get_s3_client().list_objects_v2(Bucket=self.bucket, Prefix=self.get_prefixed_path(file_path))
+        if 'Contents' in obj:
+            for field in obj["Contents"]:
+                return field["Size"]
+        else:
+            self.__log.info("Requested {} file not found".format(file_path))
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def download_file(self, filename, backup_id, blob_path=None):
+        try:
+          if blob_path:
+              only_file_name = filename.rsplit('/',1)[1]
+              file_path = f'{blob_path}/{backup_id}/{only_file_name}'
+              logging.info(f'Downloading file {file_path} to {filename}')
+
+              self.get_s3_client().download_file(self.bucket, self.get_prefixed_path(file_path), filename)
+          else:
+              logging.info("Downloading file {}" .format(self.get_prefixed_path(filename)))
+              self.get_s3_client().download_file(self.bucket, self.get_prefixed_path(filename), filename)
+        except Exception as e:
+            raise e
+        return
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def is_file_exists(self, file):
+        exists = True
+        try:
+            self.get_s3_client().head_object(Bucket=self.bucket, Key=self.get_prefixed_path(file))
+        except botocore.exceptions.ClientError as e:
+            exists = False
+        return exists
+
+    def is_s3_storage_path_exist(self, storage):
+        bucket = self.get_s3_client().list_objects_v2(Bucket=self.bucket, Prefix=self.get_prefixed_path(storage))
+        if 'Contents' in bucket:
+            s3_storage_path = bucket['Contents'][0]["Key"]
+            return True if storage in s3_storage_path else False
+        return False
+
+    def get_granular_namespaces(self, storage):
+        bucket = self.get_s3_client().list_objects_v2(Bucket=self.bucket, Prefix=self.get_prefixed_path(storage))
+        namespaces = []
+        if 'Contents' in bucket:
+            for obj in bucket["Contents"]:
+                vault = obj["Key"].split(storage, 1)[1]
+                namespace = vault.split("/", 2)[1]
+                if namespace not in namespaces:
+                    namespaces.append(namespace)
+                else:
+                    pass
+            return namespaces
+
+    def get_backup_ids(self, storage, namespace):
+        namespaced_path = self.get_prefixed_path(storage + "/" + namespace)
+        bucket = self.get_s3_client().list_objects_v2(Bucket=self.bucket, Prefix=namespaced_path)
+        backup_ids = []
+        if 'Contents' in bucket:
+            for obj in bucket["Contents"]:
+                vault = obj["Key"].split(storage, 2)[1]
+                backup_id = vault.split("/", 3)[2]
+                if backup_id not in backup_ids:
+                    backup_ids.append(backup_id)
+                else:
+                    pass
+            return backup_ids
+
+    def get_prefixed_path(self, path):
+        if self.aws_prefix:
+            return self.aws_prefix + path
+        else:
+            if path.startswith("/"):
+                return path[1:]
+            return path
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/granular/utils.py b/docker-backup-daemon/docker/granular/utils.py
new file mode 100644
index 0000000..d285fe2
--- /dev/null
+++ b/docker-backup-daemon/docker/granular/utils.py
@@ -0,0 +1,325 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+
+import tarfile
+import psycopg2
+from retrying import retry
+import fcntl
+import logging
+import configs
+import os
+import re
+import backups
+import encryption
+import subprocess
+import fnmatch
+import kube_utils
+
+import http.client
+
+from googleapiclient.discovery import build
+from googleapiclient import errors
+
+log = logging.getLogger("utils")
+
+"""
+    sometimes write operations can take more than 5 seconds (storage problems)
+    hence, file will be locked for more than 5 seconds, for this cases
+    `stop_max_delay` should be increased, so `FILE_OPERATION_DELAY`
+    env can be used
+"""
+
+
+@retry(stop_max_delay=int(os.getenv("FILE_OPERATION_DELAY", '5000')))
+def get_json_by_path(path):
+    with open(path) as fd:
+        try:
+            fcntl.lockf(fd, fcntl.LOCK_SH | fcntl.LOCK_NB)
+            return json.load(fd)
+        except IOError:  # another process accessing
+            log.info("trying to access locked file while reading")
+            raise
+        finally:
+            fcntl.lockf(fd, fcntl.LOCK_UN)
+
+
+@retry(stop_max_delay=int(os.getenv("FILE_OPERATION_DELAY", '5000')))
+def write_in_json(path, data):
+    with open(path, 'w+') as fd:
+        try:
+            fcntl.lockf(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
+            json.dump(data, fd)
+            return data
+        except IOError:  # another process accessing
+            log.info("trying to access locked file while writing")
+            raise
+        finally:
+            fcntl.lockf(fd, fcntl.LOCK_UN)
+
+
+def execute_query(conn_properties, query):
+    conn = None
+    try:
+        conn = psycopg2.connect(**conn_properties)
+        with conn.cursor() as cur:
+            cur.execute(query)
+            return cur.fetchone()[0]
+    finally:
+        if conn:
+            conn.close()
+
+def get_version_of_pgsql_server():
+    conn_properties = configs.connection_properties()
+    result = execute_query(conn_properties, 'SHOW SERVER_VERSION;')
+    return list(map(int, result.split(' ')[0].split('.')))
+
+
+# need to rewrite this one with usage of execute_query()
+def get_owner_of_db(database):
+    conn_properties = configs.connection_properties()
+    conn = None
+    try:
+        conn = psycopg2.connect(**conn_properties)
+        with conn.cursor() as cur:
+            cur.execute("""
+            SELECT pg_catalog.pg_get_userbyid(d.datdba) as "Owner"
+            FROM pg_catalog.pg_database d
+            WHERE d.datname = %s;
+            """, (database,))
+            return cur.fetchone()[0]
+    finally:
+        if conn:
+            conn.close()
+
+def get_database_list(databases):
+    databases = tuple(databases)
+    conn_properties = configs.connection_properties()
+    conn = None
+    try:
+        conn = psycopg2.connect(**conn_properties)
+        with conn.cursor() as cur:
+            cur.execute("SELECT datname from pg_database where datname in %s;", (databases,))
+            return [p[0] for p in cur.fetchall()]
+    finally:
+        if conn:
+            conn.close()
+
+def get_pg_version_from_dump(backup_path, key_name, bin_path):
+  
+    parallel_jobs = configs.get_parallel_jobs()
+    if key_name:
+        command = "openssl enc -aes-256-cbc -nosalt -d -pass " \
+                  "pass:'{}' < '{}' | {} -l ".format(
+            encryption.KeyManagement.get_object().get_password_by_name(
+                key_name), backup_path, bin_path)
+    else:
+        if int(parallel_jobs) > 1:
+            command = '{}/pg_restore -j {} -l {}'.format(bin_path, parallel_jobs, backup_path)
+        else:    
+            command = '{}/pg_restore -l {}'.format(bin_path, backup_path) 
+
+    subprocess.check_output("ls -la {}".format(backup_path), shell=True)
+
+    output = \
+        subprocess.Popen(command, shell=True,
+                         stdout=subprocess.PIPE).communicate()[0]
+
+    # pg_restore -l output format example
+    # ; Archive created at 2019-02-11 09:38:35 UTC
+    # ;     dbname: test1
+    # ;     Dumped from database version: 10.3
+    # ;     Dumped by pg_dump version: 10.6
+    # ; Selected TOC Entries:
+    # ;
+    for item in output.decode().split(";"):
+        if "Dumped from database version" in item:
+            version_as_string = item.split(": ")[1]
+            return list(map(int, version_as_string.split(' ')[0].split('.')))
+    return None
+
+
+
+class Rule:
+    magnifiers = {
+        "min": 60,
+        "h": 60 * 60,
+        "d": 60 * 60 * 24,
+        "m": 60 * 60 * 24 * 30,
+        "y": 60 * 60 * 24 * 30 * 12,
+    }
+
+    def __init__(self, rule):
+        (startStr, intervalStr) = rule.strip().split("/")
+        self.start = self.__parseTimeSpec(startStr)
+        self.interval = "delete" if (
+                intervalStr == "delete") else self.__parseTimeSpec(intervalStr)
+
+    def __parseTimeSpec(self, spec):
+        import re
+        if (spec == "0"):
+            return 0
+
+        r = re.match("^(\\d+)(%s)$" % "|".join(list(self.magnifiers.keys())), spec)
+        if (r is None):
+            raise Exception(
+                "Incorrect eviction start/interval specification: %s" % spec)
+
+        digit = int(r.groups()[0])
+        magnifier = self.magnifiers[r.groups()[1]]
+
+        return digit * magnifier
+
+    def __str__(self):
+        return "%d/%d" % (self.start, self.interval)
+
+
+def parse(rules):
+    rules = [Rule(r) for r in rules.split(",")]
+    return rules
+
+
+def is_auth_needed():
+    return os.getenv("AUTH", "false").lower() == "true"
+
+
+def get_backup_tar_file_path(backup_id, namespace):
+    path = backups.build_backup_path(backup_id, namespace)
+    if os.path.exists(path):
+        tar_path = os.path.join(path, backup_id)
+        items = [x for x in os.listdir(path) if x.endswith('.dump') or x.endswith('.sql')]
+        with tarfile.open(tar_path + ".tar.gz", "w:gz") as tar:
+            for item in items:
+                tar.add(os.path.join(path, item), arcname=item)
+        full_path_tar = tar_path + ".tar.gz"
+        return full_path_tar
+    else:
+        return None
+
+
+class GkeBackupApiCaller:
+    def __init__(self):
+        self.log = logging.getLogger('BackupRequestEndpoint')
+        self.service = build('sqladmin', 'v1beta4', cache_discovery=False)
+        self.project = os.getenv("GKE_PROJECT")
+        self.instance = os.getenv("GKE_INSTANCE")
+
+    def perform_backup(self):
+        req = self.service.backupRuns().insert(project=self.project, instance=self.instance)
+        resp = req.execute()
+        self.log.info(json.dumps(resp, indent=2))
+        if "error" in resp:
+            return resp
+        else:
+            return self.get_backup_id(resp["insertTime"])
+
+    def get_backup_id(self, insert_time):
+        req = self.service.backupRuns().list(project=self.project, instance=self.instance)
+        resp = req.execute()
+        self.log.info(json.dumps(resp, indent=2))
+        for item in resp["items"]:
+            if item["startTime"] == insert_time:
+                self.log.info("Backup requested id: {}".format(item["id"]))
+                return item["id"]
+        else:
+            return "Can't perform backup"
+
+    def delete_backup(self, backup_id):
+        req = self.service.backupRuns().delete(project=self.project, instance=self.instance, id=backup_id)
+        try:
+            resp = req.execute()
+            self.log.info(json.dumps(resp, indent=2))
+            return {
+                "backupId": backup_id,
+                "message": resp["status"]
+                   }, http.client.OK
+
+        except errors.HttpError as e:
+            if e.resp["status"] == "404":
+                return "Backup with id {} not found".format(backup_id), http.client.NOT_FOUND
+            else:
+                return http.client.BAD_REQUEST
+
+    def backup_status(self, backup_id):
+        req = self.service.backupRuns().get(project=self.project, instance=self.instance, id=backup_id)
+        try:
+            resp = req.execute()
+            self.log.info(json.dumps(resp, indent=2))
+            return resp, http.client.OK
+        except errors.HttpError as e:
+            if e.resp["status"] == "404":
+                return "Backup with id {} not found".format(backup_id), http.client.NOT_FOUND
+            else:
+                return http.client.BAD_REQUEST
+
+    def restore(self, restore_request):
+        body = {
+            "restoreBackupContext": {
+                "backupRunId": restore_request["backupId"]
+            }
+        }
+        req = self.service.instances().restoreBackup(project=self.project, instance=self.instance, body=body)
+        try:
+            resp = req.execute()
+            self.log.info(json.dumps(resp, indent=2))
+            return {
+                   'trackingId': resp["name"]
+               }, http.client.ACCEPTED
+        except errors.HttpError as e:
+            if e.resp["status"] == "404":
+                return "Backup with id {} not found".format(restore_request["backupId"]), http.client.NOT_FOUND
+            else:
+                return "Bad request", http.client.BAD_REQUEST
+
+    def restore_status(self, restore_id):
+        req = self.service.operations().get(project=self.project, operation=restore_id)
+        try:
+            resp = req.execute()
+            self.log.info(json.dumps(resp, indent=2))
+            return resp, http.client.ACCEPTED
+        except errors.HttpError as e:
+            if e.resp["status"] == "404":
+                return "Restore with id {} not found".format(restore_id), http.client.NOT_FOUND
+            else:
+                return http.client.BAD_REQUEST
+
+    def backup_list(self):
+        req = self.service.backupRuns().list(project=self.project, instance=self.instance)
+        try:
+            resp = req.execute()
+            self.log.info(json.dumps(resp, indent=2))
+            result = {}
+            result["default"] = {}
+            for item in resp["items"]:
+                result["default"][item["id"]] = {"status": item["status"]}
+            return result, http.client.OK
+        except:
+            return http.client.BAD_REQUEST
+
+
+def get_postgres_version_by_path(storage_path):
+    #storage_path = '/usr'
+    pg_dirs = fnmatch.filter(os.listdir(storage_path), 'pg*')
+    log.info(f"Possible directories for backup store using path method {pg_dirs}")
+    versions = sorted([int(re.search(r'\d+', x).group()) for x in pg_dirs], reverse=True)
+    version_postfix = "pg" + str(versions[0])
+    log.info(f"PostgreSQL server version from path method is equal to {version_postfix}, "
+             f"so will save all backups in {version_postfix} dir")
+    version_as_list = [int(version_postfix.replace("pg", "")), 0]
+    return version_as_list
+
+def is_mirror_env():
+    cm = kube_utils.get_configmap('mirror-config')
+    return cm is not None
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/health.sh b/docker-backup-daemon/docker/health.sh
new file mode 100755
index 0000000..07872fc
--- /dev/null
+++ b/docker-backup-daemon/docker/health.sh
@@ -0,0 +1,45 @@
+#!/bin/bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+LOG_FILE=/tmp/health.log
+
+. /opt/backup/utils.sh
+
+function test_postgresql() {
+  local output=$(psql -h $POSTGRES_HOST -p $POSTGRES_PORT -U replicator -l 2>&1)
+  process_exit_code $? "${ouput}"
+}
+
+if [ "$STORAGE_TYPE" = "swift" ]; then
+  test_postgresql >> ${LOG_FILE}
+  output=$(/opt/backup/scli ls $CONTAINER 2>&1)
+  process_exit_code $? "${ouput}" >> $LOG_FILE
+elif [[ "${STORAGE_TYPE}" == "s3" ]]; then
+  test_postgresql >> ${LOG_FILE}
+
+  unset flag_ssl_no_verify
+  if [[ "${AWS_S3_UNTRUSTED_CERT}" == "True" || "${AWS_S3_UNTRUSTED_CERT}" == "true" ]]; then
+    flag_ssl_no_verify="--no-verify-ssl"
+  fi
+
+  output=$(aws ${flag_ssl_no_verify} --endpoint-url "${AWS_S3_ENDPOINT_URL}" s3 ls "s3://${CONTAINER}")
+  process_exit_code $? "${ouput}" >> $LOG_FILE
+elif [[ "$STORAGE_TYPE" = "hostpath" ]] || [[ "$STORAGE_TYPE" = "pv" ]] || [[ "$STORAGE_TYPE" = "pv_label" ]] || [[ "$STORAGE_TYPE" = "provisioned" ]] || [[ "$STORAGE_TYPE" = "provisioned-default" ]]; then
+  test_postgresql >> ${LOG_FILE}
+else
+  log "wrong storage type $STORAGE_TYPE" >> $LOG_FILE
+  exit 1
+fi
diff --git a/docker-backup-daemon/docker/pip.conf b/docker-backup-daemon/docker/pip.conf
new file mode 100755
index 0000000..d547ee0
--- /dev/null
+++ b/docker-backup-daemon/docker/pip.conf
@@ -0,0 +1,4 @@
+[global]
+index-url = https://pypi.org/simple
+break-system-packages = true
+trusted-host = pypi.org
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/aws-s3-backup.sh b/docker-backup-daemon/docker/postgres/aws-s3-backup.sh
new file mode 100755
index 0000000..39d47e4
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/aws-s3-backup.sh
@@ -0,0 +1,201 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source utils.sh
+
+################################
+#
+# $1 - bucket name
+# $2 - backup id
+#
+################################
+
+readonly BUCKET="$1" # Go to AWS S3 terminology
+readonly BACKUP_ID="$2"
+BACKUP_NAME="pg_${PG_CLUSTER_NAME}_backup_${BACKUP_ID}.tar.gz"
+
+
+function log() {
+  log_module "$1" "aws-s3-backup" "$2"
+}
+
+function log_info() {
+  log "INFO" "$1"
+}
+
+function log_error() {
+  log "ERROR" "$1"
+  exit 1
+}
+
+function aws_process_exit_code() {
+  local exit_code=$1
+  local message="$2"
+  if [ ${exit_code} -ne 0 ];then
+    log_error "${message}"
+    exit 1
+  fi
+}
+
+function smoke_aws_s3() {
+  log_info "validate provided configuration and test AWS S3 storage availability"
+
+  if [[ -z "${AWS_S3_ENDPOINT_URL}" ]]; then
+    log_error "endpoint URL for AWS S3 must be specified in AWS_S3_ENDPOINT environment variable."
+  fi
+
+  # http://docs.aws.amazon.com/cli/latest/topic/config-vars.html
+  if [[ -z "${AWS_ACCESS_KEY_ID}" ]]; then
+    log_error "access key for AWS S3 must be specified in AWS_ACCESS_KEY_ID environment variable."
+  fi
+
+  if [[ -z "${AWS_SECRET_ACCESS_KEY}" ]]; then
+    log_error "secret access key for AWS S3 must be specified in AWS_SECRET_ACCESS_KEY environment variable."
+  fi
+
+  if [[ -z "${BUCKET}" ]]; then
+    log_error "bucket name must be specified in CONTAINER environment variable."
+  fi
+
+  local output=$(aws --endpoint-url "${AWS_S3_ENDPOINT_URL}" s3 ls "s3://${BUCKET}" 2>&1)
+  process_exit_code $? "${output}"
+}
+
+function stream_backup_to_aws_s3() {
+  if [[ -z "${BACKUP_ID}" ]]; then
+    log_error "Backup id must be specified explicitly"
+  fi
+
+
+  unset flag_ssl_no_verify
+  
+  if [[ "${AWS_S3_UNTRUSTED_CERT}" == "True" || "${AWS_S3_UNTRUSTED_CERT}" == "true" ]]; then
+    flag_ssl_no_verify="--no-verify-ssl"
+  fi
+
+  local s3_object_name="${BACKUP_ID}/${BACKUP_NAME}"
+
+  # S3 required to specify "expected size" of backup to divide large stream
+  # into smaller pieces for multi-part upload correctly.
+  # But, unfortunately, we don't know exact backup size after gzip,
+  # so we tell S3 that our backup is of size of our database (but a cake is a lie).
+  local expected_backup_size=$(PGPASSWORD=$POSTGRES_PASSWORD psql --no-align \
+                                    --tuples-only \
+                                    -h "${POSTGRES_HOST}" \
+                                    -p "${POSTGRES_PORT}" \
+                                    -U "${POSTGRES_USER}" \
+                                    -d postgres \
+                                    -c "select sum(pg_database_size(db.name)) from (select datname as name from pg_database) db")
+
+  log_info "expected backup size is approximately ${expected_backup_size} bytes."
+
+  local validation_pipe="pg-backup-${BACKUP_ID}.pipe"
+  local pg_basebackup_stderr_file="pg-backup-${BACKUP_ID}.error.log"
+  local validation_stderr_file="pg-backup-validation-${BACKUP_ID}.error.log"
+  local storage_client_stdout_file="storage-client-${BACKUP_ID}.log"
+
+  register_delete_on_exit "${validation_pipe}" "${storage_client_stdout_file}" "${validation_stderr_file}" "${pg_basebackup_stderr_file}"
+
+  # Validate TAR stream on the fly.
+  # This will not validate backup data itself, but will check archive's integrity.
+  mkfifo "${validation_pipe}"
+  tar -tz <"${validation_pipe}" > /dev/null 2> "${validation_stderr_file}" &
+
+  log_info "start backup streaming to AWS S3"
+    $PG_BASEBACKUP -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${REPLICATION_USER}" -D - -X fetch --format=tar --gzip 2> "${pg_basebackup_stderr_file}" \
+    | tee "${validation_pipe}" \
+    | aws --endpoint-url "${AWS_S3_ENDPOINT_URL}" ${flag_ssl_no_verify} \
+          s3 cp - "s3://${BUCKET}/${AWS_S3_PREFIX}/${s3_object_name}" \
+          --expected-size "${expected_backup_size}" 2> "${storage_client_stdout_file}"
+
+  # PIPESTATUS can be overridden, so need to keep it.
+  local exit_codes=(${PIPESTATUS[@]})
+  local pg_basebackup_exit_code=${exit_codes[0]}
+  local storage_client_exit_code=${exit_codes[2]}
+
+  # Wait for TAR validation to complete.
+  wait $!
+  local validation_exit_code=$?
+
+  local storage_client_stdout="$(cat ${storage_client_stdout_file})"
+  local validation_stderr="$(cat ${validation_stderr_file})"
+  local pg_basebackup_log="$(cat ${pg_basebackup_stderr_file})"
+
+  aws_process_exit_code "${pg_basebackup_exit_code}" "pg_basebackup has failed. Details: ${pg_basebackup_log}"
+  aws_process_exit_code "${validation_exit_code}" "Backup archive integrity validation not passed. This backup will be marked as failed. Details: ${validation_stderr}"
+  aws_process_exit_code "${storage_client_exit_code}" "Backup uploading to AWS S3 has failed. Details: ${storage_client_stdout}"
+
+  log_info "completed"
+}
+
+function main() {
+
+  version="$(PGPASSWORD=$POSTGRES_PASSWORD psql -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${POSTGRES_USER}" -d postgres -c "SHOW SERVER_VERSION;" -tA | egrep -o '[0-9]{1,}\.[0-9]{1,}')"
+  REPLICATION_USER="replicator"
+
+  log "version of pgsql server is: ${version}"
+
+  if python -c "import sys; sys.exit(0 if float("${version}") >= 17.0 else 1)"; then
+    log "Using pgsql 17 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/17/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 16.0 <= float("${version}") < 17.0 else 1)"; then
+    log "Using pgsql 16 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/16/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 15.0 <= float("${version}") < 16.0 else 1)"; then
+    log "Using pgsql 15 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/15/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 14.0 <= float("${version}") < 15.0 else 1)"; then
+    log "Using pgsql 14 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/14/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 13.0 <= float("${version}") < 14.0 else 1)"; then
+    log "Using pgsql 13 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/13/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 12.0 <= float("${version}") < 13.0 else 1)"; then
+    log "Using pgsql 12 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/12/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 11.0 <= float("${version}") < 12.0 else 1)"; then
+    log "Using pgsql 11 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/11/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 10.0 <= float("${version}") < 11.0 else 1)"; then
+    log "Using pgsql 10 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/10/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_ID}).tar.gz"
+  else
+    if [ "${PG_CLUSTER_NAME}" != "gpdb" ]
+    then
+        log "Using pgsql 9.6 bins for  pg_basebackup"
+        PG_BASEBACKUP="/usr/pgsql-9.6/bin/pg_basebackup"
+    else
+        log "Using gpdb bins for greenplum pg_basebackup"
+        TARGET_DB_ID="$(psql -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${POSTGRES_USER}" -d postgres -c "select dbid from gp_segment_configuration where content = -1 and status = 'up' and role = 'p';" -tA )"
+        PG_BASEBACKUP="/usr/local/greenplum-db/bin/pg_basebackup --target-gp-dbid="${TARGET_DB_ID}""
+        REPLICATION_USER=${POSTGRES_USER}
+
+    fi
+  fi
+
+  smoke_aws_s3
+  stream_backup_to_aws_s3
+}
+
+main "$@"
diff --git a/docker-backup-daemon/docker/postgres/backup-daemon.conf b/docker-backup-daemon/docker/postgres/backup-daemon.conf
new file mode 100644
index 0000000..60fe000
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/backup-daemon.conf
@@ -0,0 +1,14 @@
+{
+    schedule: "0 * * * *",
+    schedule: ${?BACKUP_SCHEDULE}
+  
+    eviction: "7d/delete"
+    eviction: ${?EVICTION_POLICY_BINARY}
+    eviction: ${?EVICTION_POLICY}
+	
+    storage: ${STORAGE_ROOT}
+
+    command: "/opt/backup/postgres_backup.sh %(data_folder)s"
+
+    timeout: ${?BACKUP_TIMEOUT}
+}
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/configs.py b/docker-backup-daemon/docker/postgres/configs.py
new file mode 100644
index 0000000..12d7afd
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/configs.py
@@ -0,0 +1,41 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+import logging.config
+import os
+from pyhocon import ConfigFactory
+
+GLOBAL_CONFIG_FILE_PATH = '/etc/backup-daemon.conf'
+LOGGING_CONFIG_FILE_PATH = '/opt/backup/logging.conf'
+
+
+def load_logging_configs():
+    logging.config.fileConfig(LOGGING_CONFIG_FILE_PATH)
+
+
+def load_configs():
+    default_config = os.path.join(os.path.dirname(__file__), 'backup-daemon.conf')
+    if os.path.exists(GLOBAL_CONFIG_FILE_PATH):
+        conf = ConfigFactory.parse_file(GLOBAL_CONFIG_FILE_PATH).with_fallback(default_config)
+    else:
+        conf = ConfigFactory.parse_file(default_config)
+
+    log = logging.getLogger("BackupDaemonConfiguration")
+    log.info("Loaded PostgreSQL backup configuration: %s" % json.dumps(conf))
+
+    return conf
+
+def is_external_pg():
+    return os.getenv("EXTERNAL_POSTGRESQL", "") != ""
diff --git a/docker-backup-daemon/docker/postgres/encryption.py b/docker-backup-daemon/docker/postgres/encryption.py
new file mode 100644
index 0000000..1ede33d
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/encryption.py
@@ -0,0 +1,253 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from Crypto.Cipher import AES
+import os
+import io
+import json
+import logging
+from kubernetes.client.rest import ApiException
+
+
+class FileWrapper(object):
+    def __init__(self, file_path, encrypted):
+        self.__log = logging.getLogger("EncryptionHelper")
+        self.__file_path = file_path
+        self.__encrypted = encrypted
+        self.chunk_size = 4096
+        if os.path.exists(file_path):
+            self.__file_size = os.path.getsize(file_path)
+
+    def get_file_stream(self):
+        if self.__encrypted:
+            return self.get_decrypted()
+        else:
+            return io.FileIO(self.__file_path, "r", closefd=True)
+
+    def put_file_stream(self, stream):
+        """
+        This method saves file stream to a actual file on FS
+        in case of encryption, saves encrypted file and creates .key file with
+        metadata about encryption key. Used only for WAL archives
+        :param stream: file stream that should be saved
+        :return: sha256 - sha256 of processed file
+        """
+        import hashlib
+        sha256 = hashlib.sha256()
+        if self.__encrypted:
+            password = KeyManagement.get_object().get_password()
+            cipher = EncryptionHelper.get_cipher_by_pw(password)
+        with io.FileIO(self.__file_path, "w", closefd=True) as target:
+            data = stream.read(self.chunk_size)
+            while True:
+                next_data = stream.read(self.chunk_size)
+                sha256.update(data)
+                if self.__encrypted:
+                    # pad = False
+                    # if len(data) % AES.block_size != 0:
+                    #     pad = True
+                    pad = len(next_data) == 0   # try to pad only last chunk
+                    data = cipher.encrypt(data, pad)
+                    self.create_key_file()
+                target.write(data)
+                if len(next_data) == 0:
+                    stream.close()
+                    self.__log.info("Processed stream with sha256 {}".format(
+                        sha256.hexdigest()))
+                    return sha256.hexdigest()
+                else:
+                    data = next_data
+
+    def get_decrypted(self):
+        """
+        in case of encryption everything is pretty simple,
+        need to decrypt file by chunks and then return it as a stream
+        :return: decrypted file stream
+        """
+        self.__log.info("Will try to decrypt file: %s" % self.__file_path)
+        cipher = EncryptionHelper.get_cipher_by_pw(
+            self.get_password_for_file())
+        import tempfile
+        decrypted = tempfile.TemporaryFile(mode='w+')
+        with io.FileIO(self.__file_path, "r", closefd=True) as encrypted:
+            number_of_chunks = self.__file_size / self.chunk_size
+            iteration = 0
+            while True:
+                data = encrypted.read(self.chunk_size)
+                if len(data) == 0:
+                    encrypted.close()
+                    # return stream to the start
+                    decrypted.seek(0)
+                    return decrypted
+                data = cipher.decrypt(data)
+                if iteration == number_of_chunks:
+                    data = cipher.unpad(data)
+                iteration = iteration + 1
+                decrypted.write(data)
+
+    def get_password_for_file(self):
+        """
+        this method gets password by file name.
+        in case of WAL file metadata about the key is saved to .key file,
+        in case of full backup in .metrics.
+        :return: encryption key as a string
+        """
+        if self.__file_path.endswith("tar.gz"):  # full backup
+            key_filename = os.path.dirname(self.__file_path) + "/.metrics"
+        else:  # WAL archive
+            key_filename = self.__file_path + ".key"
+
+        if os.path.exists(key_filename):
+            with open(key_filename) as f:
+                data = json.load(f)
+            key_name, key_source = data["key_name"], data["key_source"]
+            self.__log.info("Will use key_name: {} and key_source: {} "
+                            "for file decryption: {}".
+                            format(key_name, key_source, self.__file_path))
+            return KeyManagement(key_source,
+                                 key_name).get_password()
+        else:
+            # if file not exists for some of the reason, return default PW
+            return KeyManagement.get_object().get_password()
+
+    def create_key_file(self):
+        key_info = {
+            "key_name": KeyManagement.get_key_name(),
+            "key_source": KeyManagement.get_key_source()
+        }
+        key_filename = self.__file_path + ".key"
+        with open(key_filename, 'w+') as outfile:
+            json.dump(key_info, outfile)
+
+
+class EncryptionHelper(object):
+    def __init__(self, key_management):
+        self.log = logging.getLogger("EncryptionHelper")
+        self.__key_management = key_management
+
+    @staticmethod
+    def evp_bytes_to_key(password, key_len=32, iv_len=16):
+        """
+        Derive the key and the IV from the given password and salt.
+        """
+        from hashlib import md5
+        d_tot = md5(password).digest()
+        d = [d_tot]
+        while len(d_tot) < (iv_len + key_len):
+            d.append(md5(d[-1] + password).digest())
+            d_tot += d[-1]
+        return d_tot[:key_len], d_tot[key_len:key_len + iv_len]
+
+    @staticmethod
+    def get_cipher_by_pw(password):
+        key, iv = EncryptionHelper.evp_bytes_to_key(password)
+        return CipherWrapper(iv, key)
+
+
+class KeyManagement(object):
+    def __init__(self, pw_source, pw_name):
+        self.log = logging.getLogger("KeyManagement")
+        self.pw_source = pw_source
+        self.pw_name = pw_name
+        # self.pw_version = pw_version
+
+    @staticmethod
+    def get_object():
+        pw_source = os.getenv("KEY_SOURCE", 'kubernetes')
+        pw_name = os.getenv("KEY_NAME", "daemon-secret")
+        return KeyManagement(pw_source, pw_name)
+
+    def get_password(self):
+        if self.pw_source.lower() == KeySources.KUBERNETES:
+            pw_name = os.getenv("KEY_NAME", "daemon-secret")
+            return KubernetesPassword(pw_name).get_password()
+        elif self.pw_source.lower() == KeySources.VAULT:
+            pw_name = os.getenv("KEY_NAME", "daemon-secret")
+            return VaultPassword(pw_name).get_password()
+
+    def get_password_by_name(self, key_name):
+        self.log.info("Try to get key: {}".format(key_name))
+        if self.pw_source.lower() == KeySources.KUBERNETES:
+            return KubernetesPassword(key_name).get_password()
+        elif self.pw_source.lower() == KeySources.VAULT:
+            return VaultPassword(key_name).get_password()
+
+    @staticmethod
+    def get_key_name():
+        return os.getenv("KEY_NAME", "daemon-secret").lower()
+
+    @staticmethod
+    def get_key_source():
+        return os.getenv("KEY_SOURCE", "kubernetes").lower()
+
+
+# here should be some abstract class
+class KubernetesPassword(object):
+    SA_PATH = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+
+    def __init__(self, key_name):
+        self.log = logging.getLogger("KubernetesPassword")
+        self.key_name = key_name
+
+    def get_password(self):
+        from kubernetes import config
+        from kubernetes.client.apis import core_v1_api
+        config.load_incluster_config()
+        api = core_v1_api.CoreV1Api()
+        # https://github.com/kubernetes-client/python/issues/363
+        namespace = open(self.SA_PATH).read()
+        try:
+            api_response = api.read_namespaced_secret(self.key_name, namespace)
+            import base64
+            return base64.b64decode(api_response.data.get("password"))
+        except ApiException as exc:
+            self.log.error(exc)
+            raise exc
+
+
+class VaultPassword(object):
+    def __init__(self, key_name):
+        pass
+
+    def get_password(self):
+        pass
+
+
+class CipherWrapper(object):
+    def __init__(self, iv, key):
+        from Crypto.Cipher import AES
+        self.__cipher = AES.new(key, AES.MODE_CBC, iv)
+
+    def encrypt(self, data, pad=False):
+        return self.__cipher.encrypt(self.pad(data) if pad else data)
+
+    def decrypt(self, data, unpad=False):
+        return self.unpad(self.__cipher.decrypt(data)) \
+            if unpad else self.__cipher.decrypt(data)
+
+    def pad(self, data):
+        from Crypto.Util.Padding import pad
+        return pad(data, AES.block_size)
+
+    def unpad(self, data):
+        from Crypto.Util.Padding import unpad
+        return unpad(data, AES.block_size)
+
+
+class KeySources:
+    KUBERNETES = "kubernetes"
+    VAULT = "vault"
+
+    def __init__(self):
+        pass
diff --git a/docker-backup-daemon/docker/postgres/endpoints/__init__.py b/docker-backup-daemon/docker/postgres/endpoints/__init__.py
new file mode 100644
index 0000000..0ae8f9c
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/endpoints/__init__.py
@@ -0,0 +1,15 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+__all__ = ['backup', 'status', 'wal', 'restore']
diff --git a/docker-backup-daemon/docker/postgres/endpoints/backup.py b/docker-backup-daemon/docker/postgres/endpoints/backup.py
new file mode 100644
index 0000000..8d397e9
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/endpoints/backup.py
@@ -0,0 +1,154 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+Set of endpoints to performs actions with physical backups.
+"""
+
+import logging
+import utils
+from flask_restful import Resource
+from flask import Response, request, stream_with_context
+
+import requests
+from requests.exceptions import HTTPError
+from flask_httpauth import HTTPBasicAuth
+
+auth = HTTPBasicAuth()
+
+
+@auth.verify_password
+def verify(username, password):
+    return utils.validate_user(username, password)
+
+
+class BackupRequest(Resource):
+    __endpoints = [
+        '/backup',
+        '/backups/request'
+    ]
+
+    def __init__(self):
+        self.__log = logging.getLogger("BackupEndpoint")
+
+    @staticmethod
+    def get_endpoints():
+        return BackupRequest.__endpoints
+
+    @auth.login_required
+    def post(self):
+        self.__log.debug("Endpoint /backup has been called.")
+        # Redirect backup request to underlying scheduler, which keeps status of scheduled backups.
+        r = requests.post('http://localhost:8085/schedule')
+
+        if not r.ok:
+            try:
+                r.raise_for_status()
+            except HTTPError as e:
+                self.__log.exception("Something went wrong when redirecting backup request to /schedule endpoint.", e)
+
+        self.__log.debug(r.json())
+
+        return r.json()
+
+
+class Eviction(Resource):
+
+    __endpoints = [
+        '/evict',
+        '/backups/delete'
+    ]
+
+    def __init__(self, storage):
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return Eviction.__endpoints
+
+    @auth.login_required
+    def delete(self):
+        backup_id = request.args.getlist('id')[0]
+        vaults = self.__storage.list()
+        vaults.reverse()
+
+        # search for vault
+        vault_for_eviction = None
+        for vault in vaults:
+            if vault.get_id() == backup_id:
+                vault_for_eviction = vault
+                break
+        if vault_for_eviction:
+            self.__storage.evict(vault_for_eviction)
+            return "Ok"
+        else:
+            return "Not Found"
+
+
+class Download(Resource):
+
+    __endpoints = [
+        '/get',
+        '/backups/download'
+    ]
+
+    def __init__(self, storage):
+        self.__storage = storage
+        self.__log = logging.getLogger("DownloadBackupEndpoint")
+
+    @staticmethod
+    def get_endpoints():
+        return Download.__endpoints
+
+    @auth.login_required
+    def get(self):
+        def generate(storage, vault):
+            stream = storage.get_backup_as_stream(vault)
+            with stream as f:
+                chunk_size = 4096
+                while True:
+                    data = f.read(chunk_size)
+                    if len(data) == 0:
+                        f.close()
+                        return
+                    yield data
+
+        vaults = self.__storage.list()
+        vaults.reverse()
+        backup_id = request.args.getlist('id')[0] if request.args.getlist('id') else None
+
+        # search for vault
+        vault_for_streaming = None
+        if backup_id:
+            for vault in vaults:
+                if vault.get_id() == backup_id:
+                    vault_for_streaming = vault
+                    break
+        else:
+            for vault in vaults:
+                if not vault.is_failed():
+                    vault_for_streaming = vault
+                    break
+        if vault_for_streaming:
+            return Response(stream_with_context(
+                generate(self.__storage, vault_for_streaming)),
+                            mimetype='application/octet-stream',
+                            headers=[
+                                ('Content-Type', 'application/octet-stream'),
+                                ('Content-Disposition',
+                                 "pg_backup_{}.tar.gz".format(
+                                     vault_for_streaming.get_id()))
+                            ])
+        else:
+            return Response("Cannot find backup", status=500)
diff --git a/docker-backup-daemon/docker/postgres/endpoints/restore.py b/docker-backup-daemon/docker/postgres/endpoints/restore.py
new file mode 100644
index 0000000..c5d031c
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/endpoints/restore.py
@@ -0,0 +1,445 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+Set of endpoints to performs actions with external restore.
+"""
+
+import datetime
+import time
+import errno
+import hashlib
+import json
+import logging
+import os
+import subprocess
+from threading import Thread
+import boto3
+from dateutil import parser
+
+from kubernetes import client as k8s_client, config as k8s_config
+
+import utils
+import fcntl
+from flask import Response, request
+from flask_httpauth import HTTPBasicAuth
+from flask_restful import Resource
+
+auth = HTTPBasicAuth()
+
+
+@auth.verify_password
+def verify(username, password):
+    return utils.validate_user(username, password)
+
+
+class ExternalRestoreRequest(Resource):
+    __endpoints = [
+        '/external/restore',
+    ]
+
+    def __init__(self, storage):
+        self.__log = logging.getLogger("ExternalRestoreEndpoint")
+        self.restore_folder = f"{storage.root}/external/restore"
+        self.allowable_db_types = ['AZURE', 'RDS']
+        if not os.path.exists(self.restore_folder):
+            try:
+                os.makedirs(self.restore_folder)
+            except OSError as exc:
+                if exc.errno != errno.EEXIST:
+                    raise
+        self.status_file = f"{self.restore_folder}/status"
+
+    @staticmethod
+    def cleanup_restore_status(storage):
+        log=logging.getLogger("ExternalRestoreEndpoint")
+        restore_folder = f"{storage.root}/external/restore"
+        status_file = f"{restore_folder}/status"
+        if os.path.exists(status_file):
+            with open(status_file, 'r') as f:
+                status_map = json.load(f)
+                f.close()
+                restoreId = status_map['restoreId']
+                restore_file = f"{restore_folder}/{restoreId}"
+                if os.path.exists(restore_file):
+                    stuck_status = None
+                    with open(restore_file, 'r') as o:
+                        stuck_status = json.load(o)
+                        o.close()
+                    stuck_status['status'] = "Failed"
+                    with open(restore_file, 'w') as o:   
+                        o.write(json.dumps(stuck_status))
+                        o.close()
+            os.remove(status_file)
+
+
+    @staticmethod
+    def get_endpoints():
+        return ExternalRestoreRequest.__endpoints
+
+    @auth.login_required
+    def post(self):
+        self.__log.debug("Endpoint /external/restore has been called")
+
+        external_pg_type = os.getenv("EXTERNAL_POSTGRESQL", "FALSE").upper()
+
+        if external_pg_type not in self.allowable_db_types:
+            return 404
+
+        req = request.get_json()
+
+        if req.get('restore_time'):
+            restore_time = req.get('restore_time')
+        else:
+            self.__log.info("No restore_time provided")
+            return "No restore_time provided", 400
+
+        if req.get('restore_as_separate'):
+            restore_as_separate = req.get('restore_as_separate')
+        else:
+            restore_as_separate = "false"
+
+        if req.get('geo_restore'):
+            geo_restore = req.get('geo_restore')
+        else:
+            geo_restore = "false"
+
+        subnet = req.get('subnet', 'false')
+        if os.path.isfile(self.status_file):
+            with open(self.status_file, 'r') as f:
+                status_map = json.load(f)
+                f.close()
+                if status_map['status'] == "In Progress":
+                    return status_map, 200
+                else:
+                    os.remove(self.status_file)
+
+        restore_id = ExternalRestore.generate_restore_id()
+
+        if external_pg_type == "AZURE":
+            restore = ExternalRestore(self.restore_folder, restore_id, restore_time, restore_as_separate, geo_restore, subnet)
+
+        if external_pg_type == "RDS":
+            restore = RDSRestore(self.restore_folder, restore_id, restore_time, restore_as_separate)
+
+        restore.start()
+        return Response(restore_id, 202)
+
+
+class ExternalRestore(Thread):
+    def __init__(self, restore_folder, restore_id, restore_time, restore_as_separate, geo_restore, subnet):
+        Thread.__init__(self)
+        self.__log = logging.getLogger('ExternalRestore')
+        self.restore_id = restore_id
+        self.restore_time = restore_time
+        self.restore_folder = restore_folder
+        self.restore_as_separate = restore_as_separate
+        self.geo_restore = geo_restore
+        self.subnet = subnet
+
+    def run(self):
+        cmd_processed = self.__process_cmd(self.restore_id, self.restore_time, self.restore_folder,
+                                           self.restore_as_separate, self.geo_restore, self.subnet)
+        exit_code = subprocess.call(cmd_processed)
+        if exit_code != 0:
+            self.__log.error("Restore process has been failed")
+        else:
+            self.__log.info("Restore process successfully finished")
+
+    def __process_cmd(self, restore_id, restore_time, restore_folder, restore_as_separate, geo_restore, subnet):
+        self.__log.info(f"restore_id {restore_id}, restore_time {restore_time}, "
+                        f"restore_folder {restore_folder}, restore_as_separate {restore_as_separate}, "
+                        f"subnet: {subnet}")
+        cmd_processed = self.__split_command_line(
+            f"/opt/backup/azure_restore "
+            f"--restore_id {restore_id} "
+            f"--restore_time {restore_time} "
+            f"--restore_folder {restore_folder} "
+            f"--restore_as_separate {restore_as_separate} "
+            f"--geo_restore {geo_restore} "
+            f"--subnet {subnet}")
+        return cmd_processed
+
+    @staticmethod
+    def __split_command_line(cmd_line):
+        import shlex
+        lex = shlex.shlex(cmd_line)
+        lex.quotes = '"'
+        lex.whitespace_split = True
+        lex.commenters = ''
+        return list(lex)
+
+    @staticmethod
+    def generate_id():
+        return datetime.datetime.now().strftime("%Y%m%dT%H%M%S%f")
+
+    @staticmethod
+    def generate_restore_id():
+        return 'restore-%s' % ExternalRestore.generate_id()
+
+
+class RDSRestore(Thread):
+
+    NAMESPACE_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
+
+    def __init__(self, restore_folder, restore_id, restore_time, restore_as_separate):
+        Thread.__init__(self)
+        try:
+            self.__log = logging.getLogger('RDSRestore')
+            self.restore_id = restore_id
+            self.restore_time = restore_time
+            self.restore_folder = restore_folder
+            self.client = self.get_rds_client()
+            self.pg_service_name = 'pg-patroni'
+            self.namespace = open(self.NAMESPACE_PATH).read()
+            k8s_config.load_incluster_config()
+            self.k8s_core_api = k8s_client.CoreV1Api()
+            self.status = {
+                'trackingId': self.restore_id,
+                'namespace': self.namespace,
+                'status': BackupStatus.PLANNED
+            }
+            self.restore_as_separate = restore_as_separate
+        except Exception as e:
+            print("RDS: Client Error: %s " % e)
+            raise Exception(e)
+
+    def run(self):
+        try:
+            self.__log.info("RDS: Restore Cluster Running")
+            self.update_status("status", BackupStatus.IN_PROGRESS, True)
+
+            external_name = self.get_service_external_name()
+            cluster_name = external_name.split(".")[0]
+
+            response, restored_cluster_name = self.restore_cluster(cluster_name)
+            current_instances, restored_instances = self.restore_db_instances(cluster_name, restored_cluster_name)
+            self.wait_for_db_instance(restored_instances)
+            if self.restore_as_separate != 'true':
+                self.update_service_external_name(restored_cluster_name)
+                self.stop_db_cluster(cluster_name)
+            self.update_status("status", BackupStatus.SUCCESSFUL, True)
+            self.__log.info("RDS: Restore Cluster Successful")
+        except Exception as e:
+            self.__log.error("RDS: Restore failed %s" % e)
+            self.update_status("status", BackupStatus.FAILED, True)
+
+    @staticmethod
+    def generate_id():
+        return datetime.datetime.now().strftime("%Y%m%dT%H%M%S%f")
+
+    @staticmethod
+    def generate_restore_id():
+        return 'restore-%s' % RDSRestore.generate_id()
+
+    def get_rds_client(self):
+        self.__log.info("RDS: Init RDS Client")
+        return boto3.client("rds")
+
+    def restore_cluster(self, cluster_name):
+
+        self.check_name(cluster_name)
+
+        restored_cluster_name = self.get_restored_name(cluster_name)
+        self.__log.info("RDS: Restore cLuster: %s" % cluster_name)
+        self.__log.info("RDS: Restore cLuster with new name: %s" % restored_cluster_name)
+
+        try:
+            security_groups = []
+            instance_response = self.client.describe_db_instances(
+                Filters=[{'Name': 'db-cluster-id', 'Values': [cluster_name, ]}, ])
+            if instance_response.get('DBInstances'):
+                security_groups = self.extract_vpc_security_group_ids(
+                    instance_response.get('DBInstances')[0].get('VpcSecurityGroups'))
+
+            cluster_response = self.client.describe_db_clusters(DBClusterIdentifier=cluster_name)
+            if cluster_response.get('DBClusters'):
+                cluster = cluster_response.get('DBClusters')[0]
+                subnet_group_name = cluster.get('DBSubnetGroup')
+                port = cluster.get('Port')
+            else:
+                raise Exception("Cluster response is empty. Can not read parameters to continue")
+
+            restore_date = parser.parse(self.restore_time)
+
+            response = self.client.restore_db_cluster_to_point_in_time(
+                DBClusterIdentifier=restored_cluster_name,
+                SourceDBClusterIdentifier=cluster_name,
+                RestoreToTime=restore_date,
+                DBSubnetGroupName=subnet_group_name,
+                Port=port,
+                VpcSecurityGroupIds=security_groups,
+                Tags=[{
+                    'Key': 'SOURCE-CLUSTER',
+                    'Value': cluster_name
+                }, {
+                    'Key': 'RESTORE-TIME',
+                    'Value': self.restore_time
+                },
+                ],
+            )
+
+            status_code = response.get('ResponseMetadata').get('HTTPStatusCode')
+            if status_code != 200:
+                raise Exception("Error occurred while cluster restoring. http code: %s" % status_code)
+            self.__log.info("RDS: Cluster %s restored successfully" % restored_cluster_name)
+        except Exception as e:
+            raise Exception("RDS: Client Error: %s " % e)
+
+        return response, restored_cluster_name
+
+    def wait_for_db_instance(self, restored_instances):
+        for instance_name in restored_instances:
+            self.__log.info("RDS: Wait For DB Instance: %s" % instance_name)
+            waiter = self.client.get_waiter('db_instance_available')
+            waiter.wait(DBInstanceIdentifier=instance_name)
+            self.__log.info("RDS: DB Instance %s is ready" % instance_name)
+
+    def restore_db_instances(self, cluster_name, restored_cluster_name):
+        instance_response = self.client.describe_db_instances(
+            Filters=[{'Name': 'db-cluster-id', 'Values': [cluster_name, ]}, ])
+        if instance_response.get('DBInstances'):
+            restored_instances = []
+            current_instances = []
+            for current_instance in instance_response.get('DBInstances'):
+                current_instance, restored_instance = self.create_db_instance(restored_cluster_name, current_instance)
+                restored_instances.append(restored_instance)
+                current_instances.append(current_instance)
+        else:
+            raise Exception("RDS: Instance response is empty. Can not read parameters to continue")
+        return current_instances, restored_instances
+
+    def create_db_instance(self, restored_cluster_name, current_instance):
+        try:
+            current_instance_id = current_instance.get('DBInstanceIdentifier')
+            self.check_name(current_instance_id)
+            restored_instance_id = self.get_restored_name(current_instance_id)
+            self.__log.info("RDS: Create DB Instance: %s" % restored_instance_id)
+
+            db_instance_class = current_instance.get('DBInstanceClass')
+            db_engine = current_instance.get('Engine')
+            # db_master_username = current_instance.get('MasterUsername')
+
+            self.client.create_db_instance(
+                DBInstanceIdentifier=restored_instance_id,
+                DBInstanceClass=db_instance_class,
+                Engine=db_engine,
+                DBClusterIdentifier=restored_cluster_name,
+                Tags=[{
+                    'Key': 'SOURCE-CLUSTER',
+                    'Value': current_instance_id
+                },
+                ],
+            )
+        except Exception as e:
+            raise Exception("RDS: Restore DB Instance Failed: %s" % e)
+
+        return current_instance_id, restored_instance_id
+
+    def extract_vpc_security_group_ids(self, current_instance_security_groups):
+        vpc_security_group_ids = []
+        for group in current_instance_security_groups:
+            vpc_security_group_ids.append(group.get('VpcSecurityGroupId'))
+        return vpc_security_group_ids
+
+    def get_cluster_name(self, instance_name):
+        instance_response = self.client.describe_db_instances(DBInstanceIdentifier=instance_name)
+        return instance_response.get('DBClusterIdentifier')
+
+    def update_service_external_name(self, restored_cluster_name):
+        self.__log.info("RDS: update service external name: %s" % restored_cluster_name)
+        # instance_response = self.client.describe_db_instances(
+        #     Filters=[{'Name': 'db-cluster-id', 'Values': [restored_cluster_name,]},])
+        # if instance_response.get('DBInstances'):
+        #     restored_db_endpoint = instance_response.get('DBInstances')[0].get('Endpoint').get('Address')
+        cluster_response = self.client.describe_db_clusters(DBClusterIdentifier=restored_cluster_name)
+        if cluster_response.get('DBClusters'):
+            restored_db_endpoint = cluster_response.get('DBClusters')[0].get('Endpoint')
+        else:
+            raise Exception("RDS: Restored instance response is empty. Can not read parameters to continue")
+
+        service_patch = self.create_service(restored_db_endpoint)
+        self.k8s_core_api.patch_namespaced_service(name=self.pg_service_name, namespace=self.namespace, body=service_patch)
+
+    def get_service_external_name(self):
+        services = self.k8s_core_api.list_namespaced_service(
+            namespace=self.namespace,
+            field_selector="metadata.name=%s" % self.pg_service_name)
+        external_name = services.items[0].spec.external_name
+        self.__log.info("RDS: Current service external name: %s" % external_name)
+        return external_name
+
+    def create_service(self, external_name) -> k8s_client.V1Service:
+        return k8s_client.V1Service(
+            spec=k8s_client.V1ServiceSpec(
+                external_name=external_name
+            ),
+        )
+
+    def get_restored_name(self, name):
+        hashed_time = hashlib.md5(self.restore_id.encode('utf-8')).hexdigest()[0:8]
+        if "-restore-" in name:
+            old_name = name.split("-restore-")
+            return old_name[0] + "-restore-" + hashed_time
+        else:
+            return name + "-restore-" + hashed_time
+
+    #RDS cluster or instance names must contain from 1 to 63 symbols. 17 of them are reserved for restored name.
+    def check_name(self, name):
+        if len(name) > 46:
+            raise Exception("Name :%s is too long. Must me not more than 46 symbols")
+
+    def stop_db_instances(self, instances):
+        for instance in instances:
+            self.__log.info("RDS: Stop instance: %s" % instance)
+            self.client.stop_db_instance(DBInstanceIdentifier=instance)
+            self.__log.info("RDS: Stop instance: %s - Stopped" % instance)
+
+    def stop_db_cluster(self, cluster_name):
+        self.__log.info("RDS: Stop cluster: %s" % cluster_name)
+        self.client.stop_db_cluster(DBClusterIdentifier=cluster_name)
+        self.__log.info("RDS: Stop cluster: %s - Stopped" % cluster_name)
+
+    def build_restore_status_file_path(self, ):
+        return '%s/%s' % (self.restore_folder, self.restore_id)
+
+    def flush_status(self):
+        path = self.build_restore_status_file_path()
+        self.write_in_json(path, self.status)
+
+    def update_status(self, key, value, flush=False):
+        self.status[key] = value
+        if flush:
+            self.flush_status()
+
+    def write_in_json(self, path, data):
+        with open(path, 'w') as fd:
+            try:
+                fcntl.lockf(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
+                json.dump(data, fd)
+                return data
+            except IOError:  # another process accessing
+                self.__log.info("trying to access locked file while writing")
+                raise
+            finally:
+                fcntl.lockf(fd, fcntl.LOCK_UN)
+
+
+class BackupStatus:
+    SUCCESSFUL = 'Successful'
+    FAILED = 'Failed'
+    IN_PROGRESS = 'In progress'
+    PLANNED = 'Planned'
+    UNKNOWN = 'Unknown'
+    CANCELED = 'Canceled'
diff --git a/docker-backup-daemon/docker/postgres/endpoints/status.py b/docker-backup-daemon/docker/postgres/endpoints/status.py
new file mode 100644
index 0000000..42a8e5d
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/endpoints/status.py
@@ -0,0 +1,302 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+Set of endpoints that provide information about backups.
+"""
+
+import logging
+import utils
+import requests
+import os.path
+import tempfile
+import json
+
+from flask import Response
+from flask_restful import Resource
+from flask_httpauth import HTTPBasicAuth
+
+import storage_s3
+import eviction
+
+
+auth = HTTPBasicAuth()
+
+
+@auth.verify_password
+def verify(username, password):
+    return utils.validate_user(username, password)
+
+
+class Status(Resource):
+    __endpoints = [
+        '/health',
+        '/status'
+    ]
+
+    def __init__(self, storage):
+        self.__log = logging.getLogger("HealthEndpoint")
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return Status.__endpoints
+
+    def get(self):
+        self.__log.debug("Endpoint /health has been called.")
+        result = {
+            "status": "UP",
+            "storage": {},
+            "encryption": "Off"
+        }
+
+        result.update(self.__storage.get_backup_in_progress_metrics())
+
+        schedule_rs = requests.get('http://localhost:8085/schedule')
+
+        if not schedule_rs.ok:
+            result['status'] = "PROBLEM"
+            eviction_rule = None
+
+            try:
+                schedule_rs.raise_for_status()
+            except requests.HTTPError as e:
+                result['message'] = e.__str__
+        else:
+            schedule_metrics = schedule_rs.json()
+            eviction_rule = schedule_metrics['eviction_rule']
+
+            result.update({
+                'backup': schedule_metrics
+            })
+
+        vaults = list([v for v in self.__storage.list() if v.is_back_up_archive_exists()])
+        vaults.reverse()
+
+        fs_free, fs_total = self.__storage.fs_space()
+
+        dump_count = len(vaults)
+        successful_dump_count = len([x for x in vaults if not x.is_failed()])
+
+        result["storage"] = {
+            "dump_count": dump_count,
+            "successful_dump_count": successful_dump_count,
+            "size": self.__storage.size(),
+            "archive_size": self.__storage.archive_size(),
+            "free_space": fs_free,
+            "total_space": fs_total,
+            "type": self.__storage.get_type(),
+            "type_id": self.__storage.get_type_id()
+        }
+
+        if eviction_rule:
+            outdated_vaults = eviction.evict(vaults, eviction_rule,
+                                             accessor=lambda x: x.create_time())
+
+            result['storage']['outdated_backup_count'] = len(outdated_vaults)
+
+        # calculate last successful
+        for vault in vaults:
+            if not vault.is_failed():
+                result["storage"]["lastSuccessful"] = vault.to_json()
+                break
+
+        if len(vaults) > 0:
+            last_vault = vaults[:1][0]
+            result["storage"]["last"] = last_vault.to_json()
+            if last_vault.is_failed():
+                result["status"] = "WARNING"
+
+        if self.__storage.get_encryption():
+            result["encryption"] = "On"
+
+        if self.__log.isEnabledFor(logging.DEBUG):
+            debug_info = {
+                'debug': True,
+                'endpoint': '/health',
+                'response': result
+            }
+            self.__log.debug(debug_info)
+
+        return result
+
+class List(Resource):
+    __endpoints = [
+        '/list',
+        '/backups/list'
+    ]
+
+    def __init__(self, storage):
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return List.__endpoints
+
+    @auth.login_required
+    def get(self):
+        result = {
+        }
+
+        vaults = list([v for v in self.__storage.list() if v.is_back_up_archive_exists()])
+        vaults.reverse()
+
+        # calculate last successful
+        for vault in vaults:
+            result[vault.get_id()] = vault.to_json()
+
+        return result
+
+
+class BackupStatus(Resource):
+    __endpoints = [
+        '/backup/status/<backup_id>'
+    ]
+
+    def __init__(self, storage):
+        self.__log = logging.getLogger('BackupRequestEndpoint')
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return BackupStatus.__endpoints
+
+    def get(self, backup_id):
+        vault = None
+        result = None
+        vaults = self.__storage.list()
+        vaults.reverse()
+
+        for vault in vaults:
+            if vault.get_id() == backup_id:
+                result = vault
+                break
+        if not result:
+            self.__log.info("Backup %s not found" % backup_id)
+            return Response("Backup %s not found \n" % backup_id, status=404)
+        else:
+            if vault.is_locked():
+                self.__log.info("In Progress %s" % backup_id)
+                return Response("In Progress \n", status=200)
+            elif vault.is_failed():
+                self.__log.info("Backup Failed %s" % backup_id)
+                return Response("Backup Failed \n", status=200)
+            elif vault.is_done():
+                self.__log.info("Backup Done %s" % backup_id)
+                return Response("Backup Done \n", status=200)
+            else:
+                self.__log.info("Backup Failed %s" % backup_id)
+                return Response("Backup Failed \n", status=200)
+
+
+class Health(Resource):
+
+    __endpoints = [
+        '/v2/health'
+    ]
+
+    def __init__(self, storage):
+        self.__log = logging.getLogger("HealthEndpoint")
+        self.__root = storage.root
+
+    @staticmethod
+    def get_endpoints():
+        return Health.__endpoints
+
+    def get(self):
+        schedule_rs = requests.get('http://localhost:8085/schedule')
+        # we also have to check that granular app works correctly
+        protocol = "http"
+        if os.getenv("TLS", "false").lower() == "true":
+            protocol += "s"
+        gr_backups = requests.get(protocol + '://localhost:9000/health', verify=False)
+        if (schedule_rs.status_code == 200) and (self.volume_liveliness_check()) \
+                and gr_backups.status_code == 200:
+            return Response("OK", status=200)
+        else:
+            return Response("Internal server error", status=500)
+
+    def volume_liveliness_check(self):
+        mount_path = self.__root
+        if os.environ['STORAGE_TYPE'] == "s3":
+            try:
+                with open(mount_path + "/health", "w") as f:
+                    f.write("Health check")
+                if self.s3_health_check(mount_path + "/health"):
+                    return True
+                else:
+                    return False
+            except (IOError, Exception) as ex:
+                self.__log.exception(ex)
+                return False
+        else:
+            try:
+                f = tempfile.TemporaryFile(mode='w+t', suffix='.txt', prefix='volume_check_', dir=mount_path)
+                f.write("Test")
+                f.seek(0)
+                contents = f.read()
+                f.close()
+                return True
+            except (IOError, Exception) as ex:
+                self.__log.exception(ex)
+                return False
+
+    def s3_health_check(self, filepath):
+        bucket = os.getenv("CONTAINER")
+        prefixed_filepath = os.getenv("AWS_S3_PREFIX", "") + filepath
+        try:
+            storage_s3.AwsS3Vault.get_s3_client().upload_file(filepath, bucket, prefixed_filepath)
+        except (IOError, Exception) as ex:
+            self.__log.exception(ex)
+            return False
+        try:
+            storage_s3.AwsS3Vault.get_s3_client().delete_object(Bucket=bucket, Key=prefixed_filepath)
+        except (IOError, Exception) as ex:
+            self.__log.exception(ex)
+            return False
+        return True
+
+
+class ExternalRestoreStatus(Resource):
+    __endpoints = [
+        '/external/restore/<restore_id>'
+    ]
+
+    def __init__(self, storage):
+        self.__log = logging.getLogger('ExternalRestoreStatusEndpoint')
+        self.restore_folder = f"{storage.root}/external/restore"
+        self.allowable_db_types = ['AZURE', 'RDS']
+
+    @staticmethod
+    def get_endpoints():
+        return ExternalRestoreStatus.__endpoints
+
+    @auth.login_required
+    def get(self, restore_id):
+        self.__log.info("Get restore status request")
+
+        external_pg_type = os.getenv("EXTERNAL_POSTGRESQL", "FALSE").upper()
+
+        if external_pg_type not in self.allowable_db_types:
+            return Response(response=json.dumps({"status": "Not Supported"}), status=501)
+
+        restore_file_path = f"{self.restore_folder}/{restore_id}"
+        if os.path.isfile(restore_file_path):
+            with open(restore_file_path, 'r') as f:
+                status_map = json.load(f)
+                f.close()
+                return status_map, 200
+        else:
+            self.__log.info(f"Restore process not found {restore_id}")
+            return Response(response=json.dumps({"status": "Not Found"}), status=404)
diff --git a/docker-backup-daemon/docker/postgres/endpoints/wal.py b/docker-backup-daemon/docker/postgres/endpoints/wal.py
new file mode 100644
index 0000000..028b68a
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/endpoints/wal.py
@@ -0,0 +1,202 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""
+    Set of endpoints for PostgreSQL to manipulate with write-ahead logs (WAL) archives.
+"""
+import threading
+from threading import Thread
+
+from flask_restful import Resource
+from flask import Response, request, stream_with_context
+from flask_httpauth import HTTPBasicAuth
+import utils
+import logging
+from filelock import Timeout, FileLock
+
+auth = HTTPBasicAuth()
+
+RESP_NO_FILE = "Please send file using curl -XPOST -F 'file=@somefile'"
+RESP_EXIST_SHA_DIFF = "Archive file already exists with different sha256."
+RESP_EXIST_SHA_SAME = "Archive file already exists with same sha256."
+RESP_SHA_MISMATCH = "Provided sha256 does not match sha256 of stream"
+RESP_WAL_PROC_BUSY = "WAL processor is busy"
+
+@auth.verify_password
+def verify(username, password):
+    return utils.validate_user(username, password)
+
+
+class Upload(Resource):
+    __log = logging.getLogger("UploadArchive")
+
+    __endpoints = [
+        '/archive/put',
+        '/archive/upload'
+    ]
+
+    __evict_lock = threading.Lock()
+
+    def __init__(self, storage):
+        self.__storage = storage
+        self.wal_processing_lock = FileLock("/tmp/wal.processing.lock")
+        self.wal_eviction_lock = FileLock("/tmp/wal.eviction.lock")
+
+    @staticmethod
+    def get_endpoints():
+        return Upload.__endpoints
+
+    def __evict_archive_if_rule_exist(self):
+        self.__log.info("Check if eviction rule is specified and run eviction if needed")
+        if self.__storage.is_archive_evict_policy_set():
+            try:
+                with self.wal_eviction_lock.acquire(timeout=5):
+                    self.__storage.evict_archive()
+            except Timeout:
+                self.__log.warning("Evict is in progress. Skip new evict.")
+
+    def __process_wall_post(self):
+        filename = None
+        sha256 = None
+        if request.args.getlist('filename'):
+            filename = request.args.getlist('filename')[0]
+        if request.args.getlist('sha256'):
+            sha256 = request.args.getlist('sha256')[0]
+
+        self.__log.info(request.args)
+        self.__log.info(
+            "Start upload processing for {} with sha256 {}".format(
+                filename, sha256))
+
+        if self.__storage.is_archive_exists(filename):
+            stored_sha = self.__storage.get_sha256sum_for_archive(filename)
+            calc_sha = self.__storage.calculate_sha256sum_for_archive(filename)
+            if stored_sha == sha256:
+                if calc_sha != sha256:
+                    self.__log.warning(
+                        "Looks like file in storage broken. "
+                        "Will receive new one as replacement. "
+                        "Calculated sha256: {}.".format(calc_sha))
+                else:
+                    return Response(RESP_EXIST_SHA_SAME, status=208)
+            elif not stored_sha and calc_sha == sha256:
+                self.__log.warning(
+                    "Found file without sha. Will store metainfo for future.")
+                self.__storage.store_archive_checksum(filename, sha256)
+                return Response(RESP_EXIST_SHA_SAME, status=208)
+            else:
+                return Response(RESP_EXIST_SHA_DIFF, status=409)
+
+        if 'file' not in request.files:
+            return Response(RESP_NO_FILE, status=400)
+
+        file_obj = request.files['file']
+        try:
+            self.__storage.store_archive_checksum(filename, sha256)
+            sha256_processed = self.__storage.put_archive_as_stream(
+                filename, file_obj.stream)
+            if sha256 == sha256_processed:
+                return Response("Ok", status=200)
+            else:
+                self.__log.info(
+                    "Wrong result sha256 hash: {}".format(sha256_processed))
+                self.__storage.delete_archive(filename)
+                return Response(RESP_SHA_MISMATCH, status=400)
+        except Exception:
+            self.__log.exception("Cannot store archive log.")
+            for i in range(5):
+                try:
+                    self.__storage.delete_archive(filename)
+                    break
+                except Exception:
+                    self.__log.exception("Cannot cleanup failed archive log.")
+            return Response("Internal error occurred.", status=500)
+
+    @auth.login_required
+    def post(self):
+        try:
+            thread = Thread(target=self.__evict_archive_if_rule_exist)
+            thread.start()
+        except Exception as e:
+            self.__log.exception("Cannot start archive eviction during post.")
+        try:
+            with self.wal_processing_lock.acquire(timeout=5):
+                return self.__process_wall_post()
+        except Timeout:
+            self.__log.warning("Cannot process WAL because another in progress")
+            return Response(RESP_WAL_PROC_BUSY, status=503)
+
+
+class Download(Resource):
+
+    __endpoints = [
+        '/archive/get',
+        '/archive/download'
+    ]
+
+    def __init__(self, storage):
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return Download.__endpoints
+
+    @auth.login_required
+    def get(self):
+        def generate(storage, filename):
+            with storage.get_archive_as_stream(filename) as f:
+                chunk_size = 4096
+                while True:
+                    data = f.read(chunk_size)
+                    if len(data) == 0:
+                        f.close()
+                        return
+                    yield data
+
+        file_name = request.args.getlist('filename')[
+            0] if request.args.getlist('filename') else None
+        if file_name:
+            if self.__storage.is_archive_exists(file_name):
+                return Response(stream_with_context(
+                    generate(self.__storage, file_name)),
+                                mimetype='application/octet-stream',
+                                headers=[
+                                    ('Content-Type',
+                                     'application/octet-stream'),
+                                    ('Content-Disposition', file_name)
+                                ])
+            else:
+                return Response(
+                    "Cannot find file {} in archive".format(file_name),
+                    status=404)
+
+
+class Delete(Resource):
+
+    __endpoints = [
+        '/archive/delete'
+    ]
+
+    def __init__(self, storage):
+        self.__storage = storage
+
+    @staticmethod
+    def get_endpoints():
+        return Delete.__endpoints
+
+    @auth.login_required
+    def delete(self):
+        filename = request.args.getlist('filename')[0] if request.args.getlist('filename') else None
+        if filename:
+            self.__storage.delete_archive(filename)
diff --git a/docker-backup-daemon/docker/postgres/eviction.py b/docker-backup-daemon/docker/postgres/eviction.py
new file mode 100644
index 0000000..01a0311
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/eviction.py
@@ -0,0 +1,78 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import time
+from itertools import groupby
+
+
+class Rule:
+    magnifiers = {
+        "min": 60,
+        "h": 60 * 60,
+        "d": 60 * 60 * 24,
+        "m": 60 * 60 * 24 * 30,
+        "y": 60 * 60 * 24 * 30 * 12,
+    }
+
+    def __init__(self, rule):
+        (startStr, intervalStr) = rule.strip().split("/")
+        self.start = self.__parseTimeSpec(startStr)
+        self.interval = "delete" if (intervalStr == "delete") else self.__parseTimeSpec(intervalStr)
+
+    def __parseTimeSpec(self, spec):
+        import re
+        if (spec == "0"):
+            return 0
+
+        r = re.match("^(\\d+)(%s)$" % "|".join(list(self.magnifiers.keys())), spec)
+        if (r is None):
+            raise Exception("Incorrect eviction start/interval specification: %s" % spec)
+
+        digit = int(r.groups()[0])
+        magnifier = self.magnifiers[r.groups()[1]]
+
+        return digit * magnifier
+
+    def __str__(self):
+        return "%d/%d" % (self.start, self.interval)
+
+
+def parse(rules):
+    rules = [Rule(r) for r in rules.split(",")]
+    return rules
+
+
+def evict(items, rules, start_point_time=None, accessor=lambda x: x):
+    """
+		Calculate what to evict from given list of versions (version is timestamp value, when each lbackup was created)
+	"""
+
+    if start_point_time is None:
+        start_point_time = time.time()
+
+    evictionVersions = []
+    # TODO: to cache rules
+    for rule in parse(rules):
+        operateVersions = [t for t in items if accessor(t) <= start_point_time - rule.start]
+        if (rule.interval == "delete"):
+            # all versions should be evicted catched by this interval
+            evictionVersions.extend(operateVersions)
+        else:
+            # group by interval and leave only first on each
+            thursday = 3 * 24 * 60 * 60
+            for _, versionsIt in groupby(operateVersions, lambda t: int((accessor(t) - thursday) / rule.interval)):
+                grouped = sorted(list(versionsIt), key=lambda t: accessor(t))
+                evictionVersions.extend(grouped[:-1])
+
+    return sorted(list(set(evictionVersions)), reverse=True)
diff --git a/docker-backup-daemon/docker/postgres/fsutil.py b/docker-backup-daemon/docker/postgres/fsutil.py
new file mode 100644
index 0000000..87ae273
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/fsutil.py
@@ -0,0 +1,71 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import subprocess
+
+
+def touch(filepath):
+    open(filepath, "w").close()
+
+
+# def get_folder_size(dirpath):
+# 	total_size = 0
+# 	for dirpath, dirname, filenames in os.walk(dirpath):
+# 		for f in filenames:
+# 			fp = os.path.join(dirpath, f)
+# 			if os.path.exists(fp):
+# 				total_size += os.path.getsize(fp)
+# 	return total_size
+
+def get_folder_size(dirpath):
+    p1 = subprocess.Popen(["du", "-sb", dirpath], stdout=subprocess.PIPE)
+    size = p1.communicate()[0].split(b"\t")[0]
+    return size.decode()
+
+
+def get_mount_point(pathname):
+    """Get the mount point of the filesystem containing pathname"""
+    pathname = os.path.normcase(os.path.realpath(pathname))
+    parent_device = path_device = os.stat(pathname).st_dev
+    while parent_device == path_device:
+        mount_point = pathname
+        pathname = os.path.dirname(pathname)
+        if pathname == mount_point: break
+        parent_device = os.stat(pathname).st_dev
+    return mount_point
+
+
+def get_mounted_device(pathname):
+    """Get the device mounted at pathname"""
+    # uses "/proc/mounts"
+    pathname = os.path.normcase(pathname)  # might be unnecessary here
+    try:
+        with open("/proc/mounts", "r") as ifp:
+            for line in ifp:
+                fields = line.rstrip('\n').split()
+                # note that line above assumes that
+                # no mount points contain whitespace
+                if fields[1] == pathname:
+                    return fields[0]
+    except EnvironmentError:
+        pass
+    return None  # explicit
+
+
+def get_fs_space(pathname):
+    """Get the free space and total of the filesystem containing pathname"""
+
+    stat = os.statvfs(pathname)
+    return (stat.f_bfree * stat.f_bsize, stat.f_blocks * stat.f_bsize)
diff --git a/docker-backup-daemon/docker/postgres/gunicorn/__init__.py b/docker-backup-daemon/docker/postgres/gunicorn/__init__.py
new file mode 100644
index 0000000..342c6f0
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/gunicorn/__init__.py
@@ -0,0 +1,14 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
diff --git a/docker-backup-daemon/docker/postgres/gunicorn/archive.py b/docker-backup-daemon/docker/postgres/gunicorn/archive.py
new file mode 100644
index 0000000..43ef9d0
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/gunicorn/archive.py
@@ -0,0 +1,35 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from flask import Flask
+from flask_restful import Api
+
+import configs
+import endpoints.wal
+import storage
+
+
+app = Flask('ArchiveEndpoints')
+api = Api(app)
+
+conf = configs.load_configs()
+storage_instance = storage.init_storage(storageRoot=conf['storage'])
+
+api.add_resource(endpoints.wal.Upload, *endpoints.wal.Upload.get_endpoints(), resource_class_args=(storage_instance, ))
+api.add_resource(endpoints.wal.Download, *endpoints.wal.Download.get_endpoints(), resource_class_args=(storage_instance, ))
+api.add_resource(endpoints.wal.Delete, *endpoints.wal.Delete.get_endpoints(), resource_class_args=(storage_instance, ))
+
+
+if __name__ == '__main__':
+    app.run()
diff --git a/docker-backup-daemon/docker/postgres/gunicorn/private.py b/docker-backup-daemon/docker/postgres/gunicorn/private.py
new file mode 100644
index 0000000..a85eeea
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/gunicorn/private.py
@@ -0,0 +1,35 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from flask_restful import  Api
+from flask import Flask
+
+import configs
+import endpoints.backup
+import endpoints.status
+import storage
+
+
+app = Flask("InternalServiceEndpoints")
+api = Api(app)
+
+conf = configs.load_configs()
+storage_instance = storage.init_storage(storageRoot=conf['storage'])
+
+api.add_resource(endpoints.status.List, *endpoints.status.List.get_endpoints(), resource_class_args=(storage_instance, ))
+api.add_resource(endpoints.backup.Eviction, *endpoints.backup.Eviction.get_endpoints(), resource_class_args=(storage_instance, ))
+api.add_resource(endpoints.backup.Download, *endpoints.backup.Download.get_endpoints(), resource_class_args=(storage_instance, ))
+
+if __name__ == '__main__':
+    app.run()
diff --git a/docker-backup-daemon/docker/postgres/gunicorn/public.py b/docker-backup-daemon/docker/postgres/gunicorn/public.py
new file mode 100644
index 0000000..861298d
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/gunicorn/public.py
@@ -0,0 +1,60 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from flask_restful import Api
+import os
+from flask import Flask
+
+import configs
+import endpoints.backup
+import endpoints.restore
+import endpoints.status
+import storage
+
+from opentelemetry.instrumentation.flask import FlaskInstrumentor
+from opentelemetry.sdk.trace import TracerProvider
+from opentelemetry.sdk.trace.export import BatchSpanProcessor
+from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
+from opentelemetry.sdk.resources import SERVICE_NAME, Resource
+
+
+app = Flask("PublicEndpoints")
+collector_endpoint = os.getenv("OTEL_EXPORTER_OTLP_TRACES_ENDPOINT", "")
+if collector_endpoint != "":
+    collector_endpoint = "http://" + collector_endpoint
+    NAMESPACE_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
+    ns = open(NAMESPACE_PATH).read()
+    resource = Resource(attributes={
+        SERVICE_NAME: "postgresql-backup-daemon-" + ns
+    })
+    provider = TracerProvider(resource=resource)
+    processor = BatchSpanProcessor(OTLPSpanExporter(endpoint=collector_endpoint, insecure=True))
+    provider.add_span_processor(processor)
+    FlaskInstrumentor().instrument_app(app=app, tracer_provider=provider, excluded_urls="health,/health,v2/health,/v2/health")
+api = Api(app)
+
+conf = configs.load_configs()
+storage_instance = storage.init_storage(storageRoot=conf['storage'])
+
+endpoints.restore.ExternalRestoreRequest.cleanup_restore_status(storage_instance)
+
+api.add_resource(endpoints.status.Status, *endpoints.status.Status.get_endpoints(), resource_class_args=(storage_instance,))
+api.add_resource(endpoints.backup.BackupRequest, *endpoints.backup.BackupRequest.get_endpoints())
+api.add_resource(endpoints.status.Health, *endpoints.status.Health.get_endpoints(), resource_class_args=(storage_instance,))
+api.add_resource(endpoints.status.BackupStatus, *endpoints.status.BackupStatus.get_endpoints(), resource_class_args=(storage_instance,))
+api.add_resource(endpoints.status.ExternalRestoreStatus, *endpoints.status.ExternalRestoreStatus.get_endpoints(), resource_class_args=(storage_instance,))
+api.add_resource(endpoints.restore.ExternalRestoreRequest, *endpoints.restore.ExternalRestoreRequest.get_endpoints(), resource_class_args=(storage_instance,))
+
+if __name__ == '__main__':
+    app.run()
diff --git a/docker-backup-daemon/docker/postgres/locks.py b/docker-backup-daemon/docker/postgres/locks.py
new file mode 100644
index 0000000..87bffae
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/locks.py
@@ -0,0 +1,73 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import json
+import logging
+import os
+import fsutil
+
+from retrying import retry
+
+
+class BackupInProgressLock:
+    __in_progress_lock_file_path = '/tmp/backup.progress'
+
+    def __init__(self):
+        self.__log = logging.getLogger(self.__class__.__name__)
+
+    @retry(wait_fixed=300000)  # Wait for 300 seconds before retry.
+    def acquire_lock(self):
+        # could not lock the resource
+        #if not self.__lock.acquire(False):
+        #    self.__log.info("New backup can not be started, because last backup is still in progress.")
+        #    raise Exception
+        #else:
+        #    result = self.__lock.acquire()
+        self.__log.info("Backup lock has been acquired. File created")
+        if not os.path.isfile(self.get_lock_file_path()):
+            fsutil.touch(self.get_lock_file_path())
+        with open(self.get_lock_file_path(), "w+") as lock_file:
+            lock_details = {
+                'lock_acquisition_time': datetime.datetime.now().isoformat()
+            }
+            lock_file.write(json.dumps(lock_details))
+
+    def release_lock(self):
+        os.remove(self.get_lock_file_path())
+        self.__log.info("Backup lock has been released.")
+
+    @staticmethod
+    def get_lock_file_path():
+        return BackupInProgressLock.__in_progress_lock_file_path
+
+
+def backup_lock():
+    return BackupInProgressLock()
+
+
+def get_backup_lock_file_path():
+    return BackupInProgressLock.get_lock_file_path()
+
+
+def update_lock_file(**kwargs):
+    with open(get_backup_lock_file_path(), "r") as f:
+        details = json.load(f)
+
+    details.update(kwargs)
+    temp_lock = '%s.tmp' % get_backup_lock_file_path()
+    with open(temp_lock, "w") as f:
+        json.dump(details, f)
+
+    os.rename(temp_lock, get_backup_lock_file_path())
diff --git a/docker-backup-daemon/docker/postgres/logging.conf b/docker-backup-daemon/docker/postgres/logging.conf
new file mode 100644
index 0000000..1a79a1d
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/logging.conf
@@ -0,0 +1,43 @@
+[loggers]
+keys=root, gunicorn.error, gunicorn.access, werkzeug
+
+[handlers]
+keys=console
+
+[formatters]
+keys=generic, graylog
+
+[logger_root]
+level=INFO
+handlers=console
+
+[logger_gunicorn.error]
+level=INFO
+handlers=console
+qualname=gunicorn.error
+
+[logger_werkzeug]
+level=WARNING
+handlers=console
+qualname=werkzeug
+
+[logger_gunicorn.access]
+level=WARNING
+handlers=console
+qualname=gunicorn.access
+
+[handler_console]
+class=StreamHandler
+formatter=graylog
+args=(sys.stdout, )
+
+[formatter_generic]
+format=[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s
+datefmt=%Y-%m-%dT%H:%M:%S
+class=logging.Formatter
+
+
+[formatter_graylog]
+format=[%(asctime)s,%(msecs)03d][%(levelname)s][category=%(name)s] %(message)s
+datefmt=%Y-%m-%dT%H:%M:%S
+class=logging.Formatter
diff --git a/docker-backup-daemon/docker/postgres/postgres_backup.sh b/docker-backup-daemon/docker/postgres/postgres_backup.sh
new file mode 100755
index 0000000..2dfb302
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/postgres_backup.sh
@@ -0,0 +1,231 @@
+#!/bin/bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+cd $(dirname "$0")
+
+readonly AWS_S3_STORAGE="s3"
+readonly SWIFT_STORAGE="swift"
+
+# In case of Encryption password for encryption will be passed as
+# 2nd input parameter, 1st one is data_folder
+readonly ENCRYPTION_KEY="$2"
+
+BACKUP_DESTINATION_DIRECTORY="$1"
+BACKUP_NAME="pg_${PG_CLUSTER_NAME}_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+
+source utils.sh
+
+function test_swift() {
+  local out
+  out=$(/opt/backup/scli ls ${CONTAINER} 2>&1)
+  process_exit_code $? "$out"
+}
+
+function do_backup() {
+  if [[ -z "${BACKUP_ID}" ]]; then
+    log "Backup id must be specified explicitly"
+  fi
+  log BACKUP_ID
+  log "BACKUP_ID"
+  local validation_pipe="pg-backup-${BACKUP_ID}.pipe"
+  local pg_basebackup_stderr_file="pg-backup-${BACKUP_ID}.error.log"
+  local validation_stderr_file="pg-backup-validation-${BACKUP_ID}.error.log"
+
+  register_delete_on_exit "${validation_pipe}" "${validation_stderr_file}" "${pg_basebackup_stderr_file}"
+
+  # Validate TAR stream on the fly.
+  # This will not validate backup data itself, but will check archive's integrity.
+  mkfifo "${validation_pipe}"
+  tar -tz <"${validation_pipe}" > /dev/null 2> "${validation_stderr_file}" &
+
+  log "start backup streaming to mounted storage"
+  if [[ -n "$ENCRYPTION_KEY" ]]; then
+     BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY})_enc.tar.gz"
+    log "Encryption key is set will encrypt backup"
+    $PG_BASEBACKUP -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${REPLICATION_USER}" -D - -X fetch --format=tar --gzip 2> "${pg_basebackup_stderr_file}" \
+    | tee "${validation_pipe}" | openssl enc -aes-256-cbc -nosalt -pass pass:"$ENCRYPTION_KEY" > "${BACKUP_DESTINATION_DIRECTORY}/${BACKUP_NAME}"
+  else
+    $PG_BASEBACKUP -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${REPLICATION_USER}" -D - -X fetch --format=tar --gzip 2> "${pg_basebackup_stderr_file}" \
+    | tee "${validation_pipe}" > "${BACKUP_DESTINATION_DIRECTORY}/${BACKUP_NAME}"
+  fi
+
+  # PIPESTATUS can be overridden, so need to keep it.
+  local exit_codes=(${PIPESTATUS[@]})
+  local pg_basebackup_exit_code=${exit_codes[0]}
+
+  # Wait for TAR validation to complete.
+  wait $!
+  local validation_exit_code=$?
+
+  local validation_stderr="$(cat ${validation_stderr_file})"
+  local pg_basebackup_log="$(cat ${pg_basebackup_stderr_file})"
+
+  process_exit_code ${pg_basebackup_exit_code} "pg_basebackup has failed. Details: ${pg_basebackup_log}"
+  process_exit_code ${validation_exit_code} "Backup archive integrity validation not passed. This backup will be marked as failed. Details: ${validation_stderr}"
+}
+
+function do_swift_backup() {
+  local pg_backup_pipe="pg-backup-${BACKUP_DESTINATION_DIRECTORY}.pipe"
+  local pg_basebackup_error_file="pg-backup-${BACKUP_DESTINATION_DIRECTORY}.error.log"
+  local pg_backup_validation_error_file="pg-backup-validation-${BACKUP_DESTINATION_DIRECTORY}.error.log"
+  local swift_upload_log_file="pg-backup-${BACKUP_DESTINATION_DIRECTORY}.log"
+  local swift_object_path="${CONTAINER}/${BACKUP_DESTINATION_DIRECTORY}/${BACKUP_NAME}"
+
+  log "Streaming backup to Swift under path: ${swift_object_path}"
+
+  mkfifo "${pg_backup_pipe}"
+  tar -tz <"${pg_backup_pipe}" > /dev/null 2> "${pg_backup_validation_error_file}" &
+  # Validate TAR stream on the fly.
+  # This will not validate backup data itself, but will check archive's integrity.
+  if [[ -n "$ENCRYPTION_KEY" ]]; then
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY})_enc.tar.gz"
+    log "Encryption key is set will encrypt backup"
+    $PG_BASEBACKUP -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${REPLICATION_USER}" -D - -X fetch --format=tar --gzip 2> "${pg_basebackup_error_file}" \
+    | tee "${pg_backup_pipe}" \
+    | openssl enc -aes-256-cbc -nosalt -pass pass:"$ENCRYPTION_KEY" \
+    | /opt/backup/scli put "${swift_object_path}" 2>&1 > "${swift_upload_log_file}"
+  else
+    $PG_BASEBACKUP -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${REPLICATION_USER}" -D - -X fetch --format=tar --gzip 2> "${pg_basebackup_error_file}" \
+    | tee "${pg_backup_pipe}" \
+    | /opt/backup/scli put "${swift_object_path}" 2>&1 > "${swift_upload_log_file}"
+  fi
+
+  # PIPESTATUS can be overridden, so need to keep it.
+  local exit_codes=(${PIPESTATUS[@]})
+  local pg_backup_exit_code=${exit_codes[0]}
+  local swift_upload_exit_code=${exit_codes[2]}
+
+  # Wait for TAR validation to complete.
+  wait $!
+  local validation_exit_code=$?
+
+  local swift_log="$(cat ${swift_upload_log_file})"
+  local validation_log="$(cat ${pg_backup_validation_error_file})"
+  local pg_basebackup_log="$(cat ${pg_basebackup_error_file})"
+
+  rm "${pg_backup_pipe}" "${swift_upload_log_file}" "${pg_backup_validation_error_file}" "${pg_basebackup_error_file}"
+
+  process_exit_code ${pg_backup_exit_code} "pg_basebackup has failed. Details: ${pg_basebackup_log}"
+  process_exit_code ${validation_exit_code} "Backup archive integrity validation not passed. This backup will be marked as failed. Details: ${validation_log}"
+  process_exit_code ${swift_upload_exit_code} "Backup uploading to Swift has failed. Details: ${swift_log}"
+
+  log "PostgreSQL backup streaming to Swift completed successfully."
+}
+
+function remove_backup() {
+  local out
+  out=$(rm -f "${BACKUP_DESTINATION_DIRECTORY}/${BACKUP_NAME}" 2>&1)
+  process_exit_code $? "$out"
+}
+
+function main() {
+  version="$(PGPASSWORD=$POSTGRES_PASSWORD psql -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${POSTGRES_USER}" -d postgres -c "SHOW SERVER_VERSION;" -tA | egrep -o '[0-9]{1,}\.[0-9]{1,}' | awk 'END{print $1}')"
+  REPLICATION_USER="replicator"
+  log "version of pgsql server is: ${version}"
+  if python -c "import sys; sys.exit(0 if float("${version}") >= 17.0 else 1)"; then
+    log "Using pgsql 17 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/17/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 16.0 <= float("${version}") < 17.0 else 1)"; then
+    log "Using pgsql 16 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/16/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 15.0 <= float("${version}") < 16.0 else 1)"; then
+    log "Using pgsql 15 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/15/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 14.0 <= float("${version}") < 15.0 else 1)"; then
+    log "Using pgsql 14 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/14/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 13.0 <= float("${version}") < 14.0 else 1)"; then
+    log "Using pgsql 13 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/13/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 12.0 <= float("${version}") < 13.0 else 1)"; then
+    log "Using pgsql 12 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/12/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 11.0 <= float("${version}") < 12.0 else 1)"; then
+    log "Using pgsql 11 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/11/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  elif python -c "import sys; sys.exit(0 if 10.0 <= float("${version}") < 11.0 else 1)"; then
+    log "Using pgsql 10 bins for pg_basebackup"
+    PG_BASEBACKUP="/usr/lib/postgresql/10/bin/pg_basebackup"
+    BACKUP_NAME="pg_backup_$(basename ${BACKUP_DESTINATION_DIRECTORY}).tar.gz"
+  else
+    if [ "${PG_CLUSTER_NAME}" != "gpdb" ]
+    then
+        log "Using pgsql 9.6 bins for  pg_basebackup"
+        PG_BASEBACKUP="/usr/pgsql-9.6/bin/pg_basebackup"
+    else
+        log "Using gpdb bins for greenplum pg_basebackup"
+        TARGET_DB_ID="$(psql -h "${POSTGRES_HOST}" -p "${POSTGRES_PORT}" -U "${POSTGRES_USER}" -d postgres -c "select dbid from gp_segment_configuration where content = -1 and status = 'up' and role = 'p';" -tA )"
+        PG_BASEBACKUP="/usr/local/greenplum-db/bin/pg_basebackup --target-gp-dbid="${TARGET_DB_ID}""
+        REPLICATION_USER=${POSTGRES_USER}
+
+    fi
+  fi
+
+  if [ "$STORAGE_TYPE" == "${SWIFT_STORAGE}" ]; then
+    log "check swift is ready"
+    test_swift
+    log "do backup"
+    do_swift_backup
+  elif [[ "${STORAGE_TYPE}" == "${AWS_S3_STORAGE}" ]]; then
+    bash aws-s3-backup.sh "${CONTAINER}" "${BACKUP_DESTINATION_DIRECTORY}"
+    process_exit_code $? "PostgreSQL backup to AWS S3 has finished with an error."
+  elif [[ "${STORAGE_TYPE}" == "pgbackrest" ]]; then
+        log "Using pgbackrest as external backuper"
+        BACKUP_ID=$(basename ${BACKUP_DESTINATION_DIRECTORY})
+        log "'$BACKUP_ID'"
+        log "BACKUP_DESTINATION_DIRECTORY"
+        # Check cluster state via patroni API
+        PGBACKREST_SRV="backrest"
+        if [ "${BACKUP_FROM_STANDBY}" == "true" ]; then
+          PATRONI_RESPONSE=$(curl -s pg-patroni:8008/cluster)
+          if [ $? -eq 0 ]; then
+              # First verify we got valid JSON response
+              if echo "$PATRONI_RESPONSE" | jq . >/dev/null 2>&1; then
+                  # Look for healthy streaming replicas
+                  STREAMING_REPLICAS=$(echo "$PATRONI_RESPONSE" | jq -r '.members[] | select(.role=="replica" and .state=="streaming")')
+                  
+                  if [ ! -z "$STREAMING_REPLICAS" ]; then
+                      log "Found healthy streaming replica(s)"
+                      PGBACKREST_SRV="backrest-standby"
+                  else
+                      log "No healthy streaming replicas found, leader will be used"
+                  fi
+              else
+                  log "Invalid JSON response from patroni API"
+                  process_exit_code 1 "Invalid JSON response from patroni API"
+              fi
+          else
+              log "Failed to query patroni API"
+              process_exit_code 1 "Failed to query patroni API"
+          fi
+        fi
+
+        curl  -H "Content-Type: application/json" -H "Accept: application/json" -d '{"timestamp": "'$BACKUP_ID'"}' -XPOST ${PGBACKREST_SRV}:3000/backup
+  elif [[ "$STORAGE_TYPE" == "hostpath" ]] || [[ "$STORAGE_TYPE" == "pv" ]] || [[ "$STORAGE_TYPE" == "pv_label" ]] || [[ "$STORAGE_TYPE" == "provisioned" ]] || [[ "$STORAGE_TYPE" == "provisioned-default" ]]; then
+    log "do backup"
+    do_backup
+  fi
+  log "completed"
+}
+
+main "$@"
diff --git a/docker-backup-daemon/docker/postgres/postgres_backup_daemon.py b/docker-backup-daemon/docker/postgres/postgres_backup_daemon.py
new file mode 100644
index 0000000..fe9515e
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/postgres_backup_daemon.py
@@ -0,0 +1,41 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+
+import configs
+import postgres_backup_scheduler
+
+
+if __name__ == "__main__":
+    configs.load_logging_configs()
+    conf = configs.load_configs()
+
+    log = logging.getLogger("PostgreSQLBackupDaemon")
+    log.info("Backup daemon raised again.")
+
+    backups_params = {
+        'backup_command': conf.get_string("command"),
+        'storage_root': conf.get_string("storage"),  # TODO (vladislav.kaverin): Actually almost nobody needs storage root, only file-system storage.
+        'eviction_rule': conf.get_string("eviction"),
+        'timeout': conf.get_int("timeout")
+    }
+    backups_schedule = conf.get_string("schedule")
+
+    # Start scheduler and activate `/backup` endpoint.
+    backup_scheduler = postgres_backup_scheduler.start(backups_schedule, backups_params)
+
+
+    # scheduler <- backup_executor <- storage
+    # http_api <- storage
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/postgres_backup_scheduler.py b/docker-backup-daemon/docker/postgres/postgres_backup_scheduler.py
new file mode 100644
index 0000000..7b86f0c
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/postgres_backup_scheduler.py
@@ -0,0 +1,197 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+from datetime import datetime
+import threading
+import time
+import os
+
+from croniter import croniter
+from flask_restful import Resource, Api
+from flask import Flask
+from queue import Queue, Empty
+from storage import VAULT_NAME_FORMAT
+
+import locks
+import storage
+import workers
+
+
+# TODO: move to postgres/endpoints
+class SchedulerEndpoint(Resource):
+
+    __endpoint = '/schedule'
+
+    def __init__(self, scheduler):
+        self.__scheduler = scheduler
+
+    @staticmethod
+    def get_endpoint():
+        return SchedulerEndpoint.__endpoint
+
+    def post(self):
+        return self.__scheduler.enqueue_backup("http-request")
+
+    def get(self):
+        return self.__scheduler.get_metrics()
+
+
+class BackupsScheduler:
+    def __init__(self, schedule, backup_options):
+        self.__log = logging.getLogger("BackupsScheduler")
+        self.__backup_options = backup_options
+
+        self.__storage = storage.init_storage(storageRoot=self.__backup_options['storage_root'])
+        self.__task_queue = Queue()
+
+        if schedule.lower() != 'none':
+            self.__log.info("Start backup scheduler with: %s" % schedule)
+            self.__cron = croniter(schedule)
+            self.__reschedule()
+        else:
+            self.__next_timestamp = None
+            self.__log.info("Skip backup schedule.")
+
+    def __reschedule(self):
+        self.__next_timestamp = self.__cron.get_next()
+        self.__log.info("Scheduled next run at %s" % datetime.fromtimestamp(self.__next_timestamp).strftime(
+            "%Y-%m-%d %H:%M:%S"))
+
+        # TODO check on negative value after substraction
+        delay = self.__next_timestamp - time.time()
+        if delay < 0:
+            self.__log.warn("Task execution performed longer than specified repeat interval")
+            delay = 0
+
+        self.timer = threading.Timer(delay, self.__execute_and_reschedule)
+        self.timer.setDaemon(True)
+        self.timer.start()
+
+    def __execute_and_reschedule(self):
+        self.__log.info("[reason=schedule] Requesting new backup by schedule.")
+        self.enqueue_backup("schedule")
+        self.__reschedule()
+
+    def run(self):
+        # Accept manual requests for backup.
+        _activate_endpoint(self)
+        self.__log.info("Waiting for backup requests...")
+
+        while True:
+            try:
+                backup = self.__task_queue.get(True, timeout=1)
+            except Empty:
+                continue
+
+            lock = locks.backup_lock()
+            try:
+
+
+                lock.acquire_lock()
+                self.__log.info("[reason={}] Spawn new backup worker. Backup requests queue length: {}".format
+                                (backup.get_reason(), self.__task_queue.qsize()))
+                oldest_backup = self.__storage.get_oldest_backup()
+                worker = workers.spawn_backup_worker(self.__storage,
+                                                     backup_command=self.__backup_options['backup_command'],
+                                                     eviction_rule=self.__backup_options['eviction_rule'],
+                                                     backup_id=backup.get_backup_id()
+                                                     )
+
+                timeout_seconds = self.__backup_options.get('timeout')
+                if timeout_seconds:
+                    self.__log.info("Using configured timeout for backup: %ss", timeout_seconds)
+                    worker.join(timeout_seconds)
+                else:
+                    if oldest_backup:
+                        self.__log.info("Id of latest backup: {}".format(oldest_backup.get_id()))
+                        spent_time = oldest_backup.load_metrics().get('spent_time')
+
+                        if spent_time:
+                            # time stored as milliseconds converting to seconds and double the value
+                            time_out = spent_time / 1000 * 2
+                            self.__log.info("Setting timeout for backup process: {}".format(time_out))
+                            worker.join(time_out)
+
+                        else:
+                            worker.join()
+                    else:
+                        worker.join()
+
+                self.__log.info("Worker completed: {}".format(not worker.is_alive()))
+
+                if worker.is_alive():
+                    self.__log.error("Backup worker for {} is not completed after timeout: {}".format(backup.get_backup_id(), time_out))
+                    worker.fail()
+                    worker.kill()
+                    raise Exception("Backup worker timeout exceeded")
+
+            except:
+                self.__log.error("Error execute schedule callback", exc_info=1)
+            finally:
+                lock.release_lock()
+                self.__task_queue.task_done()
+
+    def enqueue_backup(self, reason="manual"):
+        backup_id = datetime.now().strftime(VAULT_NAME_FORMAT)
+        backup = Backup(backup_id, reason)
+        
+        self.__task_queue.put(backup)
+        queue_size = self.__task_queue.qsize()
+        self.__log.info("[reason={}] New backup request has been received and added to queue. Queue length: {}".format
+                        (reason, queue_size))
+
+        return {
+            'accepted': True,
+            'reason': reason,
+            'backup_requests_in_queue': queue_size,
+            'message': "PostgreSQL backup has been scheduled successfully.",
+            'backup_id': backup_id
+        }
+
+    def get_metrics(self):
+        return {
+            'requests_in_queue': self.__task_queue.qsize(),
+            'time_until_next_backup': 'none' if self.__next_timestamp is None else self.__next_timestamp - time.time(),
+            'eviction_rule': self.__backup_options['eviction_rule']
+        }
+
+
+class Backup:
+    def __init__(self, backup_id, reason):
+        self.__backup_id = backup_id
+        self.__reason = reason
+
+    def get_backup_id(self):
+        return self.__backup_id
+
+    def get_reason(self):
+        return self.__reason
+
+
+def start(backups_schedule, backups_params):
+    scheduler = BackupsScheduler(backups_schedule, backups_params)
+    scheduler.run()
+
+
+def _activate_endpoint(scheduler):
+    app = Flask("ScheduleEndpoint")
+    api = Api(app)
+    api.add_resource(SchedulerEndpoint, SchedulerEndpoint.get_endpoint(), resource_class_args=(scheduler, ))
+    backup_endpoint_thread = threading.Thread(target=app.run, args=('127.0.0.1', 8085))
+    backup_endpoint_thread.setDaemon(True)
+    backup_endpoint_thread.start()
+
+    log = logging.getLogger("ScheduleEndpoint")
+    log.info("Endpoint `/schedule` has been activated.")
diff --git a/docker-backup-daemon/docker/postgres/start_backup_daemon.sh b/docker-backup-daemon/docker/postgres/start_backup_daemon.sh
new file mode 100644
index 0000000..a13acfd
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/start_backup_daemon.sh
@@ -0,0 +1,114 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+BACKUP_DAEMON_DIR="$(cd $(dirname $0); pwd)"
+
+DEFAULT_WORKERS_NUMBER=2 # Suppose that one accepted long-running request and another deals with quick ones.
+DEFAULT_WORKERS_TIMEOUT=21600 # 6 hours
+
+PUBLIC_ENDPOINTS_WORKERS_NUMBER=${PUBLIC_ENDPOINTS_WORKERS_NUMBER:-${DEFAULT_WORKERS_NUMBER}}
+PRIVATE_ENDPOINTS_WORKERS_NUMBER=${PRIVATE_ENDPOINTS_WORKERS_NUMBER:-${DEFAULT_WORKERS_NUMBER}}
+ARCHIVE_ENDPOINTS_WORKERS_NUMBER=${ARCHIVE_ENDPOINTS_WORKERS_NUMBER:-${DEFAULT_WORKERS_NUMBER}}
+
+WORKERS_TIMEOUT=${WORKERS_TIMEOUT:-${DEFAULT_WORKERS_TIMEOUT}}
+LOG_FORMAT=${LOG_FORMAT:-generic}
+
+function update_logging_configuration() {
+  sed -i s/formatter=.*/formatter=${LOG_FORMAT}/g ${BACKUP_DAEMON_DIR}/logging.conf
+  if [[ -z "${LOG_LEVEL}" ]]; then
+    return
+  fi
+  sed -i s/level=.*/level=${LOG_LEVEL}/g ${BACKUP_DAEMON_DIR}/logging.conf
+
+}
+
+function check_ipv6() {
+  if [[ -d "/proc/sys/net/ipv6" ]]; then
+    echo "IPv6 availiable will listen on [::]"
+    export LISTEN_ADDR="[::]"
+  else
+    echo "IPv6 is not availiable will listen on 0.0.0.0"
+    export LISTEN_ADDR="0.0.0.0"
+  fi
+}
+
+
+function ride_unicorn() {
+  params="--daemon --timeout ${WORKERS_TIMEOUT} --preload --enable-stdio-inheritance --log-config ${BACKUP_DAEMON_DIR}/logging.conf"
+  if [[ $TLS ]]; then
+    params="${params} --certfile=/certs/tls.crt --keyfile=/certs/tls.key"
+  fi
+  # 2 workers
+  # Roughly, one is for /health endpoint since it's quite fast operation and should not block,
+  # second one is for /backup which just schedules backups.
+  echo ${params}
+  gunicorn --chdir "${BACKUP_DAEMON_DIR}/gunicorn" \
+           ${params} \
+           -w ${PUBLIC_ENDPOINTS_WORKERS_NUMBER} \
+           --pythonpath ${BACKUP_DAEMON_DIR} \
+           -b "${LISTEN_ADDR}":8080 \
+           public:app
+
+  # 2 workers
+  # /get is long-running operation so it will be blocked
+  # /list and /evict are quite short.
+  gunicorn --chdir "${BACKUP_DAEMON_DIR}/gunicorn" \
+           ${params} \
+           -w ${PRIVATE_ENDPOINTS_WORKERS_NUMBER} \
+           --pythonpath ${BACKUP_DAEMON_DIR} \
+           -b "${LISTEN_ADDR}":8081 \
+           private:app
+
+  gunicorn --chdir "${BACKUP_DAEMON_DIR}/gunicorn" \
+           ${params} \
+           -w ${ARCHIVE_ENDPOINTS_WORKERS_NUMBER} \
+           --pythonpath ${BACKUP_DAEMON_DIR} \
+           -b "${LISTEN_ADDR}":8082 \
+           archive:app
+
+  # Granular backup are async, so one worker should be more than enough.
+  # Timeout is just 60 seconds since it should be enough to parse and validate any request.
+  gunicorn --chdir ${BACKUP_DAEMON_DIR}/granular \
+           ${params} \
+           -w 1 \
+           --pythonpath ${BACKUP_DAEMON_DIR}/granular \
+           -b "${LISTEN_ADDR}":9000 \
+           granular:app
+}
+
+function summon_daemon() {
+  python "${BACKUP_DAEMON_DIR}/postgres_backup_daemon.py"
+}
+
+function check_user(){
+    cur_user=$(id -u)
+    if [ "$cur_user" != "0" ]
+    then
+        echo "Adding randomly generated uid to passwd file..."
+        sed -i '/backup/d' /etc/passwd
+        if ! whoami &> /dev/null; then
+          if [ -w /etc/passwd ]; then
+            echo "${USER_NAME:-backup}:x:$(id -u):0:${USER_NAME:-backup} user:/backup-storage:/sbin/nologin" >> /etc/passwd
+          fi
+        fi
+    fi
+}
+
+check_ipv6
+check_user
+update_logging_configuration
+ride_unicorn
+summon_daemon
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/storage.py b/docker-backup-daemon/docker/postgres/storage.py
new file mode 100644
index 0000000..60b4154
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/storage.py
@@ -0,0 +1,529 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import abc
+import binascii
+import hashlib
+import io
+import json
+import logging
+import re
+import os
+import time
+import locks
+import utils
+import errno
+import subprocess
+import configs
+from retrying import retry
+from traceback import format_exception
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+
+class StorageLocationAlreadyExistsException(Exception):
+    pass
+
+
+VAULT_NAME_FORMAT = "%Y%m%dT%H%M"
+VAULT_DIRNAME_MATCHER = re.compile("\\d{8}T\\d{4}", re.IGNORECASE)
+# pg_{}_archive_{}
+ARCHIVE_NAME_MATCHER = re.compile("pg_(.*)_archive_(?P<name>[\da-f]+(\.[\da-f]+\.backup)?(\.partial)?(\.bk)?)$", re.IGNORECASE)
+# print(ARCHIVE_NAME_MATCHER.match("pg_common_archive_000000320000004600000066").group("name"))
+# print(ARCHIVE_NAME_MATCHER.match("pg_common_archive_0000001B000000100000003E.bk").group("name"))
+# print(ARCHIVE_NAME_MATCHER.match("pg_common_archive_0000003200000046000000A3.00000028.backup").group("name"))
+# print(ARCHIVE_NAME_MATCHER.match("pg_common_archive_000000310000004400000020.partial").group("name"))
+# print(ARCHIVE_NAME_MATCHER.match("pg_common_archive_0000001B0000001B00000015.partial.bk").group("name"))
+
+PG_CLUSTER_NAME = os.getenv("PG_CLUSTER_NAME")
+ARCHIVE_EVICT_POLICY = os.getenv("ARCHIVE_EVICT_POLICY")
+unit_pattern = re.compile("(?P<mult>\d*)(?P<unit>.+)")
+memory_pattern = re.compile("\d+(\.\d+)?(?P<unit>[kmgt][bB]?)?")
+memory_units = {
+    "ki": 1, "mi": 1024, "gi": 1048576,
+    "k": 1, "m": 1000, "g": 1000000,
+    "kb": 1, "mb": 1024, "gb": 1048576, "tb": 1073741824
+}
+time_pattern = re.compile("\d+(\.\d+)?(?P<unit>(ms|s|min|h|d))?")
+time_utins = {
+    "ms": 1, "s": 1000, "min": 60000, "h": 3600000, "d": 86400000
+}
+log = logging.getLogger("Storage")
+
+
+class StorageException(Exception):
+    def __init__(self, msg):
+        super(Exception, self).__init__(msg)
+
+#@retry(retry_on_exception=utils.retry_if_storage_error, wait_fixed=1000)
+def init_storage(storageRoot):
+    if configs.is_external_pg():
+        version_postfix = "external"
+        log.info("External Postgres is used, storage folder is \"external\"")
+    else:
+        version_postfix = utils.get_version_of_pgsql_server()
+        if version_postfix is None:
+            import fnmatch
+            pg_dirs = fnmatch.filter(os.listdir(storageRoot), 'pg*')
+            if not pg_dirs:
+                raise StorageException("No suitable directories are found, retrying")
+            log.info(f"Possible directories for backup store {pg_dirs}")
+            versions = sorted([int(re.search(r'\d+', x).group()) for x in pg_dirs], reverse=True)
+            version_postfix = "pg" + str(versions[0])
+
+    log.info(f"PostgreSQL server version is equal to {version_postfix}, "
+             f"so will save all backups in {version_postfix} dir")
+    storageRoot = os.path.join(storageRoot, version_postfix)
+
+    # folders
+    for folder in ["archive", "granular"]:
+        dirs_to_create = os.path.join(storageRoot, folder)
+        if not os.path.exists(dirs_to_create):
+            try:
+                os.makedirs(dirs_to_create)
+            except OSError as exc:
+                if exc.errno != errno.EEXIST:
+                    raise
+    if os.environ['STORAGE_TYPE'] not in ["s3", "swift"]:
+        storage = os.path.join(storageRoot, "granular")
+        for namespace in os.listdir(storage):
+            for backup_id in os.listdir(os.path.join(storage, namespace)):
+                status_file = os.path.join(storage, namespace, backup_id, "status.json")
+                if os.path.isfile(status_file):
+                    with open(status_file , "r+") as f:
+                        try:
+                            status_json = json.load(f)
+                        except ValueError as e:
+                            log.error("Failed to read the status file {} Error: {}".format(status_file,e))
+                        else:
+                            if status_json['status'] == 'In progress':
+                                log.info("Attempt to change the status Path: {} ".format(status_file))
+                                status_json['status'] = 'Failed'
+                                f.seek(0)
+                                f.write(json.dumps(status_json))
+                                f.truncate()
+
+    if os.environ['STORAGE_TYPE'] == "swift":
+        import storage_swift
+        return storage_swift.SwiftStorage(storageRoot)
+    if os.environ['STORAGE_TYPE'] == "s3":
+        import storage_s3
+        return storage_s3.AwsS3Storage(storageRoot)
+    if os.environ['STORAGE_TYPE'] == 'pgbackrest':
+        import storage_pgbackrest
+        print("Backrest storage init")
+        return storage_pgbackrest.BackRestStorage(storageRoot)
+    import storage_fs
+    return storage_fs.FSStorage(storageRoot)
+
+
+class Storage(metaclass=abc.ABCMeta):
+    __log = logging.getLogger("Storage")
+
+    @abc.abstractmethod
+    def list(self):
+        """
+        :return: list of available except locked one.
+        :rtype: list
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def size(self):
+        """
+        :return: occupied space size in bytes
+        :rtype: int
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def archive_size(self):
+        """
+        :return: occupied space size in bytes
+        :rtype: int
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def fs_space(self):
+        """
+        :return: tuple (free, total) space on mount point where is root folder located
+        :rtype: (int, int)
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def open_vault(self):
+        """        
+        :return:
+        :rtype: (str, dict, StringIO)
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def evict_vault(self, vault):
+        """
+        Removes vault and associated files from storage
+        :param vault:
+        :return:
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_is_file_exists(self, filename):
+        """
+        Internal method. Should not use outside storage.
+        :param filename: filename with path from entry point. For fs storage entrypoint is /backup-storage/. 
+        :return: 
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_delete(self, filename):
+        """
+        Internal method. Should not use outside storage.
+        :param filename: filename with path from entry point. For fs storage entrypoint is /backup-storage/. 
+        :return: 
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_get_as_stream(self, filename):
+        """
+        Internal method. Should not use outside storage.
+        :param filename: path to file from backup root.
+        :type filename: string
+        :return: stream with requested file
+        :rtype: io.RawIOBase
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_put_as_stream(self, filename, stream):
+        """
+        Internal method. Should not use outside storage.
+        :param filename: 
+        :type filename: string
+        :param stream:
+        :type stream: io.RawIOBase
+        :return: sha256 of processed stream
+        :rtype: string
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_get_file_size(self, filename):
+        """
+        Internal method. Should not use outside storage.
+        :param filename: path to file from backup root.
+        :type filename: string
+        :return: size of file in bytes
+        :rtype: int
+        """
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def prot_list_archive(self):
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def get_type(self):
+        raise NotImplementedError
+
+    @abc.abstractmethod
+    def get_type_id(self):
+        return -1
+
+    @abc.abstractmethod
+    def get_encryption(self):
+        raise NotImplementedError
+
+    def is_archive_evict_policy_set(self):
+        return ARCHIVE_EVICT_POLICY is not None
+
+    def evict(self, vault):
+        """
+        Removes vault and associated files from storage
+        :param vault:
+        :return:
+        """
+        self.evict_vault(vault)
+        self.evict_archive()
+
+    def get_oldest_backup(self):
+        """
+        :return:
+        :rtype: Vault
+        """
+        vaults = reversed(self.list())
+        for vault in vaults:
+            if not vault.is_failed() and vault.is_back_up_archive_exists():
+                return vault
+        return None
+
+    def evict_archive(self):
+        archives = self.prot_list_archive()
+        if archives:
+            self.__log.info("Stored archive list {}".format(archives))
+            if ARCHIVE_EVICT_POLICY:
+                ups = unit_pattern.search(ARCHIVE_EVICT_POLICY)
+                evict_rule_unit = ups.group("unit")
+                evict_rule_value = int(ups.group("mult"))
+                if evict_rule_unit.lower() in memory_units:
+                    value_kib = evict_rule_value * memory_units[evict_rule_unit.lower()]
+                    self.__evict_by_size(archives, 1024 * value_kib)
+                elif evict_rule_unit.lower() in time_utins:
+                    value_ms = evict_rule_value * time_utins[evict_rule_unit.lower()]
+                    self.__evict_by_time(archives, int((time.time() * 1000 - value_ms)))
+                else:
+                    self.__log.error(
+                        "Cannot parse eviction policy {}. "
+                        "Will use oldest backup to perform eviction.".
+                        format(ARCHIVE_EVICT_POLICY))
+                    self.__evict_by_oldest_backup(archives)
+
+            else:
+                self.__evict_by_oldest_backup(archives)
+
+    def __evict_by_oldest_backup(self, archives):
+        oldest_backup = self.get_oldest_backup()
+        self.__log.info("Oldest available backup {}".format(oldest_backup))
+        if oldest_backup:
+            bct = oldest_backup.create_timestamp()
+        else:
+            self.__log.warning(
+                "Does not have oldest backup. "
+                "Will use current time and remove all files in archive.")
+            bct = int(time.time() * 1000)
+        self.__evict_by_time(archives, bct)
+
+    def __evict_by_time(self, archives, keep_time):
+        self.__log.info("Eviction timestamp {}".format(keep_time))
+        for archive in archives:
+            self.__log.debug("Check WAL".format(archive))
+            if keep_time > archive.timestamp:
+                self.delete_archive(archive.filename)
+
+    def __evict_by_size(self, archives, keep_bytes):
+        self.__log.info("Eviction limit {} bytes".format(keep_bytes))
+        sorted_archives = sorted(list(archives), key=lambda t: t.timestamp, reverse=True)
+        occupied_space = 0
+        freed_space = 0
+        for archive in sorted_archives:
+            self.__log.debug("Check WAL".format(archive))
+            full_name = self.__get_fullfilename_for_archive(archive.filename)
+            size = self.prot_get_file_size(full_name)
+            occupied_space = occupied_space + size
+            if occupied_space > keep_bytes:
+                self.delete_archive(archive.filename)
+                freed_space = freed_space + size
+        self.__log.info("Occupied before: {} bytes. Freed: {} bytes.".format(occupied_space, freed_space))
+
+    def get_backup_as_stream(self, vault):
+        """
+        :return: stream with requested backup
+        :rtype: io.RawIOBase
+        """
+        self.__log.info("Get request for vault: %s" % vault)
+        backup_name = "{}/pg_{}_backup_{}.tar.gz".format(vault.get_id(), PG_CLUSTER_NAME, vault.get_id())
+
+        # for pg10 backups there is another naming convention `pg_backup_{timestamp}`
+        if not self.prot_is_file_exists(backup_name):
+            backup_name = "{}/pg_backup_{}.tar.gz".format(vault.get_id(), vault.get_id())
+
+        # encrypted backups
+        if not self.prot_is_file_exists(backup_name):
+            backup_name = "{}/pg_backup_{}_enc.tar.gz".format(vault.get_id(),
+                                                          vault.get_id())
+
+        return self.prot_get_as_stream(backup_name)
+
+    def __get_fullfilename_for_archive(self, filename):
+        """
+        Returns full filename from storage root in form like 'archive/pg_common_archive_00000000010000000001(_enc)?'
+        :param filename:
+        :return:
+        """
+        if self.get_encryption():
+            filename = filename + "_enc"
+        archive_full_name = "archive/pg_{}_archive_{}".format(PG_CLUSTER_NAME, filename)
+        return archive_full_name
+
+    def __get_fullfilename_for_archive_sha(self, filename):
+        sha_name = "archive/pg_{}_archive_{}.sha".format(PG_CLUSTER_NAME, filename)
+        return sha_name
+
+    def is_archive_exists(self, filename):
+        return self.prot_is_file_exists(self.__get_fullfilename_for_archive(filename))
+
+    def get_archive_as_stream(self, filename):
+        """
+        :return: stream with requested archive
+        :rtype: io.RawIOBase
+        """
+        self.__log.info("Get request for archive: %s" % filename)
+        return self.prot_get_as_stream(self.__get_fullfilename_for_archive(filename))
+
+    def put_archive_as_stream(self, filename, stream):
+        """
+        :return: stream with requested archive
+        :rtype: io.RawIOBase
+        """
+        self.__log.info("Put request for archive: %s" % filename)
+        return self.prot_put_as_stream(self.__get_fullfilename_for_archive(filename), stream)
+
+    def store_archive_checksum(self, filename, sha256):
+        sha_name = self.__get_fullfilename_for_archive_sha(filename)
+        to_store = str(binascii.crc32(sha256.encode())) + "_" + sha256
+        self.prot_put_as_stream(sha_name, io.BytesIO(to_store.encode()))
+
+    def delete_archive(self, filename):
+        self.__log.info("Delete request for archive: %s" % filename)
+        self.prot_delete(self.__get_fullfilename_for_archive(filename))
+        sha_filename = self.__get_fullfilename_for_archive_sha(filename)
+        if self.prot_is_file_exists(sha_filename):
+            self.prot_delete(sha_filename)
+
+    def get_sha256sum_for_archive(self, filename):
+        sha_name = self.__get_fullfilename_for_archive_sha(filename)
+        if self.prot_is_file_exists(sha_name):
+            stream = self.prot_get_as_stream(sha_name)
+            with stream as f:
+                result = f.read().strip()
+            try:
+                crc = int(result.split("_")[0])
+                sha256 = result.split("_")[1]
+                if (crc == binascii.crc32(sha256.encode())):
+                    return sha256
+                else:
+                    self.__log.warning("CRC32 check failed for sha file {}.".format(filename))
+                    return None
+            except Exception as e:
+                return None
+        return None
+
+    def calculate_sha256sum_for_archive(self, filename):
+        sha256 = hashlib.sha256()
+        chunk_size = 4096
+        stream = self.get_archive_as_stream(filename)
+        with stream as f:
+            while True:
+                data = f.read(chunk_size)
+                if len(data) == 0:
+                    stream.close()
+                    self.__log.info(
+                        "Calculated sha256 {} for local archive".format(
+                            sha256.hexdigest()))
+                    return sha256.hexdigest()
+                sha256.update(data)
+
+    def get_backup_in_progress_metrics(self):
+        is_backup_in_progress = os.path.isfile(locks.get_backup_lock_file_path())
+        in_progress_metrics = {'backup_is_in_progress': is_backup_in_progress}
+
+        if is_backup_in_progress:
+            with open(locks.get_backup_lock_file_path(), "r") as f:
+                in_progress_metrics.update(json.load(f))
+
+        return in_progress_metrics
+
+
+class Archive:
+    def __init__(self, filename, timestamp):
+        self.filename = filename
+        self.timestamp = timestamp
+
+    def __repr__(self):
+        return "Archive name: {}, timestamp: {}".format(self.filename, self.timestamp)
+
+    def __eq__(self, other):
+        if isinstance(other, Archive):
+            return self.filename == other.filename
+        return False
+
+    def __ne__(self, other):
+        return not self.__eq__(other)
+
+
+class Vault(metaclass=abc.ABCMeta):
+    def __init__(self):
+        super(Vault, self).__init__()
+
+        self.metrics = {}
+
+    @abc.abstractmethod
+    def get_id(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def get_folder(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def load_metrics(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def is_locked(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def is_failed(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def is_done(self): raise NotImplemented
+
+    @abc.abstractmethod
+    def create_time(self): raise NotImplementedError
+
+    @abc.abstractmethod
+    def is_back_up_archive_exists(self): raise NotImplementedError
+
+    def create_timestamp(self):
+        return int(self.create_time() * 1000)
+
+    def __enter__(self):
+        self.start_timestamp = create_timestamp_as_long()
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        end_backup_timestamp = create_timestamp_as_long()
+
+        self.metrics["end_backup_timestamp"] = end_backup_timestamp
+        self.metrics["spent_time"] = end_backup_timestamp - self.start_timestamp
+
+    def __lt__(self, other):
+        return self.create_timestamp() < other.create_timestamp()
+
+    def __gt__(self, other):
+        return self.create_timestamp() > other.create_timestamp()
+
+    def to_json(self):
+        metrics = self.load_metrics()
+
+        result = {
+            "id": self.get_id(),
+            "failed": self.is_failed(),
+            "locked": self.is_locked(),
+            "ts": self.create_timestamp(),
+            "metrics": metrics
+        }
+
+        end_backup_timestamp = metrics.get("end_backup_timestamp")
+        if end_backup_timestamp:
+            result["end_timestamp_ago"] = create_timestamp_as_long() - end_backup_timestamp
+
+        return result
+
+
+def create_timestamp_as_long():
+    return int(time.time() * 1000)
diff --git a/docker-backup-daemon/docker/postgres/storage_fs.py b/docker-backup-daemon/docker/postgres/storage_fs.py
new file mode 100644
index 0000000..53abf4b
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/storage_fs.py
@@ -0,0 +1,290 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import io
+
+import errno
+import os
+from datetime import datetime
+import logging
+from traceback import format_exception
+import time
+import json
+import fsutil
+import storage
+from storage import VAULT_NAME_FORMAT, VAULT_DIRNAME_MATCHER, ARCHIVE_NAME_MATCHER, \
+    StorageLocationAlreadyExistsException
+import utils
+import encryption
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+PG_CLUSTER_NAME = os.getenv("PG_CLUSTER_NAME")
+STORAGE_ROOT = os.getenv("STORAGE_ROOT")
+
+
+class FSStorage(storage.Storage):
+    __log = logging.getLogger("FSStorage")
+
+    def __failed_filepath(self):
+        return ".failed"
+
+    def __init__(self, root):
+        self.__log.info("Init storage object with storage root: %s" % root)
+        self.root = root
+        if utils.get_encryption():
+            self.encryption = True
+        else:
+            self.encryption = False
+
+    def fail(self, backup_id):
+        self.__log.info("create .failed file: {}".format(os.path.join(self.root, backup_id, self.__failed_filepath())))
+        open(os.path.join(self.root, backup_id, self.__failed_filepath()), "w").close()
+
+    def get_encryption(self):
+        return self.encryption
+
+    def list(self):
+        if os.path.exists(self.root):
+            vaults = [FSVault(self.root + "/" + dirname)
+                      for dirname in os.listdir(self.root)
+                      if VAULT_DIRNAME_MATCHER.fullmatch(dirname) is not None]
+            vaults.sort(key=lambda v: v.create_time())
+            return vaults
+        else:
+            return []
+
+    def size(self):
+        """ Returns whole storage size in bytes """
+        return fsutil.get_folder_size(self.root)
+
+    def archive_size(self):
+        """ Returns whole storage size in bytes """
+        return fsutil.get_folder_size(os.path.join(self.root, "archive"))
+
+    def fs_space(self):
+        """ Returns tuple (free, total) space on mount point where is root folder located """
+        return fsutil.get_fs_space(self.root)
+
+    def open_vault(self, backup_id):
+        """
+        :return:
+        :rtype: (str, dict, StringIO)
+        """
+        return FSVault("%s/%s" % (self.root, backup_id))
+
+    def evict_vault(self, vault):
+        self.__log.info("Evict vault: %s" % vault)
+        self.__log.debug("Delete folder: %s" % vault.folder)
+        for root, dirs, files in os.walk(vault.folder, topdown=False):
+            for name in files:
+                try:
+                    os.remove(os.path.join(root, name))
+                except OSError as e:  # passing possible problems with permission
+                    self.__log.exception(e)
+            for name in dirs:
+                try:
+                    os.rmdir(os.path.join(root, name))
+                except OSError as e:
+                    self.__log.exception(e)
+        try:
+            os.rmdir(vault.folder)
+        except OSError as e:
+            self.__log.exception(e)
+
+    def prot_is_file_exists(self, filename):
+        self.__log.debug("Check for file: %s" % filename)
+        return os.path.isfile("{}/{}".format(self.root, filename))
+
+    def prot_delete(self, filename):
+        self.__log.info("Delete file: %s" % filename)
+        os.remove("{}/{}".format(self.root, filename))
+
+    def prot_get_as_stream(self, filename):
+        """
+        :param filename: path to file from backup root.
+        :type filename: string
+        :return: stream with requested file
+        :rtype: io.RawIOBase
+        """
+        self.__log.info("Get request for file: %s" % filename)
+        full_file_path = "{}/{}".format(self.root, filename)
+
+        return encryption.FileWrapper(full_file_path, self.encryption) \
+            .get_file_stream()
+
+    def prot_put_as_stream(self, filename, stream):
+        """
+        :param filename: 
+        :type filename: string
+        :param stream:
+        :type stream: io.RawIOBase
+        :return: sha256 of processed stream
+        :rtype: string
+        """
+        self.__log.info("Put request for file: %s" % filename)
+        fs_filename = "{}/{}".format(self.root, filename)
+        if not os.path.exists(os.path.dirname(fs_filename)):
+            try:
+                os.makedirs(os.path.dirname(fs_filename))
+            except OSError as exc:
+                if exc.errno != errno.EEXIST:
+                    raise
+        return encryption.FileWrapper(fs_filename, self.encryption) \
+            .put_file_stream(stream)
+
+    def prot_get_file_size(self, filename):
+        # self.__log.info("Size request for file: %s" % filename)
+        if os.path.exists("{}/{}".format(self.root, filename)):
+            return os.path.getsize("{}/{}".format(self.root, filename))
+        return 0
+
+    def prot_list_archive(self):
+        archive_root = os.path.join(self.root, "archive")
+        if os.path.exists(archive_root):
+            self.__log.debug("Archive directory listing {}".format(os.listdir(archive_root)))
+            archives = [storage.Archive(
+                ARCHIVE_NAME_MATCHER.match(filename).group("name"),
+                os.path.getmtime(os.path.join(archive_root, filename)) * 1000)
+                for filename in os.listdir(archive_root)
+                if ARCHIVE_NAME_MATCHER.match(filename) is not None]
+            return archives
+        else:
+            self.__log.debug("Archive directory does not exist")
+            return []
+
+    def get_type(self):
+        return "Volume"
+
+    def get_type_id(self):
+        return 0
+
+
+class FSVault(storage.Vault):
+    __log = logging.getLogger("FSVaultLock")
+
+    def __init__(self, folder):
+        super(FSVault, self).__init__()
+
+        self.folder = folder
+        self.metrics_filepath = self.folder + "/.metrics"
+        self.console = StringIO()
+
+    def get_id(self):
+        return os.path.basename(self.folder)
+
+    def get_folder(self):
+        return self.folder
+
+    def load_metrics(self):
+        self.__log.debug("Load metrics from: %s" % self.metrics_filepath)
+
+        if not os.path.isfile(self.metrics_filepath):
+            self.__log.warning("Metrics file: {} does not exists.".format(self.metrics_filepath))
+            return {}
+
+        with open(self.metrics_filepath, "r") as f:
+            try:
+                return json.load(f)
+            except Exception as e:
+                self.__log.exception(e)
+                self.__log.warning("Cannot load metrics file: {}. Metrics file can be damaged.".format(self.metrics_filepath))
+                return {}
+
+    def __lock_filepath(self):
+        return self.folder + "/.lock"
+
+    def __failed_filepath(self):
+        return self.folder + "/.failed"
+
+    def __console_filepath(self):
+        return self.folder + "/.console"
+
+    def __metrics_filepath(self):
+        return self.folder + "/.metrics"
+
+    def is_locked(self):
+        return os.path.exists(self.__lock_filepath())
+
+    def is_failed(self):
+        return os.path.exists(self.__failed_filepath())
+
+    def is_done(self):
+        if not os.path.isfile(self.__metrics_filepath()):
+            return False
+        with open(self.__metrics_filepath(), "r") as f:
+            j = json.load(f)
+        return j['exit_code'] == 0
+
+    def is_back_up_archive_exists(self):
+        # for pg10 backups there is another naming convention `pg_backup_{timestamp}`
+        return os.path.exists(self.folder + "/pg_{}_backup_{}.tar.gz".format(PG_CLUSTER_NAME, self.get_id())) \
+               or os.path.exists(self.folder + "/pg_backup_{}.tar.gz".format(self.get_id())) \
+               or os.path.exists(self.folder + "/pg_{}_backup_{}_enc.tar.gz".format(PG_CLUSTER_NAME, self.get_id())) \
+               or os.path.exists(self.folder + "/pg_backup_{}_enc.tar.gz".format(self.get_id()))
+
+    def __enter__(self):
+        self.__log.info("Init next vault: %s" % self.folder)
+        super(FSVault, self).__enter__()
+
+        if not os.path.exists(self.folder):
+            os.makedirs(self.folder)
+        else:
+            raise StorageLocationAlreadyExistsException("Destination backup folder already exists: %s" % self.folder)
+
+        self.__log.info("Create .lock file in vault: %s" % self.folder)
+        fsutil.touch(self.__lock_filepath())
+
+        return self.folder, self.metrics, self.console
+
+    def create_time(self):
+        foldername = os.path.basename(self.folder)
+        d = datetime.strptime(foldername, VAULT_NAME_FORMAT)
+        return time.mktime(d.timetuple())
+
+    def __exit__(self, tpe, exception, tb):
+        self.__log.info("Close vault")
+        self.__log.info("Save metrics to: %s" % self.metrics_filepath)
+
+        super(FSVault, self).__exit__(tpe, exception, tb)
+
+        self.metrics["size"] = fsutil.get_folder_size(self.folder)
+
+        if exception:
+            fsutil.touch(self.__failed_filepath())
+            e = "\n".join(format_exception(tpe, exception, tb))
+            self.__log.info("Don't remove vault .lock due exception in nested code")
+            self.__log.debug("Something wrong happened inside block uses vault: " + e)
+            self.metrics["exception"] = e
+            self.fail()
+
+        with open(self.metrics_filepath, "w") as f:
+            json.dump(self.metrics, f)
+
+        console_logs = self.console.getvalue()
+        self.console.close()
+        with open(self.__console_filepath(), "w") as f:
+            f.write(console_logs)
+
+        self.__log.info("Remove lock for %s" % self.get_id())
+        os.unlink(self.__lock_filepath())
+
+    def __repr__(self):
+        return "Vault(%s)" % os.path.basename(self.folder)
+
+    def fail(self):
+        open(self.__failed_filepath(), "w").close()
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/storage_pgbackrest.py b/docker-backup-daemon/docker/postgres/storage_pgbackrest.py
new file mode 100644
index 0000000..72cba48
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/storage_pgbackrest.py
@@ -0,0 +1,260 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import hashlib
+import io
+import json
+import logging
+import time
+from datetime import datetime
+from traceback import format_exception
+
+import os
+
+import storage
+from storage import VAULT_NAME_FORMAT, StorageLocationAlreadyExistsException, VAULT_DIRNAME_MATCHER
+import fsutil
+import requests
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+import subprocess
+
+class BackRestStorage(storage.Storage):
+    __log = logging.getLogger("PgBackRestFS")
+
+    def __init__(self, root):
+        self.__log.info("Init storage object with storage root: %s" % root)
+        self.root = root
+
+    def list(self):
+
+        response = requests.get("http://backrest-headless:3000/list").json()
+        print(response)
+        vault = []
+        for backup in response:
+            annotation = backup.get('annotation') or {}
+            if annotation.get('timestamp'):
+                vault.append(BackRestVault(annotation['timestamp']))
+            else:
+                self.__log.warning("Found backup without timestamp in annotation: %s", backup)
+        return vault
+
+    def size(self):
+        """ Returns whole storage size in bytes """
+        return 0
+
+    def archive_size(self):
+        """ Returns whole storage size in bytes """
+        return 0
+
+    def fs_space(self):
+        """ Returns tuple (free, total) space on mount point where is root folder located """
+        return (1, 1)
+
+    def get_type(self):
+        return "PgBackRest"
+
+    def get_type_id(self):
+        return 2
+
+    def open_vault(self, backup_id):
+        """
+
+        :return:
+        :rtype: (str, dict, StringIO)
+        """
+        vault = BackRestVault("%s/%s" % (self.root, backup_id))
+        print(f'RETURNING VAULT {vault}')
+        return vault
+
+    def prot_get_as_stream(self, filename):
+        pass
+
+    def prot_get_file_size(self, filename):
+        pass
+
+    def prot_is_file_exists(self, filename):
+        pass
+
+    def prot_list_archive(self):
+        pass
+
+    def prot_put_as_stream(self, filename, stream):
+        pass
+
+    def size(self):
+        return 0
+
+    def evict_vault(self, vault):
+        self.__log.info("Evict vault: %s" % vault)
+        self.__log.debug("Delete folder: %s" % vault.folder)
+        for root, dirs, files in os.walk(vault.folder, topdown=False):
+            for name in files:
+                try:
+                    os.remove(os.path.join(root, name))
+                except OSError as e:  # passing possible problems with permission
+                    self.__log.exception(e)
+            for name in dirs:
+                try:
+                    os.rmdir(os.path.join(root, name))
+                except OSError as e:
+                    self.__log.exception(e)
+        try:
+            os.rmdir(vault.folder)
+        except OSError as e:
+            self.__log.exception(e)
+
+    def get_encryption(self):
+        pass
+
+    def prot_delete(self, filename):
+        pass
+
+class BackRestVault(storage.Vault):
+    __log = logging.getLogger("BackRestVault")
+
+    def __init__(self, timestamp=""):
+        super(BackRestVault, self).__init__()
+
+        self.folder = timestamp
+        self.metrics_filepath = self.folder + "/.metrics"
+        self.console = StringIO()
+
+        print(f'INIT VAULT {self.folder, self.metrics_filepath, self.console}')
+
+    def get_id(self):
+        return os.path.basename(self.folder)
+
+    def get_folder(self):
+        pass
+
+    #
+    #
+    def __lock_filepath(self):
+        return self.folder + ".lock"
+    #
+    # def __failed_filepath(self):
+    #     return self.folder + ".failed"
+    #
+    # def __metrics_filepath(self):
+    #     return self.folder + ".metrics"
+    #
+    # def __console_filepath(self):
+    #     return self.folder + ".console"
+    #
+    # def __is_file_exists(self, path):
+    #     return subprocess.call(["/opt/backup/scli", "get", path], env=scli_env) == "Object Not Found"
+    #
+    # def __backup_archive_file_path(self):
+    #     return "{}/pg_{}_backup_{}.tar.gz".format(self.get_folder(), PG_CLUSTER_NAME, self.get_id())
+    #
+    # def is_locked(self):
+    #     if self.cache_state:
+    #         if not self.cached_state:
+    #             self.__cache_current_state()
+    #         return self.cached_state["is_locked"]
+    #     return self.__is_file_exists("{}/{}".format(CONTAINER, self.__lock_filepath()))
+    #
+    def is_failed(self):
+
+        return {}
+    #
+    def is_done(self):
+        return {}
+    #
+    def is_back_up_archive_exists(self):
+        return True
+
+    def is_locked(self):
+        #TODO: work with root object from pgbackrest
+        pass
+
+    def load_metrics(self):
+        return {}
+
+    def __enter__(self):
+        self.__log.info("Init next vault: %s" % self.folder)
+        super(BackRestVault, self).__enter__()
+
+        if not os.path.exists(self.folder):
+            os.makedirs(self.folder)
+        else:
+            raise StorageLocationAlreadyExistsException("Destination backup folder already exists: %s" % self.folder)
+
+        self.__log.info("Create .lock file in vault: %s" % self.folder)
+        fsutil.touch(self.__lock_filepath())
+
+        return self.folder, self.metrics, self.console
+
+    
+    def create_time(self):
+        foldername = self.get_id()
+
+        # If foldername has the 'backup-' prefix, return a default time
+        if foldername.startswith("backup-"):
+            return time.time()  # Returning the current timestamp as a fallback
+
+        try:
+            d = datetime.strptime(foldername, VAULT_NAME_FORMAT)
+            return time.mktime(d.timetuple())
+        except ValueError as e:
+            self.__log.error("Failed to parse datetime from foldername: %s, error: %s", foldername, str(e))
+            return time.time()  # Returning the current timestamp as a fallback
+    
+    #
+    # def __exit__(self, tpe, exception, tb):
+    #     self.__log.info("Close vault")
+    #     self.__log.info("Save metrics to: %s" % self.__metrics_filepath())
+    #
+    #     super(PgBackRestVault, self).__exit__(tpe, exception, tb)
+    #
+    #     backup_name = "{}/pg_{}_backup_{}.tar.gz".format(self.get_folder(), PG_CLUSTER_NAME, self.get_id())
+    #     size_str = subprocess.check_output(
+    #         ["/opt/backup/scli ls -l {} | grep {} | awk '{{A+=$2}} END{{print A}}'".format(CONTAINER_SEG, backup_name)]
+    #         , shell=True
+    #         , env=scli_env)
+    #     try:
+    #         self.metrics["size"] = int(size_str)
+    #     except Exception as e:
+    #         self.__log.error(e)
+    #         self.metrics["size"] = -1
+    #
+    #     if exception:
+    #         subprocess.call("echo .failed > /tmp/.failed", shell=True)
+    #         subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.failed", "{}/{}".format(CONTAINER, self.__failed_filepath())], env=scli_env)
+    #
+    #         e = "\n".join(format_exception(tpe, exception, tb))
+    #         self.__log.info("Don't remove vault .lock due exception in nested code")
+    #         self.__log.debug("Something wrong happened inside block uses vault: " + e)
+    #         self.metrics["exception"] = e
+    #
+    #     with open("/tmp/.metrics", "w") as f:
+    #         json.dump(self.metrics, f)
+    #     subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.metrics", "{}/{}".format(CONTAINER, self.__metrics_filepath())], env=scli_env)
+    #
+    #     console_logs = self.console.getvalue()
+    #     self.console.close()
+    #     with open("/tmp/.console", "w") as f:
+    #         f.write(console_logs)
+    #     subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.console", "{}/{}".format(CONTAINER, self.__console_filepath())], env=scli_env)
+    #
+    #     self.__log.info("Remove lock for %s" % self.get_id())
+    #     subprocess.check_call(["/opt/backup/scli", "delete", "{}/{}".format(CONTAINER, self.__lock_filepath())], env=scli_env)
+    #
+    # def __repr__(self):
+    #     return "Vault(%s)" % self.get_id()
diff --git a/docker-backup-daemon/docker/postgres/storage_s3.py b/docker-backup-daemon/docker/postgres/storage_s3.py
new file mode 100644
index 0000000..8451173
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/storage_s3.py
@@ -0,0 +1,462 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import subprocess
+
+import boto3
+import botocore
+import hashlib
+import io
+
+import urllib3
+
+import botocore.exceptions
+import errno
+import os
+from datetime import datetime
+import logging
+from traceback import format_exception
+import time
+import json
+import storage
+from storage import VAULT_NAME_FORMAT, VAULT_DIRNAME_MATCHER, StorageLocationAlreadyExistsException, ARCHIVE_NAME_MATCHER
+from retrying import retry
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+CONTAINER = os.getenv("CONTAINER")
+CONTAINER_SEG = "{}_segments".format(CONTAINER)
+PG_CLUSTER_NAME = os.getenv("PG_CLUSTER_NAME")
+
+RETRY_COUNT = 10
+RETRY_WAIT = 1000
+
+urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
+
+class StreamWrapper(io.RawIOBase):
+
+    def __init__(self, object_body):
+        self.object_body = object_body
+        self.internal_closed = False
+
+    def close(self, *args, **kwargs):
+        if not self.internal_closed:
+            self.object_body.close()
+            self.internal_closed = True
+
+    def read(self, *args, **kwargs):
+        return self.object_body.read(*args, **kwargs)
+
+    def __exit__(self, *args, **kwargs):
+        self.close()
+
+    def __enter__(self, *args, **kwargs):
+        return super(StreamWrapper, self).__enter__(*args, **kwargs)
+
+
+class AwsS3Storage(storage.Storage):
+
+    __log = logging.getLogger("AwsS3Storage")
+
+    def __init__(self, root):
+        self.__log.info("Init storage object with storage root: %s" % root)
+        self.root = root
+        self.aws_prefix = "%s/" % (os.getenv("AWS_S3_PREFIX", ""))
+
+    def get_encryption(self):
+        pass
+
+    def prot_put_as_stream(self, filename, stream):
+        """
+        :param filename: 
+        :type filename: string
+        :param stream:
+        :type stream: io.RawIOBase
+        :return: sha256 of processed stream
+        :rtype: string
+        """
+        self.__log.info("Put request for file: %s" % filename)
+        sha256 = hashlib.sha256()
+        md5 = hashlib.md5()
+        chunk_size = 4096
+        fs_filename = "{}/{}".format("/tmp", filename)
+        if not os.path.exists(os.path.dirname(fs_filename)):
+            try:
+                os.makedirs(os.path.dirname(fs_filename))
+            except OSError as exc:
+                if exc.errno != errno.EEXIST:
+                    raise
+        with io.FileIO(fs_filename, "w", closefd=True) as target:
+            while True:
+                data = stream.read(chunk_size)
+                if len(data) == 0:
+                    stream.close()
+                    self.__log.info("Processed stream with sha256 {}".format(sha256.hexdigest()))
+                    break
+                sha256.update(data)
+                md5.update(data)
+                target.write(data)
+
+        self.__log.info("Start uploading: %s" % filename)
+        # todo[anin] replace implementation
+        # AwsS3Vault.get_s3_client().upload_fileobj(data, CONTAINER, filename)
+        AwsS3Vault.get_s3_client().upload_file(fs_filename, CONTAINER, filename)
+        os.remove(fs_filename)
+        return sha256.hexdigest()
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def prot_get_as_stream(self, filename):
+        self.__log.info("Get stream request for file: %s" % self.aws_prefix + filename)
+        object_body = AwsS3Vault.get_s3_resource().Bucket(CONTAINER).Object(self.aws_prefix + filename).get()['Body']
+        return StreamWrapper(object_body)
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def prot_delete_bundle(self, filename):
+        objects_to_delete = AwsS3Vault.get_s3_client().list_objects(Bucket=CONTAINER, Prefix=self.aws_prefix + filename)
+        for obj in objects_to_delete.get('Contents', []):
+            AwsS3Vault.get_s3_client().delete_object(Bucket=CONTAINER, Key=obj['Key'])
+
+    def prot_delete(self, filename):
+        self.prot_delete_bundle(filename)
+
+    def prot_is_file_exists(self, filename):
+        exists = True
+        try:
+            AwsS3Vault.get_s3_resource().Object(CONTAINER, self.aws_prefix + filename).get()
+        except botocore.exceptions.ClientError as e:
+            if e.response['Error']['Code'] == 'NoSuchKey':
+                exists = False
+            else:
+                raise
+
+        return exists
+
+    def prot_get_file_size(self, filename):
+        if self.prot_is_file_exists(filename):
+            return int(AwsS3Vault.get_s3_resource().Object(CONTAINER, filename).get()['Size'])
+        return 0
+
+    def is_valid_backup_id(self, backup_id):
+        try:
+            datetime.strptime(backup_id, storage.VAULT_NAME_FORMAT)
+            return True
+        except ValueError:
+            return False
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def list(self):
+        bucket = AwsS3Vault.get_s3_client().list_objects(Bucket=CONTAINER)
+        aws_s3_vault_listing = []
+        if 'Contents' in bucket:
+            # Collect backups ids only
+            aws_s3_vault_listing = [obj["Key"].split('/', 2)[1]
+                                    for obj in bucket['Contents']
+                                    if '/' in obj['Key'] and self.is_valid_backup_id(obj["Key"].split('/', 2)[1])]
+
+        vaults = [
+            AwsS3Vault(backup_id
+                       , bucket=CONTAINER
+                       , cluster_name=PG_CLUSTER_NAME
+                       , cache_enabled=True
+                       , aws_s3_bucket_listing=(bucket['Contents'] if 'Contents' in bucket else None))
+            for backup_id in aws_s3_vault_listing]
+        vaults.sort(key=lambda v: v.create_time())
+        return vaults
+
+    def size(self):
+        """ Returns whole storage size in bytes """
+        total_size = 0
+        bucket = AwsS3Vault.get_s3_client().list_objects(Bucket=CONTAINER)
+
+        if 'Contents' not in bucket:
+            return 0
+
+        for obj in bucket["Contents"]:
+            total_size += obj['Size']
+        return total_size
+
+    def archive_size(self):
+        """ Returns whole storage size in bytes """
+        total_size = 0
+        bucket = AwsS3Vault.get_s3_client().list_objects(Bucket=CONTAINER, Prefix="archive/")
+
+        if 'Contents' not in bucket:
+            return 0
+
+        for obj in bucket["Contents"]:
+            total_size += obj['Size']
+        return total_size
+
+    def fs_space(self):
+        """ Returns tuple (free, total) space on mount point where is root folder located """
+        return (1, 1)
+
+    def open_vault(self, backup_id):
+        """
+        
+        :return:
+        :rtype: (str, dict, StringIO)
+        """
+        return AwsS3Vault("%s" % (datetime.now().strftime(VAULT_NAME_FORMAT)), CONTAINER, cluster_name=PG_CLUSTER_NAME)
+
+    def evict_vault(self, vault):
+        self.__log.info("Evict vault: %s" % vault)
+
+        backup_id = vault.get_folder()
+        backup_name = "{}/{}/pg_backup_{}.tar.gz".format(self.aws_prefix, backup_id, vault.get_id())
+        try:
+            self.prot_delete_bundle(backup_id)
+        except botocore.exceptions.ClientError as e:
+            return "Not Found"
+
+    def prot_list_archive(self):
+        bucket = AwsS3Vault.get_s3_client().list_objects(Bucket=CONTAINER, Prefix="archive/", Delimiter="/")
+        aws_s3_archive_listing = []
+        if 'Contents' in bucket:
+            # Collect archive ids only
+            aws_s3_archive_listing = [obj
+                  for obj in bucket['Contents']
+                  if '/' in obj['Key'] and ARCHIVE_NAME_MATCHER.match(obj["Key"].split('/', 1)[1])]
+        self.__log.info("Archives: {}".format(aws_s3_archive_listing))
+        if aws_s3_archive_listing:
+            archives = [storage.Archive(
+                ARCHIVE_NAME_MATCHER.match(archive["Key"].split('/', 1)[1]).group("name"),
+                1000 * int(archive["LastModified"].strftime("%s")))
+                for archive in aws_s3_archive_listing]
+            self.__log.info("Parsed archives: {}".format(archives))
+            return archives
+        else:
+            return []
+
+    def get_type(self):
+        return "AWS S3"
+
+    def get_type_id(self):
+        return 1
+
+
+class AwsS3VaultCreationException(Exception):
+    pass
+
+
+class AwsS3Vault(storage.Vault):
+    __log = logging.getLogger("AwsS3Vault")
+
+    @staticmethod
+    def get_s3_resource():
+        return boto3.resource("s3",
+                              region_name=os.getenv("AWS_DEFAULT_REGION") if os.getenv("AWS_DEFAULT_REGION") else None,
+                              endpoint_url=os.getenv("AWS_S3_ENDPOINT_URL"),
+                              aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
+                              aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
+                              verify=(False if os.getenv("AWS_S3_UNTRUSTED_CERT", "false").lower() == "true" else None))
+
+    @staticmethod
+    def get_s3_client():
+        return AwsS3Vault.get_s3_resource().meta.client
+
+    def __init__(self, backup_id, bucket, cluster_name=None, cache_enabled=False,
+                 aws_s3_bucket_listing=None):
+        super(AwsS3Vault, self).__init__()
+
+        self.backup_id = backup_id
+        self.aws_prefix = os.getenv("AWS_S3_PREFIX", "")
+        self.bucket = bucket
+        self.console = None
+        self.cluster_name = cluster_name
+        self.cache_enabled = cache_enabled
+        self.cached_state = {}
+        self.aws_s3_bucket_listing = aws_s3_bucket_listing
+        self.backup_name = "{}/{}/pg_backup_{}.tar.gz".format(self.aws_prefix, self.get_folder(), self.get_id())
+
+    def __get_s3_bucket(self):
+        return AwsS3Vault.get_s3_resource().Bucket(self.bucket)
+
+    def __cache_current_state(self):
+        self.__log.debug("Cache current state")
+        # todo[anin] fix cache
+        if self.aws_s3_bucket_listing:
+            # aws_s3_bucket_listing = [
+            #     {
+            #         'LastModified': datetime.datetime(2017, 6, 27, 10, 40, 8, 957000, tzinfo=tzlocal()),
+            #         'ETag': '"b3b1d912e348a08d57cb9b3ab5a92bc0"',
+            #         'StorageClass': 'STANDARD',
+            #         'Key': '20170627T1040.console',
+            #         'Owner': {'DisplayName': 'platform', 'ID': 'platform'},
+            #         'Size': 418
+            #     }
+            #         ...
+            # ]
+            self.cached_state["is_locked"] = len([x for x in self.aws_s3_bucket_listing if self.__lock_filepath() in x['Key']]) == 1
+            self.cached_state["is_failed"] = len([x for x in self.aws_s3_bucket_listing if self.__failed_filepath() in x['Key']]) == 1
+        else:
+            self.cached_state["is_locked"] = self.__is_file_exists(self.bucket, self.__lock_filepath())
+            self.cached_state["is_failed"] = self.__is_file_exists(self.bucket, self.__failed_filepath())
+        self.__log.debug("State: {}".format(self.cached_state))
+
+    def get_id(self):
+        return os.path.basename(self.backup_id)
+
+    def get_folder(self):
+        return self.backup_id
+
+    def __get_backup_name(self):
+        return self.backup_name
+
+    @retry(stop_max_attempt_number=RETRY_COUNT, wait_fixed=RETRY_WAIT)
+    def __load_metrics_from_s3(self):
+        try:
+            metrics = AwsS3Vault.get_s3_resource().Object(self.bucket, self.__metrics_filepath()).get()
+            return json.load(metrics["Body"])
+        except Exception as e:
+            self.__log.exception(e)
+            self.__log.warning("Cannot load metrics from s3. Backup can be damaged")
+            return {}
+
+    def load_metrics(self):
+        self.__log.debug("Load metrics from: %s" % self.__metrics_filepath())
+        if self.cache_enabled:
+            if "metrics" not in self.cached_state:
+                self.cached_state["metrics"] = self.__load_metrics_from_s3()
+            return self.cached_state["metrics"]
+        else:
+            return self.__load_metrics_from_s3()
+
+
+    def __lock_filepath(self):
+        return "%s/%s/%s" % (self.aws_prefix, self.backup_id, self.backup_id + ".lock")
+
+    def __failed_filepath(self):
+        return "%s/%s/%s" % (self.aws_prefix, self.backup_id, self.backup_id + ".failed")
+
+    def __metrics_filepath(self):
+        return "%s/%s/%s" % (self.aws_prefix, self.backup_id, self.backup_id + ".metrics")
+
+    def __console_filepath(self):
+        return "%s/%s/%s" % (self.aws_prefix, self.backup_id, self.backup_id + ".console")
+
+    def __is_file_exists(self, bucket, obj):
+        exists = True
+        try:
+            AwsS3Vault.get_s3_resource().Object(bucket, obj).get()
+        except botocore.exceptions.ClientError as e:
+            if e.response['Error']['Code'] == 'NoSuchKey':
+                exists = False
+            else:
+                raise
+
+        return exists
+
+    def is_locked(self):
+        if self.cache_enabled:
+            if not self.cached_state:
+                self.__cache_current_state()
+            if "is_locked" in self.cached_state.keys():
+                return self.cached_state["is_locked"]
+
+        return self.__is_file_exists(self.bucket, self.__lock_filepath())
+
+    def is_failed(self):
+        if self.cache_enabled:
+            if not self.cached_state:
+                self.__cache_current_state()
+            if "is_failed" in self.cached_state.keys():
+                return self.cached_state["is_failed"]
+        return self.__is_file_exists(self.bucket, self.__failed_filepath())
+
+    def is_done(self):
+        if not self.__is_file_exists(CONTAINER, self.__metrics_filepath()):
+            self.__log.info(self.__is_file_exists)
+            return False
+        j = self.__load_metrics_from_s3()
+        self.__log.info(j)
+        return j['exit_code'] == 0
+
+    def is_back_up_archive_exists(self):
+        return self.__is_file_exists(self.bucket, self.backup_name)
+
+    def __enter__(self):
+        self.__log.info("Init next vault: %s" % self.backup_id)
+        super(AwsS3Vault, self).__enter__()
+
+        if self.__is_file_exists(self.bucket, self.__metrics_filepath()):
+            raise AwsS3VaultCreationException("Destination backup folder already exists: %s" % self.backup_id)
+
+        self.__log.info("Create .lock file in vault: %s" % self.backup_id)
+        lock_marker = "/tmp/.lock"
+        subprocess.call("echo .lock > %s" % lock_marker, shell=True)
+        self.__get_s3_bucket().upload_file(lock_marker, self.__lock_filepath())
+
+        self.console = StringIO()
+
+        return self.backup_id, self.metrics, self.console
+
+    def create_time(self):
+        folder_name = self.get_id()
+        d = datetime.strptime(folder_name, "%Y%m%dT%H%M")
+        return time.mktime(d.timetuple())
+
+    def __exit__(self, tpe, exception, tb):
+        self.__log.debug("Closing vault [%s]" % self)
+
+        super(AwsS3Vault, self).__exit__(tpe, exception, tb)
+
+        console_logs = self.console.getvalue()
+        self.console.close()
+
+        console_marker = "/tmp/.console"
+        with open(console_marker, "w") as f:
+            f.write(console_logs)
+        self.__get_s3_bucket().upload_file(console_marker, self.__console_filepath())
+        self.__log.info("Console logs are saved to: %s" % self.__console_filepath())
+
+        if exception is None:
+            self.__on_successful_upload()
+        else:
+            e = "\n".join(format_exception(tpe, exception, tb))
+            self.__on_failed_upload(exception=e, output=console_logs)
+
+        metrics_marker = "/tmp/.metrics"
+        with open(metrics_marker, "w") as f:
+            json.dump(self.metrics, f)
+
+        self.__get_s3_bucket().upload_file(metrics_marker, self.__metrics_filepath())
+        self.__log.info("Metrics are saved to: %s" % self.__metrics_filepath())
+
+        self.__log.info("Remove lock for %s" % self.get_id())
+        self.__get_s3_bucket().Object(self.__lock_filepath()).delete()
+
+    def __on_successful_upload(self):
+        self.__log.info("Backup %s is uploaded successfully." % self.backup_id)
+        size_str = self.__get_s3_bucket().Object(self.__get_backup_name()).get()["ContentLength"]
+        self.metrics["size"] = int(size_str)
+
+    def __on_failed_upload(self, **kwargs):
+        failed_marker = "/tmp/.failed"
+        subprocess.call("echo .failed > %s" % failed_marker, shell=True)
+        self.__get_s3_bucket().upload_file(failed_marker, self.__failed_filepath())
+
+        self.__log.error("Something wrong happened inside block uses vault: " + kwargs['exception'])
+        self.__log.error("Backup script output: " + kwargs['output'])
+        self.metrics["exception"] = kwargs['exception']
+        self.metrics["size"] = -1
+
+    def __repr__(self):
+        return "Vault(%s)" % self.get_id()
+
+    def fail(self):
+        open(self.__failed_filepath(), "w").close()
diff --git a/docker-backup-daemon/docker/postgres/storage_swift.py b/docker-backup-daemon/docker/postgres/storage_swift.py
new file mode 100644
index 0000000..a0fc482
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/storage_swift.py
@@ -0,0 +1,281 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import hashlib
+import io
+import json
+import logging
+import time
+from datetime import datetime
+from traceback import format_exception
+
+import os
+
+import storage
+from storage import VAULT_NAME_FORMAT, StorageLocationAlreadyExistsException
+
+try:
+    from io import StringIO
+except ImportError:
+    from io import StringIO
+
+import subprocess
+
+CONTAINER = os.getenv("CONTAINER")
+CONTAINER_SEG = "{}_segments".format(CONTAINER)
+PG_CLUSTER_NAME = os.getenv("PG_CLUSTER_NAME")
+scli_env = os.environ.copy()
+scli_env["ST_AUTH"] = os.getenv("SWIFT_AUTH_URL")
+scli_env["ST_USER"] = os.getenv("SWIFT_USER")
+scli_env["ST_KEY"] = os.getenv("SWIFT_PASSWORD")
+scli_env["ST_TENANT"] = os.getenv("TENANT_NAME")
+
+
+class SwiftStorage(storage.Storage):
+    __log = logging.getLogger("SwiftStorage")
+
+    def __init__(self, root):
+        self.__log.info("Init storage object with storage root: %s" % root)
+        self.root = root
+
+    def list(self):
+        swift_vault_listing = subprocess.check_output(["/opt/backup/scli", "ls", CONTAINER], env=scli_env)
+        vaults = [SwiftVault(os.path.basename(backup_name[0:(len(backup_name) - len(os.path.basename(backup_name)) - 1)])
+                             , cache_state=True
+                             ,swift_vault_listing=swift_vault_listing)
+                  for backup_name in swift_vault_listing.split("\n")
+                  if backup_name.endswith(".tar.gz")]
+        vaults.sort(key=lambda v: v.create_time())
+        return vaults
+
+    def size(self):
+        """ Returns whole storage size in bytes """
+        return 0
+
+    def archive_size(self):
+        """ Returns whole storage size in bytes """
+        return 0
+
+    def fs_space(self):
+        """ Returns tuple (free, total) space on mount point where is root folder located """
+        return (1, 1)
+
+    def open_vault(self):
+        """
+        
+        :return:
+        :rtype: (str, dict, StringIO)
+        """
+        return SwiftVault("%s" % (datetime.now().strftime(VAULT_NAME_FORMAT)))
+
+    def evict(self, vault):
+        self.__log.info("Evict vault: %s" % vault)
+        backup_name = "{}/pg_{}_backup_{}.tar.gz".format(vault.get_folder(), PG_CLUSTER_NAME, vault.get_id())
+        subprocess.check_call(["/opt/backup/scli", "delete", "{}/{}".format(CONTAINER, backup_name)], env=scli_env)
+        subprocess.call(["/opt/backup/scli", "delete", "{}/{}.lock".format(CONTAINER, vault.get_folder())], env=scli_env)
+        subprocess.call(["/opt/backup/scli", "delete", "{}/{}.failed".format(CONTAINER, vault.get_folder())], env=scli_env)
+        subprocess.call(["/opt/backup/scli", "delete", "{}/{}.console".format(CONTAINER, vault.get_folder())], env=scli_env)
+        subprocess.check_call(["/opt/backup/scli", "delete", "{}/{}.metrics".format(CONTAINER, vault.get_folder())], env=scli_env)
+
+    def prot_is_file_exists(self, filename):
+        self.__log.info("File existence check for: %s" % filename)
+        swift_vault_listing = subprocess.check_output(["/opt/backup/scli", "ls", CONTAINER], env=scli_env)
+        return filename in swift_vault_listing
+
+    def prot_delete(self, filename):
+        self.__log.info("Delete file: %s" % filename)
+        subprocess.check_call(["/opt/backup/scli", "delete", "{}/{}".format(CONTAINER, filename)], env=scli_env)
+
+    def prot_get_as_stream(self, filename):
+        """
+        :param filename: path to file from backup root.
+        :type filename: string
+        :return: stream with requested file
+        :rtype: io.RawIOBase
+        """
+        self.__log.info("Get request for file: %s" % filename)
+        process = subprocess.Popen(["/opt/backup/scli", "get", "{}/{}".format(CONTAINER, filename)], env=scli_env, stdout=subprocess.PIPE)
+        return process.stdout
+
+    def prot_put_as_stream(self, filename, stream):
+        """
+        :param filename: 
+        :type filename: string
+        :param stream:
+        :type stream: io.RawIOBase
+        :return: sha256 of processed stream
+        :rtype: string
+        """
+        self.__log.info("Put request for file: %s" % filename)
+        sha256 = hashlib.sha256()
+        chunk_size = 4096
+        process = subprocess.Popen(["/opt/backup/scli", "put", "{}/{}".format(CONTAINER, filename)], env=scli_env, stdout=subprocess.PIPE, stdin=subprocess.PIPE)
+
+        while True:
+            data = stream.read(chunk_size)
+            if len(data) == 0:
+                stream.close()
+                self.__log.info("Processed stream with sha256 {}".format(sha256.hexdigest()))
+                return sha256.hexdigest()
+            sha256.update(data)
+            process.stdin.write(data)
+
+    def get_type(self):
+        return "Swift"
+
+    def get_type_id(self):
+        return 2
+
+
+class SwiftVault(storage.Vault):
+    __log = logging.getLogger("SwiftVaultLock")
+
+    def __init__(self, folder, cache_state=False, swift_vault_listing=None):
+        super(SwiftVault, self).__init__()
+
+        self.folder = folder
+        self.console = None
+        self.cache_state = cache_state
+        self.cached_state = {}
+        self.swift_vault_listing = swift_vault_listing
+
+    def __cache_current_state(self):
+        if self.cache_state:
+            swift_vault_listing = self.swift_vault_listing if self.swift_vault_listing else subprocess.check_output(["/opt/backup/scli", "ls", CONTAINER], env=scli_env)
+            self.cached_state["is_locked"] = self.__lock_filepath() in swift_vault_listing
+            self.cached_state["is_failed"] = self.__failed_filepath() in swift_vault_listing
+
+    def get_id(self):
+        return os.path.basename(self.folder)
+
+    def get_folder(self):
+        return self.folder
+
+    def __load_metrics_from_swift(self):
+        try:
+            return json.loads(subprocess.check_output([
+                "/opt/backup/scli",
+                "get",
+                "{}/{}".format(CONTAINER, self.__metrics_filepath())], env=scli_env))
+        except Exception:
+            self.__log.warning("Cannot load metrics from swift. Backup can be damaged")
+            return {}
+
+    def load_metrics(self):
+        self.__log.debug("Load metrics from: %s" % self.__metrics_filepath())
+        if self.cache_state:
+            if "metrics" not in self.cached_state:
+                self.cached_state["metrics"] = self.__load_metrics_from_swift()
+            return self.cached_state["metrics"]
+        else:
+            return self.__load_metrics_from_swift()
+
+
+    def __lock_filepath(self):
+        return self.folder + ".lock"
+
+    def __failed_filepath(self):
+        return self.folder + ".failed"
+
+    def __metrics_filepath(self):
+        return self.folder + ".metrics"
+
+    def __console_filepath(self):
+        return self.folder + ".console"
+
+    def __is_file_exists(self, path):
+        return subprocess.call(["/opt/backup/scli", "get", path], env=scli_env) == "Object Not Found"
+
+    def __backup_archive_file_path(self):
+        return "{}/pg_{}_backup_{}.tar.gz".format(self.get_folder(), PG_CLUSTER_NAME, self.get_id())
+
+    def is_locked(self):
+        if self.cache_state:
+            if not self.cached_state:
+                self.__cache_current_state()
+            return self.cached_state["is_locked"]
+        return self.__is_file_exists("{}/{}".format(CONTAINER, self.__lock_filepath()))
+
+    def is_failed(self):
+        if self.cache_state:
+            if not self.cached_state:
+                self.__cache_current_state()
+            return self.cached_state["is_failed"]
+        return self.__is_file_exists("{}/{}".format(CONTAINER, self.__failed_filepath()))
+
+    def is_done(self):
+        pass
+
+    def is_back_up_archive_exists(self):
+        return self.__is_file_exists("{}/{}".format(CONTAINER, self.__backup_archive_file_path()))
+
+    def __enter__(self):
+        self.__log.info("Init next vault: %s" % self.folder)
+        super(SwiftVault, self).__enter__()
+
+        if self.__is_file_exists(self.__metrics_filepath()):
+            raise StorageLocationAlreadyExistsException("Destination backup folder already exists: %s" % self.folder)
+
+        self.__log.info("Create .lock file in vault: %s" % self.folder)
+        subprocess.call("echo .lock > /tmp/.lock", shell=True)
+        subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.lock", "{}/{}".format(CONTAINER, self.__lock_filepath())], env=scli_env)
+        self.console = StringIO()
+        return (self.folder, self.metrics, self.console)
+
+    def create_time(self):
+        foldername = self.get_id()
+        d = datetime.strptime(foldername, VAULT_NAME_FORMAT)
+        return time.mktime(d.timetuple())
+
+    def __exit__(self, tpe, exception, tb):
+        self.__log.info("Close vault")
+        self.__log.info("Save metrics to: %s" % self.__metrics_filepath())
+
+        super(SwiftVault, self).__exit__(tpe, exception, tb)
+
+        backup_name = "{}/pg_{}_backup_{}.tar.gz".format(self.get_folder(), PG_CLUSTER_NAME, self.get_id())
+        size_str = subprocess.check_output(
+            ["/opt/backup/scli ls -l {} | grep {} | awk '{{A+=$2}} END{{print A}}'".format(CONTAINER_SEG, backup_name)]
+            , shell=True
+            , env=scli_env)
+        try:
+            self.metrics["size"] = int(size_str)
+        except Exception as e:
+            self.__log.error(e)
+            self.metrics["size"] = -1
+
+        if exception:
+            subprocess.call("echo .failed > /tmp/.failed", shell=True)
+            subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.failed", "{}/{}".format(CONTAINER, self.__failed_filepath())], env=scli_env)
+
+            e = "\n".join(format_exception(tpe, exception, tb))
+            self.__log.info("Don't remove vault .lock due exception in nested code")
+            self.__log.debug("Something wrong happened inside block uses vault: " + e)
+            self.metrics["exception"] = e
+
+        with open("/tmp/.metrics", "w") as f:
+            json.dump(self.metrics, f)
+        subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.metrics", "{}/{}".format(CONTAINER, self.__metrics_filepath())], env=scli_env)
+
+        console_logs = self.console.getvalue()
+        self.console.close()
+        with open("/tmp/.console", "w") as f:
+            f.write(console_logs)
+        subprocess.check_call(["/opt/backup/scli", "put", "/tmp/.console", "{}/{}".format(CONTAINER, self.__console_filepath())], env=scli_env)
+
+        self.__log.info("Remove lock for %s" % self.get_id())
+        subprocess.check_call(["/opt/backup/scli", "delete", "{}/{}".format(CONTAINER, self.__lock_filepath())], env=scli_env)
+
+    def __repr__(self):
+        return "Vault(%s)" % self.get_id()
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/utils.py b/docker-backup-daemon/docker/postgres/utils.py
new file mode 100644
index 0000000..e9acc48
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/utils.py
@@ -0,0 +1,89 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import psycopg2
+import logging
+
+log = logging.getLogger("utils")
+
+
+def execute_query(conn_properties, query):
+    conn = None
+    try:
+        conn = psycopg2.connect(**conn_properties)
+        with conn.cursor() as cur:
+            cur.execute(query)
+            return cur.fetchone()[0]
+    finally:
+        if conn:
+            conn.close()
+
+def get_version_of_pgsql_server():
+    # this is very bad, need to reuse code from granular backups
+    conn_properties = {
+        'host': os.getenv('POSTGRES_HOST'),
+        'port': os.getenv('POSTGRES_PORT'),
+        'user': os.getenv('POSTGRES_USER') or 'postgres',
+        'password': os.getenv('POSTGRES_PASSWORD'),
+        'database': 'postgres',
+        'connect_timeout': int(os.getenv("CONNECT_TIMEOUT", "5")),
+    }
+    try:
+        server_version = execute_query(conn_properties, 'SHOW SERVER_VERSION;')
+    except psycopg2.OperationalError as e:
+        log.exception(e)
+        return None
+    version_as_list = list(map(int, server_version.split(' ')[0].split('.')))
+    if [10, 0] <= version_as_list < [11, 0]:
+        return "pg10"
+    elif [11, 0] <= version_as_list < [12, 0]:
+        return "pg11"
+    elif [12, 0] <= version_as_list < [13, 0]:
+        return "pg12"
+    elif [13, 0] <= version_as_list < [14, 0]:
+        return "pg13"
+    elif [14, 0] <= version_as_list < [15, 0]:
+        return "pg14"
+    elif [15, 0] <= version_as_list < [16, 0]:
+        return "pg15"
+    elif [16, 0] <= version_as_list < [17, 0]:
+        return "pg16"
+    elif version_as_list >= [17, 0]:
+        return "pg17"
+    return ""
+
+
+def retry_if_storage_error(exception):
+    """
+    Return True if we should retry (in this case when it's an psycopg2.OperationalError can happen on db connect error)
+    False otherwise
+    """
+    log.info("During initialization of storage next error occurred: %s", exception)
+    import storage
+    return isinstance(exception, storage.StorageException)
+
+
+def get_encryption():
+    encrypt_backups = os.getenv("KEY_SOURCE", 'false').lower()
+    return encrypt_backups != 'false'
+
+
+def validate_user(username, password):
+    if not os.getenv("AUTH", "false").lower() == "false":
+        return username == os.getenv("POSTGRES_USER") and \
+               password == os.getenv("POSTGRES_PASSWORD")
+    else:
+        return True
+
diff --git a/docker-backup-daemon/docker/postgres/utils.sh b/docker-backup-daemon/docker/postgres/utils.sh
new file mode 100755
index 0000000..2f0270f
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/utils.sh
@@ -0,0 +1,46 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+export ST_AUTH="${SWIFT_AUTH_URL}"
+export ST_USER="${SWIFT_USER}"
+export ST_KEY="${SWIFT_PASSWORD}"
+export ST_TENANT="${TENANT_NAME}"
+
+export PGPASSWORD="$(echo ${PGPASSWORD} | tr -d '\n' | tr -d '[[:space:]]')"
+
+function log() {
+  log_module "" "backup_uploader" "$1"
+}
+
+function log_module() {
+  local priority="${1:-INFO}"
+  local module="$2"
+  local message="$3"
+
+  local timestamp=$(date --iso-8601=seconds)
+  echo "[${timestamp}][${priority}][category=${module}] ${message}"
+}
+
+function process_exit_code() {
+  local exit_code=$1
+  local message="$2"
+  if [ ${exit_code} -ne 0 ];then
+    log_module "ERROR" "" "${message}"
+    exit 1
+  fi
+}
+
+function register_delete_on_exit() {
+  trap "rm -f $*" EXIT
+}
\ No newline at end of file
diff --git a/docker-backup-daemon/docker/postgres/workers.py b/docker-backup-daemon/docker/postgres/workers.py
new file mode 100644
index 0000000..0380ef4
--- /dev/null
+++ b/docker-backup-daemon/docker/postgres/workers.py
@@ -0,0 +1,127 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import eviction
+import locks
+import utils
+import logging
+import time
+import encryption
+
+from multiprocessing import Process
+
+
+class BackupProcessException(Exception):
+    pass
+
+
+class PostgreSQLBackupWorker(Process):
+    def __init__(self, storage, backup_command, eviction_rule, backup_id):
+        Process.__init__(self)
+        self.__log = logging.getLogger("PostgreSQLBackupWorker")
+
+        self.__storage = storage
+        self.__backup_command = backup_command
+        self.__eviction_rule = eviction_rule
+        self.__backup_id = backup_id
+
+        if utils.get_encryption():
+            self.encryption = True
+            self.key = encryption.KeyManagement.get_object().get_password()
+            self.key_name = encryption.KeyManagement.get_key_name()
+            self.key_source = encryption.KeyManagement.get_key_source()
+            self.__backup_command = backup_command + " %(key)s"
+        else:
+            self.encryption = False
+
+    def run(self):
+        self.__perform_database_backup()
+        self.__cleanup_storage()
+
+    def __perform_database_backup(self):
+        with self.__storage.open_vault(self.__backup_id) as (vault_folder, metrics, console):
+            backup_id = self.__backup_id.split('/')[-1]
+
+            self.__log.info("[backup-id=%s] Start new backup streaming." % backup_id)
+            locks.update_lock_file(backup_id=backup_id)
+
+            cmd_options = {"data_folder": vault_folder}
+            if self.encryption:
+                cmd_options["key"] = self.key
+
+            cmd_processed = self.__split_command_line(self.__backup_command % cmd_options)
+            if not self.encryption:
+                self.__log.info("Run cmd template: %s\n\toptions: %s\n\tcmd: [%s]" % (
+                    self.__backup_command, str(cmd_options), ", ".join(cmd_processed)))
+            else:
+                self.__log.info("Run cmd template %s\n" % (self.__backup_command))
+            import subprocess
+            process = subprocess.Popen(cmd_processed, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
+            while True:
+                output = process.stdout.readline()
+                if output.decode() == '':
+                    if process.poll() is not None:
+                        break
+                    else:
+                        time.sleep(0.1)
+                if output:
+                    print((output.decode().strip()))
+                    console.write(output.decode().strip())
+            exit_code = process.poll()
+            metrics["exit_code"] = exit_code
+            if self.encryption:
+                metrics["key_name"] = self.key_name
+                metrics["key_source"] = self.key_source
+
+            if exit_code != 0:
+                msg = "[backup-id=%s] Execution of '%s' was finished with non zero exit code: %d" % (backup_id, cmd_processed, exit_code)
+                self.__log.error(msg)
+                raise BackupProcessException(msg)
+            else:
+                self.__log.info("[backup-id=%s] Backup streaming has been successfully finished." % backup_id)
+
+    def __cleanup_storage(self):
+        self.__log.info("Start eviction process by policy: %s" % self.__eviction_rule)
+        outdated_vaults = eviction.evict(self.__storage.list(), self.__eviction_rule,
+                                         accessor=lambda x: x.create_time())
+        if len(outdated_vaults) > 0:
+            for vault in outdated_vaults:
+                self.__storage.evict(vault)
+        else:
+            self.__log.info("No obsolete vaults to evict")
+
+    def fail(self):
+        self.__log.warning("Set failed status for backup")
+        self.__storage.fail(self.__backup_id)
+
+    @staticmethod
+    def __split_command_line(cmd_line):
+        import shlex
+        lex = shlex.shlex(cmd_line)
+        lex.quotes = '"'
+        lex.whitespace_split = True
+        lex.commenters = ''
+        return list(lex)
+
+
+def spawn_backup_worker(storage, backup_command, eviction_rule, backup_id):
+    process = PostgreSQLBackupWorker(storage, backup_command, eviction_rule, backup_id)
+    process.start()
+    return process
+
+
+def spawn_worker(runnable, *args):
+    process = Process(target=runnable, args=args)
+    process.start()
+    return process
diff --git a/docker-backup-daemon/docker/requirements.txt b/docker-backup-daemon/docker/requirements.txt
new file mode 100644
index 0000000..9b97447
--- /dev/null
+++ b/docker-backup-daemon/docker/requirements.txt
@@ -0,0 +1,54 @@
+aniso8601==9.0.1
+APScheduler==3.10.0
+boto3==1.21.21
+cachetools==4.2.4
+certifi==2024.7.4
+chardet==3.0.4
+charset-normalizer==2.0.12
+colorama==0.4.3
+croniter==1.3.8
+dictdiffer==0.9.0
+docutils==0.20
+filelock==3.4.1
+Flask==3.0.3
+Flask-HTTPAuth==4.5.0
+Flask-RESTful==0.3.9
+google-api-core==2.11.0
+google-api-python-client==2.68.0
+google-auth==2.16.1
+google-auth-httplib2==0.1.1
+googleapis-common-protos==1.57.0
+gunicorn==23.0.0
+httplib2==0.21.0
+idna==3.7
+importlib-metadata==4.8.3
+ipaddress==1.0.23
+jmespath==1.0.1
+kubernetes==27.2.0
+MarkupSafe==2.1.1
+oauth2client==4.1.3
+oauthlib==3.2.2
+protobuf==4.25.0
+psycopg2-binary==2.9.5
+pyasn1==0.5.0
+pyasn1-modules==0.3.0
+pycryptodome==3.17
+pyhocon==0.3.54
+pyparsing==3.1.0
+python-dateutil==2.9.0
+python-string-utils==1.0.0
+pytz==2022.6
+PyYAML==6.0.1
+requests==2.28.0
+requests-oauthlib==2.0.0
+retrying==1.3.3
+rsa==4.8
+ruamel.yaml==0.17.22
+six==1.16.0
+typing-extensions==4.5.0
+tzlocal==4.3
+uritemplate==4.1.1
+urllib3==1.26.17
+websocket-client==1.4.2
+Werkzeug==3.0.6
+zipp==3.6.0
\ No newline at end of file
diff --git a/docker-backup-daemon/go/cmd/main.go b/docker-backup-daemon/go/cmd/main.go
new file mode 100644
index 0000000..448584c
--- /dev/null
+++ b/docker-backup-daemon/go/cmd/main.go
@@ -0,0 +1,57 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"flag"
+
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/config"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/util"
+
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/azure"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/k8s"
+)
+
+var (
+	restoreId         = flag.String("restore_id", "", "Id of restore operation")
+	restoreTime       = flag.String("restore_time", "", "Time for restore database from")
+	restoreFolder     = flag.String("restore_folder", "", "Folder to save restore statuses")
+	restoreAsSeparate = flag.String("restore_as_separate", "false", "Flag to skip update external service")
+	geoRestore        = flag.String("geo_restore", "false", "Flag to perform geo restore")
+	subnet            = flag.String("subnet", "false", "override subnet for restored instance")
+)
+
+func main() {
+	flag.Parse()
+	restoreCfg := config.NewRestoreConfig(*restoreId, *restoreTime, *restoreFolder, *restoreAsSeparate, *geoRestore, *subnet)
+	restoreClient := azure.NewRestoreClientWithRestoreConfig(restoreCfg)
+
+	util.ConfigureAzLogging()
+
+	restoreClient.RestoreDatabase()
+
+	if restoreCfg.RestoreAsSeparate() {
+		return
+	}
+
+	err := k8s.UpdateExternalService(restoreCfg.NewServerName())
+	if err != nil {
+		panic(err)
+	}
+	err = k8s.UpdateExternalCM(restoreCfg.ServerName(), restoreCfg.NewServerName())
+	if err != nil {
+		panic(err)
+	}
+}
diff --git a/docker-backup-daemon/go/go.mod b/docker-backup-daemon/go/go.mod
new file mode 100644
index 0000000..04b0de3
--- /dev/null
+++ b/docker-backup-daemon/go/go.mod
@@ -0,0 +1,64 @@
+module github.com/Netcracker/pgskipper-backup-daemon
+
+go 1.25.3
+
+require (
+	github.com/Azure/azure-sdk-for-go/sdk/azcore v1.4.0
+	github.com/Azure/azure-sdk-for-go/sdk/azidentity v1.2.2
+	github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/postgresql/armpostgresqlflexibleservers/v2 v2.1.0
+	go.uber.org/zap v1.24.0
+	k8s.io/api v0.26.0
+	k8s.io/apimachinery v0.29.0
+	k8s.io/client-go v0.26.0
+	sigs.k8s.io/controller-runtime v0.14.1
+)
+
+require (
+	github.com/Azure/azure-sdk-for-go/sdk/internal v1.2.0 // indirect
+	github.com/AzureAD/microsoft-authentication-library-for-go v0.9.0 // indirect
+	github.com/davecgh/go-spew v1.1.1 // indirect
+	github.com/emicklei/go-restful/v3 v3.9.0 // indirect
+	github.com/evanphx/json-patch/v5 v5.6.0 // indirect
+	github.com/go-logr/logr v1.3.0 // indirect
+	github.com/go-openapi/jsonpointer v0.19.6 // indirect
+	github.com/go-openapi/jsonreference v0.20.2 // indirect
+	github.com/go-openapi/swag v0.22.3 // indirect
+	github.com/gogo/protobuf v1.3.2 // indirect
+	github.com/golang-jwt/jwt/v4 v4.5.2 // indirect
+	github.com/golang/protobuf v1.5.3 // indirect
+	github.com/google/gnostic v0.5.7-v3refs // indirect
+	github.com/google/gnostic-models v0.6.8 // indirect
+	github.com/google/gofuzz v1.2.0 // indirect
+	github.com/google/uuid v1.3.0 // indirect
+	github.com/imdario/mergo v0.3.9 // indirect
+	github.com/josharian/intern v1.0.0 // indirect
+	github.com/json-iterator/go v1.1.12 // indirect
+	github.com/kylelemons/godebug v1.1.0 // indirect
+	github.com/mailru/easyjson v0.7.7 // indirect
+	github.com/matttproud/golang_protobuf_extensions v1.0.4 // indirect
+	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
+	github.com/modern-go/reflect2 v1.0.2 // indirect
+	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
+	github.com/pkg/browser v0.0.0-20210911075715-681adbf594b8 // indirect
+	github.com/pkg/errors v0.9.1 // indirect
+	github.com/spf13/pflag v1.0.5 // indirect
+	go.uber.org/atomic v1.7.0 // indirect
+	go.uber.org/multierr v1.6.0 // indirect
+	golang.org/x/crypto v0.43.0 // indirect
+	golang.org/x/net v0.45.0 // indirect
+	golang.org/x/oauth2 v0.27.0 // indirect
+	golang.org/x/sys v0.37.0 // indirect
+	golang.org/x/term v0.36.0 // indirect
+	golang.org/x/text v0.30.0 // indirect
+	golang.org/x/time v0.3.0 // indirect
+	google.golang.org/protobuf v1.33.0 // indirect
+	gopkg.in/inf.v0 v0.9.1 // indirect
+	gopkg.in/yaml.v2 v2.4.0 // indirect
+	gopkg.in/yaml.v3 v3.0.1 // indirect
+	k8s.io/klog/v2 v2.110.1 // indirect
+	k8s.io/kube-openapi v0.0.0-20231010175941-2dd684a91f00 // indirect
+	k8s.io/utils v0.0.0-20230726121419-3b25d923346b // indirect
+	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
+	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
+	sigs.k8s.io/yaml v1.3.0 // indirect
+)
diff --git a/docker-backup-daemon/go/go.sum b/docker-backup-daemon/go/go.sum
new file mode 100644
index 0000000..a2eba08
--- /dev/null
+++ b/docker-backup-daemon/go/go.sum
@@ -0,0 +1,279 @@
+cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=
+github.com/Azure/azure-sdk-for-go/sdk/azcore v1.4.0 h1:rTnT/Jrcm+figWlYz4Ixzt0SJVR2cMC8lvZcimipiEY=
+github.com/Azure/azure-sdk-for-go/sdk/azcore v1.4.0/go.mod h1:ON4tFdPTwRcgWEaVDrN3584Ef+b7GgSJaXxe5fW9t4M=
+github.com/Azure/azure-sdk-for-go/sdk/azidentity v1.2.2 h1:uqM+VoHjVH6zdlkLF2b6O0ZANcHoj3rO0PoQ3jglUJA=
+github.com/Azure/azure-sdk-for-go/sdk/azidentity v1.2.2/go.mod h1:twTKAa1E6hLmSDjLhaCkbTMQKc7p/rNLU40rLxGEOCI=
+github.com/Azure/azure-sdk-for-go/sdk/internal v1.2.0 h1:leh5DwKv6Ihwi+h60uHtn6UWAxBbZ0q8DwQVMzf61zw=
+github.com/Azure/azure-sdk-for-go/sdk/internal v1.2.0/go.mod h1:eWRD7oawr1Mu1sLCawqVc0CUiF43ia3qQMxLscsKQ9w=
+github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/postgresql/armpostgresqlflexibleservers/v2 v2.1.0 h1:8taYXqep2de3/KFzXp8JG1ZfL/OY8VEpvr9brYLF/zE=
+github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/postgresql/armpostgresqlflexibleservers/v2 v2.1.0/go.mod h1:8Anbzn23yMdpl2DDY4qnFEPY9Vf6CoLphIt4mX59McI=
+github.com/AzureAD/microsoft-authentication-library-for-go v0.9.0 h1:UE9n9rkJF62ArLb1F3DEjRt8O3jLwMWdSoypKV4f3MU=
+github.com/AzureAD/microsoft-authentication-library-for-go v0.9.0/go.mod h1:kgDmCTgBzIEPFElEF+FK0SdjAor06dRq2Go927dnQ6o=
+github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=
+github.com/benbjohnson/clock v1.1.0 h1:Q92kusRqC1XV2MjkWETPvjJVqKetz1OzxZB7mHJLju8=
+github.com/benbjohnson/clock v1.1.0/go.mod h1:J11/hYXuz8f4ySSvYwY0FKfm+ezbsZBKZxNJlLklBHA=
+github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=
+github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
+github.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=
+github.com/cespare/xxhash/v2 v2.1.2 h1:YRXhKfTDauu4ajMg1TPgFO5jnlC2HCbmLXMcTG5cbYE=
+github.com/cespare/xxhash/v2 v2.1.2/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
+github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=
+github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/dnaeon/go-vcr v1.1.0 h1:ReYa/UBrRyQdant9B4fNHGoCNKw6qh6P0fsdGmZpR7c=
+github.com/dnaeon/go-vcr v1.1.0/go.mod h1:M7tiix8f0r6mKKJ3Yq/kqU1OYf3MnfmBWVbPx/yU9ko=
+github.com/docopt/docopt-go v0.0.0-20180111231733-ee0de3bc6815/go.mod h1:WwZ+bS3ebgob9U8Nd0kOddGdZWjyMGR8Wziv+TBNwSE=
+github.com/emicklei/go-restful/v3 v3.9.0 h1:XwGDlfxEnQZzuopoqxwSEllNcCOM9DhhFyhFIIGKwxE=
+github.com/emicklei/go-restful/v3 v3.9.0/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
+github.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=
+github.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=
+github.com/evanphx/json-patch/v5 v5.6.0 h1:b91NhWfaz02IuVxO9faSllyAtNXHMPkC5J8sJCLunww=
+github.com/evanphx/json-patch/v5 v5.6.0/go.mod h1:G79N1coSVB93tBe7j6PhzjmR3/2VvlbKOFpnXhI9Bw4=
+github.com/fsnotify/fsnotify v1.6.0 h1:n+5WquG0fcWoWp6xPWfHdbskMCQaFnG6PfBrh1Ky4HY=
+github.com/fsnotify/fsnotify v1.6.0/go.mod h1:sl3t1tCWJFWoRz9R8WJCbQihKKwmorjAbSClcnxKAGw=
+github.com/go-logr/logr v1.3.0 h1:2y3SDp0ZXuc6/cjLSZ+Q3ir+QB9T/iG5yYRXqsagWSY=
+github.com/go-logr/logr v1.3.0/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
+github.com/go-logr/zapr v1.2.3 h1:a9vnzlIBPQBBkeaR9IuMUfmVOrQlkoC4YfPoFkX3T7A=
+github.com/go-logr/zapr v1.2.3/go.mod h1:eIauM6P8qSvTw5o2ez6UEAfGjQKrxQTl5EoK+Qa2oG4=
+github.com/go-openapi/jsonpointer v0.19.6 h1:eCs3fxoIi3Wh6vtgmLTOjdhSpiqphQ+DaPn38N2ZdrE=
+github.com/go-openapi/jsonpointer v0.19.6/go.mod h1:osyAmYz/mB/C3I+WsTTSgw1ONzaLJoLCyoi6/zppojs=
+github.com/go-openapi/jsonreference v0.20.2 h1:3sVjiK66+uXK/6oQ8xgcRKcFgQ5KXa2KvnJRumpMGbE=
+github.com/go-openapi/jsonreference v0.20.2/go.mod h1:Bl1zwGIM8/wsvqjsOQLJ/SH+En5Ap4rVB5KVcIDZG2k=
+github.com/go-openapi/swag v0.22.3 h1:yMBqmnQ0gyZvEb/+KzuWZOXgllrXT4SADYbvDaXHv/g=
+github.com/go-openapi/swag v0.22.3/go.mod h1:UzaqsxGiab7freDnrUUra0MwWfN/q7tE4j+VcZ0yl14=
+github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 h1:tfuBGBXKqDEevZMzYi5KSi8KkcZtzBcTgAUUtapy0OI=
+github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572/go.mod h1:9Pwr4B2jHnOSGXyyzV8ROjYa2ojvAY6HCGYYfMoC3Ls=
+github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
+github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
+github.com/golang-jwt/jwt/v4 v4.5.2 h1:YtQM7lnr8iZ+j5q71MGKkNw9Mn7AjHM68uc9g5fXeUI=
+github.com/golang-jwt/jwt/v4 v4.5.2/go.mod h1:m21LjoU+eqJr34lmDMbreY2eSTRJ1cv77w39/MY0Ch0=
+github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=
+github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=
+github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=
+github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=
+github.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=
+github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=
+github.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=
+github.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=
+github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
+github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
+github.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=
+github.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
+github.com/google/gnostic v0.5.7-v3refs h1:FhTMOKj2VhjpouxvWJAV1TL304uMlb9zcDqkl6cEI54=
+github.com/google/gnostic v0.5.7-v3refs/go.mod h1:73MKFl6jIHelAJNaBGFzt3SPtZULs9dYrGFt8OiIsHQ=
+github.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=
+github.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=
+github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=
+github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
+github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
+github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
+github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
+github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1 h1:K6RDEckDVWvDI9JAJYCmNdQXq6neHJOYx3V6jnqNEec=
+github.com/google/pprof v0.0.0-20210720184732-4bb14d4b1be1/go.mod h1:kpwsk12EmLew5upagYY7GY0pfYCcupk39gWOCRROcvE=
+github.com/google/uuid v1.3.0 h1:t6JiXgmwXMjEs8VusXIJk2BXHsn+wx8BZdTaoZ5fu7I=
+github.com/google/uuid v1.3.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/imdario/mergo v0.3.9 h1:UauaLniWCFHWd+Jp9oCEkTBj8VO/9DKg3PV3VCNMDIg=
+github.com/imdario/mergo v0.3.9/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=
+github.com/jessevdk/go-flags v1.4.0/go.mod h1:4FA24M0QyGHXBuZZK/XkWh8h0e1EYbRYJSGM75WSRxI=
+github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
+github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
+github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
+github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
+github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
+github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
+github.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
+github.com/kr/pretty v0.2.1/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
+github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
+github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
+github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
+github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
+github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
+github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
+github.com/kylelemons/godebug v1.1.0 h1:RPNrshWIDI6G2gRW9EHilWtl7Z6Sb1BR0xunSBf0SNc=
+github.com/kylelemons/godebug v1.1.0/go.mod h1:9/0rRGxNHcop5bhtWyNeEfOS8JIWk580+fNqagV/RAw=
+github.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=
+github.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
+github.com/matttproud/golang_protobuf_extensions v1.0.4 h1:mmDVorXM7PCGKw94cs5zkfA9PSy5pEvNWRP0ET0TIVo=
+github.com/matttproud/golang_protobuf_extensions v1.0.4/go.mod h1:BSXmuO+STAnVfrANrmjBb36TMTDstsz7MSK+HVaYKv4=
+github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
+github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
+github.com/onsi/ginkgo/v2 v2.13.0 h1:0jY9lJquiL8fcf3M4LAXN5aMlS/b2BV86HFFPCPMgE4=
+github.com/onsi/ginkgo/v2 v2.13.0/go.mod h1:TE309ZR8s5FsKKpuB1YAQYBzCaAfUgatB/xlT/ETL/o=
+github.com/onsi/gomega v1.29.0 h1:KIA/t2t5UBzoirT4H9tsML45GEbo3ouUnBHsCfD2tVg=
+github.com/onsi/gomega v1.29.0/go.mod h1:9sxs+SwGrKI0+PWe4Fxa9tFQQBG5xSsSbMXOI8PPpoQ=
+github.com/pkg/browser v0.0.0-20210911075715-681adbf594b8 h1:KoWmjvw+nsYOo29YJK9vDA65RGE3NrOnUtO7a+RF9HU=
+github.com/pkg/browser v0.0.0-20210911075715-681adbf594b8/go.mod h1:HKlIX3XHQyzLZPlr7++PzdhaXEj94dEiJgZDTsxEqUI=
+github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/prometheus/client_golang v1.14.0 h1:nJdhIvne2eSX/XRAFV9PcvFFRbrjbcTUj0VP62TMhnw=
+github.com/prometheus/client_golang v1.14.0/go.mod h1:8vpkKitgIVNcqrRBWh1C4TIUQgYNtG/XQE4E/Zae36Y=
+github.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
+github.com/prometheus/client_model v0.3.0 h1:UBgGFHqYdG/TPFD1B1ogZywDqEkwp3fBMvqdiQ7Xew4=
+github.com/prometheus/client_model v0.3.0/go.mod h1:LDGWKZIo7rky3hgvBe+caln+Dr3dPggB5dvjtD7w9+w=
+github.com/prometheus/common v0.37.0 h1:ccBbHCgIiT9uSoFY0vX8H3zsNR5eLt17/RQLUvn8pXE=
+github.com/prometheus/common v0.37.0/go.mod h1:phzohg0JFMnBEFGxTDbfu3QyL5GI8gTQJFhYO5B3mfA=
+github.com/prometheus/procfs v0.8.0 h1:ODq8ZFEaYeCaZOJlZZdJA2AbQR98dSHSM1KW/You5mo=
+github.com/prometheus/procfs v0.8.0/go.mod h1:z7EfXMXOkbkqb9IINtpCn86r/to3BnA0uaxHdg830/4=
+github.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=
+github.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=
+github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
+github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
+github.com/stoewer/go-strcase v1.2.0/go.mod h1:IBiWB2sKIp3wVVQ3Y035++gc+knqhUQag1KpM8ahLw8=
+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
+github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
+github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=
+github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
+github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
+github.com/stretchr/testify v1.8.4 h1:CcVxjf3Q8PM0mHUKJCdn+eZZtm5yQwehR5yeSVQQcUk=
+github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=
+github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+go.uber.org/atomic v1.7.0 h1:ADUqmZGgLDDfbSL9ZmPxKTybcoEYHgpYfELNoN+7hsw=
+go.uber.org/atomic v1.7.0/go.mod h1:fEN4uk6kAWBTFdckzkM89CLk9XfWZrxpCo0nPH17wJc=
+go.uber.org/goleak v1.2.0 h1:xqgm/S+aQvhWFTtR0XK3Jvg7z8kGV8P4X14IzwN3Eqk=
+go.uber.org/goleak v1.2.0/go.mod h1:XJYK+MuIchqpmGmUSAzotztawfKvYLUIgg7guXrwVUo=
+go.uber.org/multierr v1.6.0 h1:y6IPFStTAIT5Ytl7/XYmHvzXQ7S3g/IeZW9hyZ5thw4=
+go.uber.org/multierr v1.6.0/go.mod h1:cdWPpRnG4AhwMwsgIHip0KRBQjJy5kYEpYjJxpXp9iU=
+go.uber.org/zap v1.24.0 h1:FiJd5l1UOLj0wCgbSE0rwwXHzEdAZS6hiiSnxJN/D60=
+go.uber.org/zap v1.24.0/go.mod h1:2kMP+WWQ8aoFoedH3T2sq6iJ2yDWpHbP0f6MQbS9Gkg=
+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
+golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.43.0 h1:dduJYIi3A3KOfdGOHX8AVZ/jGiyPa3IbBozJ5kNuE04=
+golang.org/x/crypto v0.43.0/go.mod h1:BFbav4mRNlXJL4wNeejLpWxB7wMbc79PdRGhWKncxR0=
+golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=
+golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
+golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=
+golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
+golang.org/x/net v0.45.0 h1:RLBg5JKixCy82FtLJpeNlVM0nrSqpCRYzVU1n8kj0tM=
+golang.org/x/net v0.45.0/go.mod h1:ECOoLqd5U3Lhyeyo/QDCEVQ4sNgYsqvCZ722XogGieY=
+golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=
+golang.org/x/oauth2 v0.27.0 h1:da9Vo7/tDv5RH/7nZDz1eMGS/q1Vv1N/7FCrBhI9I3M=
+golang.org/x/oauth2 v0.27.0/go.mod h1:onh5ek6nERTohokkhCD/y2cV4Do3fxFHFuAejCkRWT8=
+golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20210616045830-e2b7044e8c71/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.37.0 h1:fdNQudmxPjkdUTPnLn5mdQv7Zwvbvpaxqs831goi9kQ=
+golang.org/x/sys v0.37.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
+golang.org/x/term v0.36.0 h1:zMPR+aF8gfksFprF/Nc/rd1wRS1EI6nDBGyWAvDzx2Q=
+golang.org/x/term v0.36.0/go.mod h1:Qu394IJq6V6dCBRgwqshf3mPF85AqzYEzofzRdZkWss=
+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.30.0 h1:yznKA/E9zq54KzlzBEAWn1NXSQ8DIp/NYMy88xJjl4k=
+golang.org/x/text v0.30.0/go.mod h1:yDdHFIX9t+tORqspjENWgzaCVXgk0yYnYuSZ8UzzBVM=
+golang.org/x/time v0.3.0 h1:rg5rLMjNzMS1RkNLzCG38eapWhnYLFYXDXj2gOlr8j4=
+golang.org/x/time v0.3.0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=
+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
+golang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
+golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
+golang.org/x/tools v0.37.0 h1:DVSRzp7FwePZW356yEAChSdNcQo6Nsp+fex1SUW09lE=
+golang.org/x/tools v0.37.0/go.mod h1:MBN5QPQtLMHVdvsbtarmTNukZDdgwdwlO5qGacAzF0w=
+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+gomodules.xyz/jsonpatch/v2 v2.2.0 h1:4pT439QV83L+G9FkcCriY6EkpcK6r6bK+A5FBUMI7qY=
+gomodules.xyz/jsonpatch/v2 v2.2.0/go.mod h1:WXp+iVDkoLQqPudfQ9GBlwB2eZ5DKOnjQZCYdOS8GPY=
+google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=
+google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=
+google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=
+google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=
+google.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=
+google.golang.org/genproto v0.0.0-20201019141844-1ed22bb0c154/go.mod h1:FWY/as6DDZQgahTzZj3fqbO1CbirC29ZNUFHwi0/+no=
+google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=
+google.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=
+google.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
+google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=
+google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=
+google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=
+google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=
+google.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=
+google.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+google.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+google.golang.org/protobuf v1.24.0/go.mod h1:r/3tXBNzIEhYS9I1OUVjXDlt8tc493IdKGjtUeSXeh4=
+google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
+google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
+google.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=
+google.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
+gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
+gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
+gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.0-20200615113413-eeeca48fe776/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+honnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+k8s.io/api v0.26.0 h1:IpPlZnxBpV1xl7TGk/X6lFtpgjgntCg8PJ+qrPHAC7I=
+k8s.io/api v0.26.0/go.mod h1:k6HDTaIFC8yn1i6pSClSqIwLABIcLV9l5Q4EcngKnQg=
+k8s.io/apiextensions-apiserver v0.26.0 h1:Gy93Xo1eg2ZIkNX/8vy5xviVSxwQulsnUdQ00nEdpDo=
+k8s.io/apiextensions-apiserver v0.26.0/go.mod h1:7ez0LTiyW5nq3vADtK6C3kMESxadD51Bh6uz3JOlqWQ=
+k8s.io/apimachinery v0.29.0 h1:+ACVktwyicPz0oc6MTMLwa2Pw3ouLAfAon1wPLtG48o=
+k8s.io/apimachinery v0.29.0/go.mod h1:eVBxQ/cwiJxH58eK/jd/vAk4mrxmVlnpBH5J2GbMeis=
+k8s.io/client-go v0.26.0 h1:lT1D3OfO+wIi9UFolCrifbjUUgu7CpLca0AD8ghRLI8=
+k8s.io/client-go v0.26.0/go.mod h1:I2Sh57A79EQsDmn7F7ASpmru1cceh3ocVT9KlX2jEZg=
+k8s.io/klog/v2 v2.110.1 h1:U/Af64HJf7FcwMcXyKm2RPM22WZzyR7OSpYj5tg3cL0=
+k8s.io/klog/v2 v2.110.1/go.mod h1:YGtd1984u+GgbuZ7e08/yBuAfKLSO0+uR1Fhi6ExXjo=
+k8s.io/kube-openapi v0.0.0-20231010175941-2dd684a91f00 h1:aVUu9fTY98ivBPKR9Y5w/AuzbMm96cd3YHRTU83I780=
+k8s.io/kube-openapi v0.0.0-20231010175941-2dd684a91f00/go.mod h1:AsvuZPBlUDVuCdzJ87iajxtXuR9oktsTctW/R9wwouA=
+k8s.io/utils v0.0.0-20230726121419-3b25d923346b h1:sgn3ZU783SCgtaSJjpcVVlRqd6GSnlTLKgpAAttJvpI=
+k8s.io/utils v0.0.0-20230726121419-3b25d923346b/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
+sigs.k8s.io/controller-runtime v0.14.1 h1:vThDes9pzg0Y+UbCPY3Wj34CGIYPgdmspPm2GIpxpzM=
+sigs.k8s.io/controller-runtime v0.14.1/go.mod h1:GaRkrY8a7UZF0kqFFbUKG7n9ICiTY5T55P1RiE3UZlU=
+sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=
+sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=
+sigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=
+sigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=
+sigs.k8s.io/yaml v1.3.0 h1:a2VclLzOGrwOHDiV8EfBGhvjHvP46CtW5j6POvhYGGo=
+sigs.k8s.io/yaml v1.3.0/go.mod h1:GeOyir5tyXNByN85N/dRIT9es5UQNerPYEKK56eTBm8=
diff --git a/docker-backup-daemon/go/pkg/azure/client.go b/docker-backup-daemon/go/pkg/azure/client.go
new file mode 100644
index 0000000..1a85501
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/azure/client.go
@@ -0,0 +1,335 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package azure
+
+import (
+	"context"
+	"fmt"
+	"time"
+
+	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
+	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
+	server "github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/postgresql/armpostgresqlflexibleservers/v2"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/config"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/k8s"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/util"
+	"go.uber.org/zap"
+)
+
+const (
+	configFile        = "/app/config/config.json"
+	currentStatusFile = "status"
+	statusSuccessful  = "Successful"
+	statusInProgress  = "In Progress"
+	statusFailed      = "Failed"
+
+	resourceGroup  = "resourceGroup"
+	subscriptionId = "subscriptionId"
+	tenantId       = "tenantId"
+	clientSecret   = "clientSecret"
+	clientId       = "clientId"
+
+	restoreConfigCM = "external-restore-config"
+	mirrorCM        = "mirror-config"
+	DRCM            = "dr-config"
+)
+
+var (
+	logger = util.Logger
+)
+
+type Client struct {
+	clientConfig
+	*config.RestoreConfig
+	creds *azidentity.ClientSecretCredential
+}
+
+const (
+	TypeTagKey = "type"
+
+	DrMode     = "dr"
+	MirrorMode = "mirror"
+)
+
+type Status struct {
+	ServerName    string `json:"serverName"`
+	NewServerName string `json:"newServerName"`
+	Status        string `json:"status"`
+	RestoreId     string `json:"restoreId"`
+	StatusFolder  string `json:"-"`
+}
+
+type clientConfig map[string]string
+
+func (c clientConfig) SubscribtionId() string {
+	return c[subscriptionId]
+}
+
+func NewRestoreClientWithRestoreConfig(restoreCfg *config.RestoreConfig) *Client {
+	config, err := util.ReadConfigFile(configFile)
+	if err != nil {
+		panic(err)
+	}
+
+	return &Client{
+		clientConfig:  config,
+		RestoreConfig: restoreCfg,
+	}
+}
+
+func (azCl *Client) clientInfo(name string) string {
+	return azCl.clientConfig[name]
+}
+
+func (azCl *Client) getStatus(status string) Status {
+	return Status{
+		ServerName:    azCl.ServerName(),
+		NewServerName: azCl.NewServerName(),
+		Status:        status,
+		RestoreId:     azCl.RestoreID(),
+		StatusFolder:  azCl.StatusFolder(),
+	}
+}
+
+func (azCl *Client) RestoreDatabase() {
+	status := azCl.getStatus(statusInProgress)
+	defer checkErrorStatus(status)
+
+	ctx := context.Background()
+	client := azCl.getAzureClient()
+	oldServer, err := client.Get(ctx, azCl.ResourceGroup(), azCl.ServerName(), nil)
+	if err != nil {
+		logger.Error("cannot connect to Azure server", zap.Error(err))
+		panic(err)
+	}
+	setStatus(status)
+
+	azCl.checkRestoreValidity(oldServer)
+
+	azCl.createNewServerFromRP(ctx, client, oldServer)
+	err = azCl.updateNewServerParameters(ctx, client, oldServer)
+	if err != nil {
+		panic(err)
+	}
+
+	if azCl.StopSource() {
+		azCl.stopServer(ctx, client)
+	}
+
+	if azCl.IsGeoRestore() {
+		dataToUpdate := make(map[string]string)
+		dataToUpdate[config.GeoHaKey] = string(*oldServer.Properties.HighAvailability.Mode)
+		dataToUpdate[config.GeoSubnetKey] = *oldServer.Properties.Network.DelegatedSubnetResourceID
+		dataToUpdate[config.GeoLocationKey] = azCl.Location()
+		dataToUpdate[config.GeoRGKey] = azCl.ResourceGroup()
+		dataToUpdate[config.GeoPrivateDNSZone] = *oldServer.Properties.Network.PrivateDNSZoneArmResourceID
+		dataToUpdate[config.MainLocationKey] = azCl.GeoLocation()
+		k8s.UpdateCMData(restoreConfigCM, dataToUpdate)
+	}
+
+	status.Status = statusSuccessful
+	setStatus(status)
+}
+
+func (azCl *Client) checkRestoreValidity(oldServer server.ServersClientGetResponse) {
+	isMirror, isDr := azCl.getSiteModes()
+	// check mirror restrictions
+	if isMirror {
+		logger.Info("Mirror config is found, only mirror restore of source server and restore for mirror server are allowed")
+		tagActualVal, tagIsPresent := oldServer.Tags[TypeTagKey]
+		isMirrorServer := tagIsPresent && *tagActualVal == MirrorMode
+		if isMirrorServer {
+			logger.Info("Pg server for restore is a mirror")
+		} else {
+			logger.Info("Pg server for restore is not a mirror")
+		}
+		if !isMirrorServer && !azCl.IsMirrorRestore() {
+			errorMsg := fmt.Sprintf("Cannot perform only mirror restore on %s from mirror side", *oldServer.Name)
+			logger.Error(errorMsg)
+			panic(errorMsg)
+		}
+	}
+	// check dr restrictions
+	if isDr {
+		logger.Info("Dr config is found, only dr restore of source server and restore for dr server are allowed")
+		tagActualVal, tagIsPresent := oldServer.Tags[TypeTagKey]
+		isDrServer := tagIsPresent && *tagActualVal == DrMode
+		if isDrServer {
+			logger.Info("Pg server for restore is a dr")
+		} else {
+			logger.Info("Pg server for restore is not a dr")
+		}
+		if !isDrServer && !azCl.IsGeoRestore() {
+			errorMsg := fmt.Sprintf("Cannot perform only dr restore on %s from dr side", *oldServer.Name)
+			logger.Error(errorMsg)
+			panic(errorMsg)
+		}
+	}
+}
+
+func (azCl *Client) getSiteModes() (isMirror bool, isDR bool) {
+	isMirror, err := k8s.IsEnvTypeCmExist(mirrorCM)
+	if err != nil {
+		panic(err)
+	}
+
+	isDR, err = k8s.IsEnvTypeCmExist(DRCM)
+	if err != nil {
+		panic(err)
+	}
+
+	return
+}
+
+func (azCl *Client) getAzureClient() *server.ServersClient {
+	client, err := server.NewServersClient(azCl.clientInfo(subscriptionId), azCl.getAzureCreds(), nil)
+	if err != nil {
+		logger.Error(fmt.Sprintf("failed to get server client: %v", err))
+		panic(err)
+	}
+	return client
+}
+
+func (azCl *Client) getAzureCreds() *azidentity.ClientSecretCredential {
+	if azCl.creds == nil {
+		creds, err := azidentity.NewClientSecretCredential(azCl.clientInfo(tenantId), azCl.clientInfo(clientId), azCl.clientInfo(clientSecret), nil)
+		if err != nil {
+			logger.Error(fmt.Sprintf("failed to obtain a credential: %v", err))
+			panic(err)
+		}
+		azCl.creds = creds
+	}
+	return azCl.creds
+}
+
+func (azCl *Client) createNewServerFromRP(ctx context.Context, client *server.ServersClient, oldServer server.ServersClientGetResponse) {
+	logger.Info(fmt.Sprintf("Starting restore of database %s with new name %s by point in time %s", *oldServer.Name, azCl.NewServerName(), azCl.RestoreTime()))
+	locationName := *oldServer.Location
+	avZone := oldServer.Properties.AvailabilityZone
+	restoreType := server.CreateModePointInTimeRestore
+	subnet := oldServer.Properties.Network.DelegatedSubnetResourceID
+	backup := oldServer.Properties.Backup
+	resourceGroup := azCl.ResourceGroup()
+	privateDnsZone := oldServer.Properties.Network.PrivateDNSZoneArmResourceID
+	// Select restore mode
+	if azCl.IsGeoRestore() {
+		locationName = azCl.GeoLocation()
+		resourceGroup = azCl.GeoResourceGroup()
+		restoreType = server.CreateModeGeoRestore
+		subnet = to.Ptr(azCl.GeoSubnet())
+		avZone = nil
+		if len(azCl.GeoPrivateDNSZone()) != 0 {
+			logger.Info(fmt.Sprintf("geo restore in progress and dnsZone is specified, "+
+				"overriding it to: %s", azCl.GeoPrivateDNSZone()))
+			privateDnsZone = to.Ptr(azCl.GeoPrivateDNSZone())
+		}
+		// Disable Geo-Redundant backup on restored server due to Azure restrictions
+		disabledEnum := server.GeoRedundantBackupEnumDisabled
+		backup.GeoRedundantBackup = &disabledEnum
+	} else if azCl.IsMirrorRestore() {
+		resourceGroup = azCl.MirrorResourceGroup()
+		mirrorSubnet := azCl.MirrorSubnet()
+		subnet = &mirrorSubnet
+		if len(azCl.MirrorPrivateDNSZone()) != 0 {
+			logger.Info(fmt.Sprintf("mirror enabled and dnsZone is specified, "+
+				"overriding it to: %s", azCl.MirrorPrivateDNSZone()))
+			privateDnsZone = to.Ptr(azCl.MirrorPrivateDNSZone())
+		}
+	}
+
+	newServer := server.Server{
+		Location: &locationName,
+		Properties: &server.ServerProperties{
+			AvailabilityZone:       avZone,
+			Backup:                 backup,
+			CreateMode:             &restoreType,
+			PointInTimeUTC:         to.Ptr(func() time.Time { t, _ := time.Parse(time.RFC3339Nano, azCl.RestoreTime()); return t }()),
+			SourceServerResourceID: to.Ptr(fmt.Sprintf("/subscriptions/%s/resourceGroups/%s/providers/Microsoft.DBforPostgreSQL/flexibleServers/%s", azCl.clientInfo(subscriptionId), azCl.ResourceGroup(), *oldServer.Name)),
+			Network: &server.Network{
+				DelegatedSubnetResourceID:   subnet,
+				PrivateDNSZoneArmResourceID: privateDnsZone,
+				PublicNetworkAccess:         oldServer.Properties.Network.PublicNetworkAccess,
+			},
+		},
+	}
+
+	poller, err := client.BeginCreate(ctx,
+		resourceGroup,
+		azCl.NewServerName(),
+		newServer,
+		nil)
+
+	if err != nil {
+		logger.Error("cannot create new Flexible Postgres database", zap.Error(err))
+		panic(err)
+	}
+
+	resCreate, err := poller.PollUntilDone(ctx, nil)
+	if err != nil {
+		panic(err)
+	}
+
+	_ = resCreate
+}
+
+func (azCl *Client) stopServer(ctx context.Context, client *server.ServersClient) {
+	logger.Info(fmt.Sprintf("stopping the server %s", azCl.ServerName()))
+	pollerStop, err := client.BeginStop(ctx,
+		azCl.ResourceGroup(),
+		azCl.ServerName(),
+		nil)
+	if err != nil {
+		logger.Warn(fmt.Sprintf("error during \"Stop server\" operation starting for the server %s", azCl.ServerName()), zap.Error(err))
+		return
+	}
+
+	resStop, err := pollerStop.PollUntilDone(ctx, nil)
+	if err != nil {
+		logger.Warn(fmt.Sprintf("cannot stop the server %s", azCl.ServerName()), zap.Error(err))
+		return
+	}
+	_ = resStop
+}
+
+func checkErrorStatus(status Status) {
+	if r := recover(); r != nil {
+		status.Status = statusFailed
+		setStatus(status)
+		panic(r)
+	}
+}
+
+func setStatus(status Status) {
+	statusFilePath := fmt.Sprintf("%s/%s", status.StatusFolder, status.RestoreId)
+	err := util.WriteFile(statusFilePath, status)
+	if err != nil {
+		logger.Error("Cannot write to status file", zap.Error(err))
+	}
+
+	setCurrentStatus(status)
+}
+
+// TODO expire
+func setCurrentStatus(status Status) {
+	statusPath := fmt.Sprintf("%s/%s", status.StatusFolder, currentStatusFile)
+	if status.Status == statusInProgress {
+		err := util.WriteFile(statusPath, status)
+		if err != nil {
+			logger.Error("Cannot write to status file", zap.Error(err))
+		}
+	} else {
+		util.DeleteFile(statusPath)
+	}
+}
diff --git a/docker-backup-daemon/go/pkg/azure/parameters.go b/docker-backup-daemon/go/pkg/azure/parameters.go
new file mode 100644
index 0000000..7f2d36e
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/azure/parameters.go
@@ -0,0 +1,74 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package azure
+
+import (
+	"context"
+	"fmt"
+	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
+	server "github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/postgresql/armpostgresqlflexibleservers/v2"
+	"go.uber.org/zap"
+)
+
+func (azCl *Client) updateNewServerParameters(ctx context.Context, client *server.ServersClient, oldServer server.ServersClientGetResponse) error {
+	HAMode := oldServer.Properties.HighAvailability.Mode
+	tags := oldServer.Tags
+	resourceGroup := azCl.ResourceGroup()
+	if azCl.IsMirrorRestore() {
+		resourceGroup = azCl.MirrorResourceGroup()
+		logger.Info("turning off HA, because we do override of subnet")
+		HAMode = to.Ptr(server.HighAvailabilityModeDisabled)
+		tags[TypeTagKey] = to.Ptr(MirrorMode)
+	} else if azCl.IsGeoRestore() {
+		resourceGroup = azCl.GeoResourceGroup()
+		geoHaModeStr := azCl.GeoHA()
+		if geoHaModeStr != "" {
+			logger.Info("changing HA mode, due to configuration parameters")
+			geoHaMode := server.HighAvailabilityMode(geoHaModeStr)
+			HAMode = &geoHaMode
+		}
+		tags[TypeTagKey] = to.Ptr(DrMode)
+	}
+	retentionDays := oldServer.Properties.Backup.BackupRetentionDays
+	logger.Info(fmt.Sprintf("Setting new server HA parameters: mode=%s, backup retension days=%d", *HAMode, *retentionDays))
+	poller, err := client.BeginUpdate(ctx,
+		resourceGroup,
+		azCl.NewServerName(),
+		server.ServerForUpdate{
+			Properties: &server.ServerPropertiesForUpdate{
+				HighAvailability: &server.HighAvailability{
+					Mode: HAMode,
+				},
+				Backup: &server.Backup{BackupRetentionDays: retentionDays},
+			},
+			Tags: tags,
+		},
+		nil)
+
+	if err != nil {
+		logger.Error("cannot update new Flexible Postgres database", zap.Error(err))
+		panic(err)
+	}
+
+	resUpdate, err := poller.PollUntilDone(ctx, nil)
+	if err != nil {
+		logger.Error("error during polling HA update", zap.Error(err))
+		panic(err)
+	}
+
+	_ = resUpdate
+
+	return nil
+}
diff --git a/docker-backup-daemon/go/pkg/config/restore_config.go b/docker-backup-daemon/go/pkg/config/restore_config.go
new file mode 100644
index 0000000..1039ea9
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/config/restore_config.go
@@ -0,0 +1,176 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package config
+
+import (
+	"fmt"
+	"strconv"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/k8s"
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/util"
+)
+
+const (
+	restoreConfigCM = "external-restore-config"
+
+	MirrorSubnetKey      = "mirror.subnet"
+	MirrorPrivateDNSZone = "mirror.privateDnsZone"
+	MirrorRGKey          = "mirror.resourceGroup"
+	GeoSubnetKey         = "geo.subnet"
+	GeoLocationKey       = "geo.location"
+	GeoRGKey             = "geo.resourceGroup"
+	GeoPrivateDNSZone    = "geo.privateDnsZone"
+	GeoHaKey             = "geo.ha"
+	MainLocationKey      = "main.location"
+	MainRGKey            = "main.resourceGroup"
+)
+
+var (
+	logger = util.Logger
+)
+
+// RestoreConfig holds configuration for restore
+// if subnet is specified, means mirror restore -> don't stop source, add additional tag, override subnet from CM
+// restoreAsSeparate -> restore az pg as separe instance -> don't stop source, don't update k8s service
+type RestoreConfig struct {
+	serverName        string
+	newServerName     string
+	restoreId         string
+	restoreTime       string
+	statusFolder      string
+	restoreAsSeparate string
+	geoRestore        string
+	mirror            string
+	cmData            map[string]string
+}
+
+func (cfg *RestoreConfig) NewServerName() string {
+	return cfg.newServerName
+}
+
+func (cfg *RestoreConfig) ServerName() string {
+	return cfg.serverName
+}
+
+func (cfg *RestoreConfig) RestoreAsSeparate() bool {
+	return strings.Compare(strings.ToLower(cfg.restoreAsSeparate), "true") == 0
+}
+
+func (cfg *RestoreConfig) IsMirrorRestore() bool {
+	return strings.ToLower(cfg.mirror) == "true"
+}
+
+func (cfg *RestoreConfig) MirrorResourceGroup() string {
+	return cfg.cmData[MirrorRGKey]
+}
+
+func (cfg *RestoreConfig) MirrorSubnet() string {
+	return cfg.cmData[MirrorSubnetKey]
+}
+
+func (cfg *RestoreConfig) MirrorPrivateDNSZone() string {
+	return cfg.cmData[MirrorPrivateDNSZone]
+}
+
+func (cfg *RestoreConfig) ResourceGroup() string {
+	return cfg.cmData[MainRGKey]
+}
+
+func (cfg *RestoreConfig) Location() string {
+	return cfg.cmData[MainLocationKey]
+}
+
+func (cfg *RestoreConfig) IsGeoRestore() bool {
+	return strings.ToLower(cfg.geoRestore) == "true"
+}
+
+func (cfg *RestoreConfig) GeoSubnet() string {
+	return cfg.cmData[GeoSubnetKey]
+}
+
+func (cfg *RestoreConfig) GeoHA() string {
+	return cfg.cmData[GeoHaKey]
+}
+
+func (cfg *RestoreConfig) GeoLocation() string {
+	return cfg.cmData[GeoLocationKey]
+}
+
+func (cfg *RestoreConfig) GeoResourceGroup() string {
+	return cfg.cmData[GeoRGKey]
+}
+
+func (cfg *RestoreConfig) GeoPrivateDNSZone() string {
+	return cfg.cmData[GeoPrivateDNSZone]
+}
+
+func (cfg *RestoreConfig) RestoreID() string {
+	return cfg.restoreId
+}
+
+func (cfg *RestoreConfig) StatusFolder() string {
+	return cfg.statusFolder
+}
+
+func (cfg *RestoreConfig) RestoreTime() string {
+	return cfg.restoreTime
+}
+
+// StopSource Stop of Source should be done only in regular restore
+// restoreAsSeparate=false subnet=false
+func (cfg *RestoreConfig) StopSource() bool {
+	return !cfg.RestoreAsSeparate() && !cfg.IsMirrorRestore() && !cfg.IsGeoRestore()
+}
+
+func NewRestoreConfig(restoreId, restoreTime, statusFolder, restoreAsSeparate, geoRestore, subnet string) *RestoreConfig {
+	serverName, err := k8s.GetServerName()
+	if err != nil {
+		panic(err)
+	}
+	newServerName := GenerateNewName(serverName)
+
+	var cmData map[string]string
+	cm, err := k8s.GetCM(restoreConfigCM)
+	if err != nil {
+		panic(err)
+	}
+	cmData = cm.Data
+
+	logger.Info(fmt.Sprintf("restoreAsSeparate: %s, subnet: %s, geoRestore %s", restoreAsSeparate, subnet, geoRestore))
+	return &RestoreConfig{
+		serverName:        serverName,
+		newServerName:     newServerName,
+		restoreId:         restoreId,
+		restoreTime:       restoreTime,
+		statusFolder:      statusFolder,
+		restoreAsSeparate: restoreAsSeparate,
+		geoRestore:        geoRestore,
+		mirror:            subnet,
+		cmData:            cmData,
+	}
+}
+
+func GenerateNewName(oldServerName string) string {
+	now := time.Now()
+	timeString := strconv.Itoa(int(now.Unix()))
+	if strings.HasSuffix(oldServerName, "-restored") {
+		oldServerSlice := strings.Split(oldServerName, "-")
+		oldServerSlice[len(oldServerSlice)-2] = timeString
+		return strings.Join(oldServerSlice, "-")
+	}
+	return fmt.Sprintf("%s-%s-restored", oldServerName, timeString)
+}
diff --git a/docker-backup-daemon/go/pkg/k8s/client.go b/docker-backup-daemon/go/pkg/k8s/client.go
new file mode 100644
index 0000000..f7dea25
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/k8s/client.go
@@ -0,0 +1,173 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package k8s
+
+import (
+	"context"
+	"fmt"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/pgskipper-backup-daemon/pkg/util"
+	"go.uber.org/zap"
+	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/api/errors"
+	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/apimachinery/pkg/util/wait"
+	crclient "sigs.k8s.io/controller-runtime/pkg/client"
+)
+
+const (
+	serviceName     = "pg-patroni"
+	externalCMName  = "postgres-external"
+	connectionKey   = "connectionName"
+	mirrorSubnetKey = "mirror.subnet"
+)
+
+var (
+	logger    = util.Logger
+	k8sClient crclient.Client
+	namespace = ""
+)
+
+func init() {
+	var err error
+	namespace, err = util.GetNamespace()
+	if err != nil {
+		panic(err)
+	}
+	k8sClient, err = util.CreateClient()
+	if err != nil {
+		logger.Error("Can't create k8s client")
+		panic(err)
+	}
+}
+
+func UpdateExternalService(newDBName string) error {
+	extService, err := getExternalService()
+	if err != nil {
+		return err
+	}
+	extService.Spec.ExternalName = newDBName + ".postgres.database.azure.com"
+	if err = k8sClient.Update(context.TODO(), extService); err != nil {
+		logger.Error("Can't update external service", zap.Error(err))
+		return err
+	}
+	return nil
+}
+
+func UpdateExternalCM(dbName, newDBName string) error {
+	extCM, err := GetCM(externalCMName)
+	if err != nil {
+		return err
+	}
+	extCM.Data[connectionKey] = strings.ReplaceAll(extCM.Data[connectionKey], dbName, newDBName)
+	if err = k8sClient.Update(context.TODO(), extCM); err != nil {
+		logger.Error("Can't update external CM", zap.Error(err))
+		return err
+	}
+	return nil
+}
+
+func getExternalService() (*corev1.Service, error) {
+	extService := &corev1.Service{}
+	var k8sErr error
+	wait.PollImmediate(3*time.Second, 5*time.Minute, func() (done bool, err error) {
+		k8sErr = k8sClient.Get(context.TODO(), types.NamespacedName{
+			Name: serviceName, Namespace: namespace,
+		}, extService)
+		if k8sErr != nil {
+			logger.Error("Error during obtaining ext service info, retrying...", zap.Error(err))
+			return false, nil
+		}
+		return true, nil
+	})
+	if k8sErr != nil {
+		logger.Error("Timeout exceeded", zap.Error(k8sErr))
+	}
+	return extService, k8sErr
+}
+
+func GetServerName() (string, error) {
+	cm, err := GetCM(externalCMName)
+	if err != nil {
+		return "", err
+	}
+	fullName := cm.Data[connectionKey]
+	splitStr := strings.Split(fullName, ".")
+	return splitStr[0], nil
+}
+
+func GetCM(cmName string) (*corev1.ConfigMap, error) {
+	cm := &corev1.ConfigMap{}
+	var k8sErr error
+	wait.PollImmediate(3*time.Second, 5*time.Minute, func() (done bool, err error) {
+		k8sErr = k8sClient.Get(context.TODO(), types.NamespacedName{
+			Name: cmName, Namespace: namespace,
+		}, cm)
+		if k8sErr != nil {
+			logger.Error(fmt.Sprintf("Error during obtaining %s ConfigMap, retrying...", cmName), zap.Error(err))
+			return false, nil
+		}
+		return true, nil
+	})
+	if k8sErr != nil {
+		logger.Error("Timeout exceeded", zap.Error(k8sErr))
+	}
+	return cm, k8sErr
+}
+
+// This CM must be created by Velero in case environment has specific type
+func IsEnvTypeCmExist(cmName string) (bool, error) {
+	err := wait.PollImmediate(3*time.Second, 5*time.Minute, func() (done bool, err error) {
+		cm := &corev1.ConfigMap{}
+		k8sErr := k8sClient.Get(context.TODO(), types.NamespacedName{
+			Name: cmName, Namespace: namespace,
+		}, cm)
+		if k8sErr != nil {
+			if errors.IsNotFound(k8sErr) {
+				logger.Error(fmt.Sprintf("Cm %s is not found", cmName))
+				return false, k8sErr
+			}
+			logger.Error(fmt.Sprintf("Error during obtaining %s ConfigMap, retrying...", cmName), zap.Error(err))
+			return false, nil
+		}
+		return true, nil
+	})
+	if err != nil {
+		if errors.IsNotFound(err) {
+			return false, nil
+		}
+		return false, err
+	}
+	return true, nil
+}
+
+func UpdateCMData(cmName string, dataForUpdate map[string]string) error {
+	cm, err := GetCM(cmName)
+	if err != nil {
+		return err
+	}
+	for key, value := range dataForUpdate {
+		cm.Data[key] = value
+	}
+
+	err = k8sClient.Update(context.TODO(), cm)
+	if err != nil {
+		logger.Error("Error during updating ext CM", zap.Error(err))
+		return err
+	}
+	return nil
+}
diff --git a/docker-backup-daemon/go/pkg/util/transport.go b/docker-backup-daemon/go/pkg/util/transport.go
new file mode 100644
index 0000000..0f46c2e
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/util/transport.go
@@ -0,0 +1,76 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package util
+
+import (
+	"fmt"
+	"net/http"
+	"time"
+
+	"k8s.io/client-go/rest"
+	"k8s.io/client-go/transport"
+)
+
+const retryNumEnv = "K8S_CLIENT_RETRY_NUM"
+
+type Retry struct {
+	nums int
+	http.RoundTripper
+}
+
+func (r *Retry) RoundTrip(req *http.Request) (resp *http.Response, err error) {
+	logger := Logger
+	logger.Debug(fmt.Sprintf("executing request: %s with retries", req.URL))
+	for i := 0; i < r.nums; i++ {
+		logger.Debug(fmt.Sprintf("attempt: %v of %v", i+1, r.nums))
+		resp, err = r.RoundTripper.RoundTrip(req)
+		if err != nil && resp == nil {
+			logger.Error(fmt.Sprintf("received not retryable error %v", err))
+			return
+		} else if err != nil || resp.StatusCode >= 500 {
+			logger.Warn(fmt.Sprintf("received retryable error %v, with status code: %d ,retrying...", err, resp.StatusCode))
+			time.Sleep(10 * time.Second)
+			continue
+		} else {
+			logger.Debug("executed successfully, exiting")
+			return
+		}
+	}
+	logger.Warn("no more retries, giving up...")
+	return
+}
+
+func UpdateTransport(cfg *rest.Config) {
+	tc, err := cfg.TransportConfig()
+	if err != nil {
+		panic(err)
+	}
+	rt, err := transport.New(tc)
+	if err != nil {
+		panic(err)
+	}
+	cfg.Transport = getRetryTransport(rt)
+
+	// Security moved to transport level
+	cfg.TLSClientConfig = rest.TLSClientConfig{}
+}
+
+func getRetryTransport(rt http.RoundTripper) *Retry {
+	retryNum := getEnvInt(retryNumEnv, 10)
+	return &Retry{
+		nums:         retryNum,
+		RoundTripper: rt,
+	}
+}
diff --git a/docker-backup-daemon/go/pkg/util/util.go b/docker-backup-daemon/go/pkg/util/util.go
new file mode 100644
index 0000000..68a0620
--- /dev/null
+++ b/docker-backup-daemon/go/pkg/util/util.go
@@ -0,0 +1,150 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package util
+
+import (
+	"encoding/json"
+	"fmt"
+	"os"
+	"strconv"
+
+	azlog "github.com/Azure/azure-sdk-for-go/sdk/azcore/log"
+	"go.uber.org/zap"
+	"go.uber.org/zap/zapcore"
+	"k8s.io/apimachinery/pkg/runtime"
+	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
+	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
+	crclient "sigs.k8s.io/controller-runtime/pkg/client"
+	"sigs.k8s.io/controller-runtime/pkg/client/config"
+)
+
+const nsPath = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+
+var (
+	Logger         = getLogger()
+	AzureLogEvents = []azlog.Event{azlog.EventRequest, azlog.EventResponse, azlog.EventLRO, azlog.EventRetryPolicy}
+)
+
+func getLogger() *zap.Logger {
+	atom := zap.NewAtomicLevel()
+	encoderCfg := zap.NewProductionEncoderConfig()
+	encoderCfg.TimeKey = "timestamp"
+	encoderCfg.EncodeTime = zapcore.ISO8601TimeEncoder
+
+	logger := zap.New(zapcore.NewCore(
+		zapcore.NewJSONEncoder(encoderCfg),
+		zapcore.Lock(os.Stdout),
+		atom,
+	))
+	defer logger.Sync()
+	return logger
+}
+
+func CreateClient() (crclient.Client, error) {
+	clientConfig, err := config.GetConfig()
+	if err != nil {
+		return nil, err
+	}
+	scheme := runtime.NewScheme()
+	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
+	UpdateTransport(clientConfig)
+	client, err := crclient.New(clientConfig, crclient.Options{Scheme: scheme})
+	if err != nil {
+		return nil, err
+	}
+	return client, nil
+}
+
+func ReadConfigFile(filePath string) (map[string]string, error) {
+	file, err := os.ReadFile(filePath)
+	if err != nil {
+		Logger.Error(fmt.Sprintf("cannot read config file: %s", filePath))
+		return nil, err
+	}
+	var externalConfig map[string]string
+	err = json.Unmarshal(file, &externalConfig)
+	if err != nil {
+		Logger.Error(fmt.Sprintf("Failed to parse config file %s", filePath), zap.Error(err))
+		return nil, err
+	}
+	return externalConfig, nil
+}
+
+func WriteFile(filePath string, data interface{}) error {
+	file, err := os.Create(filePath)
+	if err != nil {
+		Logger.Error(fmt.Sprintf("cannot create %s file", filePath))
+		return err
+	}
+	defer file.Close()
+
+	dataStr, err := json.Marshal(data)
+	if err != nil {
+		Logger.Error("Cannot convert data to string")
+		return err
+	}
+	_, err = file.Write(dataStr)
+	if err != nil {
+		Logger.Error(fmt.Sprintf("cannot write into %s file", filePath))
+		return err
+	}
+	return nil
+}
+
+func DeleteFile(filepath string) {
+	err := os.Remove(filepath)
+	if err != nil {
+		Logger.Error("cannot remove current task file")
+	}
+}
+
+func ConfigureAzLogging() {
+	azlog.SetListener(func(cls azlog.Event, msg string) {
+		prefixLog := "received event: "
+		switch cls {
+		case azlog.EventLRO:
+			prefixLog = "long running event: "
+		case azlog.EventRetryPolicy:
+			prefixLog = "retry event: "
+		}
+		Logger.Info("[azlog]" + prefixLog + msg)
+	})
+	azlog.SetEvents(AzureLogEvents...)
+}
+
+func getEnvInt(name string, defValue int) int {
+	val := os.Getenv(name)
+	if val == "" {
+		return defValue
+	}
+	intVal, err := strconv.ParseInt(val, 10, 32)
+	if err != nil {
+		Logger.Warn(fmt.Sprintf("cannot parse %s env variable, value %d will be used", name, defValue))
+		return defValue
+	}
+	return int(intVal)
+}
+
+func GetNamespace() (string, error) {
+	return ReadFromFile(nsPath)
+}
+
+func ReadFromFile(filePath string) (string, error) {
+	dat, err := os.ReadFile(filePath)
+	if err != nil {
+		return "", err
+	}
+	return string(dat), nil
+}
diff --git a/docker-backup-daemon/maintenance/recovery/pg_back_rest_recovery.py b/docker-backup-daemon/maintenance/recovery/pg_back_rest_recovery.py
new file mode 100644
index 0000000..86bbd35
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/pg_back_rest_recovery.py
@@ -0,0 +1,433 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+
+from utils_oc import OpenshiftOrchestrator, OpenshiftPyClient
+from recovery import cleanup_pg_pod_data_directory
+from kubernetes import client
+from kubernetes.client import configuration
+from kubernetes.client import rest
+from kubernetes.stream import stream
+from kubernetes.stream.ws_client import ERROR_CHANNEL, STDOUT_CHANNEL, STDERR_CHANNEL
+import kubernetes
+from utils_common import retry
+
+import time
+import requests
+import logging
+import os
+import yaml
+import json
+
+
+log = logging.getLogger()
+log.setLevel(logging.DEBUG)
+retry_count = 60
+
+skip_tls_verify = os.getenv("OC_SKIP_TLS_VERIFY", "true")
+oc_openshift_url = os.getenv("OC_OPENSHIFT_URL", None)
+oc_project = os.getenv("POD_NAMESPACE", None)
+pg_cluster_name = os.getenv("PG_CLUSTER_NAME", None)
+exec_timeout = int(os.getenv("OC_EXEC_TIMEOUT", "3600"))
+
+pg_dir = "/var/lib/pgsql/data"
+pg_data_dir = "{}/postgresql_${{POD_IDENTITY}}".format(pg_dir)
+
+
+class PgBackRestRecovery():
+    def __init__(self):
+        try:
+            from kubernetes import config as k8s_config
+            k8s_config.load_incluster_config()
+            log.info("Using pyclient")
+            self.oc_client = OpenshiftPyClient()
+            self.oc_client.use_token(oc_url=oc_openshift_url, oc_token="", project=oc_project, skip_tls_verify=skip_tls_verify)
+            self._api_client = None
+            self.project = os.getenv("POD_NAMESPACE")
+            self.oc_orch = OpenshiftOrchestrator(self.oc_client, retry_count)
+            try:
+                self.apps_api = client.AppsV1Api(self._api_client)
+            except:
+                self.apps_api = client.AppsV1beta1Api(self._api_client)
+
+        except Exception as e:
+            log.exception("Failed to create OpenshiftPyClient")
+
+
+    def get_patroni_replicas_ip(self, statefulsets):
+        r = requests.get("pg-patroni:8008")
+        return r.json()['replication']
+
+
+    def get_patroni_statefulsets(self):
+        stateful_sets = self.apps_api.list_namespaced_stateful_set(self.project).items
+        return stateful_sets
+
+    def patch_statefulset_cmd(self, stateful_set, stateful_set_name, cmd):
+
+        log.info(f'Going to set {stateful_set_name} with {cmd} command')
+
+        stateful_set.spec.template.spec.containers[0].command = cmd
+
+        self.apps_api.patch_namespaced_stateful_set(stateful_set_name, self.project, stateful_set)
+
+
+    def scale_statefulset(self, stateful_set_name, replicas):
+
+        log.info(f'Going to set {stateful_set_name} with {replicas} replicas')
+
+        self.apps_api.patch_namespaced_stateful_set_scale(stateful_set_name, self.project, {"spec": {"replicas": replicas}})
+
+    def patch_configmap(self, config_map_name, config_map):
+
+        log.info(f'Going to replace {config_map_name}')
+        core_api = client.CoreV1Api(self._api_client)
+        core_api.replace_namespaced_config_map(config_map_name, self.project, config_map)
+
+
+    def delete_master_cm(self):
+        try:
+            log.info("Delete leader cm")
+            body = client.V1DeleteOptions()
+            core_api = client.CoreV1Api(self._api_client)
+            core_api.delete_namespaced_config_map("patroni-leader", self.project, body=body,)
+        except kubernetes.client.rest.ApiException as e:
+            if e.reason == "Not Found":
+                return
+            else:
+                return e
+
+    def clean_patroni_cm(self):
+        log.info("Delete initialize key")
+        cmaps = client.CoreV1Api(self._api_client).list_namespaced_config_map(self.project).items
+        for cm in cmaps:
+            if cm.metadata.name == 'patroni-config':
+                if "initialize" in cm.metadata.annotations:
+                    del cm.metadata.annotations["initialize"]
+                    self.patch_configmap(cm.metadata.name, cm)
+                return cm
+
+    def get_config_map(self, name):
+        cmaps = client.CoreV1Api(self._api_client).list_namespaced_config_map(self.project).items
+        for cm in cmaps:
+            if cm.metadata.name == name:
+                return cm
+
+    def get_template_cm(self):
+        template_cm = self.get_config_map(f"patroni-{pg_cluster_name}.config.yaml")
+        if not template_cm:
+            log.info(f"Can't find patroni-{pg_cluster_name}.config.yaml, trying to find {pg_cluster_name}-patroni.config.yaml")
+            template_cm = self.get_config_map(f"{pg_cluster_name}-patroni.config.yaml")
+        return template_cm
+
+    def create_custom_bootstrap_method(self, target, restore_type, target_timeline):
+        template_cm = self.get_template_cm()
+        patroni_config_data = template_cm.data['patroni-config-template.yaml']
+        log.info(f"Data {patroni_config_data}")
+        dict_data = yaml.load(patroni_config_data,Loader=yaml.FullLoader)
+        dict_data["bootstrap"]["pgbackrest"] = {
+            "command": f"pgbackrest --stanza=patroni --delta --type={restore_type} --target='{target}' --target-timeline={target_timeline} --target-action=promote restore",
+            "keep_existing_recovery_conf": "True",
+            "no_params": "True"
+        }
+        dict_data["bootstrap"]["method"] = "pgbackrest"
+        template_cm.data['patroni-config-template.yaml'] = yaml.dump(dict_data)
+        self.patch_configmap(template_cm.metadata.name, template_cm)
+
+    def clean_custom_bootstrap_method(self):
+        log.info(f"Pop custom bootstrap method from patroni-template config map")
+        template_cm = self.get_template_cm()
+        patroni_config_data = template_cm.data['patroni-config-template.yaml']
+        dict_data = yaml.load(patroni_config_data,Loader=yaml.FullLoader)
+        dict_data["bootstrap"].pop("method", None)
+        dict_data["bootstrap"].pop("pgbackrest", None)
+        template_cm.data['patroni-config-template.yaml'] = yaml.dump(dict_data)
+        self.patch_configmap(template_cm.metadata.name, template_cm)
+
+    @retry(tries=3600, delay=1)
+    def upgrade_stanza(self):
+        # wait for leader to upgrade stanza
+        logging.basicConfig(level=logging.DEBUG)
+        r = requests.post("http://backrest:3000/upgrade")
+        log.info(f'{r.status_code}, {r.text}')
+        
+        # Raise exception for status codes that should trigger retry
+        if r.status_code in [502, 503, 504]:
+            log.warning(f"Server error {r.status_code}: {r.text}")
+            raise requests.exceptions.RequestException(f"Server error {r.status_code}: {r.text}")
+        
+        return r
+
+    @retry(tries=3600, delay=1)
+    def restore_pod(self, pod_name, backup_id):
+        # wait for pod to restore
+        log.info(f'Will invoke restore command for pod {pod_name}')
+        logging.basicConfig(level=logging.DEBUG)
+        r = requests.post(f"http://{pod_name}.backrest-headless:3000/restore", data={'backupId':backup_id})
+        log.info(f'{r.status_code}, {r.text}')
+        
+        # Raise exception for status codes that should trigger retry
+        if r.status_code in [502, 503, 504]:
+            raise requests.exceptions.RequestException(f"Server error {r.status_code}: {r.text}")
+
+        return r.status_code
+
+    def perform_restore(self):
+        backup_id = '' if not os.getenv("SET") else os.getenv("SET")
+        restore_type = '' if not os.getenv("TYPE") else os.getenv("TYPE")
+        target = '' if not os.getenv("TARGET") else os.getenv("TARGET")
+        target_timeline = 'current' if not os.getenv("TARGET_TIMELINE") else os.getenv("TARGET_TIMELINE")
+        replicas_only_env = False if not os.getenv("REPLICAS_ONLY") else os.getenv("REPLICAS_ONLY")
+        replicas_only = replicas_only_env and replicas_only_env.lower() in ['true', '1', 'yes']
+
+        http_codes = {}
+        stateful_sets =  self.get_patroni_statefulsets()
+
+        if replicas_only:
+            return self.restore_replicas_only(stateful_sets)
+
+        for stateful_set in stateful_sets:
+            self.cleanup_data_for_stateful_set(stateful_set)
+            time.sleep(15)
+
+        stateful_sets =  self.get_patroni_statefulsets()
+
+        stateful_set = stateful_sets[0]
+        stateful_set_name = stateful_sets[0].metadata.name
+        pod_name = stateful_set.metadata.name + "-0"
+
+        # Clean up patroni config map and master config map
+        self.clean_patroni_cm()
+        self.delete_master_cm()
+
+        # Create custom bootstrap method if target is provided
+        if target:
+            log.info(f"Target has been provided, so starting PITR for pod {pod_name}")
+            self.create_custom_bootstrap_method(target, restore_type, target_timeline)
+        else:
+            log.info(f"Starting full restore procedure for pod {pod_name}")
+            http_codes[stateful_set_name] = self.restore_pod(pod_name, backup_id)
+
+        if target or http_codes[stateful_set_name] == 200:
+            log.info(f"Restore return 200 http state, so remove sleep cmd")
+            self.patch_statefulset_cmd(stateful_set, stateful_set_name, [])
+            time.sleep(5)
+            self.scale_statefulset(stateful_set_name,0)
+            time.sleep(15)
+            self.scale_statefulset(stateful_set_name,1)
+        else:
+            log.error(f'Restore procedure for {stateful_set_name} ends with error. It was {http_codes[stateful_set_name]}')
+            return
+        if not self.wait_for_pod(pod_name, attempts=5):
+            raise Exception("Pod {} is not ready".format(pod_name))
+        
+        # Check for master pod
+        try:
+            self.find_cluster_pods("master", 1)
+        except Exception as e:
+            log.error(f"Failed to find master pod after retries: {e}")
+            raise
+
+        # Clean up custom bootstrap method and upgrade stanza
+        self.clean_custom_bootstrap_method()
+        self.upgrade_stanza()
+
+        log.info("Leader database has been restored")
+        if len(stateful_sets) > 1:
+            # Restore replicas
+            self.restore_replicas(stateful_sets[1:])
+
+        log.info("All pods have been restored")
+        log.info("Done")
+
+    def restore_replicas_only(self, stateful_sets):
+        log.info("Restoring replicas only")
+        master_pods = self.find_cluster_pods("master", 1)
+        # Extract stateful set name from master pod name (remove "-0" suffix)
+        master_statefulset_name = master_pods[0].metadata.name.rsplit("-", 1)[0]
+        log.info(f"Master stateful set name: {master_statefulset_name}")
+        
+        # Clean up non-master stateful sets
+        for stateful_set in stateful_sets:
+            if stateful_set.metadata.name != master_statefulset_name:
+                self.cleanup_data_for_stateful_set(stateful_set)
+                time.sleep(15)
+        
+        # Filter replicas after getting fresh state
+        stateful_sets = self.get_patroni_statefulsets()
+        replicas_stateful_sets = [
+            stateful_set for stateful_set in stateful_sets 
+            if stateful_set.metadata.name != master_statefulset_name
+        ]
+        
+        if replicas_stateful_sets:
+            self.restore_replicas(replicas_stateful_sets)
+            log.info("Replicas have been restored")
+        else:
+            log.info("No replicas to restore")
+        return
+
+    def cleanup_data_for_stateful_set(self, stateful_set):
+        stateful_set_name = stateful_set.metadata.name
+        pod_name = stateful_set.metadata.name + "-0"
+
+        cmd = ["sh", "-c", "while true ; do sleep 3600; done"]
+        self.patch_statefulset_cmd(stateful_set, stateful_set_name, cmd)
+        time.sleep(5)
+        # Just in case when pods could be scaled 0
+        self.scale_statefulset(stateful_set_name,1)
+        if not self.wait_for_pod(pod_name, attempts=5):
+            raise Exception("Pod {} is not ready".format(pod_name))
+        self.cleanup_patroni_data(pod_name, stateful_set_name, False)
+
+    def restore_replicas(self, stateful_sets):
+        # Trigger incremental backup
+        incr_backup_id = self.trigger_diff_backup()
+
+        # Wait for backup to appear in list
+        self.wait_diff_backup(incr_backup_id)
+        
+        for stateful_set in stateful_sets:
+            stateful_set_name = stateful_set.metadata.name
+            pod_name = stateful_set.metadata.name + "-0"
+            self.patch_statefulset_cmd(stateful_set, stateful_set_name, [])
+            time.sleep(5)
+            self.scale_statefulset(stateful_set_name,0)
+            time.sleep(15)
+            self.scale_statefulset(stateful_set_name,1)
+            if not self.wait_for_pod(pod_name, attempts=5):
+                raise Exception("Pod {} is not ready".format(pod_name))
+
+        try:
+            self.find_cluster_pods("replica", len(stateful_sets))
+        except Exception as e:
+            log.error(f"Failed to find replica pod after retries: {e}")
+            raise
+
+    @retry(tries=3600, delay=5)
+    def trigger_diff_backup(self):  
+        log.info("Triggering differential backup")
+        try:
+            backup_response = requests.post("http://localhost:9000/backup/diff")
+            backup_response.raise_for_status()
+            backup_id = backup_response.json()['backupId']
+            log.info(f"Differential backup triggered with ID: {backup_id}")
+            return backup_id
+        except requests.exceptions.RequestException as e:
+            log.error(f"Failed to trigger differential backup: {e}")
+            raise
+
+    @retry(tries=3600, delay=5)
+    def wait_diff_backup(self, backup_id):
+        log.info(f"Waiting for backup {backup_id} to appear in list")
+        try:
+            list_response = requests.get("http://backrest-headless:3000/list")
+            list_response.raise_for_status()
+            backups = list_response.json()
+                
+            if any((backup.get('annotation') or {}).get('timestamp') == backup_id for backup in backups):
+                log.info(f"Backup {backup_id} found in list")
+            else:
+                log.info(f"Backup {backup_id} not found in list")
+                raise Exception(f"Backup {backup_id} not found in list")
+        except requests.exceptions.RequestException as e:
+            log.error(f"Failed to check backup list: {e}")
+
+
+    @retry(tries=3600, delay=5)
+    def find_cluster_pods(self, pod_type, count):
+        core_api = client.CoreV1Api(self._api_client)
+        pods = core_api.list_namespaced_pod(
+            self.project,
+            label_selector=f"pgtype={pod_type}"
+        )
+        if len(pods.items) < count:
+            log.info(f"Pods with pgtype={pod_type} count:{len(pods.items)}, required:{count}")
+            raise Exception(f"Pods with pgtype={pod_type} count:{len(pods.items)}, required:{count}")
+        log.info(f"Found {pod_type} pods count:{len(pods.items)}")
+        return pods.items
+
+    def cleanup_patroni_data(self, pod_name, container_name, preserve_old_files):
+        log.info("Try to cleanup data directory for pod {}".format(pod_name))
+        if preserve_old_files == "yes":
+            self.oc_client.oc_exec(pod_name, container_name, "sh -c 'mv {} {}_backup_$(date +%s); ls -ll {}'".format(pg_data_dir, pg_data_dir, pg_dir))
+            log.info("Old files were preserved on volume. Cleanup if needed.")
+        self.oc_exec(pod_name, container_name, "sh -c 'rm -rf {}; rm -rf {}/pgbackrest; mkdir {}; chmod 700 {}'; echo Done".format(pg_data_dir, pg_dir, pg_data_dir, pg_data_dir))
+
+
+    @retry(tries=3600, delay=5)
+    def oc_exec(self, pod_id, container_name, command):
+        log.debug(f"Try to execute '{command}' on pod {pod_id}")
+        core_api = client.CoreV1Api(self._api_client)
+
+        exec_command = [
+            '/bin/sh', '-c', command
+        ]
+
+        try:
+            resp = stream(core_api.connect_get_namespaced_pod_exec,
+                          pod_id,
+                          self.project,
+                          container=container_name,
+                          command=exec_command,
+                          stderr=True, stdin=False,
+                          stdout=True, tty=False, _preload_content=True, _request_timeout=exec_timeout)
+
+            log.info(f"Command executed. Result: {resp}")
+
+            if resp:
+                log.debug(f"Command output: {resp}")
+                if "No such file or directory" in resp or "cannot remove" in resp:
+                    log.info("Directory already cleaned up or removal issue detected.")
+                    return resp  # Exit early if the directory is already cleaned up or a removal issue was detected
+
+            return resp
+
+        except Exception as e:
+            log.error(f"Exception occurred while executing command: {e}")
+            raise
+
+
+    def wait_for_pod(self, pod_name, attempts=5):
+        for i in range(1, attempts):
+            time.sleep(15)
+            status = self.get_pod_status(pod_name)
+            log.info("Pod state is {}".format(status))
+            if status and status.lower() == "running":
+                return True
+            else:
+                log.info("Retrying...")
+        log.info("Can't get pod {} status".format(pod_name))
+        return False
+
+
+    def get_pod_status(self, pod_name):
+        core_api = client.CoreV1Api(self._api_client)
+        pods = core_api.list_namespaced_pod(self.project).items
+        for x in pods:
+            if x.metadata.name == pod_name:
+                return x.status.phase
+            else:
+                log.info("Pod {} not found".format(pod_name))
+
+if __name__ == "__main__":
+    recovery = PgBackRestRecovery()
+    recovery.perform_restore()
+
+
+
+
+
+
diff --git a/docker-backup-daemon/maintenance/recovery/recovery.py b/docker-backup-daemon/maintenance/recovery/recovery.py
new file mode 100644
index 0000000..528cd4c
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/recovery.py
@@ -0,0 +1,790 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import argparse
+import os
+import random
+import requests
+import string
+import json
+import yaml
+import logging
+import time
+import sys
+import utils_oc
+import utils_pg
+from utils_common import RecoveryException
+from utils_dcs import PatroniDCS, PatroniDCSEtcd, PatroniDCSKubernetes
+
+import dateutil.parser
+import dateutil.zoneinfo
+import dateutil.tz
+
+from multiprocessing.pool import ThreadPool
+import concurrent.futures
+import asyncio
+
+
+retry_count = 60
+pg_dir = "/var/lib/pgsql/data"
+pg_data_dir = "{}/postgresql_${{POD_IDENTITY}}".format(pg_dir)
+
+log = logging.getLogger()
+log.setLevel(logging.DEBUG)
+
+ch = logging.StreamHandler(sys.stdout)
+ch.setLevel(logging.INFO)
+formatter = logging\
+    .Formatter('%(asctime)s - %(thread)d - %(name)s:%(funcName)s#%(lineno)d - %(levelname)s - %(message)s')
+ch.setFormatter(formatter)
+log.addHandler(ch)
+
+ch = logging.FileHandler("recovery.debug.{}.log".format(int(time.time())), mode='w', encoding=None, delay=False)
+ch.setLevel(logging.DEBUG)
+formatter = logging \
+    .Formatter('%(asctime)s - %(thread)d - %(name)s:%(funcName)s#%(lineno)d - %(levelname)s - %(message)s')
+ch.setFormatter(formatter)
+log.addHandler(ch)
+
+RECOVERY_EXCEPTION_NO_RESTORE_COMMAND = """FAILURE: cannot find restore_command in dcs and in patroni config template.
+Put valid value into dcs (etcd or '{}-config' configmap) on path 'postgresql.recovery_conf.restore_command'.
+If dcs uninitialized put it into configmap 'patroni-{}.config.yaml' on path 'bootstrap.dcs.postgresql.recovery_conf.restore_command'
+Value can be '' if wal_archive is disabled and 'curl -v -S -f --connect-timeout 3 postgres-backup-daemon:8082/archive/get?filename=%f -o %p' if enabled. 
+"""
+
+
+class PoolLogger(object):
+
+    def __init__(self, callable):
+        self.__callable = callable
+
+    def __call__(self, *args, **kwargs):
+        try:
+            result = self.__callable(*args, **kwargs)
+        except Exception as e:
+            log.exception("Task failed with error")
+            raise e
+        return result
+
+
+class LoggingPool(ThreadPool):
+
+    def apply_async(self, func, args=(), kwds={}, callback=None):
+        return ThreadPool.apply_async(self, PoolLogger(func), args, kwds, callback)
+
+# todo[anin] OpenshiftPyClient cannot work in parallel!
+
+
+
+
+def cleanup_pg_pod_data_directory(oc_client, pod_id, preserve_old_files):
+    log.info("Try to cleanup data directory for pod {}".format(pod_id))
+    if preserve_old_files == "yes":
+        oc_client.oc_exec(pod_id, "sh -c 'mv {} {}_backup_$(date +%s); ls -ll {}'".format(pg_data_dir, pg_data_dir, pg_dir))
+        log.info("Old files were preserved on volume. Cleanup if needed.")
+    oc_client.oc_exec(pod_id, " sh -c 'rm -rf {}; mkdir {}; chmod 700 {}' ".format(pg_data_dir, pg_data_dir, pg_data_dir))
+
+
+def update_configmap(oc_client, dcs_storage, recovery_pod_id, pg_cluster_name, restore_version,
+                     recovery_target_timeline, recovery_target_inclusive,
+                     recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target,):
+    log.info("Update configmap with actual bootstrap method")
+    patroni_config_cm = oc_client.get_configmap("patroni-{}.config.yaml".format(pg_cluster_name))
+    if not patroni_config_cm:
+        log.info("Can't find patroni-{}.config.yaml, trying to find {}-patroni.config.yaml".format(pg_cluster_name, pg_cluster_name))
+        patroni_config_cm = oc_client.get_configmap("{}-patroni.config.yaml".format(pg_cluster_name))
+    patroni_config_data = patroni_config_cm['data']['patroni-config-template.yaml']
+    log.debug("Patroni config template from configmap: \n {}".format(patroni_config_data))
+    patroni_config = yaml.safe_load(patroni_config_data)
+    log.debug("Patroni config template from configmap (parsed): \n {}".format(patroni_config))
+
+    patroni_dsc, patroni_dsc_data = dcs_storage.get_dcs_config(oc_client, recovery_pod_id, pg_cluster_name)
+
+    targets = {
+        "recovery_target_name": recovery_target_name,
+        "recovery_target_time": recovery_target_time,
+        "recovery_target_xid": recovery_target_xid,
+        "recovery_target": recovery_target
+    }
+
+    recovery_target_key = ""
+    recovery_target_value = ""
+
+    for (key, value) in list(targets.items()):
+        if value:
+            recovery_target_key = key
+            recovery_target_value = value
+
+    patroni_config["bootstrap"]["daemon_recovery"] = {
+        "command": "/daemon-recovery.sh  --restore-version={}".format(restore_version),
+        "recovery_conf": {
+            "recovery_target_action": "promote",
+            "recovery_target_timeline": recovery_target_timeline,
+            "recovery_target_inclusive": recovery_target_inclusive
+        }
+    }
+
+    restore_command = get_restore_command(oc_client, dcs_storage, pg_cluster_name, recovery_pod_id)
+    # This is a WA for PG12. In PG12 next IF condition were introduced:
+    # https://github.com/postgres/postgres/blob/REL_12_STABLE/src/backend/access/transam/xlog.c#L5397
+    if not restore_command:
+        restore_command = "echo"
+    patroni_config["bootstrap"]["daemon_recovery"]["recovery_conf"]["restore_command"] = restore_command
+
+    if recovery_target_key:
+        patroni_config["bootstrap"]["daemon_recovery"]["recovery_conf"][recovery_target_key] = recovery_target_value
+
+    patroni_config["bootstrap"]["method"] = "daemon_recovery"
+    log.debug("Patroni config template after update: {}".format(patroni_config))
+    updated_patroni_config = yaml.safe_dump(patroni_config, default_flow_style=False, encoding="utf-8", allow_unicode=True)
+    log.debug("Patroni config template after update: {}".format(updated_patroni_config))
+    # encoded_patroni_config = json.dumps(updated_patroni_config)
+    # log.debug("Encoded patroni config template after update: {}".format(encoded_patroni_config))
+    patroni_config_cm['data']['patroni-config-template.yaml'] = updated_patroni_config.decode()
+    oc_client.apply_object(patroni_config_cm)
+
+
+def remove_bootstrap_method(oc_client, pg_cluster_name):
+    log.info("Remove bootstrap method from configmap")
+    patroni_config_cm = oc_client.get_configmap("patroni-{}.config.yaml".format(pg_cluster_name))
+    if not patroni_config_cm:
+        log.info("Can't find patroni-{}.config.yaml, trying to find {}-patroni.config.yaml".format(pg_cluster_name, pg_cluster_name))
+        patroni_config_cm = oc_client.get_configmap("{}-patroni.config.yaml".format(pg_cluster_name))
+    patroni_config_data = patroni_config_cm['data']['patroni-config-template.yaml']
+    log.debug("Patroni config template from configmap: \n {}".format(patroni_config_data))
+    patroni_config = yaml.safe_load(patroni_config_data)
+    log.debug("Patroni config template from configmap (parsed): \n {}".format(patroni_config))
+
+    patroni_config["bootstrap"].pop("method", None)
+    log.debug("Patroni config template after update: {}".format(patroni_config))
+    updated_patroni_config = yaml.safe_dump(patroni_config, default_flow_style=False, encoding="utf-8", allow_unicode=True)
+    log.debug("Patroni config template after update: {}".format(updated_patroni_config))
+
+    patroni_config_cm['data']['patroni-config-template.yaml'] = updated_patroni_config.decode()
+    oc_client.apply_object(patroni_config_cm)
+
+
+
+def perform_bootstrap_recovery(oc_client, oc_orch, pg, dcs_storage,
+                               pg_depl_name, pg_cluster_name,
+                               preserve_old_files, restore_version,
+                               recovery_target_timeline, recovery_target_inclusive,
+                               recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target,
+                               deployment_type):
+    '''
+    :type oc_client: OpenshiftClient
+    :type oc_orch: OpenshiftOrchestrator
+    :type pg: PostgresqlClient
+    :type dcs_storage: PatroniDCS
+    :param pg_depl_name:
+    :param pg_cluster_name:
+    :param preserve_old_files:
+    :param restore_version:
+    :param recovery_target_timeline:
+    :param recovery_target_inclusive:
+    :param recovery_target_name:
+    :param recovery_target_time:
+    :param recovery_target_xid:
+    :param recovery_target:
+    :param deployment_type:
+    :return:
+    '''
+    log.info("Start recovery procedure using bootstrap config")
+
+    log.info("Replace current postgresql deployments with test versions")
+
+    pool = concurrent.futures.ThreadPoolExecutor(max_workers=4)
+    loop = asyncio.get_event_loop()
+
+    if deployment_type == "dc":
+        deployments = oc_client.get_deployment_names(pg_depl_name)
+        tasks = [loop.run_in_executor(
+            pool,
+            oc_orch.replace_command_on_dc,
+            p, ["sh", "-c", "while true ; do sleep 3600; done"]) for p in deployments]
+        loop.run_until_complete(asyncio.gather(*tasks))
+    elif deployment_type == "deployment":
+        log.info("Will set test version on postgresql deployments: {}".format(pg_depl_name))
+        deployments = oc_client.get_deployment_names(pg_depl_name, deployment_type)
+        tasks = [loop.run_in_executor(
+            pool,
+            oc_orch.replace_command_on_deployment,
+            p, ["sh", "-c", "while true ; do sleep 3600; done"]) for p in deployments]
+
+        loop.run_until_complete(asyncio.gather(*tasks))
+
+    elif deployment_type == "statefulset":
+        oc_orch.replace_command_on_statefulset("pg-patroni-node1", ["sh", "-c", "while true ; do sleep 3600; done"])
+        oc_orch.replace_command_on_statefulset("pg-patroni-node2", ["sh", "-c", "while true ; do sleep 3600; done"])
+
+
+
+    initial_replicas_desc = oc_client.get_cluster_pods_desc(pg_cluster_name)
+    initial_replicas = [pod["metadata"]["name"] for pod in initial_replicas_desc]
+
+    # in case of statefulset need to get number of replicas for scaling back
+    if deployment_type == "statefulset":
+        initial_replicas_num = oc_client.get_stateful_set_replicas_count("patroni")
+
+    recovery_pod_id = list([pod for pod in initial_replicas_desc if not list([env for env in pod["spec"]["containers"][0]["env"] if env["name"] == "DR_MODE" and "value" in env and env["value"].lower() == "true"])])[0]["metadata"]["name"]
+    log.info("Will use for procedure pod {} from {}".format(recovery_pod_id, initial_replicas))
+
+    log.info("Update configmap")
+
+    update_configmap(
+         oc_client, dcs_storage, recovery_pod_id, pg_cluster_name, restore_version,
+         recovery_target_timeline, recovery_target_inclusive,
+         recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target)
+
+    log.info("Cleanup initialization key")
+    dcs_storage.cleanup_initialization_key(
+    oc_client, pg_cluster_name, recovery_pod_id)
+
+    log.info("Cleanup data directories")
+    for p in initial_replicas:
+        cleanup_pg_pod_data_directory(
+        oc_client, p, preserve_old_files)
+
+    # restore command on deployments
+    if deployment_type == "dc":
+        recovery_dc = list([p for p in deployments if p in recovery_pod_id])[0]
+        log.info("Restore command for replica deployment and leave them shut down")
+        tasks = [loop.run_in_executor(
+            pool,
+            oc_orch.replace_command_on_dc,
+            p, None, False) for p in [d for d in deployments if d != recovery_dc]]
+        log.info("Restore command for deployment {}".format(recovery_dc))
+        tasks.append(loop.run_in_executor(
+            pool,
+            oc_orch.replace_command_on_dc,
+            recovery_dc, None))
+
+        loop.run_until_complete(asyncio.gather(*tasks))
+
+        # determine new pod for recovery deployment
+        recovery_replicas = oc_client.get_replicas_names(recovery_dc)
+        recovery_pod_id = recovery_replicas[0]
+    elif deployment_type == "deployment":
+        deployments = oc_client.get_deployment_names(pg_depl_name, deployment_type)
+        log.info("Deployments: {}".format(deployments))
+        recovery_dc = list([p for p in deployments if p in recovery_pod_id])[0]
+        log.info("Restore command for replica deployment and leave them shut down")
+        tasks = [loop.run_in_executor(
+            pool,
+            oc_orch.replace_command_on_deployment,
+            p, None, True) for p in [d for d in deployments if d != recovery_dc]]
+        log.info("Restore command for deployment {}".format(recovery_dc))
+        loop.run_until_complete(asyncio.gather(*tasks))
+        oc_orch.replace_command_on_deployment(recovery_dc, None)
+        time.sleep(60)
+        recovery_pod_id = oc_client.get_pods_by_label("app={}".format(pg_cluster_name))[0]
+    elif deployment_type == "statefulset":
+        # set command as None for statefulset
+        oc_orch.replace_command_on_statefulset("pg-patroni-node1", None, False)
+        oc_orch.replace_command_on_statefulset("pg-patroni-node2", None, False)
+        oc_orch.scale_stateful_set("pg-patroni-node1", 1)
+        recovery_pod_id = oc_client.get_cluster_pods(pg_cluster_name)[0]
+    log.info(recovery_pod_id)
+    # wait while pod is up
+    if not oc_client.is_pod_ready(recovery_pod_id, attempts=5):
+        raise Exception("Pod {} is not ready".format(recovery_pod_id))
+
+    log.info("Cleanup initialization key")
+    dcs_storage.cleanup_initialization_key(
+    oc_client, pg_cluster_name, recovery_pod_id)
+
+    cleanup_pg_pod_data_directory(oc_client, recovery_pod_id, preserve_old_files)
+
+    # wait while database will complete bootstrap and exit from recovery mode
+    log.info("Wait while patroni pod: {} will complete bootstrap from backup.".format(recovery_pod_id))
+    oc_orch.wait_for_one_of_records_in_logs_since(
+        recovery_pod_id,
+        ["no action. I am ({}), the leader with the lock".format(recovery_pod_id)],  # prefix can be "INFO:" or  "[INFO][source=patroni]"
+        time.time(),
+        "Bootstrap from backup daemon is in progress.",
+        [" bootstrap in progress", " waiting for end of recovery after bootstrap"]
+    )
+    log.info("Wait while postgres will exit from recovery mode.")
+    pg.wait_pg_recovery_complete(recovery_pod_id)
+
+    log.info("Try to set password for postgres and replicator users")
+
+    def split(line):
+        pos = line.find("=")
+        if pos > 0:
+            return line[0:pos], line[pos+1:]
+        else:
+            return line, ""
+
+    pg_user, pg_password = oc_client.get_secret_data("postgres-credentials")
+    log.info(
+        pg.execute_local_query(recovery_pod_id, "ALTER USER {} WITH PASSWORD '{}'"
+                               .format(pg_user, pg_password)))
+    replicator_user, replicator_password = oc_client.get_secret_data("replicator-credentials")
+    log.info(
+        pg.execute_local_query(recovery_pod_id, "ALTER USER {} WITH PASSWORD '{}'"
+                               .format(replicator_user, replicator_password)))
+
+    bootstrap_method_cleanup = loop.run_in_executor(
+        pool,
+        remove_bootstrap_method,
+        oc_client,
+        pg_cluster_name)
+    loop.run_until_complete(asyncio.gather(bootstrap_method_cleanup))
+    # restore replicas
+    if deployment_type == "dc":
+        for dc in deployments:
+            if dc != recovery_dc:
+                log.info("Scale up dc {}".format(dc))
+                oc_orch.ensure_scale(dc, 1)
+                oc_orch.wait_replicas(dc, 1, running=True)
+    elif deployment_type == "deployment":
+        for dc in deployments:
+            if dc != recovery_dc:
+                log.info("Scale up deployment {}".format(dc))
+                oc_orch.ensure_scale(dc, 1, "deployment")
+    elif deployment_type == "statefulset":
+        oc_orch.scale_stateful_set("pg-patroni-node2", initial_replicas_num)
+
+    # check if database working on all nodes
+    replicas = oc_client.get_cluster_pods(pg_cluster_name)
+    for pod_id in replicas:
+        if pod_id != recovery_pod_id:
+            cleanup_pg_pod_data_directory(oc_client, pod_id, preserve_old_files)
+            log.info("Wait while patroni on replica {} will complete bootstrap from master.".format(pod_id))
+            if not oc_client.is_pod_ready(pod_id, attempts=5):
+                raise Exception("Pod {} is not ready".format(pod_id))
+            oc_orch.wait_for_record_in_logs_since(
+                pod_id,
+                "no action. I am ({}), a secondary, and following a leader ({})".format(pod_id, recovery_pod_id),
+                time.time(),
+                "Bootstrap from master is in progress.",
+                " bootstrap from leader"
+            )
+            log.info("Wait while replica database will start on pod {}.".format(pod_id))
+            pg.wait_database(pod_id)
+
+
+
+def download_archive(oc_client, recovery_pod_id, restore_version):
+    if restore_version:
+        oc_client.oc_exec(recovery_pod_id, "sh -c 'cd {} ; curl -u postgres:\"$PG_ROOT_PASSWORD\" postgres-backup-daemon:8081/get?id={} | tar -xzf - '"
+                .format(pg_data_dir, restore_version))
+    else:
+        oc_client.oc_exec(recovery_pod_id, "sh -c 'cd {} ; curl -u postgres:\"$PG_ROOT_PASSWORD\" postgres-backup-daemon:8081/get | tar -xzf - '"
+                .format(pg_data_dir))
+
+
+def update_string_value_in_map(map, key, value):
+    if value:
+        map[key] = value
+    else:
+        map.pop(key, None)
+
+
+def prepare_recovery_conf(recovery_pod_id, patroni_dsc,
+                          recovery_target_timeline, recovery_target_inclusive,
+                          recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target):
+    if "recovery_conf" not in patroni_dsc["postgresql"]:
+        patroni_dsc["postgresql"]["recovery_conf"] = {}
+
+    recovery_conf = patroni_dsc["postgresql"]["recovery_conf"]
+    update_string_value_in_map(recovery_conf, "recovery_target_inclusive", recovery_target_inclusive)
+    update_string_value_in_map(recovery_conf, "recovery_target_timeline", recovery_target_timeline)
+
+    update_string_value_in_map(recovery_conf, "recovery_target_name", recovery_target_name)
+    update_string_value_in_map(recovery_conf, "recovery_target_time", recovery_target_time)
+    update_string_value_in_map(recovery_conf, "recovery_target_xid", recovery_target_xid)
+    update_string_value_in_map(recovery_conf, "recovery_target", recovery_target)
+
+def is_recovery_target_specified(recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target):
+    return recovery_target_name or recovery_target_time or recovery_target_xid or recovery_target
+
+
+def validate_restore_version(backups, restore_version):
+    if not backups:
+        raise RecoveryException("FAILURE: cannot find any available backups.")
+
+    if restore_version:
+        if restore_version not in backups:
+            raise RecoveryException("FAILURE: RESTORE_VERSION={} does not match any existing backup. "
+                                    "List of available backups: {}.".format(restore_version, backups))
+    else:
+        log.info("Proceed with empty RESTORE_VERSION.")
+
+
+def validate_recovery_target_parameters(
+        oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id,  restore_version,
+        recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target):
+    '''
+
+    :type oc_client: OpenshiftClient
+    :type dcs_storage: PatroniDCS
+    :param pg_cluster_name:
+    :param backup_daemon_pod_id:
+    :param restore_version:
+    :param recovery_target_name:
+    :param recovery_target_time:
+    :param recovery_target_xid:
+    :param recovery_target:
+    :return:
+    '''
+    if is_recovery_target_specified(recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target) \
+            and (not restore_version and not recovery_target_time):
+        raise RecoveryException("FAILURE: cannot perform PITR "
+                                "without specified backup or recovery_target_time")
+
+    if is_recovery_target_specified(recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target):
+        archive_mode = get_dcs_config_params(oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id,
+                                             ["postgresql", "parameters", "archive_mode"], use_template=True)
+        if archive_mode != "on":
+            raise RecoveryException("FAILURE: Cannot perform PITR recovery without WAL archive.")
+
+        if not get_restore_command(oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id):
+            raise RecoveryException("FAILURE: Cannot perform PITR recovery without restore_command.")
+
+
+def check_if_replication_works(oc_client, pg, pg_cluster_name):
+    log.info("Start replication check")
+    replicas = oc_client.get_cluster_pods_desc(pg_cluster_name)
+    master_replicas = list([p for p in replicas if "pgtype" in p["metadata"]["labels"] and
+                                            p["metadata"]["labels"]["pgtype"] == "master"])
+
+    if len(master_replicas) > 1:
+        raise RecoveryException("FAILURE: Several masters detected. Healthy PostgreSQL cluster should have one master.")
+
+    if len(master_replicas) == 0:
+        raise RecoveryException("FAILURE: Cannot find master. Healthy PostgreSQL cluster should have one master.")
+
+    master_pod_id = master_replicas[0]["metadata"]["name"]
+
+    id = int(time.time()) * 100000 + random.randint(1, 99999)
+    uuid = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(32))
+    pg.execute_local_query(master_pod_id, "create table IF NOT EXISTS test_recovery (id bigint primary key not null, value text not null)")
+    pg.execute_local_query(master_pod_id, "insert into test_recovery values ({}, '{}')".format(id, uuid))
+    resulted_uuid = pg.execute_local_query(master_pod_id, "select value from test_recovery where id={}".format(id))
+    log.debug("Select result from master {}: {}".format(master_pod_id, resulted_uuid))
+
+    if resulted_uuid != uuid:
+        raise RecoveryException("FAILURE: Unexpected result '{}' on master while expected '{}'. "
+                                "Master does not work or does not perform write operations.".format(resulted_uuid, uuid))
+
+    log.info("Record was inserted in master")
+
+    replica_names = list([p["metadata"]["name"] for p in replicas])
+
+    for replica in replica_names:
+        if replica != master_pod_id:
+            query_result = None
+            for i in range(1, retry_count):
+                log.info("Try to check if database on pod {} receives data from master".format(replica))
+                query_result = pg.execute_local_query(replica, "select value from test_recovery where id={}".format(id))
+                log.debug("Query result from replica: {}".format(query_result))
+                if query_result == uuid:
+                    break
+                time.sleep(1)
+
+            if query_result != uuid:
+                raise RecoveryException("FAILURE: Unexpected result '{}' while expected '{}'. "
+                                        "Replica was not able to receive changes from master.".format(query_result, uuid))
+
+
+def get_tzinfos():
+    # todo[anin] implement for different versions of dateutils
+    if "get_zonefile_instance" in dir(dateutil.zoneinfo):
+        return dateutil.zoneinfo.get_zonefile_instance().zones
+    if "getzoneinfofile_stream" in dir(dateutil.zoneinfo):
+        return dateutil.zoneinfo.ZoneInfoFile(dateutil.zoneinfo.getzoneinfofile_stream()).zones
+    return {}
+
+
+def enrich_backup(backup_info, tzinfos, cluster_tz_name):
+    backup_info["parsed"] = dateutil.parser.parse(backup_info["id"], tzinfos=tzinfos)
+    if not backup_info["parsed"].tzinfo:
+        backup_info["parsed"] = backup_info["parsed"].replace(tzinfo=tzinfos[cluster_tz_name])
+    return backup_info
+
+
+def safe_dict_get(data, path, default=None):
+    value = data
+    for x in path:
+        value = value.get(x)
+        if not value:
+            break
+    return value if value is not None else default
+
+
+def get_dcs_config_params(oc_client, dcs_storage, pg_cluster_name, pod_id, path, use_template=True):
+    patroni_dsc, _ = dcs_storage.get_dcs_config(oc_client, pod_id, pg_cluster_name)
+    value = safe_dict_get(patroni_dsc, path)
+    if value is None and use_template:
+        log.warning("Cannot get '{}' from dcs. It is possible that dcs state was removed or lost.".format(path))
+        patroni_template_cm = oc_client.get_configmap("patroni-{}.config.yaml".format(pg_cluster_name))
+        if not patroni_template_cm:
+            log.info("Can't find patroni-{}.config.yaml, trying to find {}-patroni.config.yaml".format(pg_cluster_name, pg_cluster_name))
+            patroni_template_cm = oc_client.get_configmap("{}-patroni.config.yaml".format(pg_cluster_name))
+        patroni_template_data = patroni_template_cm.get("data", {}).get("patroni-config-template.yaml")
+        patroni_template = yaml.load(patroni_template_data)
+        log.debug("Patroni config template from configmap (parsed): \n {}".format(patroni_template))
+        patroni_template_dcs = safe_dict_get(patroni_template, ["bootstrap", "dcs"], {})
+        log.debug("Patroni dcs config template from configmap (parsed): \n {}".format(patroni_template_dcs))
+        value = safe_dict_get(patroni_template_dcs, path)
+    return value
+
+
+def get_restore_command(oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id):
+    return get_dcs_config_params(oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id,
+                                 ["postgresql", "recovery_conf", "restore_command"], use_template=True)
+
+
+def perform_recovery(oc_openshift_url, oc_username, oc_password, oc_project,
+                     pg_cluster_name, pg_depl_name, preserve_old_files, restore_version,
+                     recovery_target_timeline, recovery_target_inclusive,
+                     recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target,
+                     oc_path, oc_config_file, skip_tls_verify=False):
+
+    # todo[anin] OpenshiftPyClient requires more testing (statefulsets for example)
+    # so leave OpenshiftShellClient as default for backward compatibility
+    try:
+        from kubernetes import config as k8s_config
+        k8s_config.load_incluster_config()
+        log.info("Using pyclient")
+        oc_client = utils_oc.OpenshiftPyClient()
+        oc_client.use_token(oc_url=oc_openshift_url, oc_token="", project=oc_project, skip_tls_verify=skip_tls_verify)
+    except Exception as e:
+        log.exception("Failed to create OpenshiftPyClient, proceeding with OpenshiftShellClient")
+        oc_client = utils_oc.OpenshiftShellClient(oc_path, oc_config_file)
+        oc_client.login(oc_url=oc_openshift_url, username=oc_username, password=oc_password, project=oc_project,
+                       skip_tls_verify=skip_tls_verify)
+
+    oc_orch = utils_oc.OpenshiftOrchestrator(oc_client, retry_count)
+    pg = utils_pg.PostgresqlClient(oc_client, retry_count)
+
+    if oc_client.get_entity_safe("configmap", "{}-config".format(pg_cluster_name)):
+        log.info("Use kubernetes as DCS storage")
+        dcs_storage = PatroniDCSKubernetes()
+    elif oc_client.get_entity_safe("svc", "etcd"):
+        log.info("Use etcd as DCS storage")
+        dcs_storage = PatroniDCSEtcd()
+    else:
+        raise RecoveryException("Cannot find configmap {}-config or service etcd and guess dcs type."
+                                .format(pg_cluster_name))
+    deployment_type = "dc"
+
+    # here we are trying to check if this is a deployment via operator
+    if oc_client.get_entity_safe("deployment", "postgres-backup-daemon"):
+        log.info("Deployments are used for Pods management")
+        deployment_type = "deployment"
+
+    #Changed Order here of deployment check
+    if oc_client.get_entity_safe("statefulset", "patroni"):
+        log.info("Statefulset is used for Pod management")
+        deployment_type = "statefulset"
+
+    deployment_type = "statefulset"
+
+    log.info("Try to validate if backup daemon running")
+    backup_daemon_replicas = oc_client.get_postgres_backup_daemon_pod()
+    if not backup_daemon_replicas:
+        raise RecoveryException("FAILURE: cannot find backup daemon pod."
+                                "Recovery procedure requires running backup daemon pod.")
+    backup_daemon_pod_id = backup_daemon_replicas[0]
+    log.info("Found backup daemon pod {}".format(backup_daemon_pod_id))
+
+    log.info("Try to validate recovery target parameters")
+    validate_recovery_target_parameters(
+        oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id, restore_version,
+        recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target)
+
+    log.info("Try to validate restore_command setting")
+    restore_command = get_restore_command(oc_client, dcs_storage, pg_cluster_name, backup_daemon_pod_id)
+    log.info("Recovery command printed in start ={}".format(restore_command))
+    if restore_command is None:
+        raise RecoveryException(RECOVERY_EXCEPTION_NO_RESTORE_COMMAND.format(pg_cluster_name, pg_cluster_name))
+
+    ##Added backup status check here
+    log.info("Try to validate if the backup status is successful")
+    backup_status = check_backup_status(restore_version)
+    if not backup_status:
+        raise RecoveryException("FAILURE: Backup with id {} has an unsuccessful status. "
+                                     "Recovery cannot proceed.".format(restore_version))
+
+
+    if restore_version:
+        log.info("Try to validate backup {} against list of backups from {}".format(restore_version,
+                                                                                    backup_daemon_pod_id))
+
+        backup_list = requests.get("http://localhost:8081/list", auth=('postgres', os.getenv('POSTGRES_PASSWORD')))
+        validate_restore_version(backup_list.json(), restore_version)
+    elif recovery_target_time:
+        log.info("Try to find backup id from specified recovery_target_time={}".format(recovery_target_time))
+        backup_list = requests.get("http://localhost:8081/list", auth=('postgres', os.getenv('POSTGRES_PASSWORD')))
+        cluster_tz_name = oc_client.oc_exec(backup_daemon_pod_id, 'date "+%Z"').strip()
+        log.debug("Cluster time zone: {}" + cluster_tz_name)
+
+        tzinfos = get_tzinfos()
+        rt_time = dateutil.parser.parse(recovery_target_time, tzinfos=tzinfos)
+        if not rt_time.tzinfo:
+            rt_time = rt_time.replace(tzinfo=tzinfos[cluster_tz_name])
+
+        log.debug("Parsed time: {}".format(rt_time))
+        possible_backup_list = list([p for p in [enrich_backup(p, tzinfos, cluster_tz_name) for p in [p for p in list(backup_list.json().values()) if not p["failed"]]] if p["parsed"] and p["parsed"] < rt_time])
+
+        if not possible_backup_list:
+            raise RecoveryException("FAILURE: Cannot find backup for specified recovery_target_time='{}'. "
+                                    "Try to specify restore_version manually. "
+                                    "Available backups: {}".format(recovery_target_time, backup_list.json()))
+
+        possible_backup = max(possible_backup_list, key=lambda p: p["parsed"])
+        restore_version = possible_backup["id"]
+
+        log.info("Selected restore_version is {}".format(restore_version))
+    else:
+        raise RecoveryException("FAILURE: Cannot perform recovery without restore_version and recovery_target_time. "
+                                "Please specify at least one of them.")
+
+    patroni_cm = oc_client.get_entity_safe("configmap", "patroni-{}.config.yaml".format(pg_cluster_name))
+    if not patroni_cm:
+        log.info("Can't find patroni-{}.config.yaml, trying to find {}-patroni.config.yaml".format(pg_cluster_name, pg_cluster_name))
+        patroni_cm = oc_client.get_entity_safe("configmap", "{}-patroni.config.yaml".format(pg_cluster_name))
+    if not patroni_cm:
+        raise RecoveryException("FAILURE: Cannot find configmap patroni-{}.config.yaml. "
+                                "Check if recovery scripts version complies with cluster version".format(pg_cluster_name))
+    perform_bootstrap_recovery(oc_client, oc_orch, pg, dcs_storage,
+                               pg_depl_name, pg_cluster_name,
+                               preserve_old_files, restore_version,
+                               recovery_target_timeline, recovery_target_inclusive,
+                               recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target,
+                               deployment_type)
+
+    if not os.getenv("SKIP_REPLICATION_CHECK", False):
+        check_if_replication_works(oc_client, pg, pg_cluster_name)
+    log.info("Recovery is completed successfully")
+
+
+def check_backup_status(backup_id):
+    try:
+        url = f"http://postgres-backup-daemon:8080/backup/status/{backup_id}"
+        headers = {"Accept": "application/json"}
+        response = requests.get(url, headers=headers)
+        response_text = response.text
+
+        log.info("Backup Status Check Result: %s", response_text)
+
+        if "Backup Done" in response_text:
+            log.info("Backup Status: Backup done")
+            return True
+        else:
+            log.info("Backup Status: Backup not done")
+            raise Exception(f"Backup not done. Status: {response_text}")
+
+    except requests.RequestException as e:
+        log.error("Error occurred while checking backup status: %s", e)
+        return False
+
+
+
+def prepare_parameters_and_perform_recovery():
+    log.info("Start parameters preparation")
+
+    # import env variables
+    oc_openshift_url = os.getenv("OC_OPENSHIFT_URL", None)
+    oc_username = os.getenv("OC_USERNAME", None)
+    oc_password = os.getenv("OC_PASSWORD", None)
+    oc_project = os.getenv("OC_PROJECT", None)
+
+    pg_cluster_name = os.getenv("PG_CLUSTER_NAME", None)
+    pg_depl_name = os.getenv("PG_DEPL_NAME", None)
+    preserve_old_files = os.getenv("PRESERVE_OLD_FILES", "no")
+
+    restore_version = os.getenv("RESTORE_VERSION", "")
+
+    recovery_target_timeline = os.getenv("RECOVERY_TARGET_TIMELINE", "latest")
+    recovery_target_inclusive = os.getenv("RECOVERY_TARGET_INCLUSIVE", "true")
+
+    recovery_target_name = os.getenv("RECOVERY_TARGET_NAME", None)
+    recovery_target_time = os.getenv("RECOVERY_TARGET_TIME", None)
+    recovery_target_xid = os.getenv("RECOVERY_TARGET_XID", None)
+    recovery_target = os.getenv("RECOVERY_TARGET", None)
+
+    oc_path = os.getenv("OC_PATH", "oc")
+    skip_tls_verify = os.getenv("OC_SKIP_TLS_VERIFY", "true")
+    oc_config_file = os.getenv("OC_CONFIG_FILE", "./oc_config_file.yaml")
+
+    # parse args
+    parser = argparse.ArgumentParser(description='Recovery procedure for Postgresql cluster')
+    parser.add_argument('--oc-openshift-url', dest='oc_openshift_url', default=None, help='address of openshift console')
+    parser.add_argument('--oc-username', dest='oc_username', default=None, help='user of openshift console')
+    parser.add_argument('--oc-password', dest='oc_password', default=None, help='password of openshift console')
+    parser.add_argument('--oc-project', dest='oc_project', default=None, help='address of openshift console')
+
+    parser.add_argument('--pg-cluster-name', dest='pg_cluster_name', default=None, help='Postgresql cluster name')
+    parser.add_argument('--pg-depl-name', dest='pg_depl_name', default=None,
+                        help='Template of postgresql deployment name like "pg-common-node"')
+
+    parser.add_argument('--preserve-old-files', dest='preserve_old_files', default=None, choices=["yes", "no"],
+                        help='If "yes" then store old files on volume otherwise remove old files.')
+
+    parser.add_argument('--restore-version', dest='restore_version', default=None,
+                        help='ID of backup to recovery procedure')
+
+    parser.add_argument('--timeline', dest='recovery_target_timeline', default=None,
+                        help='Only for point in time recovery. Desired timeline for recovery procedure')
+    parser.add_argument('--inclusive', dest='recovery_target_inclusive', default=None, choices=['true', 'false'],
+                        help='Only for point in time recovery. Specifies if recovery procedure should inslude specified recovery target')
+
+    parser.add_argument('--recovery-target-time', dest='recovery_target_time', default=None,
+                        help='Only for point in time recovery. Specifies recovery_target_time.')
+    parser.add_argument('--recovery-target-name', dest='recovery_target_name', default=None,
+                        help='Only for point in time recovery. Specifies recovery_target_name.')
+    parser.add_argument('--recovery-target-xid', dest='recovery_target_xid', default=None,
+                        help='Only for point in time recovery. Specifies recovery_target_xid.')
+    parser.add_argument('--recovery-target', dest='recovery_target', default=None, choices=['immediate'],
+                        help='Only for point in time recovery. Specifies recovery_target.')
+
+    parser.add_argument('--oc-path', dest='oc_path', default=None, help='path to oc client (with oc)')
+    parser.add_argument('--oc-config', dest='oc_config_file', default=None, help='path to oc client (with oc)')
+    parser.add_argument('--oc-skip-tls-verify', dest='skip_tls_verify', default=None, choices=['true', 'false'],
+                        help='Set value to "true" to turn off ssl validation')
+
+    args = parser.parse_args()
+
+    for (key, value) in list(vars(args).items()):
+        if value and key in locals():
+            locals()[key] = value
+
+    if not pg_depl_name:
+        pg_depl_name = "pg-{}-node".format(pg_cluster_name)
+
+    log.info("Parameters were parsed")
+    log.debug("Local vars : {}".format(locals()))
+    log.debug("Global vars : {}".format(globals()))
+
+    try:
+        perform_recovery(oc_openshift_url, oc_username, oc_password, oc_project,
+                         pg_cluster_name, pg_depl_name,
+                         preserve_old_files, restore_version,
+                         recovery_target_timeline, recovery_target_inclusive,
+                         recovery_target_name, recovery_target_time, recovery_target_xid, recovery_target,
+                         oc_path, oc_config_file,
+                         # force_manual_recovery,
+                         skip_tls_verify=(True if skip_tls_verify == 'true' else False))
+    except RecoveryException as ex:
+        log.exception("Recovery procedure failed.")
+        log.error(str(ex))
+        sys.exit(1)
+
+
+if __name__ == "__main__":
+    prepare_parameters_and_perform_recovery()
diff --git a/docker-backup-daemon/maintenance/recovery/recovery.sh b/docker-backup-daemon/maintenance/recovery/recovery.sh
new file mode 100755
index 0000000..eb853a8
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/recovery.sh
@@ -0,0 +1,24 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source recovery_utils.sh
+source recovery_setEnv.sh
+
+validate_binary jq
+validate_binary python3
+validate_python_package "yaml" "PyYAML"
+
+python3 ./recovery.py
\ No newline at end of file
diff --git a/docker-backup-daemon/maintenance/recovery/recovery_launcher.sh b/docker-backup-daemon/maintenance/recovery/recovery_launcher.sh
new file mode 100755
index 0000000..075d61b
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/recovery_launcher.sh
@@ -0,0 +1,63 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source recovery_utils.sh
+source recovery_setEnv.sh
+
+validate_binary jq
+
+
+# check if need to setup RESTORE_VERSION
+if [[ -z "${RESTORE_VERSION}" ]] ; then
+    log "Current RESTORE_VERSION value is empty."
+
+    if [[ -n "${RECOVERY_TARGET_TIME}" ]] ; then
+        log "RECOVERY_TARGET_TIME is specified. Leave RESTORE_VERSION value is empty to allow procedure to guess RESTORE_VERSION."
+    fi
+
+    backup_list_json=$(curl http://postgres-backup-daemon:8081/list)
+    backup_list=$(echo ${backup_list_json} | jq -r '.[].id')
+    for item in ${backup_list[@]}; do
+        echo ${item}
+    done
+    RESTORE_VERSION_CHECK=false
+    while [ ${RESTORE_VERSION_CHECK} == "false" ] ; do
+        log "Please select one of backups. Empty input means last available backup or guessed backup."
+        read RESTORE_VERSION
+        # empty value allowed
+        if [[ -z "${RESTORE_VERSION}" ]] ; then
+            RESTORE_VERSION_CHECK=true
+        fi
+        # check non empty value for correctness
+        for item in ${backup_list[@]}; do
+            if [[ "${RESTORE_VERSION}" == ${item} ]] ; then
+                RESTORE_VERSION_CHECK=true
+                break
+            fi
+        done
+    done
+fi
+
+
+if [[ -z "${RESTORE_VERSION}" ]] ; then
+    log "Will try to restore last available backup or guessed backup."
+    confirm || exit 1
+else
+    log "Will try to restore backup ${RESTORE_VERSION}."
+    confirm || exit 1
+fi
+
+RESTORE_VERSION=${RESTORE_VERSION} ./recovery.sh
diff --git a/docker-backup-daemon/maintenance/recovery/recovery_setEnv.sh b/docker-backup-daemon/maintenance/recovery/recovery_setEnv.sh
new file mode 100755
index 0000000..abe1a36
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/recovery_setEnv.sh
@@ -0,0 +1,56 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+NAMESPACE=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
+
+# user defined variables
+export OC_PROJECT=${OC_PROJECT:-${NAMESPACE}}
+export OC_CONFIG_FILE=${OC_CONFIG_FILE:-oc_config_file.yaml}
+export PG_CLUSTER_NAME=${PG_CLUSTER_NAME:-patroni}
+export PRESERVE_OLD_FILES=${PRESERVE_OLD_FILES:-no}
+export RESTORE_VERSION=${RESTORE_VERSION:-}
+export SKIP_REPLICATION_CHECK=${SKIP_REPLICATION_CHECK:-False}
+
+#############################################################################
+#### Recovery target settings according to
+#### https://www.postgresql.org/docs/9.6/static/recovery-target-settings.html
+#############################################################################
+
+export RECOVERY_TARGET_INCLUSIVE=${RECOVERY_TARGET_INCLUSIVE:-true}
+export RECOVERY_TARGET_TIMELINE=${RECOVERY_TARGET_TIMELINE:-latest}
+
+# specify only one parameter
+export RECOVERY_TARGET_TIME=${RECOVERY_TARGET_TIME:-}
+export RECOVERY_TARGET_NAME=${RECOVERY_TARGET_NAME:-}
+export RECOVERY_TARGET_XID=${RECOVERY_TARGET_XID:-}
+export RECOVERY_TARGET=${RECOVERY_TARGET:-}
+#############################################################################
+
+
+#############################################################################
+#### oc client settings
+#############################################################################
+export OC_PATH=${OC_PATH:-oc}
+export OC_SKIP_TLS_VERIFY=${OC_SKIP_TLS_VERIFY:-true}
+#############################################################################
+
+# system vars (do not change)
+export PG_DEPL_NAME=${PG_DEPL_NAME:-pg-${PG_CLUSTER_NAME}}
+
+export LOG_ERROR="\e[0;101m"
+export LOG_SUCCESS="\e[0;32m"
+export LOG_INFO="\e[0;104m"
+
diff --git a/docker-backup-daemon/maintenance/recovery/recovery_utils.sh b/docker-backup-daemon/maintenance/recovery/recovery_utils.sh
new file mode 100755
index 0000000..237064f
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/recovery_utils.sh
@@ -0,0 +1,77 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+function log {
+    echo -e "[$(date +'%H:%M:%S')] $2$1\e[m"
+}
+
+function validate_binary {
+    command -v $1 &> /dev/null
+    if [ $? -gt 0 ] ; then
+        log "Please install $1"
+        exit 1
+    fi
+}
+
+function validate_python_package {
+    local package="$1"
+    local package_pip_name="$2"
+    local package_version="$3"
+    python -c "import $package"
+    if [ $? -gt 0 ] ; then
+        log "Please install python package $package"
+        [[ -n "$package_pip_name" ]] && log "To install $package execute command 'sudo pip install $package_pip_name'."
+        exit 1
+    fi
+
+    if [[ -n "${package_version}" ]] ; then
+        package_current_version=$(pip3 show ${package_pip_name} | grep -e '^Version:' | cut -d ':'  -f2)
+        if [ $? -gt 0 ] ; then
+            log "Cannot get version of $2 via pip"
+            exit 1
+        fi
+        cmp_result=$(python -c "from pkg_resources import parse_version;  print(parse_version('${package_current_version}'.strip()) >= parse_version('${package_version}'))")
+        if [ $? -gt 0 ] ; then
+            log "Cannot compare version of ${package_pip_name} with desired version ${package_version}. Current version: ${package_current_version}."
+            exit 1
+        fi
+        if [[ "${cmp_result}" != "True" ]] ; then
+            log "Installed version of ${package_pip_name} is too old. Please install version ${package_version} or above. Current version: ${package_current_version}."
+            exit 1
+        fi
+    fi
+
+}
+
+function get_replicas {
+  REPL_NAME=${1}
+  gr_replicas=($(oc get pods --config="$OC_CONFIG_FILE" | grep -v deploy | grep Running | grep ${REPL_NAME} | cut -d\  -f1))
+  echo "${gr_replicas[*]}"
+}
+
+function get_pod_ip(){
+    oc --config="$OC_CONFIG_FILE" get pod $1 -o json | jq -r '.status.podIP'
+}
+
+confirm() {
+	while(true); do
+		read -p "Continue (y/n)?" choice
+		case "$choice" in
+			y|Y ) return 0;;
+			n|N ) return 1;;
+		esac
+	done
+}
\ No newline at end of file
diff --git a/docker-backup-daemon/maintenance/recovery/utils_common.py b/docker-backup-daemon/maintenance/recovery/utils_common.py
new file mode 100644
index 0000000..4fe53a6
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/utils_common.py
@@ -0,0 +1,146 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import copy
+import time
+
+
+class RecoveryException(Exception):
+    """
+    This class should be used if exception contains meaningful message to user.
+    """
+
+    def __init__(self, *args):
+        super(RecoveryException, self).__init__(*args)
+
+
+class Differ:
+
+    def is_primitive_array(self, array):
+        for entity in array:
+            if isinstance(entity, list) or isinstance(entity, dict):
+                return False
+        return True
+
+    def ensure_path(self, path, result):
+        target = result
+        for i in range(0, len(path)):
+            el = path[i]
+            nextEl = path[i + 1] if i + 1 < len(path) else None
+            if isinstance(el, str):
+                if el in target:
+                    target = target[el]
+                else:
+                    if nextEl is not None:
+                        target[el] = {} if isinstance(nextEl, str) else []
+                        target = target[el]
+                    else:
+                        target[el] = None
+            else:
+                if len(target) > 0:
+                    target = target[el]
+                else:
+                    if nextEl is not None:
+                        target.append({} if isinstance(nextEl, str) else [])
+                        target = target[el]
+                    else:
+                        target[el] = None
+
+    def set_value_for_path(self, data, path, result):
+        self.ensure_path(path, result)
+        target = result
+        for i in range(0, len(path) - 1):
+            el = path[i]
+            target = target[el]
+        target[path[len(path) - 1]] = data
+
+    def trace_tree_for_diffs(self, source, data, path, result, keep_name=False):
+        if isinstance(data, list):
+            if source != data:
+                if source and self.is_primitive_array(source) or data and self.is_primitive_array(data):
+                    self.set_value_for_path(data, path, result)
+                else:
+                    if len(data) != len(source):
+                        raise AssertionError("Cannot get diff for different arrays")
+                    for i in range(0, len(data)):
+                        newPath = copy.copy(path)
+                        newPath.append(i)
+                        self.trace_tree_for_diffs(source[i], data[i], newPath, result, keep_name=keep_name)
+        elif isinstance(data, dict):
+            if source != data:
+                for k, v in data.items():
+                    newPath = copy.copy(path)
+                    newPath.append(k)
+                    if k not in source:
+                        self.set_value_for_path(data[k], newPath, result)
+                    else:
+                        self.trace_tree_for_diffs(source[k], data[k], newPath, result, keep_name=keep_name)
+                for k, v in source.items():
+                    if k not in data:
+                        newPath = copy.copy(path)
+                        newPath.append(k)
+                        self.set_value_for_path(None, newPath, result)
+        else:
+            if source != data:
+                self.set_value_for_path(data, path, result)
+            elif keep_name and len(path) > 0 and path[len(path) - 1] == "name":
+                self.set_value_for_path(data, path, result)
+
+    def get_json_diff(self, source, data, keep_name=False):
+        result = {}
+        self.trace_tree_for_diffs(source, data, [], result, keep_name=keep_name)
+        return result
+
+
+def retry(exceptions=None, tries=5, delay=1, backoff=1, logger=None):
+    """
+    :param exceptions: if defined - only specified exceptions will be checked
+    :type exceptions: tuple of Exception or Exception
+    :param tries: how much to try before fail. <=0 means no limits.
+    :param delay: basic delay between tries
+    :param backoff: delay increase factor after each retry
+    :param logger:
+    :type logger: logging.Logger
+    :return:
+    """
+    def deco_retry(f):
+
+        def handle_error(e, mtries, mdelay):
+            msg = "Error occurred during execution: {}. Will retry in {} seconds.".format(str(e), delay)
+            if logger:
+                logger.exception(msg)
+            else:
+                print(msg)
+            time.sleep(mdelay)
+            mtries -= 1
+            mdelay *= backoff
+            return mtries, mdelay
+
+        def f_retry(*args, **kwargs):
+            mtries, mdelay = tries, delay
+            while tries <= 0 or mtries > 1:
+                if exceptions:
+                    try:
+                        return f(*args, **kwargs)
+                    except exceptions as e:
+                        mtries, mdelay = handle_error(e, mtries, mdelay)
+                else:
+                    try:
+                        return f(*args, **kwargs)
+                    except Exception as e:
+                        mtries, mdelay = handle_error(e, mtries, mdelay)
+            return f(*args, **kwargs)
+
+        return f_retry
+    return deco_retry
diff --git a/docker-backup-daemon/maintenance/recovery/utils_dcs.py b/docker-backup-daemon/maintenance/recovery/utils_dcs.py
new file mode 100644
index 0000000..16ca845
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/utils_dcs.py
@@ -0,0 +1,124 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+import logging
+from abc import ABCMeta, abstractmethod
+
+log = logging.getLogger()
+
+
+class PatroniDCS(metaclass=ABCMeta):
+    @abstractmethod
+    def get_dcs_config(self, client, recovery_pod_id, pg_cluster_name):
+        """
+        returns 2 dict with data from dcs - config from dcs and dcs data itself.
+        :type client: OpenshiftClient
+        :type recovery_pod_id: str
+        :type pg_cluster_name: str
+        :rtype: tuple(dict, str)
+        """
+        pass
+
+    @abstractmethod
+    def update_dcs_config(self, oc_client, recovery_pod_id, patroni_dsc):
+        pass
+
+    @abstractmethod
+    def cleanup_initialization_key(self, oc_client, pg_cluster_name, pod_id):
+        """
+        :type oc_client: OpenshiftClient
+        :param pg_cluster_name:
+        :param pod_id:
+        :return:
+        """
+        pass
+
+
+class PatroniDCSEtcd(PatroniDCS):
+
+    def get_dcs_config(self, oc_client, recovery_pod_id, pg_cluster_name):
+        patroni_dsc_data = json.loads(oc_client.oc_exec(recovery_pod_id, "curl etcd:2379/v2/keys/patroni/{}/config"
+                                                        .format(pg_cluster_name)))["node"]["value"]
+        log.debug("Patroni dcd: {}".format(patroni_dsc_data))
+        patroni_dsc = json.loads(patroni_dsc_data)
+        log.debug("Patroni dcd (parsed): {}".format(patroni_dsc))
+        return patroni_dsc, patroni_dsc_data
+
+    def update_dcs_config(self, oc_client, recovery_pod_id, patroni_dsc):
+        log.info("Start dsc configuration update.")
+        recovery_conf = patroni_dsc["postgresql"]["recovery_conf"]
+        recovery_conf["recovery_target_action"] = "promote"
+        with open("dsc.config.tmp", mode="w") as fd:
+            json.dump(patroni_dsc, fd)
+        oc_client.rsync("./", "{}:/tmp".format(recovery_pod_id))
+        log.debug(oc_client.oc_exec(recovery_pod_id,
+                                    'python -c \'import etcd, os; '
+                                    'fd=open("/tmp/dsc.config.tmp"); data=fd.read(); fd.close(); '
+                                    'client = etcd.Client(host="etcd", protocol="http", port=2379); '
+                                    'client.write("patroni/{}/config".format(os.getenv("PG_CLUST_NAME")), data)\''))
+
+    def cleanup_initialization_key(self, oc_client, pg_cluster_name, pod_id):
+        oc_client.oc_exec(pod_id, "sh -c '"
+                                  "curl -XDELETE -s etcd:2379/v2/keys/patroni/${PG_CLUST_NAME}/initialize; "
+                                  "curl -XDELETE etcd:2379/v2/keys/patroni/${PG_CLUST_NAME}/optime?recursive=true'")
+
+
+class PatroniDCSKubernetes(PatroniDCS):
+
+    def get_dcs_config(self, oc_client, recovery_pod_id, pg_cluster_name):
+        """
+        :type oc_client: OpenshiftClient
+        :param recovery_pod_id:
+        :param pg_cluster_name:
+        :return:
+        """
+        patroni_dcs_cm = oc_client.get_configmap("{}-config".format(pg_cluster_name))
+        log.debug("DCS config map: {}".format(patroni_dcs_cm))
+        patroni_dsc_data = patroni_dcs_cm["metadata"]["annotations"]["config"]
+        log.debug("Patroni dcs: {}".format(patroni_dsc_data))
+        patroni_dsc = json.loads(patroni_dsc_data)
+        log.debug("Patroni dcd (parsed): {}".format(patroni_dsc))
+        return patroni_dsc, patroni_dsc_data
+
+    def update_dcs_config(self, oc_client, recovery_pod_id, patroni_dsc):
+        raise NotImplementedError()
+
+    def cleanup_initialization_key(self, oc_client, pg_cluster_name, pod_id):
+        """
+        :type oc_client: OpenshiftClient
+        :param pg_cluster_name:
+        :param pod_id:
+        :return:
+        """
+        log.debug("Try to delete configmap {}-leader".format(pg_cluster_name))
+        oc_client.delete_entity("configmap", "{}-leader".format(pg_cluster_name))
+
+        # in case of sync replication we should also remove leader key from cm
+        if oc_client.get_entity_safe("configmap", "{}-sync".format(pg_cluster_name)):
+            log.debug("Try to delete configmap {}-sync".format(pg_cluster_name))
+            patroni_dcs_sync_cm = oc_client.get_configmap("{}-sync".format(pg_cluster_name))
+            patroni_dcs_sync_cm["metadata"]["annotations"].pop('leader', None)
+            oc_client.replace_object(patroni_dcs_sync_cm)
+
+        log.debug("Try to remove initialize key from configmap {}-config".format(pg_cluster_name))
+        for _ in range(1, 5):
+            # this is a dirty WA because sometimes initialize did not removed
+            patroni_dcs_cm = oc_client.get_configmap("{}-config".format(pg_cluster_name))
+            log.debug("DCS config map before : {}".format(patroni_dcs_cm))
+            if "initialize" in patroni_dcs_cm["metadata"]["annotations"]:
+                del patroni_dcs_cm["metadata"]["annotations"]["initialize"]
+            oc_client.replace_object(patroni_dcs_cm)
+            patroni_dcs_cm = oc_client.get_configmap("{}-config".format(pg_cluster_name))
+            log.debug("DCS config map after applying: {}".format(patroni_dcs_cm))
\ No newline at end of file
diff --git a/docker-backup-daemon/maintenance/recovery/utils_oc.py b/docker-backup-daemon/maintenance/recovery/utils_oc.py
new file mode 100644
index 0000000..915d559
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/utils_oc.py
@@ -0,0 +1,1113 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import json
+from abc import abstractmethod, ABCMeta
+import subprocess
+import logging
+from utils_common import *
+import pprint
+import time
+
+log = logging.getLogger()
+
+try:
+    from kubernetes import client
+    from kubernetes.client import configuration
+    from kubernetes.client import rest
+    #from openshift import client as op_client
+    from kubernetes.client.rest import ApiException
+    from kubernetes.stream import stream
+    from kubernetes.stream.ws_client import ERROR_CHANNEL, STDOUT_CHANNEL, STDERR_CHANNEL
+    import kubernetes
+    from six import iteritems
+
+    def to_dict(self):
+        """
+        Returns the model properties as a dict
+        """
+        result = {}
+
+        for attr, _ in iteritems(self.swagger_types):
+            if attr in self.attribute_map:
+                attr_name = self.attribute_map[attr]
+            else:
+                attr_name = attr
+
+            value = getattr(self, attr)
+            if isinstance(value, list):
+                result[attr_name] = list([x.to_dict() if hasattr(x, "to_dict") else x for x in value])
+            elif hasattr(value, "to_dict"):
+                result[attr_name] = value.to_dict()
+            elif isinstance(value, dict):
+                result[attr_name] = dict([(item[0], item[1].to_dict())
+                    if hasattr(item[1], "to_dict") else item for item in list(value.items())])
+            else:
+                result[attr_name] = value
+
+        return result
+
+    # kubernetes.client.models.v1_pod.V1Pod.to_dict = to_dict
+    # kubernetes.client.models.v1_container.V1Container.to_dict = to_dict
+    # kubernetes.client.models.v1_probe.V1Probe.to_dict = to_dict
+    # kubernetes.client.models.v1_config_map.V1ConfigMap.to_dict = to_dict
+    # kubernetes.client.models.v1_deployment.V1Deployment.to_dict = to_dict
+    # openshift.client.models.v1_deployment_config.V1DeploymentConfig.to_dict = to_dict
+    # kubernetes.client.models.v1_replication_controller.V1ReplicationController.to_dict = to_dict
+    # kubernetes.client.models.v1_stateful_set.V1StatefulSet.to_dict = to_dict
+    # kubernetes.client.models.v1beta1_stateful_set.V1beta1StatefulSet.to_dict = to_dict
+    # kubernetes.client.models.apps_v1beta1_deployment.AppsV1beta1Deployment.to_dict = to_dict
+    # kubernetes.client.models.v1_object_meta.V1ObjectMeta.to_dict = to_dict
+    # kubernetes.client.models.v1_stateful_set_status.V1StatefulSetStatus.to_dict = to_dict
+    # kubernetes.client.models.v1beta1_stateful_set_status.V1beta1StatefulSetStatus.to_dict = to_dict
+
+    class ObjectEncoder(json.JSONEncoder):
+
+        def default(self, o):
+            if isinstance(o, datetime.datetime):
+                return o.strftime("%Y-%m-%dT%H:%M:%SZ")
+            return super(ObjectEncoder, self).default(o)
+
+    use_kube_client = True
+except ImportError as e:
+    log.exception("Cannot use python client")
+    use_kube_client = False
+
+
+def get_api_token(oc_url, username, password):
+    import base64
+    import requests
+    import re
+
+    auth_url = oc_url + "/oauth/authorize?" \
+                        "client_id=openshift-challenging-client&" \
+                        "response_type=token"
+    headers = {}
+    user_password = username + b":" + password
+    encoding = base64.b64encode(user_password)
+    headers["Authorization"] = b"Basic " + encoding
+    headers["X-CSRF-Token"] = b"1"
+
+    result = requests.get(url=auth_url, headers=headers, verify=False, allow_redirects=False)
+    if result.status_code != 302:
+        raise ApiException(status=result.status_code, http_resp=result.text)
+    location = result.headers["Location"]
+    p = re.compile("access_token=([a-zA-Z0-9-_]+)")
+    token = p.findall(location)[0]
+    return token
+
+
+class OpenshiftClient(metaclass=ABCMeta):
+    @abstractmethod
+    def login(self, oc_url, username, password, project, skip_tls_verify=False):
+        pass
+
+    @abstractmethod
+    def use_token(self, oc_url, oc_token, project, skip_tls_verify=False):
+        pass
+
+    @abstractmethod
+    def get_entities(self, entity_type):
+        """
+        :param entity_type:
+        :return: parsed entity
+        :rtype: []
+        """
+        pass
+
+    @abstractmethod
+    def get_entity(self, entity_type, entity_name):
+        """
+        :param entity_type:
+        :param entity_name:
+        :return: parsed entity
+        :rtype: {}
+        """
+        pass
+
+    @abstractmethod
+    def get_entity_safe(self, entity_type, entity_name):
+        """
+        :param entity_type:
+        :param entity_name:
+        :return: parsed entity or None if entity does not exist
+        :rtype: {}
+        """
+        pass
+
+    def get_configmap(self, cm_name):
+        """
+        :param cm_name:
+        :return: parsed entity
+        :rtype: {}
+        """
+        return self.get_entity_safe("configmap", cm_name)
+
+    def get_deployment(self, dc_name, type="dc"):
+        """
+        :param dc_name:
+        :return: parsed entity
+        :rtype: {}
+        """
+        return self.get_entity(type, dc_name)
+
+    def get_stateful_set(self, stateful_set_name):
+        return self.get_entity("statefulset", stateful_set_name)
+
+    def get_env_for_pod(self, pod_id, env_name, default_value=None):
+        pod = self.get_entity("pod", pod_id)
+        envs = pod["spec"]["containers"][0]["env"]
+        env = list([x for x in envs if x["name"] == env_name])
+        return default_value if not env else env[0]["value"]
+
+    def get_env_for_dc(self, dc_name, env_name, default_value=None):
+        dc = self.get_entity("dc", dc_name)
+        envs = dc["spec"]["template"]["spec"]["containers"][0]["env"]
+        env = list([x for x in envs if x["name"] == env_name])
+        return default_value if not env else env[0]["value"]
+
+    @abstractmethod
+    def set_env_for_dc(self, dc_name, env_name, env_value):
+        pass
+
+    @abstractmethod
+    def get_deployment_names(self, dc_name_part):
+        """
+        :param dc_name_part:
+        :return: list of deployment configs names which contain `dc_name_part`
+        :rtype: [string]
+        """
+        pass
+
+    def get_deployment_replicas_count(self, dc_name, type="dc"):
+        """
+        :param dc_name:
+        :return: replica parameter from deployment corresponding to dc_name
+        :rtype: int
+        """
+        deployment = self.get_deployment(dc_name, type)
+        return int(deployment["spec"]["replicas"])
+
+    def get_stateful_set_replicas_count(self, stateful_set_name):
+        stateful_set = self.get_stateful_set(stateful_set_name)
+        return int(stateful_set.get("spec").get("replicas"))
+
+    def get_running_stateful_set_replicas_count(self, stateful_set_name):
+        stateful_set = self.get_stateful_set(stateful_set_name)
+        status = stateful_set.get("status")
+        return min(int(status.get("ready_replicas") or "0"),
+                   int(status.get("updated_replicas") or "0"))
+
+    def get_liveness_probe_from_stateful_set(self, stateful_set_name):
+        stateful_set = self.get_stateful_set(stateful_set_name)
+        return stateful_set["spec"]["template"]["spec"]["containers"][0]["livenessProbe"]
+
+    @abstractmethod
+    def get_replicas_desc(self, dc_name, running=True):
+        """
+        :param dc_name:
+        :param running:
+        :return: list of replicas descriptions
+        :rtype: []
+        """
+        pass
+
+    def get_replicas_names(self, dc_name, running=True):
+        """
+        :param dc_name:
+        :param running:
+        :return:
+        :rtype: [string]
+        """
+        pod_names = list([p["metadata"]["name"] for p in self.get_replicas_desc(dc_name, running)])
+        return pod_names
+
+    @abstractmethod
+    def delete_entity(self, entity_type, entity_name, ignore_not_found=True):
+        pass
+
+    @abstractmethod
+    def oc_exec(self, pod_id, command):
+        """
+        :param pod_id:
+        :param command:
+        :return:
+        :rtype: string
+        """
+        pass
+
+    @abstractmethod
+    def get_logs(self, pod_id, since=None):
+        """
+        :param pod_id:
+        :param since:
+        :return:
+        :rtype: string
+        """
+        pass
+
+    @abstractmethod
+    def rsync(self, source, target):
+        pass
+
+    @abstractmethod
+    def delete_pod(self, pod_id, grace_period=None):
+        pass
+
+    @abstractmethod
+    def scale(self, dc_name, count, entity="dc"):
+        pass
+
+    @abstractmethod
+    def apply_object(self, data):
+        pass
+
+    @abstractmethod
+    def replace_object(self, data):
+        pass
+
+    @abstractmethod
+    def get_cluster_pods(self, cluster_name, running=True):
+        pass
+
+    @abstractmethod
+    def get_cluster_pods_desc(self, cluster_name, running=True):
+        pass
+
+    @abstractmethod
+    def get_pods_by_label(self, label_selector):
+        pass
+
+    def get_pod_status(self, pod_id):
+        items = self.get_entities("pod")
+        for x in items:
+            if x["metadata"]["name"] == pod_id:
+                return x["status"]["phase"]
+            else:
+                log.info("Pod {} not found".format(pod_id))
+
+    def is_pod_ready(self, pod_id, attempts=5):
+        for i in range(1, attempts):
+            time.sleep(5)
+            status = self.get_pod_status(pod_id)
+            log.info("Pod state is {}".format(status))
+            if status.lower() == "running":
+                return True
+            else:
+                log.info("Retrying...")
+        log.info("Can't get pod {} status".format(pod_id))
+        return False
+
+    def get_postgres_backup_daemon_pod(self):
+        pods_from_dcs = self.get_pods_by_label("app=postgres-backup-daemon")
+        pods_from_deployments = self.get_pods_by_label("component=postgres-backup-daemon")
+        return pods_from_dcs + pods_from_deployments
+
+
+class OpenshiftShellClient(OpenshiftClient):
+    def __init__(self, oc_path="oc", config_file="./oc_config_file.yaml"):
+        self.oc = oc_path + " --config={}".format(config_file)
+
+    def login(self, oc_url, username, password, project, skip_tls_verify=False):
+        log.info("Log in as {} to {}".format(username, oc_url))
+        subprocess.check_call(
+            '{} login -u "{}" -p "{}" {} {}'
+                .format(self.oc,
+                        username, password, oc_url,
+                        ("--insecure-skip-tls-verify=true" if skip_tls_verify else "")),
+            shell=True, stdout=subprocess.PIPE)
+
+        if project:
+            log.info("Change project to {}".format(project))
+            subprocess.check_call(
+                '{} project {}'.format(self.oc, project),
+                shell=True, stdout=subprocess.PIPE)
+
+    def use_token(self, oc_url, oc_token, project, skip_tls_verify=False):
+        raise NotImplementedError("Cannot use token with oc client")
+
+    def get_entities(self, entity_type):
+        entity = json.loads(subprocess.Popen(
+            '{} get {} -o json'.format(self.oc, entity_type),
+            shell=True, stdout=subprocess.PIPE).stdout.read())
+        return entity["items"]
+
+    def get_entity(self, entity_type, entity_name):
+        entity = json.loads(subprocess.Popen(
+            '{} get {} {} -o json'.format(self.oc, entity_type, entity_name),
+            shell=True, stdout=subprocess.PIPE).stdout.read())
+        return entity
+
+    def get_entity_safe(self, entity_type, entity_name):
+        entities_data = json.loads(subprocess.Popen(
+            '{} get {} -o json'.format(self.oc, entity_type), shell=True, stdout=subprocess.PIPE).stdout.read())
+        entities = list([p for p in entities_data["items"] if entity_name == p["metadata"]["name"]])
+        if entities:
+            return entities[0]
+        return None
+
+    def set_env_for_dc(self, dc_name, env_name, env_value):
+        log.info("Try to set env {}={} to deployment {}".format(env_name, env_value, dc_name))
+        env_repr = "{}={}".format(env_name, env_value) if env_value else "{}-".format(env_name)
+        subprocess.check_call("{} set env dc {} {}".format(self.oc, dc_name, env_repr), shell=True)
+
+    def get_deployment_names(self, dc_name_part, type="dc"):
+        deployments_data = json.loads(subprocess.Popen(
+            '{} get {} -o json'.format(self.oc, type), shell=True, stdout=subprocess.PIPE).stdout.read())
+        deployments = list(
+            [p["metadata"]["name"] for p in [p for p in deployments_data["items"] if dc_name_part in p["metadata"]["name"]]])
+        return deployments
+
+    def get_replicas_desc(self, dc_name, running=True):
+        pods_data = json.loads(subprocess.Popen(
+            "{} get pods {} -o json".format(self.oc, ("-a" if not running else "")),
+            shell=True, stdout=subprocess.PIPE).stdout.read())
+        pods = list(
+            [p for p in pods_data["items"] if "deploymentconfig" in p["metadata"]["labels"] and
+                   dc_name in p["metadata"]["labels"]["deploymentconfig"]])
+        if running:
+            pods = list([p for p in pods if "running" == p["status"]["phase"].lower()])
+        return pods
+
+    def delete_entity(self, entity_type, entity_name, ignore_not_found=True):
+        log.debug("Try to delete entity {} {}".format(entity_type, entity_name))
+        inf_value = "true" if ignore_not_found else "false"
+        subprocess.check_call("{} delete {} {} --ignore-not-found={}"
+                              .format(self.oc, entity_type, entity_name, inf_value),
+                              shell=True)
+
+    def oc_exec(self, pod_id, command):
+        log.info("Try to execute '{}' on pod {}".format(command, pod_id))
+        process = subprocess.Popen("{} exec {} -- {}".format(self.oc, pod_id, command), shell=True,
+                                   stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+        if process.wait() != 0:
+            raise Exception("Error occured during execution. "
+                            "Return code: {}, stderr: {}, stdout: {}"
+                            .format(process.returncode, process.stderr.read(), process.stdout.read()))
+        return process.stdout.read().decode()
+
+    def get_logs(self, pod_id, since=None):
+        log.debug("Try to obtain logs from pod {} for last {}s.".format(pod_id, since))
+        cmd = "{} logs {} --since={}s".format(self.oc, pod_id, since)
+        if not since:
+            cmd = "{} logs {}".format(self.oc, pod_id)
+        process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+        result = process.communicate()
+        log.debug("Received data len: {}".format(0 if not result[0] else len(result[0])))
+        if process.returncode != 0 or result[1]:
+            log.warning("Error during log obtain. code{}, message: {}".format(process.returncode, result[1]))
+        return result[0].decode()
+
+    def rsync(self, source, target):
+        subprocess.check_call("{} rsync {} {}".format(self.oc, source, target), shell=True)
+
+    def delete_pod(self, pod_id, grace_period=None):
+        log.debug("Remove pod {} with grace-period {}".format(pod_id, grace_period))
+        grace_period = "--grace-period={}".format(grace_period) if grace_period else ""
+        p = subprocess.Popen("{} delete pod {} {}".format(self.oc, pod_id, grace_period),
+                             shell=True, stdin=subprocess.PIPE, stderr=subprocess.PIPE)
+        if p.wait() != 0:
+            error = p.stderr.read()
+            if 'pods "{}" not found'.format(pod_id) in error.decode():
+                log.warning("Cannot remove pod {} - no such pod.".format(pod_id))
+                pass
+            else:
+                raise Exception("Cannot remove pod. Error: {}".format(error))
+
+    @retry(tries=5)  # handle case when DC version is Unknown
+    def scale(self, name, count, entity="dc"):
+        subprocess.check_call("{} scale --replicas={} {} {}"
+                              .format(self.oc, count, entity, name), shell=True)
+
+    def apply_object(self, data):
+        p = subprocess.Popen("{} apply -f -".format(self.oc), shell=True, stdin=subprocess.PIPE)
+        p.communicate(input=json.dumps(data).encode())
+
+    def replace_object(self, data):
+        p = subprocess.Popen("{} replace -f -".format(self.oc), shell=True, stdin=subprocess.PIPE)
+        p.communicate(input=json.dumps(data).encode())
+
+    def get_cluster_pods(self, cluster_name, running=True):
+        pods_data = json.loads(
+            subprocess.Popen("{} get pods {} --selector=\"pgcluster={}\" -o json"
+                             .format(self.oc, ("-a" if not running else ""), cluster_name),
+                             shell=True, stdout=subprocess.PIPE).stdout.read())
+        pods = [pod["metadata"]["name"] for pod in pods_data["items"]]
+        return pods
+
+    def get_cluster_pods_desc(self, cluster_name, running=True):
+        pods_data = json.loads(
+            subprocess.Popen("{} get pods {} --selector=\"pgcluster={}\" -o json"
+                             .format(self.oc, ("-a" if not running else ""), cluster_name),
+                             shell=True, stdout=subprocess.PIPE).stdout.read())
+        return pods_data["items"]
+
+    def get_pods_by_label(self, label_selector):
+        pods_data = json.loads(
+            subprocess.Popen("{0} get pods {1} -l={2} -o json".format(self.oc, "-a", label_selector),
+                             shell=True, stdout=subprocess.PIPE).stdout.read())
+        pods = [pod["metadata"]["name"] for pod in pods_data["items"]]
+        return pods
+
+
+class OpenshiftPyClient(OpenshiftClient):
+
+    def __init__(self):
+        super(OpenshiftPyClient, self).__init__()
+        self._api_client = None
+        self.project = None
+
+    def login(self, oc_url, username, password, project, skip_tls_verify=False):
+        log.info("Log in as {} to {}".format(username, oc_url))
+        
+        # Configuration for Kubernetes client
+        os_config = client.Configuration()
+        os_config.verify_ssl = not skip_tls_verify
+        os_config.assert_hostname = False
+        os_config.host = oc_url
+        
+        openshift_token = get_api_token(oc_url, b"admin", b"admin")
+        os_config.api_key = {"authorization": "Bearer " + openshift_token}
+        
+        self._api_client = client.ApiClient(configuration=os_config)
+
+        log.info("Will use namespace {}".format(project))
+        self.project = project
+
+    # This method is called if OC_TOKEN env variable is presented,
+    # In our cases, this env is set only in case of robot tests
+    def use_token(self, oc_url, oc_token, project, skip_tls_verify=False):
+        log.info("Log in to {} with token".format(oc_url))
+        from kubernetes import config
+        config.load_incluster_config()
+        log.info("Will use namespace {}".format(project))
+        self.project = project
+
+    def __list_entities(self, entity_type):
+        if entity_type == "pod":
+            core_api = client.CoreV1Api(self._api_client)
+            items = core_api.list_namespaced_pod(self.project).items
+        elif entity_type == "configmap":
+            core_api = client.CoreV1Api(self._api_client)
+            items = core_api.list_namespaced_config_map(self.project).items
+        elif entity_type == "dc":
+            core_api = client.CoreV1Api(self._api_client)
+            items = core_api.list_namespaced_deployment_config(self.project).items
+        elif entity_type == "rc":
+            core_api = client.CoreV1Api(self._api_client)
+            items = core_api.list_namespaced_replication_controller(self.project).items
+        elif entity_type == "statefulset":
+            try:
+                apps_api = client.AppsV1Api(self._api_client)
+                items = apps_api.list_namespaced_stateful_set(self.project).items
+            except:
+                apps_api = client.AppsV1beta1Api(self._api_client)
+                items = apps_api.list_namespaced_stateful_set(self.project).items
+        elif entity_type == "deployment":
+            try:
+                apps_api = client.AppsV1beta1Api(self._api_client)
+                items = apps_api.list_namespaced_deployment(self.project).items
+            except:
+                apps_api = client.AppsV1Api(self._api_client)
+                items = apps_api.list_namespaced_deployment(self.project).items
+        else:
+            raise NotImplementedError("Cannot list {}".format(entity_type))
+        return items
+
+    def __to_dict(self, entity):
+        data = entity.to_dict()
+        if isinstance(entity, kubernetes.client.models.v1_pod.V1Pod):
+            data["kind"] = "Pod"
+        elif isinstance(entity,
+                        kubernetes.client.models.v1_config_map.V1ConfigMap):
+            data["kind"] = "ConfigMap"
+        elif isinstance(entity,
+                        kubernetes.client.models.v1_deployment.V1Deployment):
+            data["kind"] = "Deployment"
+        elif isinstance(entity,
+                        kubernetes.client.models.v1_replication_controller.V1ReplicationController):
+            data["kind"] = "ReplicationController"
+        elif isinstance(entity, kubernetes.client.models.v1_stateful_set.V1StatefulSet):
+            data["kind"] = "StatefulSet"
+        else:
+            raise NotImplementedError("Cannot transform to dict entity {}"
+                                      .format(entity))
+        data = json.loads(json.dumps(data, cls=ObjectEncoder))
+        return data
+
+    def get_entities(self, entity_type):
+        items = self.__list_entities(entity_type)
+        return list([self.__to_dict(x) for x in items])
+
+    def get_entity(self, entity_type, entity_name):
+        items = self.__list_entities(entity_type)
+        if entity_type == "statefulset":
+            filtered_items = [item for item in items if "patroni" in item.metadata.name]
+        else:
+            filtered_items = list(filter(lambda x: x.metadata.name == entity_name, items))
+        if entity_name == "pg-patroni-node1":
+            returned_value = self.__to_dict(filtered_items[0])
+        elif entity_name == "pg-patroni-node2":
+            returned_value = self.__to_dict(filtered_items[1])
+        else:
+            returned_value = self.__to_dict(filtered_items[0])
+        return returned_value
+
+    def get_entity_safe(self, entity_type, entity_name):
+        try:
+            return self.get_entity(entity_type, entity_name)
+        except Exception as e:
+            return None
+
+    def set_env_for_dc(self, dc_name, env_name, env_value):
+        log.info("Try to set env {}={} to deployment {}".format(env_name, env_value, dc_name))
+        core_api = client.CoreV1Api(self._api_client)
+        dc = core_api.read_namespaced_deployment_config(dc_name, self.project)
+        base_envs = dc.spec.template.spec.containers[0].env
+        base_env = list([x for x in base_envs if x.name == env_name])
+        if env_value:
+            if base_env:
+                base_env[0].value = env_value
+            else:
+                base_envs.append(client.V1EnvVar(name=env_name, value=env_value))
+            log.info(core_api.patch_namespaced_deployment_config(dc_name, self.project, dc))
+        else:
+            if base_env:
+                base_envs.remove(base_env[0])
+                log.info(core_api.replace_namespaced_deployment_config(dc_name, self.project, dc))
+
+    def get_deployment_names(self, dc_name_part, type="dc"):
+        items = self.__list_entities(type)
+        deployments = list(
+            [p.metadata.name for p in [p for p in items if dc_name_part in p.metadata.name]])
+        return deployments
+
+    def get_replicas_desc(self, dc_name, type="dc", running=True):
+        items = self.__list_entities("pod")
+        pods = list(
+            [p for p in items if "deploymentconfig" in p.metadata.labels and
+                   dc_name in p.metadata.labels["deploymentconfig"]])
+        if running:
+            pods = list(
+                [self.__to_dict(x) for x in [p for p in pods if "running" == p.status.phase.lower()]])
+        else:
+            pods = list([self.__to_dict(x) for x in pods])
+        return pods
+
+    def delete_entity(self, entity_type, entity_name, ignore_not_found=True):
+        log.debug("Try to delete entity {} {}".format(entity_type, entity_name))
+        try:
+            if entity_type == "pod":
+                core_api = client.CoreV1Api(self._api_client)
+                core_api.delete_namespaced_pod(entity_name, self.project, {})
+            elif entity_type == "configmap":
+                body = client.V1DeleteOptions()
+                core_api = client.CoreV1Api(self._api_client)
+                core_api.delete_namespaced_config_map(entity_name, self.project, body=body,)
+            elif entity_type == "dc":
+                core_api = client.CoreV1Api(self._api_client)
+                core_api.delete_namespaced_deployment_config(entity_name,
+                                                             self.project, {})
+            elif entity_type == "rc":
+                core_api = client.CoreV1Api(self._api_client)
+                core_api.delete_namespaced_replication_controller(entity_name,
+                                                                  self.project, {})
+            elif entity_type == "statefulset":
+                core_api = client.AppsV1Api(self._api_client)
+                core_api.delete_namespaced_stateful_set(entity_name,
+                                                        self.project, {})
+            else:
+                raise NotImplementedError("Cannot delete {}".format(entity_type))
+        except kubernetes.client.rest.ApiException as e:
+            if ignore_not_found and e.reason == "Not Found":
+                return
+            else:
+                raise e
+
+    @retry(tries=30, delay=5)
+    def oc_exec(self, pod_id, command):
+        log.debug(f"Try to execute '{command}' on pod {pod_id}")
+        core_api = client.CoreV1Api(self._api_client)
+
+        exec_command = [
+            '/bin/sh', '-c', command
+        ]
+
+        try:
+            resp = stream(core_api.connect_get_namespaced_pod_exec,
+                          pod_id,
+                          self.project,
+                          command=exec_command,
+                          stderr=True, stdin=False,
+                          stdout=True, tty=False, _preload_content=True, _request_timeout=60)
+
+            log.info(f"Command executed. Result: {resp}")
+
+            if resp:
+                log.debug(f"Command output: {resp}")
+                if "No such file or directory" in resp or "cannot remove" in resp:
+                    log.info("Directory already cleaned up or removal issue detected.")
+                    return resp  # Exit early if the directory is already cleaned up or a removal issue was detected
+
+            return resp
+
+        except Exception as e:
+            log.error(f"Exception occurred while executing command: {e}")
+            raise
+
+        log.debug(f"Command '{command}' completed for pod {pod_id}")
+        return None
+
+    def get_logs(self, pod_id, since=None):
+        log.debug("Try to obtain logs from pod {} for last {}s."
+                  .format(pod_id, since))
+        # time.sleep(20)
+        core_api = client.CoreV1Api(self._api_client)
+        if since:
+            return core_api.read_namespaced_pod_log(pod_id, self.project, since_seconds=since)
+        else:
+            return core_api.read_namespaced_pod_log(pod_id, self.project)
+
+    def rsync(self, source, target):
+        if ":" in source:
+            raise Exception("Cannot load files from pod yet")
+        (pod_id, target_dir) = target.split(":")
+        log.debug("Try to upload files from {} to pod {} in dir {}"
+                  .format(source, pod_id, target_dir))
+
+        import tarfile
+        import tempfile
+        tempfile_fd = tempfile.TemporaryFile()
+        tar = tarfile.open(fileobj=tempfile_fd, mode='w:gz')
+        tar.add(source)
+        tar.close()
+        tempfile_fd.flush()
+        tempfile_fd.seek(0)
+
+        core_api = client.CoreV1Api(self._api_client)
+        exec_command = ['tar', 'xzvf', '-', '-C', target_dir]
+        resp = stream(core_api.connect_get_namespaced_pod_exec, pod_id,
+                      self.project,
+                      command=exec_command,
+                      stderr=True, stdin=True,
+                      stdout=True, tty=False,
+                      _preload_content=False)
+
+        resp.write_stdin(tempfile_fd.read())
+        resp.update(1)
+        if resp.peek_stdout():
+            log.debug("STDOUT: %s" % resp.read_stdout())
+        if resp.peek_stderr():
+            log.debug("STDERR: %s" % resp.read_stderr())
+        resp.close()
+        tempfile_fd.close()
+        error = resp.read_channel(ERROR_CHANNEL)
+        if error and "Success" != json.loads(error).get("status"):
+            raise Exception("Error occurred during execution: {}. "
+                            .format(error))
+
+    def delete_pod(self, pod_id, grace_period=None):
+        log.debug("Try to remove pod {} with grace-period {}"
+                  .format(pod_id, grace_period))
+        core_api = client.CoreV1Api(self._api_client)
+        try:
+            status = core_api.delete_namespaced_pod(pod_id, self.project, {},
+                                                    grace_period_seconds=grace_period)
+            log.debug(status)
+        except ApiException as ae:
+            if 'pods \\"{}\\" not found'.format(pod_id) in ae.body:
+                log.warning("Cannot remove pod {} - no such pod."
+                            .format(pod_id))
+                pass
+            else:
+                raise ae
+
+    @retry(tries=5)  # handle case when DC version is Unknown
+    def scale(self, name, count, entity="dc"):
+        log.debug("Try to scale {} {} to {} replicas".format(entity, name, count))
+        if entity == "dc":
+            core_api = client.CoreV1Api(self._api_client)
+            data = core_api.patch_namespaced_deployment_config(name, self.project, {"spec": {"replicas": count}})
+        elif entity == "statefulset":
+            core_api = client.AppsV1Api(self._api_client)
+            data = core_api.patch_namespaced_stateful_set(name, self.project, {"spec": {"replicas": count}})
+        elif entity == "deployment":
+            try:
+                core_api = client.AppsV1beta1Api(self._api_client)
+                data = core_api.patch_namespaced_deployment(name, self.project, {"spec": {"replicas": count}})
+            except:
+                core_api = client.AppsV1Api(self._api_client)
+                data = core_api.patch_namespaced_deployment(name, self.project, {"spec": {"replicas": count}})
+        else:
+            raise NotImplementedError("Cannot scale entity {} of type {}".format(name, entity))
+        return self.__to_dict(data)
+
+    def get_json_diff(self, source, data):
+        return Differ().get_json_diff(source, data, keep_name=True)
+
+    def apply_object(self, data):
+        log.debug("Try to apply {}".format(data))
+        entity_type = data["kind"]
+        entity_name = data["metadata"]["name"]
+        if entity_type == "Pod":
+            core_api = client.CoreV1Api(self._api_client)
+            source = reset_last_applied(self.get_entity("pod", entity_name))
+            diff = self.get_json_diff(source, data)
+            if diff:
+                return self.__to_dict(core_api.patch_namespaced_pod(entity_name, self.project, diff))
+            return data
+        elif entity_type == "ConfigMap":
+            core_api = client.CoreV1Api(self._api_client)
+            source = self.get_entity("configmap", entity_name)
+            diff = self.get_json_diff(source, data)
+            if diff:
+                return self.__to_dict(core_api.patch_namespaced_config_map(entity_name, self.project, diff))
+            return data
+        elif entity_type == "DeploymentConfig":
+            core_api = client.CoreV1Api(self._api_client)
+            source = reset_last_applied(self.get_entity("dc", entity_name))
+            diff = self.get_json_diff(source, data)
+            if diff:
+                return self.__to_dict(core_api.patch_namespaced_deployment_config(entity_name, self.project, diff))
+            return data
+        elif entity_type == "ReplicationController":
+            # todo[anin] check container for same bug as above
+            core_api = client.CoreV1Api(self._api_client)
+            source = self.get_entity("rc", entity_name)
+            diff = self.get_json_diff(source, data)
+            if diff:
+                return self.__to_dict(core_api.patch_namespaced_replication_controller(entity_name, self.project, diff))
+            return data
+        elif entity_type == "StatefulSet":
+            # todo[anin] check container for same bug as above
+            apps_api = client.AppsV1Api(self._api_client)
+            source = reset_last_applied(self.get_entity("statefulset", entity_name))
+            # return self.__to_dict(apps_api.patch_namespaced_stateful_set(entity_name, self.project, data))
+            # TODO: It's not working at all, we are just returning same without patching, just rolled back coz of release
+            return data
+        elif entity_type == "Deployment":
+            try:
+                apps_api = client.AppsV1beta1ApiApi(self._api_client)
+                source = reset_last_applied(self.get_entity("deployment", entity_name))
+            except:
+                apps_api = client.AppsV1Api(self._api_client)
+                source = reset_last_applied(self.get_entity("deployment", entity_name))
+
+            diff = self.get_json_diff(source, data)
+            if diff:
+                return self.__to_dict(apps_api.patch_namespaced_deployment(entity_name, self.project, diff))
+            return data
+        else:
+            raise NotImplementedError("Cannot apply {}".format(entity_type))
+
+    def replace_object(self, data):
+        log.debug("Try to apply {}".format(data))
+        entity_type = data["kind"]
+        entity_name = data["metadata"]["name"]
+        if entity_type == "Pod":
+            core_api = client.CoreV1Api(self._api_client)
+            return self.__to_dict(core_api.replace_namespaced_pod(entity_name, self.project, data))
+        elif entity_type == "ConfigMap":
+            core_api = client.CoreV1Api(self._api_client)
+            return self.__to_dict(core_api.replace_namespaced_config_map(entity_name, self.project, data))
+        elif entity_type == "DeploymentConfig":
+            core_api = client.CoreV1Api(self._api_client)
+            return self.__to_dict(core_api.replace_namespaced_deployment_config(entity_name, self.project, data))
+        elif entity_type == "ReplicationController":
+            core_api = client.CoreV1Api(self._api_client)
+            return self.__to_dict(core_api.replace_namespaced_replication_controller(entity_name, self.project, data))
+        elif entity_type == "StatefulSet":
+            core_api = client.AppsV1Api(self._api_client)
+            return self.__to_dict(core_api.replace_namespaced_stateful_set(entity_name, self.project, data))
+        else:
+            raise NotImplementedError("Cannot replace {}".format(entity_type))
+
+    def get_cluster_pods(self, cluster_name, running=True):
+        pods = [x for x in self.__list_entities("pod") if "pgcluster" in x.metadata.labels and x.metadata.labels["pgcluster"] == cluster_name]
+        pods = list([x.metadata.name for x in pods])
+        return pods
+
+    def get_cluster_pods_desc(self, cluster_name, running=True):
+        pods = [x for x in self.__list_entities("pod") if "pgcluster" in x.metadata.labels and x.metadata.labels["pgcluster"] == cluster_name]
+        pods = list([self.__to_dict(x) for x in pods])
+        return pods
+
+    def get_pods_by_label(self, label_selector):
+        core_api = client.CoreV1Api(self._api_client)
+        items = core_api.list_namespaced_pod(self.project, label_selector=label_selector).items
+        pods = list([x.metadata.name for x in items])
+        return pods
+
+    def get_secret_data(self, secret_name):
+        core_api = client.CoreV1Api()
+        try:
+            api_response = core_api.read_namespaced_secret(secret_name, self.project)
+            import base64
+            data = api_response.data
+            password = base64.b64decode(data.get("password")).decode('utf-8')
+            user_data = data.get("user")
+            if not user_data:
+                user_data = data.get("username")
+            user = base64.b64decode(user_data).decode('utf-8')
+            return user, password
+        except ApiException as exc:
+            log.error(exc)
+            raise exc
+
+
+def get_client(oc_path="oc", oc_config_file="./oc_config_file.yaml"):
+    """
+    Returns wrapper over shell if oc client present
+    otherwise tries to return wrapper over kubernetes.client
+    :return:
+    :rtype: OpenshiftClient
+    """
+    if use_kube_client:
+        return OpenshiftPyClient()
+    else:
+        return OpenshiftShellClient(oc_path, oc_config_file)
+
+
+def reset_last_applied(entity):
+    entity["metadata"].pop("namespace", None)
+    entity["metadata"].pop("selfLink", None)
+    entity["metadata"].pop("uid", None)
+    entity["metadata"].pop("resourceVersion", None)
+    entity["metadata"].pop("generation", None)
+    entity["metadata"].pop("creationTimestamp", None)
+    entity["metadata"].pop("managedFields", None)
+    entity.pop("status", None)
+    return entity
+
+
+class OpenshiftOrchestrator:
+    def __init__(self, client, retry_count=100):
+        self.oc = client
+        self.retry_count = retry_count
+
+    def ensure_scale(self, dc_name, replicas, type="dc"):
+        log.info("Try to scale dc {} to {} replicas.".format(dc_name, replicas))
+        for i in range(1, self.retry_count):
+            self.oc.scale(dc_name, replicas, type)
+            time.sleep(1)
+            if replicas == self.oc.get_deployment_replicas_count(dc_name, type):
+                log.debug("dc {} was scaled successfully.".format(dc_name))
+                return
+        raise Exception("Was not able to scale deployment {}".format(dc_name))
+
+    def wait_replicas(self, dc_name, replicas, running=False):
+        log.info("Wait {} replicas of dc {}".format(replicas, dc_name))
+        replica_names = None
+        for i in range(1, self.retry_count):
+            replica_names = self.oc.get_replicas_names(dc_name, running=running)
+            log.info("Wait {} replicas of dc {}. Actual replicas: {}".format(replicas, dc_name, replica_names))
+            log.debug("Wait {} replicas of dc {}. Actual replicas: {}".format(replicas, dc_name, replica_names))
+            if len(replica_names) == replicas:
+                log.debug("Found {} replicas of dc {}.".format(replicas, dc_name))
+                return
+            time.sleep(1)
+        raise Exception("Expected replicas count was {} but actual replicas: {}".format(replicas, replica_names))
+
+    def wait_replicas_statefulset(self, stateful_set_name, replicas_number):
+        for i in range(1, self.retry_count):
+            log.info("Waiting till all replicas are ready")
+            time.sleep(1)
+            ready_replicas = self.oc.get_running_stateful_set_replicas_count(stateful_set_name)
+            if replicas_number == ready_replicas:
+                log.debug("Statefulset {} was scaled successfully.".format(stateful_set_name))
+                return
+        raise Exception("Was not able to scale statefulset {}".format(stateful_set_name))
+
+    def set_env_on_dc(self, dc_name, env_name, env_value, scale_up=True):
+        log.info("Try to set env {}={} to deployment {}".format(env_name, env_value, dc_name))
+        log.info("Scale down before changes")
+        self.ensure_scale(dc_name, 0)
+        replica_names = self.oc.get_replicas_names(dc_name, running=False)
+        for replica in replica_names:
+            self.oc.delete_pod(replica, 1)
+        self.wait_replicas(dc_name, 0)
+
+        log.info("Change env")
+        self.oc.set_env_for_dc(dc_name, env_name, env_value)
+
+        log.debug("Check if env present on actual version of dc")
+        for i in range(1, self.retry_count):
+            dc = self.oc.get_deployment(dc_name)
+            envs = dc["spec"]["template"]["spec"]["containers"][0]["env"]
+            env = list([x for x in envs if x["name"] == env_name])
+            version = dc["metadata"].get("resourceVersion") if dc["metadata"].get("resourceVersion") else dc["metadata"].get("resource_version")
+            log.debug("Env: {}. Version: {}".format(env, version))
+            if env_value:
+                if env and env[0]["value"] == env_value and version != "Unknown":
+                    break
+            else:
+                if not env and version != "Unknown":
+                    break
+            log.debug("Wait for changes to apply")
+            time.sleep(1)
+
+        if scale_up:
+            log.info("Scale up after changes")
+            self.ensure_scale(dc_name, 1)
+            self.wait_replicas(dc_name, 1, running=True)
+
+    def replace_command_on_deployment(self, deployment_name, command, scale_down=False):
+        log.info("Try to set command {} to deployment {}".format(command, deployment_name))
+        deployment_entity = self.oc.get_deployment(deployment_name, "deployment")
+        deployment_entity["spec"]["template"]["spec"]["containers"][0]["command"] = command
+        old_generation = deployment_entity["status"]["observed_generation"]
+        log.info("Observed generation before update: {}".format(old_generation))
+        if scale_down:
+            deployment_entity["spec"]["replicas"] = 0
+        self.oc.apply_object(deployment_entity)
+        for _ in range(1, self.retry_count):
+            updated_deployment = self.oc.get_deployment(deployment_name, "deployment")
+            new_generation = updated_deployment["status"].get("observed_generation")
+            ready_replicas = updated_deployment["status"].get("ready_replicas")
+            if new_generation == (old_generation + 1) and ready_replicas:
+                break
+            else:
+                time.sleep(1)
+
+
+    def replace_command_on_dc(self, dc_name, command, scale_up=True):
+        log.info("Try to set command {} to deployment config {}".format(command, dc_name))
+
+        log.info("Scale down before changes")
+        self.ensure_scale(dc_name, 0)
+        replica_names = self.oc.get_replicas_names(dc_name, running=False)
+        for replica in replica_names:
+            self.oc.delete_pod(replica, 1)
+        self.wait_replicas(dc_name, 0)
+
+        log.info("Change command")
+        dc = self.oc.get_deployment(dc_name)
+        dc["spec"]["template"]["spec"]["containers"][0]["command"] = command
+
+        last_applied = dc["metadata"]["annotations"]["kubectl.kubernetes.io/last-applied-configuration"]
+        if last_applied:
+            dc = reset_last_applied(dc)
+            log.debug(json.dumps(dc))
+            self.oc.apply_object(dc)
+        else:
+            log.debug(json.dumps(dc))
+            self.oc.replace_object(dc)
+
+        log.debug("Check if command present on actual version of dc")
+        for i in range(1, self.retry_count):
+            dc = self.oc.get_deployment(dc_name)
+            container_def = dc["spec"]["template"]["spec"]["containers"][0]
+            current_command = None
+            if "command" in container_def:
+                current_command = container_def["command"]
+            version = dc["metadata"].get("resourceVersion") if dc["metadata"].get("resourceVersion") else dc["metadata"].get("resource_version")
+            log.debug("Command: {}. Version: {}".format(current_command, version))
+            if current_command == command and version != "Unknown":
+                break
+            log.debug("Wait for changes to apply")
+            time.sleep(1)
+
+        if scale_up:
+            log.info("Scale up after changes")
+            self.ensure_scale(dc_name, 1)
+            self.wait_replicas(dc_name, 1, running=True)
+
+    def replace_command_on_statefulset(self, stateful_set_name, command, scale_up=True):
+        log.info("Try to set command {} to statefulset {}".format(command, stateful_set_name))
+
+        stateful_set = self.oc.get_stateful_set(stateful_set_name)
+        replicas_number = self.oc.get_stateful_set_replicas_count(stateful_set_name)
+        log.info("Scale down before changes")
+        self.scale_stateful_set(stateful_set_name, 0)
+
+        log.info("Change command")
+
+        stateful_set["spec"]["template"]["spec"]["containers"][0]["command"] = command
+
+        stateful_set = reset_last_applied(stateful_set)
+        log.debug(json.dumps(stateful_set))
+        self.oc.apply_object(stateful_set)
+
+        time.sleep(5)
+        if scale_up:
+            log.info("Scale up after changes")
+            self.scale_stateful_set(stateful_set_name, replicas_number)
+
+    def wait_for_one_of_records_in_logs_since(self, pod_id, records, start_time,
+                                              wait_message=None, restart_timer_records=None):
+        """
+        Receives logs from specified pod and checks if logs contain one of records since start_time.
+        Process will wait for record until retry counter reaches self.retry_count or
+        logs will be filled with one of restart_timer_records.
+        Sleep interval is 5 seconds. Between intervals Method can inform user about process with wait_message
+        :param pod_id:
+        :param records:
+        :type records: list
+        :param start_time:
+        :param wait_message:
+        :param restart_timer_records:
+        :type restart_timer_records: list
+        :return: Nothing.
+        :raise: Exception if record was not found
+        """
+        log.debug("Wait for records {} in pod {} from {}.".format(records, pod_id, start_time))
+        counter = 0
+        sleep_time = 20
+        fetch_start_time = start_time
+        while counter < self.retry_count / 5:
+            fetch_end_time = time.time()
+            # add small overlap to ensure that we dont miss peace of logs
+            time_passed = int(fetch_end_time - fetch_start_time) + sleep_time
+            logs = self.oc.get_logs(pod_id, time_passed)
+            # check if logs contains expected record
+            for record in records:
+                record_count = logs.count(record)
+                if record_count > 0:
+                    log.debug("Found record '{}' in logs. Found {} records.".format(record, record_count))
+                    return
+            # check if logs contains new restart records.
+            if restart_timer_records:
+                for record in restart_timer_records:
+                    record_count = logs.count(record)
+                    if record_count > 0:
+                        if wait_message:
+                            log.info(wait_message)
+                        log.debug("Found record '{}' in logs. Will prolong wait time. ".format(record))
+                        counter = 0
+            time.sleep(sleep_time)
+            counter = counter + 1
+            fetch_start_time = fetch_end_time
+        raise Exception("Cannot find records in logs")
+
+    def wait_for_record_in_logs_since(self, pod_id, record, start_time, wait_message=None, restart_timer_record=None):
+        self.wait_for_one_of_records_in_logs_since(pod_id,
+                                                   [record],
+                                                   start_time,
+                                                   wait_message,
+                                                   [restart_timer_record] if restart_timer_record else None)
+
+    def scale_stateful_set(self, stateful_set_name, replicas_number):
+        log.info("Try to scale statefulset {} to {} replicas.".format(stateful_set_name, replicas_number))
+        self.oc.scale(stateful_set_name, replicas_number, entity="statefulset")
+        self.wait_replicas_statefulset(stateful_set_name, replicas_number)
+
+    def get_liveness_probe_from_stateful_set(self, stateful_set_name):
+        return self.oc.get_liveness_probe_from_stateful_set(stateful_set_name)
+
+    def return_liveness_readiness_probes_for_stateful_set(self, stateful_set_name, probe):
+        stateful_set = self.oc.get_stateful_set(stateful_set_name)
+        stateful_set["spec"]["template"]["spec"]["containers"][0]["readinessProbe"] = probe
+        stateful_set["spec"]["template"]["spec"]["containers"][0]["livenessProbe"] = probe
+        self.oc.apply_object(stateful_set)
\ No newline at end of file
diff --git a/docker-backup-daemon/maintenance/recovery/utils_pg.py b/docker-backup-daemon/maintenance/recovery/utils_pg.py
new file mode 100644
index 0000000..39cf8e1
--- /dev/null
+++ b/docker-backup-daemon/maintenance/recovery/utils_pg.py
@@ -0,0 +1,87 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import time
+import logging
+import sys
+from utils_common import RecoveryException
+
+log = logging.getLogger()
+
+
+class PostgresqlClient:
+
+    def __init__(self, oc_client, retry_count=100):
+        self.oc = oc_client
+        self.retry_count = retry_count
+
+    def execute_select_query(self, query):
+        import psycopg2
+        from psycopg2 import Error
+        import os
+        try:
+            connection = psycopg2.connect(user="postgres",
+                                          password=os.getenv('POSTGRES_PASSWORD'),
+                                          host=os.getenv('POSTGRES_HOST'),
+                                          port="5432",
+                                          database="postgres")
+
+            cursor = connection.cursor()
+            cursor.execute(query)
+            for p in cursor.fetchall():
+                return p[0]
+        except (Exception, Error) as error:
+            log.error("Postgres communication failed: {}".format(error))
+
+    def execute_local_query(self, pod_id, query):
+        return self.oc.oc_exec(pod_id, "psql -h localhost -p 5432 postgres -t -c \"{}\"".format(query)).strip()
+
+    def wait_db_response(self, pod_id, query, result):
+        log.info("Start waiting for response '{}' for query '{}' from DB on {}".format(result, query, pod_id))
+        wait_database_start_time = time.time()
+        query_result = None
+        select_counter = 0
+        for i in range(1, self.retry_count):
+            log.debug("{} try to check DB response.".format(i))
+            if query_result == result:
+                select_counter = select_counter + 1
+                if select_counter == 5:
+                    break
+            else:
+                select_counter = 0
+            time.sleep(1)
+            try:
+                query_result = self.execute_select_query(query)
+                log.debug("Response from DB: {}".format(query_result))
+            except Exception as e:
+                if 'current phase is Pending' in str(e) or \
+                   'Is the server running on host' in str(e) or \
+                   'server closed the connection' in str(e):
+                    log.debug("One of allowed error occurred during request.")
+                else:
+                    raise e
+        wait_database_time = time.time() - wait_database_start_time
+        if query_result == result and select_counter == 5:
+            log.info("SUCCESS: Received response {} in {} sec".format(query_result, wait_database_time))
+        else:
+            raise RecoveryException("FAILURE: Cannot get expected result '{}' for query '{}' "
+                                    "from DB on pod {} in {} sec. Check if database working properly."
+                                    .format(result, query, pod_id, wait_database_time))
+
+    def wait_pg_recovery_complete(self, pod_id):
+        self.wait_db_response(pod_id, "select pg_is_in_recovery()", False)
+
+    def wait_database(self, pod_id):
+        self.wait_db_response(pod_id, "select 1", 1)
+
diff --git a/docker-dbaas-adapter/.gitignore b/docker-dbaas-adapter/.gitignore
new file mode 100644
index 0000000..63e2c39
--- /dev/null
+++ b/docker-dbaas-adapter/.gitignore
@@ -0,0 +1,7 @@
+.idea/
+.history
+target/
+
+# Temporary Build Files
+build/_output
+build/_test
\ No newline at end of file
diff --git a/docker-dbaas-adapter/CODE-OF-CONDUCT.md b/docker-dbaas-adapter/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-dbaas-adapter/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-dbaas-adapter/CONTRIBUTING.md b/docker-dbaas-adapter/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-dbaas-adapter/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-dbaas-adapter/LICENSE b/docker-dbaas-adapter/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-dbaas-adapter/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-dbaas-adapter/Makefile b/docker-dbaas-adapter/Makefile
new file mode 100644
index 0000000..44c8cfe
--- /dev/null
+++ b/docker-dbaas-adapter/Makefile
@@ -0,0 +1,46 @@
+DOCKER_FILE := build/Dockerfile
+
+NAMESPACE := 
+
+ifndef TAG_ENV
+override TAG_ENV = local
+endif
+
+ifndef DOCKER_NAMES
+override DOCKER_NAMES = "ghcr.io/netcracker/pgskipper-dbaas-adapter:${TAG_ENV}"
+endif
+
+sandbox-build: deps docker-build
+
+all: sandbox-build docker-push
+
+local: fmt deps docker-build
+
+deps:
+	go mod tidy
+	GO111MODULE=on
+
+fmt:
+	gofmt -l -s -w .
+
+compile:
+	CGO_ENABLED=0 go build -o ./build/_output/bin/postgresql-dbaas \
+				-gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./adapter
+
+
+docker-build:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker build --file="${DOCKER_FILE}" --pull -t $(docker_tag) ./;)
+
+docker-push:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker push $(docker_tag);)
+
+clean:
+	rm -rf build/_output
+
+test:
+	git config --global url."https://${GH_ACCESS_TOKEN}@github.com/".insteadOf "https://github.com/"
+	go test -v ./...
+
+replace-image: local
+	$(foreach docker_tag,$(DOCKER_NAMES),kubectl patch deployment dbaas-postgres-adapter -n $(NAMESPACE) --type "json" -p '[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":'$(docker_tag)'},{"op":"replace","path":"/spec/template/spec/containers/0/imagePullPolicy","value":"Always"}, {"op":"replace","path":"/spec/replicas","value":0}]';)
+	$(foreach docker_tag,$(DOCKER_NAMES),kubectl patch deployment dbaas-postgres-adapter -n $(NAMESPACE) --type "json" -p '[{"op":"replace","path":"/spec/replicas","value":1}]';)
\ No newline at end of file
diff --git a/docker-dbaas-adapter/README.md b/docker-dbaas-adapter/README.md
new file mode 100644
index 0000000..b281e56
--- /dev/null
+++ b/docker-dbaas-adapter/README.md
@@ -0,0 +1,8 @@
+# pgskipper-dbaas-adapter
+
+Component provides REST API for logical database management.
+
+## Repository structure
+
+* `./adapter` - directory with dbaas adapter source code.
+* `./build` - directory with dbaas adapter build files.
diff --git a/docker-dbaas-adapter/SECURITY.md b/docker-dbaas-adapter/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-dbaas-adapter/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-dbaas-adapter/adapter/backup/backup.go b/docker-dbaas-adapter/adapter/backup/backup.go
new file mode 100644
index 0000000..87db09c
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/backup/backup.go
@@ -0,0 +1,498 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backup
+
+import (
+	"context"
+	"crypto/tls"
+	"crypto/x509"
+	"encoding/base64"
+	"encoding/json"
+	"fmt"
+	"net/http"
+	"os"
+	"strings"
+	"sync"
+	"time"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/service"
+	coreUtils "github.com/Netcracker/qubership-dbaas-adapter-core/pkg/utils"
+	uuid "github.com/google/uuid"
+	"github.com/valyala/fasthttp"
+	"go.uber.org/zap"
+)
+
+const (
+	Post = "POST"
+	Get  = "GET"
+)
+
+var generatorMutex = sync.Mutex{}
+
+func newPostgresAdapterBackupActionTrack(task *PostgresBackupStatus) dao.DatabaseAdapterBaseTrack {
+	var details *dao.DatabasesBackupAdapt = nil
+
+	if task.Status == "Successful" {
+		details = &dao.DatabasesBackupAdapt{
+			LocalId: task.BackupId,
+		}
+	}
+
+	return dao.DatabaseAdapterBaseTrack{
+		Status:  mapStatus(task.Status),
+		TrackId: task.BackupId,
+		Action:  "BACKUP",
+		Details: details,
+	}
+}
+
+func newPostgresAdapterRestoreActionTrack(task *PostgresRestoreStatus) dao.DatabaseAdapterRestoreTrack {
+	return dao.DatabaseAdapterRestoreTrack{
+		DatabaseAdapterBaseTrack: dao.DatabaseAdapterBaseTrack{
+			Status:  mapStatus(task.Status),
+			TrackId: task.TrackingId,
+			Action:  "RESTORE",
+		},
+	}
+}
+
+func newPostgresAdapterRestoreActionTrackWithRequest(task *PostgresRestoreResponse, req PostgresDaemonRestoreRequest) *dao.DatabaseAdapterRestoreTrack {
+	if req.DatabasesMapping != nil {
+		return &dao.DatabaseAdapterRestoreTrack{
+			DatabaseAdapterBaseTrack: dao.DatabaseAdapterBaseTrack{
+				Status:  "PROCEEDING",
+				TrackId: task.TrackingId,
+				Action:  "RESTORE",
+			},
+			ChangedNameDb: req.DatabasesMapping,
+		}
+	}
+
+	return &dao.DatabaseAdapterRestoreTrack{
+		DatabaseAdapterBaseTrack: dao.DatabaseAdapterBaseTrack{
+			Status:  "PROCEEDING",
+			TrackId: task.TrackingId,
+			Action:  "RESTORE",
+		},
+	}
+}
+
+func certFileExists() bool {
+	path := "/certs/tls.crt"
+	_, err := os.Stat(path)
+	return !os.IsNotExist(err)
+}
+
+func NewServiceAdapter(clusterAdapter cluster.ClusterAdapter, backupAddress string, keep string, auth bool, user string, password string, pgSSl string) *BackupAdapter {
+	httpClient := &fasthttp.Client{}
+	if pgSSl == "on" && certFileExists() {
+		httpClient.TLSConfig = setTLSConfig()
+	}
+
+	// Client for default backup service
+	httpClientForDefault := &http.Client{}
+	if pgSSl == "on" && certFileExists() {
+		httpClientForDefault.Transport = &http.Transport{
+			TLSClientConfig: setTLSConfig(),
+		}
+	}
+
+	client := &BackupAdapter{
+		ClusterAdapter:       clusterAdapter,
+		DaemonAddress:        backupAddress,
+		Keep:                 keep,
+		Auth:                 auth,
+		User:                 user,
+		Password:             password,
+		PgSSl:                pgSSl,
+		Client:               httpClient,
+		log:                  util.GetLogger(),
+		DefaultBackupService: service.DefaultBackupAdministrationService(util.GetLogger(), backupAddress, user, password, false, httpClientForDefault, util.GetPgDBLength(), []string{}),
+	}
+	return client
+}
+
+func setTLSConfig() *tls.Config {
+	cert, err := tls.LoadX509KeyPair("/certs/tls.crt", "/certs/tls.key")
+	if err != nil {
+		panic(fmt.Sprintf("Error during load a key pair %v", zap.Error(err)))
+	}
+
+	// Load CA cert
+	caCert, err := os.ReadFile("/certs/ca.crt")
+	if err != nil {
+		panic(fmt.Sprintf("Error during load a key pair %v", zap.Error(err)))
+	}
+	caCertPool := x509.NewCertPool()
+	caCertPool.AppendCertsFromPEM(caCert)
+
+	// Setup HTTPS client
+	tlsConfig := &tls.Config{
+		Certificates: []tls.Certificate{cert},
+		RootCAs:      caCertPool,
+	}
+	return tlsConfig
+}
+func (ba BackupAdapter) sendRequest(ctx context.Context, method string, path string, body interface{}) (*BackupDaemonResponse, error) {
+	logger := util.ContextLogger(ctx)
+	url := ba.getUrl(path)
+	req := fasthttp.AcquireRequest()
+	defer fasthttp.ReleaseRequest(req)
+	res := fasthttp.AcquireResponse()
+	defer fasthttp.ReleaseResponse(res)
+
+	req.Header.SetMethod(method)
+	req.SetRequestURI(url)
+
+	if ba.Auth {
+		auth := ba.User + ":" + ba.Password
+		authHeaderValue := base64.StdEncoding.EncodeToString([]byte(auth))
+		req.Header.Set("Authorization", authHeaderValue)
+	}
+
+	if body != nil {
+		var requestBody []byte
+		req.Header.Set("Accept", "application/json")
+		req.Header.SetContentType("application/json")
+		requestBody, err := json.Marshal(body)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during marshal body: %+v", body), zap.Error(err))
+			return nil, err
+		}
+		req.SetBody(requestBody)
+	}
+
+	if err := ba.Client.Do(req, res); err != nil {
+		logger.Error(fmt.Sprintf("Error during send %s request %s", method, ba.getUrl("/backup/request")), zap.Error(err))
+		return nil, err
+	}
+
+	responseBody := res.Body()
+	responseBodyCopy := make([]byte, len(responseBody))
+	copy(responseBodyCopy, responseBody)
+
+	return &BackupDaemonResponse{Status: res.StatusCode(), Body: responseBodyCopy}, nil
+}
+
+func (ba BackupAdapter) CollectBackup(ctx context.Context, databases []string, keepFromRequest string, allowEviction bool) dao.DatabaseAdapterBaseTrack {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Send request to collect backup to %s", ba.DaemonAddress))
+	keep := "forever"
+
+	if allowEviction {
+		if keepFromRequest != "" {
+			keep = keepFromRequest
+		} else {
+			keep = ba.Keep
+		}
+	}
+
+	requestBody := PostgresDaemonBackupRequest{
+		Databases: databases,
+		Keep:      keep,
+	}
+
+	response, err := ba.sendRequest(ctx, Post, "/backup/request", requestBody)
+	if err != nil {
+		panic(err)
+	}
+
+	var postgresDaemonBackup PostgresDaemonBackup
+	err = json.Unmarshal(response.Body, &postgresDaemonBackup)
+	if err != nil {
+		logger.Error("Error during unmarshal response from /backup/request", zap.Error(err))
+		panic(err)
+	}
+
+	trackBackup, _ := ba.TrackBackup(ctx, postgresDaemonBackup.BackupId)
+
+	return trackBackup
+}
+
+func (ba BackupAdapter) TrackRestore(ctx context.Context, trackId string) (dao.DatabaseAdapterRestoreTrack, bool) {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Request status information for restore %s to daemon %s", trackId, ba.DaemonAddress))
+	status, found := ba.getRestoreStatus(ctx, trackId)
+	if !found {
+		return dao.DatabaseAdapterRestoreTrack{}, false
+	}
+
+	return newPostgresAdapterRestoreActionTrack(status), true
+}
+
+func (ba BackupAdapter) TrackBackup(ctx context.Context, trackId string) (dao.DatabaseAdapterBaseTrack, bool) {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Request status information for backup %s to daemon %s", trackId, ba.DaemonAddress))
+	status, found := ba.getBackupStatus(ctx, trackId)
+	if !found {
+		return dao.DatabaseAdapterBaseTrack{}, false
+	}
+
+	return newPostgresAdapterBackupActionTrack(status), true
+}
+
+func (ba BackupAdapter) getBackupStatus(ctx context.Context, trackId string) (*PostgresBackupStatus, bool) {
+	response, err := ba.sendRequest(ctx, Get, "/backup/status/"+trackId, nil)
+	if err != nil {
+		panic(err)
+	}
+	if response.Status == http.StatusNotFound {
+		return nil, false
+	}
+
+	var backupStatus PostgresBackupStatus
+	err = json.Unmarshal(response.Body, &backupStatus)
+	if err != nil {
+		util.ContextLogger(ctx).Error("Error during unmarshal response from /backup/status", zap.Error(err))
+		panic(err)
+	}
+
+	return &backupStatus, true
+}
+
+func (ba BackupAdapter) getRestoreStatus(ctx context.Context, trackId string) (*PostgresRestoreStatus, bool) {
+	logger := util.ContextLogger(ctx)
+	response, err := ba.sendRequest(ctx, Get, "/restore/status/"+trackId, nil)
+	if err != nil {
+		panic(err)
+	}
+	if response.Status == http.StatusNotFound {
+		return nil, false
+	}
+
+	var restoreStatus PostgresRestoreStatus
+	err = json.Unmarshal(response.Body, &restoreStatus)
+	if err != nil {
+		logger.Error("Error during unmarshal response from /restore/request", zap.Error(err))
+		panic(err)
+	}
+
+	return &restoreStatus, true
+}
+
+func (ba BackupAdapter) EvictBackup(ctx context.Context, backupId string) (string, bool) {
+	logger := util.ContextLogger(ctx)
+	response, err := ba.sendRequest(ctx, Post, "/delete/"+backupId, nil)
+	if err != nil {
+		panic(err)
+	}
+	if response.Status == http.StatusNotFound {
+		return "", false
+	}
+	var deleteResponse PostgresBackupDeleteResponse
+	err = json.Unmarshal(response.Body, &deleteResponse)
+	if err != nil {
+		logger.Error("Error during parse delete response", zap.Error(err))
+		panic(err)
+	}
+
+	var status = "Unknown"
+	if deleteResponse.Status != "" {
+		if deleteResponse.Status == "Successful" {
+			status = "SUCCESS"
+		} else {
+			status = "FAIL"
+		}
+	}
+	//todo
+	return status, true
+
+}
+
+func (ba BackupAdapter) RestoreBackup(ctx context.Context, backupId string, databases []dao.DbInfo, regenerateNames bool, oldNameFormat bool) (*dao.DatabaseAdapterRestoreTrack, error) {
+	logger := util.ContextLogger(ctx)
+	restoreRoles := "true"
+	dbaasClone := false
+	var databaseMapping map[string]string
+	if regenerateNames {
+		restoreRoles = "false"
+		dbaasClone = true
+		databaseMapping = make(map[string]string, len(databases))
+
+		for _, database := range databases {
+			newDbName, err := generateNewDBName(ctx, database, oldNameFormat)
+			if err != nil {
+				return nil, err
+			}
+			databaseMapping[database.Name] = newDbName
+		}
+	}
+
+	req := PostgresDaemonRestoreRequest{
+		BackupId:          backupId,
+		Databases:         getDbNames(databases),
+		DatabasesMapping:  databaseMapping,
+		Force:             true,
+		RestoreRoles:      restoreRoles,
+		SingleTransaction: "true",
+		DbaasClone:        dbaasClone,
+	}
+
+	response, err := ba.sendRequest(ctx, Post, "/restore/request", req)
+	if err != nil {
+		return nil, err
+	}
+
+	var postgresRestoreResponse PostgresRestoreResponse
+	err = json.Unmarshal(response.Body, &postgresRestoreResponse)
+	if err != nil {
+		logger.Error("Error during parse response from backupDaemon", zap.Error(err))
+		return nil, err
+	}
+
+	return newPostgresAdapterRestoreActionTrackWithRequest(&postgresRestoreResponse, req), nil
+}
+
+func generateNewDBName(ctx context.Context, database dao.DbInfo, oldNameFormat bool) (newDbName string, err error) {
+	generatorMutex.Lock()
+	defer generatorMutex.Unlock()
+	// perform sleep to avoid timestamp collision
+	time.Sleep(1 * time.Millisecond)
+
+	if oldNameFormat {
+		newDbName = generateDbNameWithUUID(ctx, database.Name)
+	} else if database.Prefix != nil {
+		newDbName = coreUtils.RegenerateDbName(*database.Prefix, util.GetPgDBLength())
+	} else {
+		newDbName, err = coreUtils.PrepareDatabaseName(database.Namespace, database.Microservice, util.GetPgDBLength())
+		if err != nil {
+			return newDbName, err
+		}
+	}
+	return newDbName, err
+}
+
+func getDbNames(dbInfo []dao.DbInfo) []string {
+	result := make([]string, 0, len(dbInfo))
+	for _, db := range dbInfo {
+		result = append(result, db.Name)
+	}
+	return result
+}
+
+func generateDbNameWithUUID(ctx context.Context, dbName string) string {
+	logger := util.ContextLogger(ctx)
+
+	uuidName := uuid.New()
+	newDbName := uuidName.String()
+	newDbName = strings.ReplaceAll(newDbName, "-", "")
+
+	maxLen := util.GetPgDBLength()
+	prefixFromDb := dbName
+	if len(dbName) > maxLen-len(newDbName)-1 {
+		prefixFromDb = dbName[:maxLen-1-len(dbName)]
+	}
+	newDbName = fmt.Sprintf("%s_%s", prefixFromDb, uuidName.String())
+
+	logger.Info(fmt.Sprintf("New name %s was generated for db with name %s", newDbName, dbName))
+	return strings.ToLower(newDbName)
+}
+
+func (ba BackupAdapter) getUrl(route string) string {
+	return ba.DaemonAddress + route
+}
+
+func mapStatus(daemonStatus string) dao.DatabaseAdapterBackupAdapterTrackStatus {
+	switch daemonStatus {
+	case "Planned":
+		return "PROCEEDING"
+	case "In progress":
+		return "PROCEEDING"
+	case "Failed":
+		return "FAIL"
+	case "Successful":
+		return "SUCCESS"
+	default:
+		return "FAIL"
+	}
+}
+
+func (ba *BackupAdapter) CollectBackupV2(ctx context.Context, storageName string, blobPath string, databaseNames []string) (*dao.BackupResponse, bool) {
+	response, found := ba.DefaultBackupService.CollectBackupV2(ctx, storageName, blobPath, databaseNames)
+	if found {
+		convertBackupResponseStatus(response)
+	}
+	return response, found
+}
+
+func (ba *BackupAdapter) RestoreBackupV2(ctx context.Context, backupId string, restoreRequest dao.CreateRestoreRequest, dryRun bool) (*dao.RestoreResponse, bool) {
+	response, found := ba.DefaultBackupService.RestoreBackupV2(ctx, backupId, restoreRequest, dryRun)
+	if found {
+		convertRestoreResponseStatus(response)
+	}
+	return response, found
+}
+
+func (ba *BackupAdapter) EvictBackupV2(ctx context.Context, backupId string, blobPath string) bool {
+	return ba.DefaultBackupService.EvictBackupV2(ctx, backupId, blobPath)
+}
+
+func (ba *BackupAdapter) EvictRestoreV2(ctx context.Context, restoreId string, blobPath string) bool {
+	return ba.DefaultBackupService.EvictRestoreV2(ctx, restoreId, blobPath)
+}
+
+func (ba *BackupAdapter) TrackBackupV2(ctx context.Context, backupId string, blobPath string) (*dao.BackupResponse, bool) {
+	response, found := ba.DefaultBackupService.TrackBackupV2(ctx, backupId, blobPath)
+	if found {
+		convertBackupResponseStatus(response)
+	}
+	return response, found
+}
+
+func (ba *BackupAdapter) TrackRestoreV2(ctx context.Context, restoreId string, blobPath string) (*dao.RestoreResponse, bool) {
+	response, found := ba.DefaultBackupService.TrackRestoreV2(ctx, restoreId, blobPath)
+	if found {
+		convertRestoreResponseStatus(response)
+	}
+	return response, found
+}
+
+func convertBackupResponseStatus(response *dao.BackupResponse) {
+	if response != nil {
+		response.Status = convertBackupResponse(response.Status)
+		for i, database := range response.Databases {
+			database.Status = convertBackupResponse(database.Status)
+			response.Databases[i] = database
+		}
+	}
+}
+
+func convertRestoreResponseStatus(response *dao.RestoreResponse) {
+	if response != nil {
+		response.Status = convertBackupResponse(response.Status)
+		for i, database := range response.Databases {
+			database.Status = convertBackupResponse(database.Status)
+			response.Databases[i] = database
+		}
+	}
+}
+
+func convertBackupResponse(status dao.BackupRestoreStatus) dao.BackupRestoreStatus {
+	switch status {
+	case "Successful":
+		return dao.CompletedStatus
+	case "Failed", "Canceled", "Unknown":
+		return dao.FailedStatus
+	case "In progress":
+		return dao.InProgressStatus
+	case "Planned":
+		return dao.NotStartedStatus
+	default:
+		return status
+	}
+}
diff --git a/docker-dbaas-adapter/adapter/backup/backup_test.go b/docker-dbaas-adapter/adapter/backup/backup_test.go
new file mode 100644
index 0000000..b8f134e
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/backup/backup_test.go
@@ -0,0 +1,415 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+//nolint
+
+package backup
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"net/http"
+	"net/url"
+	"strings"
+	"testing"
+	"time"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	coreUtils "github.com/Netcracker/qubership-dbaas-adapter-core/pkg/utils"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/mock"
+)
+
+var (
+	host     = "test_host"
+	user     = "test_user"
+	port     = 1234
+	password = "password"
+)
+
+type PostgresClusterAdapterMock struct {
+	mock.Mock
+	cluster.ClusterAdapter
+}
+
+func (ca *PostgresClusterAdapterMock) GetHost() string {
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetPort() int {
+	return ca.Called().Int(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetUser() string {
+	ca.Called()
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetPassword() string {
+	ca.Called()
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetConnection(ctx context.Context) (cluster.Conn, error) {
+	args := ca.Called()
+	return args.Get(0).(cluster.Conn), args.Error(1)
+}
+
+type MockConn struct {
+	mock.Mock
+	cluster.Conn
+}
+
+type Request interface {
+	URI() *url.URL
+}
+
+type Response interface {
+	Body() []byte
+}
+
+func startTestServer() {
+	// Handle requests on "/backup/request" and "/backup/status/{backupId}"
+	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
+		fmt.Println("string url")
+		fmt.Println(r.URL.Path)
+
+		// Check if the URL path starts with "/backup/request"
+		if strings.HasPrefix(r.URL.Path, "/backup/request") {
+
+			response := PostgresDaemonBackup{
+				BackupId: "20240105T0836",
+			}
+
+			jsonResponse, err := json.Marshal(response)
+			fmt.Println("string json respnse")
+			fmt.Println(string(jsonResponse))
+			if err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+
+			w.Header().Set("Content-Type", "application/json")
+			if _, err := w.Write(jsonResponse); err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+			return
+		}
+
+		// Check if the URL path starts with "/backup/status/"
+		statusPrefix := "/backup/status/"
+		if strings.HasPrefix(r.URL.Path, statusPrefix) {
+			backupId := strings.TrimPrefix(r.URL.Path, statusPrefix)
+
+			response := PostgresBackupStatus{
+				BackupId: backupId,
+				Status:   "Successful",
+				Databases: map[string]PostgresDatabaseBackupStatus{
+					"db1": {Status: "Successful"},
+					"db2": {Status: "In progress"},
+				},
+			}
+
+			jsonResponse, err := json.Marshal(response)
+			fmt.Println("string json respnse")
+			fmt.Println(string(jsonResponse))
+			if err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+
+			w.Header().Set("Content-Type", "application/json")
+			if _, err := w.Write(jsonResponse); err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+			return
+		}
+
+		//restore status
+		restoreStatusPrefix := "/restore/status/"
+		if strings.HasPrefix(r.URL.Path, restoreStatusPrefix) {
+			trackId := strings.TrimPrefix(r.URL.Path, restoreStatusPrefix)
+
+			response := PostgresRestoreStatus{
+				TrackingId: trackId,
+				Status:     "Successful",
+			}
+
+			jsonResponse, err := json.Marshal(response)
+			fmt.Println("string json respnse")
+			fmt.Println(string(jsonResponse))
+			if err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+
+			w.Header().Set("Content-Type", "application/json")
+			if _, err := w.Write(jsonResponse); err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+			return
+		}
+
+		//EVictBackup
+		EvictBackupPrefix := "/delete/20240105T0836"
+		if strings.HasPrefix(r.URL.Path, EvictBackupPrefix) {
+			trackId := strings.TrimPrefix(r.URL.Path, EvictBackupPrefix)
+
+			response := PostgresBackupDeleteResponse{
+				BackupId: trackId,
+				Status:   "Successful",
+				Message:  "Backup successfully evicted.",
+			}
+
+			jsonResponse, err := json.Marshal(response)
+			fmt.Println("string json respnse")
+			fmt.Println(string(jsonResponse))
+			if err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+
+			w.Header().Set("Content-Type", "application/json")
+			if _, err := w.Write(jsonResponse); err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+			return
+		}
+
+		//RestoreRequest
+		RestoreRequestPrefix := "/restore/request"
+		if strings.HasPrefix(r.URL.Path, RestoreRequestPrefix) {
+			trackId := "20240105T0836"
+
+			response := PostgresRestoreResponse{
+				TrackingId: trackId,
+			}
+
+			jsonResponse, err := json.Marshal(response)
+			fmt.Println("string json respnse")
+			fmt.Println(string(jsonResponse))
+			if err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+
+			w.Header().Set("Content-Type", "application/json")
+			if _, err := w.Write(jsonResponse); err != nil {
+				http.Error(w, err.Error(), http.StatusInternalServerError)
+				return
+			}
+			return
+		}
+
+		http.Error(w, "Invalid Endpoint", http.StatusNotFound)
+
+	})
+
+	go func() {
+		if err := http.ListenAndServe(":8080", nil); err != nil {
+			panic(err)
+		}
+	}()
+
+	time.Sleep(100 * time.Millisecond)
+
+}
+
+func init() {
+	startTestServer()
+}
+
+func TestCollectBackup(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetUser").Return(user)
+	ca.On("GetPassword").Return(password)
+
+	databases := []string{"db1", "db2"}
+
+	aggAddress := "http://localhost:8080"
+
+	backupAdapater := NewServiceAdapter(ca, aggAddress, "forever", false, user, password, "off")
+
+	keepFromRequest := "30 days"
+	allowEviction := true
+
+	trackBackup := backupAdapater.CollectBackup(context.Background(), databases, keepFromRequest, allowEviction)
+
+	assert.Equal(t, trackBackup.Action, dao.DatabaseAdapterAction("BACKUP"))
+	assert.Equal(t, trackBackup.TrackId, "20240105T0836")
+	assert.Equal(t, trackBackup.Status, dao.DatabaseAdapterBackupAdapterTrackStatus("SUCCESS"))
+
+}
+
+func TestTrackBackupTest(t *testing.T) {
+
+	aggAddress := "http://localhost:8080"
+
+	statusRequestBody := PostgresBackupStatus{
+		BackupId: "20240105T0836",
+	}
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	trackStatus, found := backupAdapater.TrackBackup(context.Background(), "20240105T0836")
+	assert.Equal(t, found, true)
+	backupStatus, found := backupAdapater.getBackupStatus(context.Background(), "20240105T0836")
+	assert.Equal(t, found, true)
+	BackupActionTrack := newPostgresAdapterBackupActionTrack(backupStatus)
+
+	fmt.Printf("Backup new statysStatus: %v\n", trackStatus)
+
+	assert.Equal(t, BackupActionTrack.TrackId, statusRequestBody.BackupId)
+	assert.Equal(t, BackupActionTrack.Status, dao.DatabaseAdapterBackupAdapterTrackStatus("SUCCESS"))
+	assert.Equal(t, BackupActionTrack.Action, dao.DatabaseAdapterAction("BACKUP"))
+
+}
+
+func TestTrackRestoreStatus(t *testing.T) {
+
+	aggAddress := "http://localhost:8080"
+
+	statusRequestBody := PostgresRestoreStatus{
+		TrackingId: "20240105T0836",
+	}
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	backupStatus, found := backupAdapater.TrackRestore(context.Background(), "20240105T0836")
+	assert.Equal(t, found, true)
+	backupRestoreStatus, found := backupAdapater.getRestoreStatus(context.Background(), "20240105T0836")
+	assert.Equal(t, found, true)
+	fmt.Printf("Backup restire Status small: %v\n", backupRestoreStatus)
+	RestoreActionTrack := newPostgresAdapterRestoreActionTrack(backupRestoreStatus)
+	fmt.Printf("Backup restire Status: %v\n", backupStatus)
+
+	assert.Equal(t, RestoreActionTrack.TrackId, statusRequestBody.TrackingId)
+	assert.Equal(t, RestoreActionTrack.Status, dao.DatabaseAdapterBackupAdapterTrackStatus("SUCCESS"))
+	assert.Equal(t, RestoreActionTrack.Action, dao.DatabaseAdapterAction("RESTORE"))
+
+}
+
+func TestEvictBackupStatus(t *testing.T) {
+
+	aggAddress := "http://localhost:8080"
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	backupStatus, isFound := backupAdapater.EvictBackup(context.Background(), "20240105T0836")
+
+	fmt.Printf("Backup EVICT NewStatus: %v\n", backupStatus)
+	assert.Equal(t, backupStatus, "SUCCESS")
+	assert.Equal(t, true, isFound)
+
+}
+
+func TestEvictBackupIdNotFound(t *testing.T) {
+
+	aggAddress := "http://localhost:8080"
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	backupStatus, isFound := backupAdapater.EvictBackup(context.Background(), "20240105T0833")
+
+	fmt.Printf("Backup EVICT NewStatus: %v\n", backupStatus)
+	assert.Equal(t, backupStatus, "")
+	assert.Equal(t, false, isFound)
+
+}
+
+func TestRestoreRequestStatus(t *testing.T) { // TODO: Check test logic
+
+	databaseNames := []string{"db1", "db2"}
+	databases := []dao.DbInfo{}
+	var regenerateNames = true
+
+	var databaseMapping map[string]string
+	if regenerateNames {
+		databaseMapping = make(map[string]string, len(databaseNames))
+
+		for _, database := range databaseNames {
+			databaseMapping[database] = coreUtils.RegenerateDbName("test", util.GetPgDBLength())
+			databases = append(databases, dao.DbInfo{Name: database, Namespace: "testNamespace", Microservice: "testMicroservice"})
+		}
+	}
+
+	req := PostgresDaemonRestoreRequest{
+		BackupId:          "20240105T0836",
+		Databases:         databaseNames,
+		DatabasesMapping:  databaseMapping,
+		Force:             true,
+		RestoreRoles:      "true",
+		SingleTransaction: "true",
+	}
+
+	aggAddress := "http://localhost:8080"
+
+	statusResponseBody := PostgresRestoreResponse{
+		TrackingId: "20240105T0836",
+	}
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	backupStatus, err := backupAdapater.RestoreBackup(context.Background(), "20240105T0836", databases, regenerateNames, false)
+	if err != nil {
+		panic(err)
+	}
+
+	fmt.Printf("Status response body: %v\n", statusResponseBody)
+	RestoreActionWithRequest := newPostgresAdapterRestoreActionTrackWithRequest(&statusResponseBody, req)
+	fmt.Printf("Backup Restore request NEwStatus: %v\n", backupStatus)
+
+	assert.Equal(t, RestoreActionWithRequest.Status, dao.DatabaseAdapterBackupAdapterTrackStatus("PROCEEDING"))
+	assert.Equal(t, RestoreActionWithRequest.Action, dao.DatabaseAdapterAction("RESTORE"))
+	assert.NotNil(t, RestoreActionWithRequest.ChangedNameDb)
+	assert.NotNil(t, RestoreActionWithRequest.TrackId)
+
+}
+
+func TestInvalidEndpointStatus(t *testing.T) {
+
+	aggAddress := "http://localhost:8080"
+
+	statusRequestBody := PostgresBackupDeleteResponse{
+		BackupId: "20240105T0836",
+		Status:   "Successful",
+		Message:  "Backup successfully evicted.",
+	}
+
+	backupAdapater := NewServiceAdapter(nil, aggAddress, "", false, "", "", "")
+
+	ResponseStatus, err := backupAdapater.sendRequest(context.Background(), "POST", "/postgres/", statusRequestBody)
+	assert.Nil(t, err)
+
+	str := asciiToStr(ResponseStatus.Body)
+	assert.Equal(t, str, "Invalid Endpoint\n")
+
+}
+
+func asciiToStr(asciiBytes []byte) string {
+	return string(asciiBytes)
+}
diff --git a/docker-dbaas-adapter/adapter/backup/type.go b/docker-dbaas-adapter/adapter/backup/type.go
new file mode 100644
index 0000000..16ac4e1
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/backup/type.go
@@ -0,0 +1,85 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backup
+
+import (
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/service"
+	"github.com/valyala/fasthttp"
+
+	"go.uber.org/zap"
+)
+
+type BackupAdapter struct {
+	ClusterAdapter       cluster.ClusterAdapter
+	DaemonAddress        string
+	Keep                 string
+	Auth                 bool
+	User                 string
+	Password             string
+	PgSSl                string
+	Client               *fasthttp.Client
+	log                  *zap.Logger
+	DefaultBackupService service.BackupAdministrationService
+}
+
+type PostgresDaemonBackupRequest struct {
+	Databases []string `json:"databases"`
+	Keep      string   `json:"keep"`
+}
+
+type PostgresDaemonBackup struct {
+	BackupId string `json:"backupId"`
+}
+
+type PostgresBackupStatus struct {
+	BackupId  string                                  `json:"backupId"`
+	Status    string                                  `json:"status"`
+	Databases map[string]PostgresDatabaseBackupStatus `json:"databases"`
+}
+
+type PostgresDatabaseBackupStatus struct {
+	Status string `json:"status"`
+}
+
+type PostgresDaemonRestoreRequest struct {
+	BackupId          string            `json:"backupId"`
+	Databases         []string          `json:"databases"`
+	DatabasesMapping  map[string]string `json:"databasesMapping"`
+	Force             bool              `json:"force"`
+	RestoreRoles      string            `json:"restoreRoles"`
+	SingleTransaction string            `json:"singleTransaction"`
+	DbaasClone        bool              `json:"dbaasClone"`
+}
+
+type PostgresRestoreResponse struct {
+	TrackingId string `json:"trackingId"`
+}
+
+type PostgresRestoreStatus struct {
+	TrackingId string `json:"trackingId"`
+	Status     string `json:"status"`
+}
+
+type PostgresBackupDeleteResponse struct {
+	BackupId string `json:"backupId"`
+	Status   string `json:"status"`
+	Message  string `json:"message"`
+}
+
+type BackupDaemonResponse struct {
+	Status int
+	Body   []byte
+}
diff --git a/docker-dbaas-adapter/adapter/basic/basic.go b/docker-dbaas-adapter/adapter/basic/basic.go
new file mode 100644
index 0000000..893f436
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/basic.go
@@ -0,0 +1,1614 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"os"
+	"regexp"
+	"strings"
+	"sync"
+
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+
+	uuid "github.com/google/uuid"
+	"go.uber.org/zap"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	coreUtils "github.com/Netcracker/qubership-dbaas-adapter-core/pkg/utils"
+)
+
+const (
+	DbKind             = "database"
+	UserKind           = "user"
+	DeletedStatus      = "DELETED"
+	DeleteFailedStatus = "DELETE_FAILED"
+	BadRequest         = "BAD_REQUEST"
+	Failed             = "FAILED"
+	Skipped            = "SKIPPED"
+	Successful         = "SUCCESSFUL"
+
+	ExtensionPath          = "/app/extensions/"
+	ExtensionConfigNameNew = "dbaas-postgres-adapter.extensions-config"
+	ExtensionConfigNameOld = "nc-dbaas-postgres-adapter.extensions-config"
+	ExtensionName          = "dbaas.default_extensions.json"
+
+	UpdateRequiredKey     = "updateRequired"
+	ExtensionsKey         = "extensions"
+	Metadata              = "metadata"
+	RolesVersionKey       = "rolesVersion"
+	RolesKey              = "roles"
+	DefaultPrefix         = "dbaas"
+	CreationParametersKey = "dbCreationParameters"
+	PGExtensionsKey       = "pgExtensions"
+)
+
+var (
+	log           = util.GetLogger()
+	defaultSchema = "public"
+	DbaasMetadata = "_DBAAS_METADATA"
+)
+
+type PgSettings struct {
+	Schemas            []string
+	CreationParameters map[string]string
+	Extensions         []string
+	NotSupported       []string
+}
+
+type Generator interface {
+	Generate() string
+}
+
+type UUIDGenerator struct{}
+
+func NewServiceAdapter(clusterAdapter cluster.ClusterAdapter, version dao.ApiVersion, roles []string, features map[string]bool) *ServiceAdapter {
+
+	extensionConfig := readDefaultExtFile()
+
+	log.Debug(fmt.Sprintf("Extension config: %v", *extensionConfig))
+
+	return &ServiceAdapter{
+		ClusterAdapter:  clusterAdapter,
+		Mutex:           &sync.Mutex{},
+		ExtensionConfig: extensionConfig,
+		ApiVersion:      version,
+		roles:           roles,
+		features:        features,
+		Generator:       UUIDGenerator{},
+		log:             util.GetLogger(),
+	}
+}
+
+func (sa ServiceAdapter) GetVersion() dao.ApiVersion {
+	return sa.ApiVersion
+}
+
+func (sa ServiceAdapter) validateSettings(ctx context.Context, settings map[string]interface{}) error {
+	if settings != nil && settings["pgExtensions"] != nil {
+		extensions, err := sa.getPostgresAvailableExtensions(ctx)
+		if err != nil {
+			return err
+		}
+
+		for _, requestExtension := range util.GetStringArrayFromInterface(settings["pgExtensions"]) {
+			if !util.Contains(extensions, requestExtension) {
+				return fmt.Errorf("request contains not valid Postgres extensions. %s", requestExtension)
+			}
+		}
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) getPostgresAvailableExtensions(ctx context.Context) ([]string, error) {
+	logger := util.ContextLogger(ctx)
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		return nil, err
+	}
+
+	defer conn.Close()
+
+	rows, _ := conn.Query(ctx, selectPostgresAvailableExtensions)
+
+	var extensions []string
+	for rows.Next() {
+		var extension string
+		err = rows.Scan(&extension)
+		if err != nil {
+			logger.Error("Error occurred during obtain extension rows", zap.Error(err))
+			return nil, err
+		}
+
+		extensions = append(extensions, extension)
+	}
+	return extensions, nil
+}
+
+func (sa ServiceAdapter) dropDatabase(ctx context.Context, dbName string) {
+	logger := util.ContextLogger(ctx)
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		return
+	}
+
+	defer conn.Close()
+
+	_, err = conn.Exec(ctx, dropDatabase(dbName))
+	if err != nil {
+		logger.Error("Failed to clear up db resource", zap.Error(err))
+	}
+}
+
+func (sa ServiceAdapter) dropUsers(ctx context.Context, users map[string]string) {
+	for user := range users {
+		sa.dropUser(ctx, user)
+	}
+}
+
+func (sa ServiceAdapter) dropUser(ctx context.Context, username string) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Perform drop for user %s", username))
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		return
+	}
+
+	defer conn.Close()
+
+	_, err = conn.Exec(ctx, dropUser(username))
+
+	if err != nil {
+		logger.Error("Failed to clear up user resource", zap.Error(err))
+	}
+}
+
+func (sa ServiceAdapter) GetFeatures() map[string]bool {
+	return sa.features
+}
+
+func (sa ServiceAdapter) CreateDatabase(ctx context.Context, requestOnCreateDb dao.DbCreateRequest) (string, *dao.LogicalDatabaseDescribed, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Info("Start database creation")
+
+	var dbName string
+
+	metadata := requestOnCreateDb.Metadata
+
+	// if client provided its own dbName, userName or password then use them, otherwise - generate random ones
+	if requestOnCreateDb.DbName != "" {
+		dbName = requestOnCreateDb.DbName
+	} else if requestOnCreateDb.NamePrefix != nil {
+		if !validateDbIdentifierParam(ctx, "namePrefix", *requestOnCreateDb.NamePrefix, DbPrefixPattern) || *requestOnCreateDb.NamePrefix == "" {
+			return "", nil, fmt.Errorf("namePrefix must comply to the pattern %s", DbPrefixPattern)
+		}
+		dbName = coreUtils.RegenerateDbName(*requestOnCreateDb.NamePrefix, util.GetPgDBLength())
+	} else {
+		namespace, msName, err := coreUtils.GetNsAndMsName(metadata)
+		if err != nil {
+			return "", nil, err
+		}
+		dbGeneratedName, err := coreUtils.PrepareDatabaseName(namespace, msName, util.GetPgDBLength())
+		if err != nil {
+			logger.Error("error during database name preparation", zap.Error(err))
+			panic(err)
+		}
+		dbName = dbGeneratedName
+	}
+
+	if !validateDbIdentifierParam(ctx, "dbName", dbName, DbIdentifiersPattern) {
+		return "", nil, fmt.Errorf("dbName must comply to the pattern %s", DbIdentifiersPattern)
+	}
+
+	if !validateDbIdentifierParam(ctx, "username", requestOnCreateDb.Username, DbIdentifiersPattern) {
+		return "", nil, fmt.Errorf("userName must comply to the pattern %s", DbIdentifiersPattern)
+	}
+
+	err := sa.validateSettings(ctx, requestOnCreateDb.Settings)
+	if err != nil {
+		logger.Error("Error occurred during validate extensions", zap.Error(err))
+		return "", nil, err
+	}
+
+	logger.Debug(fmt.Sprintf("Database name: %s", dbName))
+
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close()
+
+	pgSettings, err := sa.getPgSettings(ctx, requestOnCreateDb.Settings)
+	if err != nil {
+		return "", nil, err
+	}
+	logger.Debug(fmt.Sprintf("Result settings: %s", pgSettings))
+
+	_, err = conn.Exec(ctx, createDatabase(dbName, pgSettings.CreationParameters))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't create DB %s", dbName), zap.Error(err))
+		panic(err)
+	}
+
+	users := make(map[string]string)
+
+	var username string
+	var password string
+	var adminUser string
+	var adminPassword string
+
+	connectionProps := make([]dao.ConnectionProperties, 0)
+	resources := []dao.DbResource{{Kind: DbKind, Name: dbName}}
+
+	if sa.GetVersion() == "v1" {
+		if requestOnCreateDb.Username != "" {
+			username = requestOnCreateDb.Username
+		} else {
+			username = fmt.Sprintf("dbaas_%s", sa.Generate())
+		}
+
+		if requestOnCreateDb.Password != "" {
+			password = requestOnCreateDb.Password
+		} else {
+			password = sa.Generate()
+		}
+
+		adminUser = username
+		adminPassword = password
+
+		_, err = conn.Exec(ctx, createUser(username, password))
+		if err == nil {
+			users[username] = ""
+			if util.GetEnv("EXTERNAL_POSTGRESQL", "") != "" {
+				_, err = conn.Exec(ctx, grantUserToAdmin(username, sa.GetUser()))
+				if err != nil {
+					logger.Error(fmt.Sprintf("Couldn't GRANT %s TO %s;", username, sa.GetUser()), zap.Error(err))
+					sa.dropDatabase(ctx, dbName)
+					sa.dropUser(ctx, username)
+					return "", nil, err
+				}
+			}
+			_, err = conn.Exec(ctx, grantAllRightsOnDatabase(dbName, username))
+			if err != nil {
+				logger.Error(fmt.Sprintf("Cannot grant User %s", username), zap.Error(err))
+				sa.dropDatabase(ctx, dbName)
+				sa.dropUsers(ctx, users)
+				return "", nil, err
+			}
+
+			err = sa.createMetadata(ctx, dbName, adminUser, adminPassword, metadata)
+			if err != nil {
+				sa.dropDatabase(ctx, dbName)
+				sa.dropUser(ctx, adminUser)
+				return "", nil, err
+			}
+
+			connectionProps = append(connectionProps, sa.getConnectionProperties(dbName, username, "", password))
+			resources = append(resources, dao.DbResource{Kind: UserKind, Name: username})
+		}
+	} else {
+		for _, role := range sa.GetSupportedRoles() {
+			username = fmt.Sprintf("dbaas_%s", sa.Generate())
+			password = sa.Generate()
+
+			if role == "admin" {
+				if requestOnCreateDb.Username != "" {
+					username = requestOnCreateDb.Username
+				}
+				if requestOnCreateDb.Password != "" {
+					password = requestOnCreateDb.Password
+				}
+				adminUser = username
+				adminPassword = password
+			}
+			logger.Debug(fmt.Sprintf("Create user %s with role %s", username, role))
+			_, err = conn.Exec(ctx, createUser(username, password))
+			if err != nil {
+				panic(err)
+			}
+			extDB := util.GetEnv("EXTERNAL_POSTGRESQL", "")
+			if extDB != "" {
+				logger.Debug(fmt.Sprintf("External database is %s, granting user to admin", extDB))
+				_, err = conn.Exec(ctx, grantUserToAdmin(username, sa.GetUser()))
+				if err != nil {
+					logger.Error(fmt.Sprintf("Couldn't GRANT %s TO %s;", username, sa.GetUser()), zap.Error(err))
+					sa.dropDatabase(ctx, dbName)
+					sa.dropUser(ctx, username)
+					return "", nil, err
+				}
+			}
+			users[username] = role
+			connectionProps = append(connectionProps, sa.getConnectionProperties(dbName, username, role, password))
+			metadata[RolesKey] = users
+
+			resources = append(resources, dao.DbResource{Kind: UserKind, Name: username})
+		}
+
+		err = sa.GrantAllOnSchemaToUser(ctx, dbName, defaultSchema, adminUser)
+		if err != nil {
+			fmt.Printf("Error granting CREATE privilege on public schema to %s: %v\n", adminUser, err)
+		} else {
+			fmt.Printf("Successfully granted ALL privilege on public schema to %s\n", adminUser)
+		}
+
+		// explicitely providing create grants for PG15
+		err = sa.grantCreateOnSchema(ctx, dbName, defaultSchema, adminUser)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't create User %s", username), zap.Error(err))
+			sa.dropDatabase(ctx, dbName)
+			sa.dropUsers(ctx, users)
+			return "", nil, err
+		}
+
+		err = sa.createMetadata(ctx, dbName, adminUser, adminPassword, metadata)
+		if err != nil {
+			sa.dropDatabase(ctx, dbName)
+			sa.dropUser(ctx, username)
+			return "", nil, err
+		}
+
+		// grant roles for all schemas
+
+		err = sa.GrantUsersForAllSchemas(ctx, dbName, users)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't create User %s", username), zap.Error(err))
+			sa.dropDatabase(ctx, dbName)
+			sa.dropUsers(ctx, users)
+			return "", nil, err
+		}
+	}
+
+	sa.createExtensions(dbName, adminUser, pgSettings.Extensions)
+
+	logger.Info(fmt.Sprintf("Created db resources: %+v", resources))
+	// NO NAME Name: dbName, todo
+	response := &dao.LogicalDatabaseDescribed{ConnectionProperties: connectionProps, Resources: resources}
+	return dbName, response, nil
+}
+
+func getUsernameWithoutHost(username string) string {
+	return strings.Split(username, "@")[0]
+}
+
+func (sa ServiceAdapter) enableDatabase(ctx context.Context, dbName string) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Enable database %s", dbName))
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		return
+	}
+
+	defer conn.Close()
+	extDB := util.GetEnv("EXTERNAL_POSTGRESQL", "")
+	if extDB != "" {
+		logger.Debug(fmt.Sprintf("Enable database for external DB: %s", extDB))
+		_, err = conn.Exec(context.Background(), allowConnectionsToDbExt(dbName))
+	} else {
+		_, err = conn.Exec(context.Background(), allowConnectionsToDb(dbName))
+	}
+
+	if err != nil {
+		logger.Error(fmt.Sprintf("Failed to enable database %s after restoration, skip it", dbName), zap.Error(err))
+		return
+	}
+}
+
+func (sa ServiceAdapter) CreateUser(ctx context.Context, username string, postgresUserRequest dao.UserCreateRequest) (*dao.CreatedUser, error) {
+
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Creation of user with role %s started", postgresUserRequest.Role))
+	prefix := postgresUserRequest.UsernamePrefix
+	if username == "" {
+		if prefix == "" {
+			prefix = DefaultPrefix
+		}
+		username = fmt.Sprintf("%s_%s", prefix, sa.Generate())
+	} else {
+		if prefix != "" {
+			prefix = prefix + sa.GetDBPrefixDelimiter()
+		}
+		username = fmt.Sprintf("%s%s", prefix, getUsernameWithoutHost(username))
+	}
+	logger.Debug(fmt.Sprintf("User name with prefix: %s", username))
+	if !validateDbIdentifierParam(ctx, "username", username, DbIdentifiersPattern) {
+		return nil, fmt.Errorf("username must comply to the pattern %s", DbIdentifiersPattern)
+	}
+
+	if postgresUserRequest.DbName != "" && !validateDbIdentifierParam(ctx, "dbName", postgresUserRequest.DbName, DbIdentifiersPattern) {
+		return nil, fmt.Errorf("dbName must comply to the pattern %s", DbIdentifiersPattern)
+	}
+
+	version := sa.GetVersion()
+	if version == "v2" && !(util.Contains(sa.GetSupportedRoles(), postgresUserRequest.Role) || postgresUserRequest.Role == "none") {
+		return nil, fmt.Errorf("unsupported role. Role should be one of the list %s", sa.GetSupportedRoles())
+	}
+	if version == "v2" && (postgresUserRequest.Role == "none" && postgresUserRequest.DbName == "") {
+		return nil, fmt.Errorf("user with empty role can be created for defined database name. Specify dbName parameter in request")
+	}
+
+	var resources []dao.DbResource
+
+	userCreated := false
+	password := postgresUserRequest.Password
+
+	if password == "" {
+		password = sa.Generate()
+	}
+
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		panic(err)
+	}
+
+	defer conn.Close()
+
+	isUserExist, err := sa.isUserExist(ctx, conn, username)
+	if err != nil {
+		return nil, err
+	}
+
+	if !isUserExist {
+		logger.Info(fmt.Sprintf("User %s is not exist, creation...", username))
+		_, err = conn.Exec(ctx, createUser(username, password))
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't create user %s", username), zap.Error(err))
+			panic(err)
+		}
+
+		extPostgres := util.GetEnv("EXTERNAL_POSTGRESQL", "")
+		if extPostgres != "" {
+			logger.Debug(fmt.Sprintf("external database: %s, grant user to admin", extPostgres))
+			_, err = conn.Exec(ctx, grantUserToAdmin(username, sa.GetUser()))
+			if err != nil {
+				logger.Error(fmt.Sprintf("User %s not created! Couldn't GRANT %s TO %s;", username, username, sa.GetUser()), zap.Error(err))
+				sa.dropUser(ctx, username)
+				panic(err)
+			}
+		}
+		userCreated = true
+	} else {
+		logger.Info(fmt.Sprintf("Change password for existing user %s", username))
+		_, err = conn.Exec(ctx, changeUserPassword(username, password))
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't update user %s", username), zap.Error(err))
+			panic(err)
+		}
+	}
+
+	dbName := postgresUserRequest.DbName
+	if dbName != "" {
+		if version == "v1" {
+			_, err = conn.Exec(ctx, grantAllRightsOnDatabase(dbName, username))
+		} else {
+			err = sa.grantUserAndSave(ctx, dbName, username, postgresUserRequest.Role)
+			if err != nil {
+				if userCreated {
+					sa.dropUser(ctx, username)
+				}
+
+				logger.Error(fmt.Sprintf("Couldn't update metadata for user %s", username), zap.Error(err))
+			}
+		}
+
+		if err != nil {
+			if userCreated {
+				sa.dropUser(ctx, username)
+			}
+			logger.Error(fmt.Sprintf("Couldn't update grants for user %s", username), zap.Error(err))
+			panic(err)
+		}
+
+		resources = append(resources, dao.DbResource{
+			Kind: DbKind,
+			Name: dbName,
+		})
+
+		sa.enableDatabase(ctx, dbName)
+		if version == "v1" {
+			if err = sa.reassignGrantsToNewUserForDb(ctx, dbName, username); err != nil {
+				if userCreated {
+					sa.dropUser(ctx, username)
+				}
+				panic(err)
+			}
+		}
+	}
+
+	connectionProperties := sa.getConnectionProperties(dbName, username, postgresUserRequest.Role, password)
+	resources = append(resources, dao.DbResource{
+		Kind: UserKind,
+		Name: username,
+	})
+
+	response := &dao.CreatedUser{
+		ConnectionProperties: connectionProperties,
+		Name:                 dbName,
+		Resources:            resources,
+		//Created:              userCreated, todo
+	}
+	return response, nil
+}
+
+func (sa ServiceAdapter) isUserExist(ctx context.Context, conn cluster.Conn, username string) (bool, error) {
+	logger := util.ContextLogger(ctx)
+	rows, err := conn.Query(ctx, getUser, username)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't get user %s", username), zap.Error(err))
+		return false, err
+	}
+	defer rows.Close()
+
+	return rows.Next(), nil
+}
+
+func (sa ServiceAdapter) createExtensions(dbName string, username string, extensions []string) {
+	if username == "" {
+		sa.log.Warn("Can't create extensions. Username is Empty")
+		return
+	}
+	ctx := context.Background()
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return
+	}
+
+	defer conn.Close()
+
+	if len(extensions) > 0 {
+		sa.log.Debug(fmt.Sprintf("Create extensions %s", extensions))
+		CreateExtFromSlice(ctx, conn, username, extensions)
+	}
+
+	if sa.DefaultExt != nil {
+		sa.log.Debug(fmt.Sprintf("Create default extensions %s", sa.DefaultExt))
+		CreateExtFromSlice(ctx, conn, username, sa.DefaultExt)
+	}
+}
+
+func (sa ServiceAdapter) createMetadata(ctx context.Context, dbName string, username string, password string, metadata map[string]interface{}) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Metadata creation initiated for %s", dbName))
+
+	username = getUsernameWithoutHost(username)
+	conn, err := sa.GetConnectionToDbWithUser(ctx, dbName, username, password)
+	if err != nil {
+		return err
+	}
+
+	defer conn.Close()
+
+	_, err = conn.Exec(ctx, createMetaTable)
+	if err != nil {
+		logger.Error("Couldn't create metadata table", zap.Error(err))
+		return err
+	}
+
+	if len(metadata) > 0 {
+		metadata[RolesVersionKey] = util.GetRolesVersion()
+		logger.Debug(fmt.Sprintf("Insert metadata to %s for %s", DbaasMetadata, dbName))
+		metadataJson, errParse := json.Marshal(metadata)
+		if errParse != nil {
+			logger.Error("Error during marshal metadata", zap.Error(err))
+			return errParse
+		}
+
+		_, err = conn.Exec(ctx, insertIntoMetaTable, Metadata, metadataJson)
+		if err != nil {
+			logger.Error("Couldn't insert data in metadata table", zap.Error(err))
+			return err
+		}
+
+		_, err = conn.Exec(ctx, alterOwnerMetaTable(username))
+		if err != nil {
+			logger.Error("Couldn't change owner for metadata table", zap.Error(err))
+			return err
+		}
+	}
+
+	return nil
+}
+
+func (sa ServiceAdapter) GetDBPrefixDelimiter() string {
+	return "_"
+}
+
+func (sa ServiceAdapter) GetDBPrefix() string {
+	return "dbaas"
+}
+
+func (sa ServiceAdapter) GetDefaultCreateRequest() dao.DbCreateRequest {
+	return dao.DbCreateRequest{}
+}
+
+func (sa ServiceAdapter) GetDefaultUserCreateRequest() dao.UserCreateRequest {
+	return dao.UserCreateRequest{}
+}
+
+func (sa ServiceAdapter) PreStart() {}
+
+func (sa ServiceAdapter) UpdateMetadata(ctx context.Context, metadata map[string]interface{}, dbName string) {
+	metadataOld, err := sa.GetMetadataInternal(ctx, dbName)
+	if err != nil {
+		log.Warn(fmt.Sprintf("cannot get metadata from %s", dbName))
+	} else {
+		if roles := sa.GetRolesFromMetadata(metadataOld); len(roles) > 0 {
+			metadata[RolesKey] = roles
+		}
+		if _, ok := metadataOld[RolesVersionKey]; ok {
+			metadata[RolesVersionKey] = metadataOld[RolesVersionKey]
+		}
+	}
+	err = sa.UpdateMetadataInternal(ctx, metadata, dbName)
+	if err != nil {
+		panic(err)
+	}
+}
+
+func (sa ServiceAdapter) UpdateMetadataInternal(ctx context.Context, metadata map[string]interface{}, dbName string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Update metadata initiated for %s", dbName))
+	if !validateDbIdentifierParam(ctx, "dbName", dbName, DbIdentifiersPattern) {
+		return fmt.Errorf("dbName must comply to the pattern %s", DbIdentifiersPattern)
+	}
+
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	defer conn.Close()
+
+	if _, ok := metadata[RolesVersionKey]; !ok {
+		metadata[RolesVersionKey] = util.GetRolesVersion()
+	}
+
+	metadataJson, err := json.Marshal(metadata)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Error during marshal metadata in %s", dbName), zap.Error(err))
+		return err
+	}
+
+	_, err = conn.Exec(ctx, createMetaTable)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't create metadata table in %s", dbName), zap.Error(err))
+		return err
+	}
+
+	_, err = conn.Exec(ctx, deleteMetaData)
+	if err != nil {
+		logger.Error(fmt.Sprintf("cannot delete from metadata table for %s", dbName), zap.Error(err))
+		return err
+	}
+
+	_, err = conn.Exec(ctx, insertIntoMetaTable, Metadata, metadataJson)
+	if err != nil {
+		logger.Error(fmt.Sprintf("cannot insert in metadata table for %s", dbName), zap.Error(err))
+		return err
+	}
+
+	return nil
+}
+
+func (sa ServiceAdapter) DescribeDatabases(ctx context.Context, logicalDatabases []string, showResources bool, showConnections bool) map[string]dao.LogicalDatabaseDescribed {
+	logger := util.ContextLogger(ctx)
+	logger.Info("Describe databases is executed")
+	result := make(map[string]dao.LogicalDatabaseDescribed)
+	return result
+}
+
+func (sa ServiceAdapter) GetRolesFromMetadata(metadata map[string]interface{}) map[string]string {
+	roles := make(map[string]string)
+	if _, ok := metadata[RolesKey]; ok {
+		if rolesFromMeta, ok := metadata[RolesKey].(map[string]interface{}); ok {
+			roles = util.GetMapStringFromMapInterface(rolesFromMeta)
+		}
+	}
+	return roles
+}
+
+func (sa ServiceAdapter) GetValidRolesFromMetadata(ctx context.Context, dbName string, metadata map[string]interface{}) (map[string]string, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Start roles validation from metadata for %s", dbName))
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return nil, err
+	}
+	defer conn.Close()
+
+	roles := sa.GetRolesFromMetadata(metadata)
+	logger.Debug(fmt.Sprintf("Roles from metadata: %s", roles))
+
+	for username, role := range roles {
+		isUserExist, err := sa.isUserExist(ctx, conn, username)
+		if err != nil {
+			return nil, err
+		}
+		if !isUserExist {
+			logger.Warn(fmt.Sprintf("Role %s is not exist", username))
+			delete(roles, username)
+			continue
+		}
+
+		isValid, err := sa.validateRole(ctx, conn, username, role)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during validation user %s for database %s", username, dbName))
+			return nil, err
+		}
+		if !isValid {
+			logger.Warn(fmt.Sprintf("Role %s is not valid for database %s", username, dbName))
+			delete(roles, username)
+		}
+	}
+
+	logger.Debug(fmt.Sprintf("Valid roles from metadata: %s", roles))
+	return roles, nil
+}
+
+func (sa ServiceAdapter) saveRolesInMetadata(ctx context.Context, dbName string, roles map[string]string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Save roles %s in metadata for %s", roles, dbName))
+	metadata, err := sa.GetMetadataInternal(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	metadata[RolesKey] = roles
+	err = sa.UpdateMetadataInternal(ctx, metadata, dbName)
+	return err
+}
+
+func (sa ServiceAdapter) GetMetadataInternal(ctx context.Context, logicalDatabase string) (map[string]interface{}, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Get metadata from %s", logicalDatabase))
+
+	var metadata map[string]interface{}
+
+	conn, err := sa.GetConnectionToDb(ctx, logicalDatabase)
+	if err != nil {
+		return nil, err
+	}
+	defer conn.Close()
+
+	rows, err := conn.Query(ctx, getMetadata, "metadata")
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't obtain data from metadata table for %s", logicalDatabase), zap.Error(err))
+		return nil, err
+	}
+	defer rows.Close()
+
+	for rows.Next() {
+		err = rows.Scan(&metadata)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't scan data from metadata table for %s", logicalDatabase), zap.Error(err))
+			return nil, err
+		}
+	}
+
+	if len(metadata) == 0 {
+		rows.Close()
+		conn.Close()
+
+		metadata, err = sa.getOldFormatMetadata(ctx, logicalDatabase)
+		if err != nil {
+			return nil, err
+		}
+	}
+
+	return metadata, nil
+}
+
+func (sa ServiceAdapter) getOldFormatMetadata(ctx context.Context, logicalDatabase string) (map[string]interface{}, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Get metadata in old format for %s", logicalDatabase))
+	metadata := make(map[string]interface{})
+	conn, err := sa.GetConnectionToDb(ctx, logicalDatabase)
+	if err != nil {
+		return nil, err
+	}
+	defer conn.Close()
+
+	rows, err := conn.Query(ctx, getAllMetadata)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't obtain data from metadata table for %s", logicalDatabase), zap.Error(err))
+		return nil, err
+	}
+	defer rows.Close()
+
+	for rows.Next() {
+		var key string
+		var value interface{}
+		err = rows.Scan(&key, &value)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't scan data from metadata table for %s", logicalDatabase), zap.Error(err))
+			return nil, err
+		}
+		metadata[key] = value
+	}
+	return metadata, err
+}
+
+func (sa ServiceAdapter) GetMetadata(ctx context.Context, logicalDatabase string) map[string]interface{} {
+	metadata, err := sa.GetMetadataInternal(ctx, logicalDatabase)
+	if err != nil {
+		panic(err)
+	}
+	return metadata
+}
+
+func (sa ServiceAdapter) MigrateToVault(ctx context.Context, logicalDatabase string, userName string) error {
+	return nil
+}
+
+func (sa ServiceAdapter) disableDatabase(ctx context.Context, conn cluster.Conn, dbName string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Disable database %s", dbName))
+	var err error
+
+	err = sa.rollbackPrepared(ctx, conn, dbName)
+	if err != nil {
+		return err
+	}
+
+	if util.GetEnv("EXTERNAL_POSTGRESQL", "") != "" {
+		_, err = conn.Exec(ctx, fmt.Sprintf(prohibitConnectionsToDbExt, dbName))
+	} else {
+		_, err = conn.Exec(ctx, prohibitConnectionsToDb, dbName)
+	}
+	if err != nil {
+		logger.Error(fmt.Sprintf("Error during prohibit connections to db %s", dbName), zap.Error(err))
+		return err
+	}
+
+	if util.GetEnv("EXTERNAL_POSTGRESQL", "") == "" {
+		err := sa.terminateListenConnections(ctx, conn)
+		if err != nil {
+			logger.Warn(fmt.Sprintf("Error during LISTEN connections cleanup: %v", err))
+		}
+	}
+
+	_, err = conn.Exec(ctx, dropConnectionsToDb, dbName)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Error during drop connections to db %s", dbName), zap.Error(err))
+		return err
+	}
+	return nil
+}
+
+func transactionsExists(ctx context.Context, conn cluster.Conn, dbName string) (error, bool) {
+	var prep bool
+	row := conn.QueryRow(ctx, existsRollbackTransactions, dbName)
+	err := row.Scan(&prep)
+	if err != nil {
+		return err, false
+	}
+	return nil, prep
+}
+
+func (sa ServiceAdapter) rollbackPrepared(ctx context.Context, conn cluster.Conn, dbName string) error {
+	logger := util.ContextLogger(ctx)
+	var idTransaction string
+	var trans []string
+	err, prep := transactionsExists(ctx, conn, dbName)
+	if err != nil {
+		logger.Error("error when checking if rollback transactions are exists")
+		return err
+	}
+
+	if prep {
+		connDb, err := sa.GetConnectionToDb(ctx, dbName)
+		if err != nil {
+			logger.Error("taking connection failed while rollback prepared transactions")
+			return err
+		}
+		defer connDb.Close()
+
+		rows, err := connDb.Query(ctx, selectPreparedTransactions, dbName)
+		if err != nil {
+			logger.Error("select idTransaction failed while rollback prepared transactions")
+			return err
+		}
+		defer rows.Close()
+
+		for rows.Next() {
+			err = rows.Scan(&idTransaction)
+			if err != nil {
+				logger.Error("scan idTransaction failed while rollback prepared transactions")
+				return err
+			}
+			trans = append(trans, idTransaction)
+		}
+
+		for _, tr := range trans {
+			_, err = connDb.Exec(ctx, rollbackPreparedByGid(tr))
+			if err != nil {
+				logger.Error(fmt.Sprintf("rollback prepared failed with %s", tr))
+				return err
+			}
+		}
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) terminateListenConnections(ctx context.Context, conn cluster.Conn) error {
+	logger := util.ContextLogger(ctx)
+	logger.Debug("Cleaning up problematic LISTEN connections globally")
+
+	_, err := conn.Exec(ctx, terminateListenConnections)
+	if err != nil {
+		logger.Warn(fmt.Sprintf("Error during terminate LISTEN connections: %v", err))
+	}
+
+	return nil
+}
+
+func (sa ServiceAdapter) disableUser(ctx context.Context, dbName string, resource dao.DbResource) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Disable user %s for database %s", resource.Name, dbName))
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+
+	defer connDb.Close()
+
+	// if user deleted, we need to rollback prepared transactions in order to avoid lock on reassign grants
+	err = sa.rollbackPrepared(ctx, connDb, dbName)
+	if err != nil {
+		return err
+	}
+
+	_, err = connDb.Exec(context.Background(), reassignGrants(getUsernameWithoutHost(resource.Name), sa.GetUser()))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't reassign grants for user %s", resource.Name), zap.Error(err))
+		return err
+	}
+
+	_, err = connDb.Exec(context.Background(), dropOwnedObjects(resource.Name))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't drop owned objects for user %s", resource.Name), zap.Error(err))
+		return err
+	}
+
+	_, err = connDb.Exec(context.Background(), dropUserConnectionsToDb, resource.Name)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't drop user connections for user %s", resource.Name), zap.Error(err))
+		return err
+	}
+
+	return nil
+}
+
+func (sa ServiceAdapter) dropResource(ctx context.Context, resource dao.DbResource, conn cluster.Conn) dao.DbResource {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Drop resource %s with name %s", resource.Kind, resource.Name))
+	sa.Mutex.Lock()
+	defer sa.Mutex.Unlock()
+
+	if resource.Kind == DbKind {
+		rows, err := conn.Query(ctx, getDatabase, resource.Name)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during get information for DB %s", resource.Name), zap.Error(err))
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+
+		dbExist := rows.Next()
+		rows.Close()
+
+		if dbExist {
+			logger.Debug(fmt.Sprintf("Database %s is already exist, skip name validation for DDL", resource.Name))
+		} else {
+			logger.Debug(fmt.Sprintf("Database %s is not exist, skip deletion", resource.Name))
+
+			return dao.DbResource{
+				Kind:   resource.Kind,
+				Name:   resource.Name,
+				Status: DeletedStatus,
+			}
+		}
+
+		err = sa.disableDatabase(ctx, conn, resource.Name)
+		if err != nil {
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+
+		_, err = conn.Exec(context.Background(), dropOldDatabase(resource.Name))
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during drop DB %s", resource.Name), zap.Error(err))
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+
+		return dao.DbResource{
+			Kind:   resource.Kind,
+			Name:   resource.Name,
+			Status: DeletedStatus,
+		}
+
+	} else if resource.Kind == UserKind {
+		rows, err := conn.Query(context.Background(), getUser, resource.Name)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during get information for user %s", resource.Name), zap.Error(err))
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+
+		userExist := rows.Next()
+		rows.Close()
+		if userExist {
+			logger.Debug(fmt.Sprintf("User %s is already exist, skip name validation for DDL", resource.Name))
+		} else {
+			logger.Debug(fmt.Sprintf("User %s is not exist, skip deletion", resource.Name))
+
+			return dao.DbResource{
+				Kind:   resource.Kind,
+				Name:   resource.Name,
+				Status: DeletedStatus,
+			}
+		}
+
+		rows, err = conn.Query(context.Background(), getDependenceDbForRole, resource.Name, resource.Name)
+		if err != nil {
+			logger.Error("Couldn't get user roles", zap.Error(err))
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+
+		defer rows.Close()
+
+		for rows.Next() {
+			var dbName string
+			err = rows.Scan(&dbName)
+
+			if err != nil {
+				logger.Error(fmt.Sprintf("Error occurred during scan db for user %s", resource.Name), zap.Error(err))
+
+				return dao.DbResource{
+					Kind:         resource.Kind,
+					Name:         resource.Name,
+					Status:       DeleteFailedStatus,
+					ErrorMessage: err.Error(),
+				}
+			}
+
+			err = sa.disableUser(ctx, dbName, resource)
+			if err != nil {
+				return dao.DbResource{
+					Kind:         resource.Kind,
+					Name:         resource.Name,
+					Status:       DeleteFailedStatus,
+					ErrorMessage: err.Error(),
+				}
+			}
+		}
+
+		_, err = conn.Exec(context.Background(), dropUser(resource.Name))
+		if err != nil {
+			logger.Error(fmt.Sprintf("Couldn't drop user %s", resource.Name), zap.Error(err))
+			return dao.DbResource{
+				Kind:         resource.Kind,
+				Name:         resource.Name,
+				Status:       DeleteFailedStatus,
+				ErrorMessage: err.Error(),
+			}
+		}
+	}
+
+	return dao.DbResource{
+		Kind:   resource.Kind,
+		Name:   resource.Name,
+		Status: DeletedStatus,
+	}
+}
+
+func (sa ServiceAdapter) dropResourceByKind(ctx context.Context, resources []dao.DbResource, kind string, conn cluster.Conn) []dao.DbResource {
+	var result []dao.DbResource
+	for _, resource := range resources {
+		if resource.Kind == kind {
+			dropResult := sa.dropResource(ctx, resource, conn)
+			result = append(result, dropResult)
+		}
+	}
+	return result
+}
+
+func (sa ServiceAdapter) DropResources(ctx context.Context, resources []dao.DbResource) []dao.DbResource {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Drop resources %s", resources))
+	var droppedResources []dao.DbResource
+
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		panic(err)
+	}
+
+	defer conn.Close()
+
+	users := sa.dropResourceByKind(ctx, resources, UserKind, conn)
+	droppedResources = append(droppedResources, users...)
+
+	dataBases := sa.dropResourceByKind(ctx, resources, DbKind, conn)
+	droppedResources = append(droppedResources, dataBases...)
+
+	return droppedResources
+}
+
+func (sa ServiceAdapter) GetDatabases(ctx context.Context) []string {
+
+	logger := util.ContextLogger(ctx)
+	logger.Info("Get databases")
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		panic(err)
+	}
+
+	defer conn.Close()
+
+	rows, err := conn.Query(ctx, getDatabases)
+	if err != nil {
+		logger.Error("Error occurred during obtain databases rows", zap.Error(err))
+		panic(err)
+	}
+
+	defer rows.Close()
+
+	var databasesNames []string
+	for rows.Next() {
+		var databaseName string
+		err = rows.Scan(&databaseName)
+		if err != nil {
+			logger.Error("Error occurred during scan databases row", zap.Error(err))
+			panic(err)
+		}
+
+		databasesNames = append(databasesNames, databaseName)
+	}
+
+	return databasesNames
+}
+
+func (sa ServiceAdapter) getConnectionProperties(dbName, username, role, password string) dao.ConnectionProperties {
+	url := fmt.Sprintf("jdbc:postgresql://%s:%d/%s", sa.GetHost(), sa.GetPort(), dbName)
+	connectionProps := dao.ConnectionProperties{
+		"name":     dbName,
+		"url":      url,
+		"host":     sa.GetHost(),
+		"port":     sa.GetPort(),
+		"username": sa.getUserNameWithHostName(username),
+		"password": password,
+	}
+	if len(role) > 0 {
+		connectionProps["role"] = role
+	}
+	return connectionProps
+}
+
+func (generator UUIDGenerator) Generate() string {
+	uuidString := uuid.New().String()
+	return strings.ReplaceAll(uuidString, "-", "")
+}
+
+func validateDbIdentifierParam(ctx context.Context, paramName string, paramValue string, pattern string) bool {
+	logger := util.ContextLogger(ctx)
+	if paramValue != "" {
+		matched, err := regexp.MatchString(pattern, paramValue)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during check %s", paramName), zap.Error(err))
+			return false
+		}
+
+		if !matched {
+			logger.Info(fmt.Sprintf("Provided %s does not meet the requirements", paramName))
+		}
+
+		return matched
+	}
+	return true
+}
+
+func (sa ServiceAdapter) ValidatePostgresAvailableExtensions(ctx context.Context, extensions []string) (bool, error) {
+	logger := util.ContextLogger(ctx)
+	availableExtensions, err := sa.getPostgresAvailableExtensions(ctx)
+	if err != nil {
+		return false, err
+	}
+
+	for _, extension := range extensions {
+		if !util.Contains(availableExtensions, extension) {
+			logger.Error(fmt.Sprintf("request contains not valid Postgres extensions. %s", extension))
+			return false, nil
+		}
+	}
+	return true, nil
+}
+
+func validateCreationParameters(ctx context.Context, parameters map[string]string) error {
+	logger := util.ContextLogger(ctx)
+	for k := range parameters {
+		if !util.Contains(crParamsKeys, strings.ToUpper(k)) {
+			logger.Error(fmt.Sprintf("Invalid parameter for create database: '%s'", k))
+			return fmt.Errorf("request contains not valid parameter '%s' in the creation parameters", k)
+		}
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) deleteExtension(ctx context.Context, extension string, conn cluster.Conn) error {
+	logger := util.ContextLogger(ctx)
+	_, err := conn.Exec(context.Background(), deleteExtension(extension))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Error during delete database extension %s", extension), zap.Error(err))
+		return err
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) updateExtensions(ctx context.Context, conn cluster.Conn, newPgExtensions []string, currentPgExtensions []string) (*SettingUpdateResult, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug("Update extensions started")
+	isValid, err := sa.ValidatePostgresAvailableExtensions(ctx, newPgExtensions)
+	if err != nil {
+		logger.Error("Error during validate new extensions", zap.Error(err))
+		return nil, err
+	}
+
+	if !isValid {
+		return getBadRequestSettingsResult(newPgExtensions)
+	}
+
+	if len(currentPgExtensions) == 0 && len(newPgExtensions) == 0 {
+		message := "Nothing to update. Extensions list is empty."
+		logger.Info(message)
+		return &SettingUpdateResult{
+			Status:  Skipped,
+			Message: message,
+		}, nil
+	}
+
+	if len(currentPgExtensions) > 0 {
+		isValid, err = sa.ValidatePostgresAvailableExtensions(ctx, currentPgExtensions)
+		if err != nil {
+			logger.Error("Error during validate current extensions", zap.Error(err))
+			return nil, err
+		}
+
+		if !isValid {
+			return getBadRequestSettingsResult(currentPgExtensions)
+		}
+
+		for _, extension := range currentPgExtensions {
+			if !util.Contains(newPgExtensions, extension) {
+				err = sa.deleteExtension(ctx, extension, conn)
+				if err != nil {
+					return nil, err
+				}
+			}
+		}
+	}
+
+	for _, extension := range newPgExtensions {
+		_, err = conn.Exec(context.Background(), createExtensionIfNotExist(extension))
+		if err != nil {
+			logger.Error(fmt.Sprintf("Error during update database extension %s", extension), zap.Error(err))
+			return nil, err
+		}
+	}
+
+	return &SettingUpdateResult{
+		Status:  Successful,
+		Message: "Update database extensions successful",
+	}, nil
+}
+
+func (sa ServiceAdapter) updateSettings(dbName string, request PostgresUpdateSettingsRequest) (*PostgresUpdateSettingsResult, error) {
+	sa.log.Debug("Update setting started")
+	ctx := context.Background()
+	results := make(map[string]SettingUpdateResult)
+	if request.NewSettings == nil {
+		return &PostgresUpdateSettingsResult{
+			SettingUpdateResult: map[string]SettingUpdateResult{"newSettings": {
+				Status:  BadRequest,
+				Message: "newSettings parameter is empty",
+			}}}, nil
+	}
+
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return nil, err
+	}
+	defer conn.Close()
+
+	currentSettings, err := sa.getPgSettings(ctx, request.CurrentSettings)
+	if err != nil {
+		return nil, err
+	}
+	newSettings, err := sa.getPgSettings(ctx, request.NewSettings)
+	if err != nil {
+		return nil, err
+	}
+
+	updatedExtensions, err := sa.updateExtensions(ctx, conn, newSettings.Extensions, currentSettings.Extensions)
+	if err != nil {
+		return nil, err
+	}
+	results["pgExtensions"] = *updatedExtensions
+
+	for _, key := range newSettings.NotSupported {
+		message := fmt.Sprintf("Setting %s not supported", key)
+		sa.log.Info(message)
+		results[key] = SettingUpdateResult{
+			Status:  Skipped,
+			Message: message,
+		}
+	}
+	return &PostgresUpdateSettingsResult{results}, nil
+}
+
+func (sa ServiceAdapter) getUserNameWithHostName(userName string) string {
+	values := strings.Split(sa.ClusterAdapter.GetUser(), "@")
+	if len(values) > 1 {
+		return fmt.Sprintf("%s@%s", userName, values[1])
+	}
+	return userName
+}
+
+func (sa ServiceAdapter) reassignGrantsToNewUserForDb(ctx context.Context, dbName string, newUserName string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Will reassign grants to new user %s for db %s", newUserName, dbName))
+
+	currentUserName, err := sa.getCurrentUserForDb(ctx, dbName)
+	if err != nil {
+		logger.Error("", zap.Error(err))
+		return err
+	}
+
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	defer connDb.Close()
+
+	_, err = connDb.Exec(context.Background(), reassignGrants(currentUserName, newUserName))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't reassign grants from %s to %s", currentUserName, newUserName),
+			zap.Error(err))
+		return err
+	}
+	logger.Info(fmt.Sprintf("Reassign grants to new user %s for db %s has been completed", newUserName, dbName))
+	return nil
+}
+
+func (sa ServiceAdapter) getCurrentUserForDb(ctx context.Context, dbName string) (string, error) {
+	logger := util.ContextLogger(ctx)
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return "", err
+	}
+	defer connDb.Close()
+	rows, err := connDb.Query(context.Background(), getOwnerForMetaData)
+
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't get owner for meta table for db %s", dbName), zap.Error(err))
+		return "", err
+	}
+	defer rows.Close()
+	var currentUserForDb string
+	for rows.Next() {
+		err = rows.Scan(&currentUserForDb)
+		if err != nil {
+			logger.Error("Error occurred during scan databases row", zap.Error(err))
+			return "", err
+		}
+	}
+	logger.Debug(fmt.Sprintf("Current admin for database %s: %s", dbName, currentUserForDb))
+	return currentUserForDb, nil
+}
+
+func (sa ServiceAdapter) getDBInfo(ctx context.Context, dbName string) (*DbInfo, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Get info for database %s", dbName))
+	userName, err := sa.getCurrentUserForDb(ctx, dbName)
+	if err != nil {
+		return nil, err
+	}
+	return &DbInfo{Owner: userName}, nil
+}
+
+func getBadRequestSettingsResult(extensionsArray []string) (*SettingUpdateResult, error) {
+	message := fmt.Sprintf("Update settings Request contains not valid PostgreSQL extensions %s", extensionsArray)
+	log.Info(message)
+	return &SettingUpdateResult{
+		Status:  BadRequest,
+		Message: message,
+	}, nil
+}
+
+func readDefaultExtFile() *ExtensionConfig {
+	extensionFile := ExtensionPath + ExtensionName
+	file, err := os.ReadFile(extensionFile)
+	if err != nil {
+		log.Info(fmt.Sprintf("Skipping extensions file, cannot read it: %s", extensionFile))
+		return &ExtensionConfig{
+			DefaultExt:        make([]string, 0),
+			ExtUpdateRequired: false,
+		}
+	}
+	var extensionMap map[string]interface{}
+	err = json.Unmarshal(file, &extensionMap)
+	if err != nil {
+		log.Warn(fmt.Sprintf("Failed to parse extensions file %s", extensionFile), zap.Error(err))
+		extensionMap = make(map[string]interface{})
+	}
+
+	extensionSlice := util.GetStringArrayFromInterface(extensionMap[ExtensionsKey])
+	log.Info(fmt.Sprintf("Default extensions: %s", extensionSlice))
+	updateRequired := extensionMap[UpdateRequiredKey].(bool)
+	log.Info(fmt.Sprintf("Extensions should be updated: %t", updateRequired))
+
+	return &ExtensionConfig{
+		DefaultExt:        extensionSlice,
+		ExtUpdateRequired: updateRequired,
+	}
+}
+
+func CreateExtFromSlice(ctx context.Context, conn cluster.Conn, username string, extensions []string) {
+	for _, extension := range extensions {
+		log.Info(fmt.Sprintf("Create extension %s", extension))
+		_, err := conn.Exec(ctx, createExtensionIfNotExist(extension))
+		if err != nil {
+			log.Warn(fmt.Sprintf("Couldn't create extension %s", extension), zap.Error(err))
+		}
+
+		switch extension {
+		case OracleFdw:
+			log.Debug(fmt.Sprintf("Grant %s for OracleFDW", username))
+			_, err = conn.Exec(ctx, grantRightsForFdw(extension, username))
+			if err != nil {
+				log.Warn(fmt.Sprintf("Couldn't set grants for extension %s", extension), zap.Error(err))
+			}
+		case Dblink:
+			log.Debug(fmt.Sprintf("Grant %s for dblink", username))
+			_, err = conn.Exec(ctx, grantRightsForFdw("dblink_fdw", username))
+			if err != nil {
+				log.Warn(fmt.Sprintf("Couldn't set grants for extension %s", extension), zap.Error(err))
+			}
+			for _, function := range dblinkFuncs {
+				log.Debug(fmt.Sprintf("Grant %s for dblink function %s", username, function))
+				_, err = conn.Exec(ctx, setGrantOnFunction(function, username))
+				if err != nil {
+					log.Warn(fmt.Sprintf("Couldn't set grants for function %s", function), zap.Error(err))
+				}
+			}
+		}
+	}
+}
+
+func (sa ServiceAdapter) getConnectionLimitFromTemplate1Db(ctx context.Context) string {
+	logger := util.ContextLogger(ctx)
+	connDb, err := sa.GetConnection(ctx)
+	if err != nil {
+		return "-1"
+	}
+	defer connDb.Close()
+	rows, err := connDb.Query(context.Background(), getConnectionLimitFromTemplate1)
+	if err != nil {
+		logger.Error("Couldn't get connection limit from template1", zap.Error(err))
+		return "-1"
+	}
+	defer rows.Close()
+	var connectionLimit string
+	for rows.Next() {
+		err = rows.Scan(&connectionLimit)
+		if err != nil {
+			logger.Error("Error occurred during scan databases row", zap.Error(err))
+			return "-1"
+		}
+	}
+	logger.Debug(fmt.Sprintf("Connection limit for template1: %s", connectionLimit))
+	return connectionLimit
+}
+
+func (sa ServiceAdapter) GetSchemas(dbName string) ([]string, error) {
+	result := []string{}
+
+	// existing schemas
+	schemasFromDB, err := sa.getSchemasFromDB(dbName)
+	if err != nil {
+		return result, err
+	}
+	result = append(result, schemasFromDB...)
+
+	return result, err
+}
+
+func (sa ServiceAdapter) getSchemasFromDB(dbName string) ([]string, error) {
+	ctx := context.Background()
+	result := []string{}
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return result, nil
+	}
+	defer conn.Close()
+
+	rows, err := conn.Query(ctx, getSchemasQuery())
+	if err != nil {
+		return result, err
+	}
+
+	for rows.Next() {
+		var schema string
+		err := rows.Scan(&schema)
+		if err != nil {
+			sa.log.Error("cant scan schema name", zap.Error(err))
+			return result, err
+		}
+		result = append(result, schema)
+	}
+
+	sa.log.Debug(fmt.Sprintf("Schemas from DB: %s", result))
+	return result, nil
+}
+
+func (sa ServiceAdapter) getPgSettings(ctx context.Context, settings map[string]interface{}) (PgSettings, error) {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Database settings for process: %s", settings))
+	creationParameters := map[string]string{}
+	extensions := []string{}
+	notSupported := []string{}
+	for key, value := range settings {
+		switch key {
+		case CreationParametersKey:
+			creationParameters = util.GetMapStringFromMapInterface(value)
+			logger.Info(fmt.Sprintf("Database will be created with the next parameters %s", creationParameters))
+			err := validateCreationParameters(ctx, creationParameters)
+			if err != nil {
+				return PgSettings{}, err
+			}
+			creationParameters["connection limit"] = sa.getConnectionLimitFromTemplate1Db(ctx)
+		case PGExtensionsKey:
+			extensions = util.GetStringArrayFromInterface(settings[PGExtensionsKey])
+		default:
+			notSupported = append(notSupported, key)
+		}
+	}
+
+	return PgSettings{
+		CreationParameters: creationParameters,
+		Extensions:         extensions,
+		NotSupported:       notSupported,
+	}, nil
+}
diff --git a/docker-dbaas-adapter/adapter/basic/basic_roles.go b/docker-dbaas-adapter/adapter/basic/basic_roles.go
new file mode 100644
index 0000000..577c52f
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/basic_roles.go
@@ -0,0 +1,449 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"context"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	"go.uber.org/zap"
+)
+
+const FeatureMultiUsers = "multiusers"
+const FeatureTls = "tls"
+const FeatureNotStrictTLS = "tlsNotStrict"
+
+func (sa ServiceAdapter) IsMultiUsersEnabled() bool {
+	return sa.features[FeatureMultiUsers]
+}
+
+func (sa ServiceAdapter) GetSupportedRoles() []string {
+	if sa.IsMultiUsersEnabled() {
+		return sa.roles
+	}
+	return []string{"admin"}
+}
+
+func (sa ServiceAdapter) CreateRoles(ctx context.Context, roles []dao.AdditionalRole) (success []dao.Success, failure *dao.Failure) {
+	logger := util.ContextLogger(ctx)
+	logger.Info("Roles creation started")
+	var additionalRoleIdInProcess string
+
+	defer func() {
+		if r := recover(); r != nil {
+			log.Error(fmt.Sprintf("error during additional roles creation %s", r))
+			if failure == nil {
+				failure = &dao.Failure{
+					Id:      additionalRoleIdInProcess,
+					Message: fmt.Sprintf("%s", r),
+				}
+			}
+		}
+	}()
+
+	for _, additionalRole := range roles {
+		existingRoles := make([]string, 0)
+		additionalRoleIdInProcess = additionalRole.Id
+		dbName := additionalRole.DbName
+		logger.Info(fmt.Sprintf("Additional role in process id=%s, dbName=%s", additionalRoleIdInProcess, dbName))
+		err := sa.createMetadata(ctx, dbName, sa.GetUser(), sa.GetPassword(), nil)
+		if err != nil {
+			return success, &dao.Failure{
+				Id:      additionalRole.Id,
+				Message: err.Error(),
+			}
+		}
+		for _, connectionProperties := range additionalRole.ConnectionProperties {
+			roleForCheck := connectionProperties["role"].(string) //TODO
+			existingRoles = append(existingRoles, roleForCheck)
+
+			// handle managed databases case
+			username := connectionProperties["username"].(string)
+			logger.Debug(fmt.Sprintf("Role %s is found with name %s, perform grants preparation", roleForCheck, username))
+
+			if util.GetEnv("EXTERNAL_POSTGRESQL", "") != "" {
+				if err := sa.grantRightsToPostgres(ctx, username); err != nil {
+					logger.Error(fmt.Sprintf("Couldn't GRANT %s TO %s;", username, sa.GetUser()), zap.Error(err))
+					return success, &dao.Failure{
+						Id:      additionalRole.Id,
+						Message: err.Error(),
+					}
+				}
+			}
+
+			if roleForCheck != "admin" {
+				continue
+			}
+
+			err = sa.grantUserAndSave(ctx, dbName, username, roleForCheck)
+			if err != nil {
+				return success, &dao.Failure{
+					Id:      additionalRole.Id,
+					Message: err.Error(),
+				}
+			}
+		}
+		newConProps := make([]dao.ConnectionProperties, 0)
+		newResources := make([]dao.DbResource, 0)
+		for _, role := range sa.GetSupportedRoles() {
+			if !util.Contains(existingRoles, role) {
+				logger.Info(fmt.Sprintf("Role %s is not exist for database %s, creation...", role, dbName))
+				userCreateRequest := dao.UserCreateRequest{
+					DbName: dbName,
+					Role:   role,
+				}
+
+				createdUser, err := sa.CreateUser(ctx, "", userCreateRequest)
+				if err != nil {
+					return success, &dao.Failure{
+						Id:      additionalRole.Id,
+						Message: err.Error(),
+					}
+				}
+
+				newConProps = append(newConProps, createdUser.ConnectionProperties)
+				newResources = append(newResources, createdUser.Resources...)
+			}
+		}
+		success = append(success, dao.Success{
+			Id:                   additionalRole.Id,
+			ConnectionProperties: newConProps,
+			Resources:            newResources,
+			DbName:               dbName,
+		})
+	}
+	return success, failure
+}
+
+func (sa ServiceAdapter) GrantUsersAccordingRoles(ctx context.Context, dbName string, users map[string]string, schemas []string) error {
+	logger := util.ContextLogger(ctx)
+	isExternalPg := util.IsExternalPostgreSQl()
+	queries := make([]string, 0)
+	isOwnerChanged := false
+	metadataOwner, _ := sa.getCurrentUserForDb(ctx, dbName)
+	logger.Debug(fmt.Sprintf("Metadata owner: %s", metadataOwner))
+	adminUsers := []string{sa.GetUser()}
+	RWUsers := make([]string, 0)
+	ROUsers := make([]string, 0)
+
+	for userName, role := range users {
+		switch role {
+		case "admin":
+			adminUsers = append(adminUsers, userName)
+		case "streaming":
+			adminUsers = append(adminUsers, userName)
+		case "rw":
+			RWUsers = append(RWUsers, userName)
+		case "ro":
+			ROUsers = append(ROUsers, userName)
+		}
+	}
+
+	// grant admin users for external database
+	if isExternalPg {
+		for _, adminUser := range adminUsers {
+			if adminUser != sa.GetUser() {
+				log.Debug(fmt.Sprintf("Grant %s role to %s query was added to queries list", adminUser, sa.GetUser()))
+				queries = append(queries, grantUserToAdmin(adminUser, sa.GetUser()))
+			}
+		}
+	}
+
+	for _, schema := range schemas {
+		queries = append(queries, revokeRights())
+
+		for userName, role := range users {
+			// grant connection to database
+			queries = append(queries, grantConnectionToDB(dbName, userName))
+			if util.Contains(sa.GetSupportedRoles(), role) {
+				// grant schema usage
+				queries = append(queries, grantSchemaUsage(schema, userName))
+				// grant for already existing tables
+				queries = append(queries, grantUserOnTablesQuery(schema, userName, role))
+				// grant for already existing sequences
+				queries = append(queries, grantUserOnSequencesQuery(schema, userName, role))
+			}
+		}
+
+		for _, adminUserName := range adminUsers {
+			// grant create and previous admin grants
+			queries = append(queries, grantSchemaCreate(schema, adminUserName))
+		}
+
+		// change metadata owner in case owner was postgres
+		if metadataOwner == sa.GetUser() {
+			for _, adminUser := range adminUsers {
+				// select future owner for metadata
+				if adminUser != sa.GetUser() && users[adminUser] != "streaming" {
+					metadataOwner = adminUser
+					isOwnerChanged = true
+					logger.Debug(fmt.Sprintf("New metadata owner: %s", metadataOwner))
+					break
+				}
+			}
+		}
+		if isOwnerChanged {
+			// altering owners for DB resources
+			alterOperations := []DbResource{
+				{getTablesListQuery(schema, isExternalPg), alterOwnerForTable},
+				{getSequenceListQuery(schema, isExternalPg), alterOwnerForSequence},
+				{getLargeObjectsListQuery(isExternalPg), alterOwnerForLargeObject},
+				{getViewsListQuery(schema, isExternalPg), alterViewOwnerQuery},
+				{getFunctionsListQuery(schema, isExternalPg), alterFunctionOwnerQuery},
+				{getProceduresListQuery(schema, isExternalPg), alterProcedureOwnerQuery},
+				{getCustomTypesListQuery(schema, isExternalPg), alterTypeOwnerQuery},
+			}
+
+			for _, dbResource := range alterOperations {
+				alterOwnerQueries, errCh := sa.executeForResourceQueries(ctx, schema, dbName, metadataOwner, dbResource.SelectQuery, dbResource.AlterQueryFunction)
+				if errCh != nil {
+					return errCh
+				}
+				queries = append(queries, alterOwnerQueries...)
+			}
+		}
+
+		if metadataOwner != "" {
+			queries = append(queries, alterOwnerForSchema(schema, metadataOwner))
+			queries = append(queries, alterDatabaseOwnerQuery(dbName, metadataOwner))
+		}
+	}
+
+	for _, adminUserName := range adminUsers {
+		// grant all database privileges
+		if adminUserName != sa.GetUser() {
+			queries = append(queries, grantAllRightsOnDatabase(dbName, adminUserName))
+
+			// grant replication for admin
+			queries = append(queries, AllowReplicationForUser(adminUserName))
+		}
+
+		// grant user if not metadata owner and owner is not postgres
+		if metadataOwner != "" && adminUserName != metadataOwner && metadataOwner != sa.ClusterAdapter.GetUser() {
+			queries = append(queries, setGrantAsForRole(metadataOwner, adminUserName))
+		}
+
+		// grant RO and RW for future tables and schemas created by admin
+		for _, ROUserName := range ROUsers {
+			queries = append(queries, setReadOnlyRoleDefaultGrants(adminUserName, ROUserName))
+			queries = append(queries, setSchemasDefaultGrants(adminUserName, ROUserName))
+			queries = append(queries, setSequencesRODefaultGrants(adminUserName, ROUserName))
+		}
+		for _, RWUserName := range RWUsers {
+			queries = append(queries, setReadWriteRoleDefaultGrants(adminUserName, RWUserName))
+			queries = append(queries, setSchemasDefaultGrants(adminUserName, RWUserName))
+			queries = append(queries, setSequencesRWDefaultGrants(adminUserName, RWUserName))
+		}
+	}
+
+	// should be the last operation to ensure all objects are owned by new owner
+	queries = append(queries, alterOwnerMetaTable(metadataOwner))
+
+	return sa.executeQueries(ctx, dbName, queries)
+}
+
+func (sa ServiceAdapter) executeQueries(ctx context.Context, dbName string, queries []string) error {
+	logger := util.ContextLogger(ctx)
+	sa.Mutex.Lock()
+	defer sa.Mutex.Unlock()
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	defer connDb.Close()
+	for _, query := range queries {
+		log.Debug(fmt.Sprintf("[%s] Query for exec: %s", dbName, query))
+		if _, err = connDb.Exec(ctx, query); err != nil {
+			logger.Error(fmt.Sprintf("cannot execute in database %s the query %s", dbName, query), zap.Error(err))
+			return err
+		}
+	}
+
+	return nil
+}
+
+func grantUserOnTablesQuery(schema, username, role string) string {
+	var query string
+	switch role {
+	case "rw":
+		query = setReadWriteRoleGrants(schema, username)
+	case "ro":
+		query = setReadOnlyRoleGrants(schema, username)
+	default:
+		query = setAdminGrants(schema, username)
+	}
+	return query
+}
+
+func grantUserOnSequencesQuery(schema, username, role string) string {
+	var query string
+	switch role {
+	case "rw":
+		query = setReadWriteRoleSequencesGrants(schema, username)
+	case "ro":
+		query = setReadOnlyRoleSequencesGrants(schema, username)
+	default:
+		query = setAdminRoleSequencesGrants(schema, username)
+	}
+	return query
+}
+
+func (sa ServiceAdapter) grantCreateOnSchema(ctx context.Context, dbName, schema, username string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Debug(fmt.Sprintf("Grant create on schema %s for %s", schema, username))
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	defer connDb.Close()
+	_, err = connDb.Exec(ctx, grantSchemaCreate(schema, username))
+	if err != nil {
+		return err
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) GrantAllOnSchemaToUser(ctx context.Context, dbName, schema, username string) error {
+	grantUserSQL := fmt.Sprintf("GRANT ALL ON SCHEMA %s TO %s", schema, username)
+	connDb, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return err
+	}
+	defer connDb.Close()
+	_, err = connDb.Exec(ctx, grantUserSQL)
+	if err != nil {
+		return err
+	}
+	return nil
+}
+
+func (sa ServiceAdapter) validateRole(ctx context.Context, conn cluster.Conn, username, role string) (bool, error) {
+	logger := util.ContextLogger(ctx)
+	privilegeType := "TRUNCATE"
+	switch role {
+	case "ro":
+		privilegeType = "SELECT"
+	case "rw":
+		privilegeType = "UPDATE"
+	}
+	rows, err := conn.Query(ctx, getMetadataGrantQuery(username, privilegeType))
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't get user %s", username), zap.Error(err))
+		return false, err
+	}
+	defer rows.Close()
+	isUserValid := rows.Next()
+
+	return isUserValid, nil
+}
+
+func (sa ServiceAdapter) executeForResourceQueries(ctx context.Context, schema, dbName, userName, resourceQuery string, appliedQuery func(schema string, resource string, newOwner string) string) ([]string, error) {
+	queries := make([]string, 0)
+	resources, err := sa.getStringsFromQuery(ctx, dbName, resourceQuery)
+	if err != nil {
+		return nil, err
+	}
+	for _, resource := range resources {
+		queries = append(queries, appliedQuery(schema, resource, userName))
+	}
+	return queries, nil
+}
+
+func (sa ServiceAdapter) getStringsFromQuery(ctx context.Context, dbName, query string) ([]string, error) {
+	logger := util.ContextLogger(ctx)
+	conn, err := sa.GetConnectionToDb(ctx, dbName)
+	if err != nil {
+		return nil, err
+	}
+	defer conn.Close()
+
+	log.Debug(fmt.Sprintf("[%s] Resource query for exec: %s", dbName, query))
+	rows, err := conn.Query(ctx, query)
+	if err != nil {
+		logger.Error(fmt.Sprintf("failed to perform in db %s the next query: %s", dbName, query), zap.Error(err))
+		return nil, err
+	}
+	defer rows.Close()
+
+	resultStrings := make([]string, 0)
+	for rows.Next() {
+		currentString := ""
+		err = rows.Scan(&currentString)
+		if err != nil {
+			logger.Error(fmt.Sprintf("failed to scan string from db %s", dbName), zap.Error(err))
+			return nil, err
+		}
+		resultStrings = append(resultStrings, currentString)
+	}
+	return resultStrings, nil
+}
+
+func (sa *ServiceAdapter) grantUserAndSave(ctx context.Context, dbName, username, role string) error {
+	logger := util.ContextLogger(ctx)
+	metadata, err := sa.GetMetadataInternal(ctx, dbName)
+	if err != nil {
+		logger.Error(fmt.Sprintf("cannot get metadata for database %s", dbName), zap.Error(err))
+		return err
+	}
+	rolesMap, err := sa.GetValidRolesFromMetadata(ctx, dbName, metadata)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Couldn't read valid roles from metadata for db %s", dbName), zap.Error(err))
+		return err
+	}
+	rolesMap[username] = role
+
+	err = sa.GrantUsersForAllSchemas(ctx, dbName, rolesMap)
+	return err
+}
+
+func (sa *ServiceAdapter) GrantUsersForAllSchemas(ctx context.Context, dbName string, rolesMap map[string]string) error {
+	logger := util.ContextLogger(ctx)
+	logger.Info(fmt.Sprintf("Grant roles on all schemas for %s", dbName))
+	if err := sa.saveRolesInMetadata(ctx, dbName, rolesMap); err == nil {
+		schemasList, err := sa.GetSchemas(dbName)
+		if err != nil {
+			return err
+		}
+		logger.Debug(fmt.Sprintf("Schemas list: %s", schemasList))
+		err = sa.GrantUsersAccordingRoles(context.Background(), dbName, rolesMap, schemasList)
+		if err != nil {
+			return err
+		}
+	} else {
+		logger.Error(fmt.Sprintf("can't save roles in metadata for %s", dbName))
+		return err
+	}
+	return nil
+}
+
+func (sa *ServiceAdapter) grantRightsToPostgres(ctx context.Context, user string) error {
+
+	conn, err := sa.GetConnection(ctx)
+	if err != nil {
+		return err
+	}
+	defer conn.Close()
+
+	_, err = conn.Exec(ctx, grantUserToAdmin(user, sa.GetUser()))
+
+	if err != nil {
+		return err
+	}
+
+	return nil
+}
diff --git a/docker-dbaas-adapter/adapter/basic/basic_roles_test.go b/docker-dbaas-adapter/adapter/basic/basic_roles_test.go
new file mode 100644
index 0000000..d086299
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/basic_roles_test.go
@@ -0,0 +1,532 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"context"
+	"encoding/json"
+	"errors"
+	"fmt"
+	"reflect"
+	"testing"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	"github.com/jackc/pgconn"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/mock"
+	"go.uber.org/zap"
+)
+
+func Test_SupportedRoles(t *testing.T) {
+	cl := new(PostgresClusterAdapterMock)
+
+	// check multiusers
+	feature := map[string]bool{FeatureMultiUsers: true}
+	sa := NewServiceAdapter(cl, apiVersion, roles, feature)
+
+	expectedRoles := roles
+	supportedRoles := sa.GetSupportedRoles()
+
+	assert.ElementsMatch(t, expectedRoles, supportedRoles)
+
+	// check singleuser
+	sa = NewServiceAdapter(cl, apiVersion, roles, features)
+
+	expectedRoles = []string{"admin"}
+	supportedRoles = sa.GetSupportedRoles()
+
+	assert.ElementsMatch(t, expectedRoles, supportedRoles)
+}
+
+func Test_GetFeatures(t *testing.T) {
+	cl := new(PostgresClusterAdapterMock)
+
+	// check multiusers
+	features := map[string]bool{FeatureMultiUsers: true}
+	sa := NewServiceAdapter(cl, apiVersion, roles, features)
+
+	expectedFeatures := features
+	supportedFeatures := sa.GetFeatures()
+	fmt.Printf("supportedFeatures: %v\n", supportedFeatures)
+
+	if !reflect.DeepEqual(expectedFeatures, supportedFeatures) {
+		t.Errorf("Expected features %v, but got %v", expectedFeatures, supportedFeatures)
+	}
+
+}
+
+func Test_GetVersion(t *testing.T) {
+	cl := new(PostgresClusterAdapterMock)
+
+	// check multiusers
+	feature := map[string]bool{FeatureMultiUsers: true}
+	sa := NewServiceAdapter(cl, apiVersion, roles, feature)
+
+	expectedversion := dao.ApiVersion("v2")
+	version := sa.GetVersion()
+
+	assert.Equal(t, expectedversion, version)
+
+}
+
+func TestCreateRoles(t *testing.T) {
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	userName := "user1"
+	userPassword := "password"
+	dbName := "test_db1"
+	emptyUser := ""
+	schema := "public"
+	prefixedUser := "dbaas_test_user"
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"dbaas_test_user": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(errParse))
+		return
+	}
+
+	ca.On("GetUser").Return(user)
+	ca.On("GetPassword").Return(userPassword)
+
+	generator := new(GeneratorMock)
+	generator.On("Generate").Return(user)
+	generator.On("Generate").Return(userPassword)
+
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbName, user, userPassword).Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+
+	conn.On("Query", context.Background(), getUser, prefixedUser).Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Exec", context.Background(), createUser(prefixedUser, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), AllowReplicationForUser(prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleSequencesGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleSequencesGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(prefixedUser, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Close").Return(nil).Times(15)
+	rows.On("Next").Return(false).Times(11)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, map[string]bool{FeatureMultiUsers: false})
+	sa.Generator = generator
+	rolesReq := []dao.AdditionalRole{{Id: "123", DbName: dbName, ConnectionProperties: []dao.ConnectionProperties{{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": user,
+		"password": password,
+		"role":     RORole,
+	}},
+		Resources: []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: user}},
+	}}
+	succ, failed := sa.CreateRoles(context.Background(), rolesReq)
+	fmt.Printf("my succ: %v\n", succ)
+	fmt.Printf("my failed: %v\n", failed)
+	fmt.Printf("my succ resources: %v\n", succ[0].Resources)
+	fmt.Printf("my succ id: %v\n", succ[0].ConnectionProperties)
+
+	assert.Equal(t, 1, len(succ[0].ConnectionProperties))
+	assert.Equal(t, 2, len(succ[0].Resources))
+
+	assert.Nil(t, failed)
+}
+
+func Test_CreateRoles_MultiUsers(t *testing.T) {
+
+	var roles = []string{AdminRole, RWRole}
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	userex := "dbaas_test_user"
+	passwordex := "test_user"
+	dbName := "test_db1"
+	schema := "public"
+
+	for _, role := range roles {
+		metadata := map[string]interface{}{
+			RolesKey: map[string]string{
+				"dbaas_test_user": role,
+			},
+			RolesVersionKey: util.GetRolesVersion(),
+		}
+
+		metadataJson, errParse := json.Marshal(metadata)
+		if errParse != nil {
+			log.Error("Error during marshal metadata", zap.Error(errParse))
+			return
+		}
+		conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	}
+
+	ca.On("GetConnection").Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	ca.On("GetUser").Return(user)
+	ca.On("GetPassword").Return(password)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetConnectionToDbWithUser", dbName, user, password).Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	generator := new(GeneratorMock)
+	generator.On("Generate").Return(user)
+
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getUser, userex).Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Exec", context.Background(), setSchemasDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userex, passwordex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable("")).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), AllowReplicationForUser(userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleSequencesGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setSequencesRWDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userex, passwordex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	rows.On("Next").Return(true).Times(7)
+	rows.On("Close").Return(nil)
+
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(15)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, map[string]bool{FeatureMultiUsers: true})
+	sa.Generator = generator
+	rolesReq := []dao.AdditionalRole{{Id: "123", DbName: dbName, ConnectionProperties: []dao.ConnectionProperties{{
+		"name":     dbName,
+		"url":      fmt.Sprintf("clickhouse://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": user,
+		"password": password,
+		"role":     RORole,
+	}},
+		Resources: []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: user}},
+	}}
+	succ, failure := sa.CreateRoles(context.Background(), rolesReq)
+	fmt.Printf("CreateRoles succ: %v\n", succ)
+	fmt.Printf("CreateRoles failure: %v\n", failure)
+	assert.NotEmpty(t, succ)
+	assert.Nil(t, failure)
+	assert.Equal(t, 2, len(succ[0].ConnectionProperties))
+	assert.Equal(t, 4, len(succ[0].Resources))
+
+}
+
+func Test_CreateRoles_MultiUsersFailedCase(t *testing.T) {
+
+	var roles = []string{RWRole, AdminRole}
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	userex := "dbaas_test_user"
+	passwordex := "test_user"
+	dbName := "test_db1"
+	dbName2 := "test_db2"
+	schema := "public"
+	expectedErrorC := errors.New("can't set database grants")
+
+	for _, role := range roles {
+		metadata := map[string]interface{}{
+			RolesKey: map[string]string{
+				"dbaas_test_user": role,
+			},
+			RolesVersionKey: util.GetRolesVersion(),
+		}
+
+		metadataJson, errParse := json.Marshal(metadata)
+		if errParse != nil {
+			log.Error("Error during marshal metadata", zap.Error(errParse))
+			return
+		}
+		conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	}
+
+	ca.On("GetConnection").Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	ca.On("GetUser").Return(user)
+	ca.On("GetPassword").Return(password)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetConnectionToDbWithUser", dbName, user, password).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbName2, user, password).Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName2).Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	generator := new(GeneratorMock)
+	generator.On("Generate").Return(user)
+
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getUser, userex).Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Exec", context.Background(), dropUser(userex)).Return(pgconn.CommandTag("DELETED"), nil)
+	conn.On("Exec", context.Background(), dropUser(user)).Return(pgconn.CommandTag("DELETED"), nil)
+	conn.On("Exec", context.Background(), setSchemasDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userex, passwordex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName2)).Return(pgconn.CommandTag("UPDATED"), expectedErrorC)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable("")).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), AllowReplicationForUser(userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName2, userex)).Return(pgconn.CommandTag("UPDATED"), expectedErrorC)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleSequencesGrants(schema, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setSequencesRWDefaultGrants(user, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName2, userex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userex, passwordex)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	rows.On("Next").Return(true).Times(7)
+	rows.On("Close").Return(nil)
+
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(29)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, map[string]bool{FeatureMultiUsers: true})
+	sa.Generator = generator
+	rolesReq := []dao.AdditionalRole{{Id: "123", DbName: dbName, ConnectionProperties: []dao.ConnectionProperties{{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": user,
+		"password": password,
+		"role":     RORole,
+	}},
+		Resources: []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: user}},
+	}}
+
+	rolesReq = append(rolesReq, dao.AdditionalRole{
+		Id:     "124",
+		DbName: dbName2,
+		ConnectionProperties: []dao.ConnectionProperties{{
+			"name":     dbName2,
+			"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+			"host":     host,
+			"port":     port,
+			"username": user,
+			"password": password,
+			"role":     RORole,
+		}},
+		Resources: []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: user}},
+	})
+
+	succ, failure := sa.CreateRoles(context.Background(), rolesReq)
+	fmt.Printf("CreateRoles succ from failed case: %v\n", succ)
+	fmt.Printf("CreateRoles failure: %v\n", failure)
+	assert.NotEmpty(t, succ)
+	assert.NotNil(t, failure)
+	assert.NotEqual(t, 4, len(succ[0].ConnectionProperties))
+	assert.NotEqual(t, 6, len(succ[0].Resources))
+	assert.Equal(t, "can't set database grants", failure.Message)
+}
+
+func TestCreateRolesWithErrorCase(t *testing.T) {
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	userName := "user1"
+	userPassword := "password"
+	dbName := "test_db1"
+	emptyUser := ""
+	schema := "public"
+	prefixedUser := "dbaas_test_user"
+	expectedErrorC := errors.New("can't set database grants")
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"dbaas_test_user": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(errParse))
+		return
+	}
+
+	ca.On("GetUser").Return(user)
+	ca.On("GetPassword").Return(userPassword)
+
+	generator := new(GeneratorMock)
+	generator.On("Generate").Return(user)
+	generator.On("Generate").Return(userPassword)
+
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbName, user, userPassword).Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+
+	conn.On("Query", context.Background(), getUser, prefixedUser).Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, expectedErrorC)
+	conn.On("Exec", context.Background(), createUser(prefixedUser, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), AllowReplicationForUser(prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleSequencesGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(prefixedUser, user)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, prefixedUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Close").Return(nil).Times(15)
+	rows.On("Next").Return(false).Times(11)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, map[string]bool{FeatureMultiUsers: false})
+	sa.Generator = generator
+	rolesReq := []dao.AdditionalRole{{Id: "123", DbName: dbName, ConnectionProperties: []dao.ConnectionProperties{{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": user,
+		"password": password,
+		"role":     RORole,
+	}},
+		Resources: []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: user}},
+	}}
+	succ, failed := sa.CreateRoles(context.Background(), rolesReq)
+	fmt.Printf("my succ: %v\n", succ)
+	fmt.Printf("my failed: %v\n", failed)
+	assert.Equal(t, 0, len(succ))
+	assert.NotNil(t, failed)
+	assert.Equal(t, "can't set database grants", failed.Message)
+}
+
+func TestValidateRole(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	username := "valid_user"
+
+	ca.On("GetConnection").Return(conn, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	feature := map[string]bool{FeatureMultiUsers: true}
+	sa := NewServiceAdapter(ca, apiVersion, roles, feature)
+
+	t.Run("Valid user with 'ro' role", func(t *testing.T) {
+
+		rows.On("Next").Return(true).Once()
+		rows.On("Close").Return(nil)
+		conn.On("Query", context.Background(), getMetadataGrantQuery(username, "SELECT")).Return(rows, nil)
+
+		isValid, err := sa.validateRole(context.Background(), conn, "valid_user", "ro")
+
+		assert.NoError(t, err)
+		assert.True(t, isValid)
+		rows.AssertExpectations(t)
+	})
+
+	t.Run("Valid user with 'rw' role", func(t *testing.T) {
+
+		rows.On("Next").Return(true).Once()
+		rows.On("Close").Return(nil)
+		conn.On("Query", context.Background(), getMetadataGrantQuery(username, "UPDATE")).Return(rows, nil)
+
+		isValid, err := sa.validateRole(context.Background(), conn, "valid_user", "rw")
+
+		assert.NoError(t, err)
+		assert.True(t, isValid)
+		rows.AssertExpectations(t)
+	})
+}
diff --git a/docker-dbaas-adapter/adapter/basic/basic_test.go b/docker-dbaas-adapter/adapter/basic/basic_test.go
new file mode 100644
index 0000000..4243758
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/basic_test.go
@@ -0,0 +1,1437 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+//nolint
+
+package basic
+
+import (
+	"context"
+	"database/sql"
+	"encoding/json"
+	"errors"
+	"fmt"
+	"strings"
+	"testing"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	"github.com/jackc/pgconn"
+	"github.com/jackc/pgx/v4"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/mock"
+	"go.uber.org/zap"
+)
+
+const (
+	usernamePrefix1 = "dbaas"
+	apiVersion      = dao.ApiVersion("v2")
+)
+
+const (
+	AdminRole = "admin"
+	RORole    = "ro"
+	RWRole    = "rw"
+
+	cKey  = "classifier"
+	nsKey = "namespace"
+	msKey = "microserviceName"
+)
+
+var roles = []string{AdminRole, RWRole, RORole}
+
+var (
+	user     = "test_user"
+	password = "password"
+	host     = "test_host"
+	port     = 54321
+	features = map[string]bool{}
+	Database = "test_db"
+	SSL      = "off"
+)
+
+type PostgresClusterAdapterMock struct {
+	mock.Mock
+	cluster.ClusterAdapter
+}
+
+type GeneratorMock struct {
+	mock.Mock
+	Generator
+}
+
+func (m *GeneratorMock) Generate() string {
+	args := m.Called()
+	return args.String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetHost() string {
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetPort() int {
+	return ca.Called().Int(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetUser() string {
+	ca.Called()
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetPassword() string {
+	ca.Called()
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetDatabase() string {
+	return ca.Called().String(0)
+}
+
+func (ca *PostgresClusterAdapterMock) GetConnection(ctx context.Context) (cluster.Conn, error) {
+	args := ca.Called()
+	return args.Get(0).(cluster.Conn), args.Error(1)
+}
+
+func (ca *PostgresClusterAdapterMock) GetConnectionToDb(ctx context.Context, database string) (cluster.Conn, error) {
+	args := ca.Called(database)
+	return args.Get(0).(cluster.Conn), args.Error(1)
+}
+
+func (ca *PostgresClusterAdapterMock) GetConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (cluster.Conn, error) {
+	args := ca.Called(database, username, password)
+	return args.Get(0).(cluster.Conn), args.Error(1)
+}
+
+type MockRow struct {
+	mock.Mock
+	pgx.Row
+}
+
+func (m *MockRow) Scan(dest ...interface{}) error {
+	args := m.Called(dest)
+	return args.Error(0)
+}
+
+type MockRows struct {
+	mock.Mock
+	pgx.Rows
+}
+
+func (m *MockRows) Next() bool {
+	args := m.Called()
+	return args.Bool(0)
+}
+
+func (m *MockRows) Name() string {
+	args := m.Called()
+	return args.String(0)
+}
+
+func (m *MockRows) Scan(dest ...interface{}) error {
+	args := m.Called(dest)
+	return args.Error(0)
+}
+
+func (m *MockRows) Close() {
+	m.Called()
+}
+
+type MockConn struct {
+	mock.Mock
+	cluster.Conn
+}
+
+func (m *MockConn) Query(ctx context.Context, query string, args ...interface{}) (pgx.Rows, error) {
+	args = append([]interface{}{ctx, query}, args...)
+	ret := m.Called(args...)
+	return ret.Get(0).(pgx.Rows), ret.Error(1)
+}
+
+func (m *MockConn) QueryRow(ctx context.Context, query string, args ...interface{}) pgx.Row {
+	args = append([]interface{}{ctx, query}, args...)
+	ret := m.Called(args...)
+	return ret.Get(0).(pgx.Row)
+}
+
+func (m *MockConn) Exec(ctx context.Context, sql string, arguments ...interface{}) (pgconn.CommandTag, error) {
+	arguments = append([]interface{}{ctx, sql}, arguments...)
+	ret := m.Called(arguments...)
+	return ret.Get(0).(pgconn.CommandTag), ret.Error(1)
+}
+
+func (m *MockConn) Databases(ctx context.Context) ([]sql.DB, error) {
+	args := m.Called(ctx)
+	databases := args.Get(0)
+	if databases != nil {
+		return args.Get(0).([]sql.DB), args.Error(1)
+	}
+	return nil, args.Error(1)
+}
+
+func (m *MockConn) Close() {
+}
+
+func TestGetDatabases(t *testing.T) {
+	expectedDBNames := []string{"test_db_1", "test_db_2"}
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	//var err error
+
+	ca.On("GetConnection").Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Query", context.Background(), getDatabases).Return(rows, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Next").Return(false).Once()
+	rows.On("Close").Return(nil)
+
+	for _, roles := range expectedDBNames {
+		roles := roles
+		rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+			arg := args.Get(0).([]interface{})
+			strArg := arg[0].(*string)
+			*strArg = roles
+		}).Once()
+	}
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	databases := sa.GetDatabases(context.Background())
+	assert.ElementsMatch(t, expectedDBNames, databases)
+}
+
+func TestDescribeDatabases(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	databaseDesc := sa.DescribeDatabases(context.Background(), []string{Database}, false, false)
+	fmt.Printf("databaseDesc: %v\n", databaseDesc)
+	assert.NotNil(t, databaseDesc)
+
+}
+
+func (m *MockConn) Database(ctx context.Context) ([]sql.DB, error) {
+	args := m.Called(ctx)
+	databases := args.Get(0)
+	if databases != nil {
+		return args.Get(0).([]sql.DB), args.Error(1)
+	}
+	return nil, args.Error(1)
+}
+
+func (m *MockConn) DatabaseExists(ctx context.Context, name string) (bool, error) {
+	args := m.Called(ctx, name)
+	return args.Bool(0), args.Error(1)
+}
+
+func (m *MockConn) Remove(ctx context.Context) error {
+	args := m.Called(ctx)
+	return args.Error(0)
+}
+
+func TestDropResources_Database(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	row := new(MockRow)
+	connDb := new(MockConn)
+	testId := "foobar"
+
+	ca.On("GetConnection").Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+	dbName := "db1"
+	conn.On("GetDatabase").Return(dbName)
+	conn.On("Query", context.Background(), getDatabase, dbName).Return(rows, nil)
+	conn.On("QueryRow", context.Background(), existsRollbackTransactions, dbName).Return(row)
+	conn.On("Exec", context.Background(), dropDatabase(dbName)).Return(pgconn.CommandTag("DELETED"), nil)
+	conn.On("Exec", context.Background(), prohibitConnectionsToDb, dbName).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), terminateListenConnections).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), dropConnectionsToDb, dbName).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Close", context.Background()).Return(nil)
+	row.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		answ := arg[0].(*bool)
+		*answ = true
+	})
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Next").Return(false).Once()
+	rows.On("Close").Return(nil)
+
+	ca.On("GetConnectionToDb", dbName).Return(connDb, nil)
+	connDb.On("Query", context.Background(), selectPreparedTransactions, dbName).Return(rows, nil)
+	connDb.On("Exec", context.Background(), rollbackPreparedByGid(testId)).Return(pgconn.CommandTag("DELETED"), nil)
+	connDb.On("Close", context.Background()).Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		answ := arg[0].(*string)
+		*answ = testId
+	})
+
+	resourceForDrop := []dao.DbResource{{Kind: DbKind, Name: dbName}}
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	droppedRes := sa.DropResources(context.Background(), resourceForDrop)
+	fmt.Printf("my dropped resources: %v\n", droppedRes)
+	assert.Equal(t, dao.DELETED, droppedRes[0].Status)
+
+}
+
+func TestDropResources_DatabaseFailedCase(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	row := new(MockRow)
+	connDb := new(MockConn)
+	testId := "foobar"
+
+	ca.On("GetConnection").Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+	dbName := "db1"
+	expectedErrorC := errors.New("error while execute drop database")
+	conn.On("GetDatabase").Return(dbName)
+	conn.On("Query", context.Background(), getDatabase, dbName).Return(rows, nil)
+	conn.On("QueryRow", context.Background(), existsRollbackTransactions, dbName).Return(row)
+	conn.On("Exec", context.Background(), dropDatabase(dbName)).Return(pgconn.CommandTag("DELETED"), expectedErrorC)
+	conn.On("Exec", context.Background(), prohibitConnectionsToDb, dbName).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), terminateListenConnections).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), dropConnectionsToDb, dbName).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Close", context.Background()).Return(nil)
+	row.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		answ := arg[0].(*bool)
+		*answ = true
+	})
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Next").Return(false).Once()
+	rows.On("Close").Return(nil)
+
+	ca.On("GetConnectionToDb", dbName).Return(connDb, nil)
+	connDb.On("Query", context.Background(), selectPreparedTransactions, dbName).Return(rows, nil)
+	connDb.On("Exec", context.Background(), rollbackPreparedByGid(testId)).Return(pgconn.CommandTag("DELETED"), nil)
+	connDb.On("Close", context.Background()).Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		answ := arg[0].(*string)
+		*answ = testId
+	})
+
+	resourceForDrop := []dao.DbResource{{Kind: DbKind, Name: dbName}}
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	droppedRes := sa.DropResources(context.Background(), resourceForDrop)
+	fmt.Printf("dropResource : %v\n", droppedRes)
+	assert.Equal(t, dao.DELETE_FAILED, droppedRes[0].Status)
+	assert.Equal(t, "error while execute drop database", droppedRes[0].ErrorMessage)
+
+}
+
+type UserMock struct {
+	mock.Mock
+	dao.CreatedUser
+}
+
+func (m *MockConn) User(ctx context.Context, name string) (string, error) {
+	args := m.Called(ctx, name)
+	user := args.Get(0)
+	if user != nil {
+		return args.Get(0).(string), args.Error(1)
+	}
+	return name, args.Error(1)
+}
+
+func TesDropResources__UserFailedCase(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+
+	ca.On("GetConnection").Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+	expectedErrorC := errors.New("error while execute drop database")
+
+	user := new(UserMock)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+	userName := "user1"
+	resourceForDrop := []dao.DbResource{{Kind: UserKind, Name: userName}}
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(user, nil).Once()
+	conn.On("Exec", context.Background(), dropUser(userName)).Return(pgconn.CommandTag("DELETED"), expectedErrorC)
+	droppedRes := sa.DropResources(context.Background(), resourceForDrop)
+	fmt.Printf("my dropped resources from user: %v\n", droppedRes)
+	fmt.Printf("dropResource : %v\n", droppedRes)
+	assert.Equal(t, dao.DELETE_FAILED, droppedRes[0].Status)
+	assert.Equal(t, "error while execute drop user", droppedRes[0].ErrorMessage)
+
+}
+
+func TesDropResources_User(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+
+	ca.On("GetConnection").Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	user := new(UserMock)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+	userName := "user1"
+	resourceForDrop := []dao.DbResource{{Kind: UserKind, Name: userName}}
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(user, nil).Once()
+	conn.On("Exec", context.Background(), dropUser(userName)).Return(pgconn.CommandTag("DELETED"), nil)
+	droppedRes := sa.DropResources(context.Background(), resourceForDrop)
+	fmt.Printf("my dropped resources from user: %v\n", droppedRes)
+	assert.Equal(t, resourceForDrop, droppedRes)
+	assert.ElementsMatch(t, resourceForDrop, droppedRes)
+
+}
+
+func TestCreateUser_UserExist(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	dbName := "db1"
+
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Close", context.Background()).Return(nil)
+
+	userName := "user1"
+	userPassword := "password1"
+	userRole := AdminRole
+	var err error
+	schema := "public"
+	emptyUser := ""
+
+	featuresWithMult := map[string]bool{FeatureMultiUsers: true}
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"user1": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+	userCreateRequest := dao.UserCreateRequest{DbName: dbName, Role: userRole, Password: userPassword}
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("Query", context.Background(), getUser, userName).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleSequencesGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleSequencesGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setSequencesRWDefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setSequencesRODefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(userName, nil)
+	conn.On("DatabaseExists", context.Background(), mock.AnythingOfType("string")).Return(true, nil)
+
+	rows.On("Name").Return(userName)
+
+	rows.On("Next").Return(true).Times(3)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(6)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	createdUser, err := sa.CreateUser(context.Background(), userName, userCreateRequest)
+	assert.Empty(t, err)
+	fmt.Printf("my created user: %v\n", createdUser.Name)
+	assert.Equal(t, dbName, createdUser.Name)
+	assert.Equal(t, []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: userName}}, createdUser.Resources)
+	assert.Equal(t, dao.ConnectionProperties{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": userName,
+		"password": userPassword,
+		"role":     userRole,
+	}, createdUser.ConnectionProperties)
+
+	//RW USer
+	featuresWithMult = map[string]bool{FeatureMultiUsers: true}
+	metadata = map[string]interface{}{
+		RolesKey: map[string]string{
+			"user1": "rw",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+	metadataJson, errParse = json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+	userCreateRequest = dao.UserCreateRequest{DbName: dbName, Role: "rw", Password: userPassword}
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("Query", context.Background(), getUser, userName).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setSchemasDefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadWriteRoleDefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(userName, nil)
+	conn.On("DatabaseExists", context.Background(), mock.AnythingOfType("string")).Return(true, nil)
+
+	rows.On("Name").Return(userName)
+
+	rows.On("Next").Return(true).Times(3)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(6)
+
+	fmt.Printf("my connection from create user: %v\n", conn)
+
+	sa = NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	createdUser, err = sa.CreateUser(context.Background(), userName, userCreateRequest)
+	assert.Empty(t, err)
+	fmt.Printf("my created user: %v\n", createdUser.Name)
+	assert.Equal(t, dbName, createdUser.Name)
+	assert.Equal(t, []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: userName}}, createdUser.Resources)
+	assert.Equal(t, dao.ConnectionProperties{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": userName,
+		"password": userPassword,
+		"role":     "rw",
+	}, createdUser.ConnectionProperties)
+
+	//ROUser
+	featuresWithMult = map[string]bool{FeatureMultiUsers: true}
+	metadata = map[string]interface{}{
+		RolesKey: map[string]string{
+			"user1": "ro",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+	metadataJson, errParse = json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+	userCreateRequest = dao.UserCreateRequest{DbName: dbName, Role: "ro", Password: userPassword}
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("Query", context.Background(), getUser, userName).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleDefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(userName, nil)
+	conn.On("DatabaseExists", context.Background(), mock.AnythingOfType("string")).Return(true, nil)
+
+	rows.On("Name").Return(userName)
+
+	rows.On("Next").Return(true).Times(3)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(6)
+
+	fmt.Printf("my connection from create user: %v\n", conn)
+
+	sa = NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	createdUser, err = sa.CreateUser(context.Background(), userName, userCreateRequest)
+	assert.Empty(t, err)
+	fmt.Printf("my created user: %v\n", createdUser.Name)
+	assert.Equal(t, dbName, createdUser.Name)
+	assert.Equal(t, []dao.DbResource{{Kind: DbKind, Name: dbName}, {Kind: UserKind, Name: userName}}, createdUser.Resources)
+	assert.Equal(t, dao.ConnectionProperties{
+		"name":     dbName,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbName),
+		"host":     host,
+		"port":     port,
+		"username": userName,
+		"password": userPassword,
+		"role":     "ro",
+	}, createdUser.ConnectionProperties)
+
+	//InvalidRole - Here Error will not be empty
+	featuresWithMult = map[string]bool{FeatureMultiUsers: true}
+	metadata = map[string]interface{}{
+		"roles": map[string]string{
+			"user1": "ro",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+	metadataJson, errParse = json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+	userCreateRequest = dao.UserCreateRequest{DbName: dbName, Role: "Invalid Role", Password: userPassword}
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("Query", context.Background(), getUser, userName).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setReadOnlyRoleDefaultGrants(userName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), allowConnectionsToDb(dbName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), changeUserPassword(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantAllRightsOnDatabase(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(userName, nil)
+	conn.On("DatabaseExists", context.Background(), mock.AnythingOfType("string")).Return(true, nil)
+
+	rows.On("Name").Return(userName)
+
+	rows.On("Next").Return(true).Times(3)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(6)
+
+	sa = NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	createdUser, err = sa.CreateUser(context.Background(), userName, userCreateRequest)
+	fmt.Printf("my created user with error: %v\n", createdUser)
+	assert.NotNil(t, err)
+}
+
+func TestCreateDatabase(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	generator := new(GeneratorMock)
+	var err error
+
+	userName := "user1"
+	userPassword := "test_password"
+	namePrefix := "test_prefix"
+	dbName := "test_database_name"
+	schema := "public"
+	emptyUser := ""
+	grantUserSQL := fmt.Sprintf("GRANT ALL ON SCHEMA %s TO %s", schema, userName)
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"user1": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+
+	user := new(UserMock)
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbName, userName, userPassword).Return(conn, nil)
+	ca.On("GetUser").Return(userName)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Close", context.Background()).Return(nil)
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+
+	generator.On("Generate").Return(dbName).Once()
+	for i := range roles {
+		generator.On("Generate").Return(fmt.Sprintf("user%d", i)).Once()
+		generator.On("Generate").Return(fmt.Sprintf("password%d", i)).Once()
+	}
+
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(user, nil)
+
+	conn.On("Name").Return(dbName, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantUserSQL).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	// multiusers disabled
+	featuresWithMult := map[string]bool{FeatureMultiUsers: false}
+
+	request := dao.DbCreateRequest{Metadata: map[string]interface{}{}, NamePrefix: &namePrefix, Password: userPassword, DbName: dbName, Settings: map[string]interface{}{}, Username: userName}
+
+	rows.On("Next").Return(true).Times(4)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(8)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	sa.Generator = generator
+
+	setting, err := sa.getPgSettings(context.Background(), request.Settings)
+	assert.Nil(t, err)
+
+	conn.On("Exec", context.Background(), createDatabase(dbName, setting.CreationParameters)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	dbNameExp := dbName
+
+	dbNameRes, dbDesc, err := sa.CreateDatabase(context.Background(), request)
+	assert.Nil(t, err)
+
+	conn.On("Exec", context.Background(), createDatabase(dbName, setting.CreationParameters)).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	assert.Equal(t, dbNameExp, dbNameRes)
+	expCp := dao.ConnectionProperties{
+		"host":     host,
+		"name":     dbNameExp,
+		"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, dbNameExp),
+		"port":     port,
+		"username": "user1",
+		"password": "test_password",
+		"role":     AdminRole,
+	}
+	assert.Equal(t, expCp, dbDesc.ConnectionProperties[0])
+}
+
+func TestCreateDatabaseMultiUser(t *testing.T) {
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	generator := new(GeneratorMock)
+	var err error
+
+	userName := "user"
+	userPassword := "password"
+	namePrefix := "test_prefix"
+	dbName := "test_database_name"
+	schema := "public"
+	emptyUser := ""
+
+	namespace := "test-namespace"
+	msName := "test-microservice"
+	classifier := map[string]interface{}{nsKey: namespace, msKey: msName}
+
+	// initialize database part with matcher
+	genDBNameWithoutTS := fmt.Sprintf("%s_", namePrefix)
+	dbArgMatcher := mock.MatchedBy(func(dbName string) bool { return strings.Contains(dbName, genDBNameWithoutTS) })
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"dbaas_user0": "admin",
+			"dbaas_user1": "rw",
+			"dbaas_user2": "ro",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+		cKey:            classifier,
+	}
+
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(err))
+		return
+	}
+
+	user := new(UserMock)
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbArgMatcher).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbArgMatcher, userName, userPassword).Return(conn, nil)
+
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Close", context.Background()).Return(nil)
+
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(user, nil)
+
+	for i := range roles {
+		userName = fmt.Sprintf("dbaas_user%d", i)
+		generator.On("Generate").Return(fmt.Sprintf("user%d", i)).Once()
+		generator.On("Generate").Return(fmt.Sprintf("password%d", i)).Once()
+
+		grantUserSQL := fmt.Sprintf("GRANT ALL ON SCHEMA %s TO %s", schema, (fmt.Sprintf("dbaas_user%d", i)))
+
+		ca.On("GetConnectionToDbWithUser", dbArgMatcher, (fmt.Sprintf("dbaas_user%d", i)), (fmt.Sprintf("password%d", i))).Return(conn, nil)
+		conn.On("Name").Return(dbName, nil)
+		ca.On("GetUser").Return(userName)
+		conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+		conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+		conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+		conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+		conn.On("Exec", context.Background(), setReadOnlyRoleSequencesGrants(schema, "dbaas_user2")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setReadWriteRoleSequencesGrants(schema, "dbaas_user1")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setAdminRoleSequencesGrants(schema, "dbaas_user0")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setSequencesRWDefaultGrants("dbaas_user0", "dbaas_user1")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setSequencesRODefaultGrants("dbaas_user0", "dbaas_user2")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setSchemasDefaultGrants("dbaas_user0", "dbaas_user1")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setReadWriteRoleDefaultGrants("dbaas_user0", "dbaas_user1")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setReadOnlyRoleDefaultGrants("dbaas_user0", "dbaas_user2")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setSchemasDefaultGrants("dbaas_user0", "dbaas_user2")).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), grantSchemaCreate(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setReadOnlyRoleGrants(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setReadWriteRoleGrants(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), mock.MatchedBy(getMatcherFuncWithUser(genDBNameWithoutTS, i))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), createUser((fmt.Sprintf("dbaas_user%d", i)), (fmt.Sprintf("password%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), alterOwnerMetaTable((fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), grantSchemaCreate(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), grantConnectionToDB((fmt.Sprintf("dbaas_user%d", i)), (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), grantSchemaUsage(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), setAdminGrants(schema, (fmt.Sprintf("dbaas_user%d", i)))).Return(pgconn.CommandTag("UPDATED"), nil)
+		conn.On("Exec", context.Background(), grantUserSQL).Return(pgconn.CommandTag("UPDATED"), nil)
+	}
+
+	// multiusers enabled
+	featuresWithMult := map[string]bool{FeatureMultiUsers: true}
+
+	request := dao.DbCreateRequest{Metadata: metadata, NamePrefix: &namePrefix, Password: "", DbName: "", Settings: map[string]interface{}{}, Username: ""}
+
+	rows.On("Next").Return(true).Times(4)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		metadataForObtain := arg[0].(*map[string]interface{})
+		*metadataForObtain = metadata
+	}).Return(nil)
+	rows.On("Next").Return(false).Times(8)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+	sa.Generator = generator
+
+	setting, err := sa.getPgSettings(context.Background(), request.Settings)
+	fmt.Printf("my settings for pg db: %v\n", setting)
+	assert.Nil(t, err)
+
+	conn.On("Exec", context.Background(), mock.MatchedBy(
+		func(query string) bool {
+			genEmpty := createDatabase("", setting.CreationParameters)
+			queryArr := strings.Split(query, "\"")
+			emptyQueryArr := strings.Split(genEmpty, "\"")
+			return queryArr[0] == emptyQueryArr[0] && strings.Contains(queryArr[1], genDBNameWithoutTS)
+		})).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	dbNameRes, dbDesc, err := sa.CreateDatabase(context.Background(), request)
+	assert.Nil(t, err)
+	assert.Contains(t, dbNameRes, genDBNameWithoutTS)
+
+	for i, r := range roles {
+		found := false
+		if dbDesc != nil {
+			for _, cp := range dbDesc.ConnectionProperties {
+				if cp["role"] == r {
+					found = true
+					expCp := dao.ConnectionProperties{
+						"host":     host,
+						"name":     cp["name"],
+						"url":      fmt.Sprintf("jdbc:postgresql://%s:%d/%s", host, port, cp["name"]),
+						"port":     port,
+						"username": fmt.Sprintf("dbaas_user%d", i),
+						"password": fmt.Sprintf("password%d", i),
+						"role":     r,
+					}
+					assert.Equal(t, expCp, cp)
+					assert.Contains(t, cp["name"], genDBNameWithoutTS)
+				}
+
+			}
+		}
+		assert.True(t, found)
+	}
+}
+
+func getMatcherFuncWithUser(dbName string, i int) func(query string) bool {
+	return func(query string) bool {
+		genEmpty := grantConnectionToDB("", fmt.Sprintf("dbaas_user%d", i))
+		queryArr := strings.Split(query, "\"")
+		emptyQueryArr := strings.Split(genEmpty, "\"")
+		return queryArr[0] == emptyQueryArr[0] && strings.Contains(queryArr[1], dbName) && queryArr[2] == emptyQueryArr[2]
+	}
+}
+
+func TestCreateDatabasePanics(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+	var err error
+
+	userName := "user1"
+	userPassword := "test_password"
+	namePrefix := "test_prefix"
+	dbName := "test_database_name"
+	schema := "public"
+	emptyUser := ""
+	grantUserSQL := fmt.Sprintf("GRANT ALL ON SCHEMA %s TO %s", schema, userName)
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"user1": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+	metadataJson, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata", zap.Error(errParse))
+		return
+	}
+
+	user := new(UserMock)
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", dbName).Return(conn, nil)
+	ca.On("GetConnectionToDbWithUser", dbName, userName, userPassword).Return(conn, nil)
+	ca.On("GetUser").Return(userName)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Close", context.Background()).Return(nil)
+
+	ca.On("GetUser").Return(userName)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+
+	conn.On("isUserExist", context.Background(), mock.Anything, mock.AnythingOfType("string")).Return(true, nil)
+	conn.On("User", context.Background(), mock.AnythingOfType("string")).Return(user, nil)
+
+	conn.On("Name").Return(dbName, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getOwnerForMetaData).Return(rows, nil)
+	conn.On("Exec", context.Background(), createUser(userName, userPassword)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaCreate(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createSchema(schema)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), alterOwnerMetaTable(emptyUser)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), revokeRights()).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantConnectionToDB(dbName, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantSchemaUsage(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), setAdminGrants(schema, userName)).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), grantUserSQL).Return(pgconn.CommandTag("UPDATED"), nil)
+
+	// multiusers disabled
+	featuresWithMult := map[string]bool{FeatureMultiUsers: false}
+	request := dao.DbCreateRequest{Metadata: map[string]interface{}{
+		"roles": map[string]string{
+			"user1": "admin",
+		}}, NamePrefix: &namePrefix, Password: userPassword, DbName: dbName, Settings: map[string]interface{}{}, Username: userName}
+
+	rows.On("Next").Return(true).Times(4)
+	rows.On("Close").Return(nil)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(8)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, featuresWithMult)
+
+	setting, err := sa.getPgSettings(context.Background(), request.Settings)
+	assert.Nil(t, err)
+
+	conn.On("Exec", context.Background(), createDatabase(dbName, setting.CreationParameters)).Return(pgconn.CommandTag("UPDATED"), errors.New("Error while create database"))
+
+	dbNameExp := dbName
+
+	assert.PanicsWithError(t, "Error while create database", func() {
+		_, _, err := sa.CreateDatabase(context.Background(), request)
+		assert.Error(t, err)
+	})
+
+	fmt.Printf("data : %v\n", dbNameExp)
+
+}
+
+func Test_UpdateMetadata(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetDatabase").Return(Database)
+
+	conn := new(MockConn)
+	rows := new(MockRows)
+
+	ca.On("GetConnectionToDb", Database).Return(conn, nil)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+	// New metadata
+	metadata := map[string]interface{}{
+		cKey: map[string]string{msKey: "test", nsKey: "test-ns"},
+	}
+
+	// Old metadata
+	oldMetadata := map[string]interface{}{
+		RolesKey: map[string]interface{}{
+			"test_user_name": "admin",
+		},
+		cKey: map[string]string{msKey: "testOld", nsKey: "test-ns-old"},
+	}
+
+	// Merged metadata to save
+	resultMetadata := map[string]interface{}{
+		cKey: map[string]string{msKey: "test", nsKey: "test-ns"},
+		RolesKey: map[string]string{
+			"test_user_name": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	metadataJson, errParse := json.Marshal(resultMetadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata")
+		return
+	}
+
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil).Once()
+	rows.On("Next").Return(true).Once()
+	rows.On("Scan", mock.Anything).Run(func(args mock.Arguments) {
+		arg := args.Get(0).([]interface{})
+		strArg1 := arg[0].(*map[string]interface{})
+		*strArg1 = oldMetadata
+	}).Return(nil).Once()
+
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), deleteMetaData).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), insertIntoMetaTable, "metadata", metadataJson).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Exec", context.Background(), createMetaTable).Return(pgconn.CommandTag("UPDATED"), nil)
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Close", context.Background()).Return(nil)
+	rows.On("Close").Return(nil)
+
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Scan", mock.Anything).Return(nil)
+	rows.On("Next").Return(false).Times(2)
+
+	sa.UpdateMetadata(context.Background(), metadata, Database)
+}
+
+func Test_GetMetadataInternal(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetDatabase").Return(Database)
+
+	conn := new(MockConn)
+	rows := new(MockRows)
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"test_user_name": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	ca.On("GetConnectionToDb", Database).Return(conn, nil)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Close", context.Background()).Return(nil)
+	rows.On("Close").Return(nil)
+
+	rows.On("Next").Return(true).Times(2)
+
+	rolesMap := metadata[RolesKey].(map[string]string)
+
+	for userName, role := range rolesMap {
+		rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+			arg := args.Get(0).([]interface{})
+			strArg1 := arg[0].(*map[string]interface{})
+
+			if *strArg1 == nil {
+				*strArg1 = make(map[string]interface{})
+			}
+
+			rolesMap := (*strArg1)[RolesKey]
+			if rolesMap == nil {
+				rolesMap = make(map[string]string)
+				(*strArg1)[RolesKey] = rolesMap
+			}
+
+			rolesMap.(map[string]string)[userName] = role
+			(*strArg1)[RolesVersionKey] = util.GetRolesVersion()
+		}).Times(2)
+	}
+
+	rows.On("Next").Return(false).Times(2)
+
+	_, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata")
+		return
+	}
+
+	fmt.Printf("metadata : %v\n", metadata)
+	fmt.Printf("metadatajsonerror : %v\n", errParse)
+
+	data, _ := sa.GetMetadataInternal(context.Background(), Database)
+	fmt.Printf("data : %v\n", data)
+
+	assert.Equal(t, metadata, data)
+
+}
+
+func TestGetSchema(t *testing.T) {
+
+	expectedSchemaNames := []string{"test_schema_1", "test_schema_2"}
+
+	ca := new(PostgresClusterAdapterMock)
+	conn := new(MockConn)
+	rows := new(MockRows)
+
+	ca.On("GetConnection").Return(conn, nil)
+	ca.On("GetConnectionToDb", Database).Return(conn, nil)
+	fmt.Printf("my ac from connect func connection: %v\n", conn)
+
+	conn.On("Query", context.Background(), getDatabases).Return(rows, nil)
+	conn.On("Query", context.Background(), getSchemasQuery()).Return(rows, nil)
+	conn.On("Close", context.Background()).Return(nil)
+
+	rows.On("Next").Return(true).Times(2)
+	rows.On("Next").Return(false).Once()
+	rows.On("Close").Return(nil)
+
+	for _, schemas := range expectedSchemaNames {
+		schemas := schemas
+		rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+			arg := args.Get(0).([]interface{})
+			strArg := arg[0].(*string)
+			*strArg = schemas
+		}).Once()
+	}
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	desiredSchema, err := sa.GetSchemas(Database)
+
+	fmt.Printf("my schemas from connect func connection: %v\n", desiredSchema)
+	if err != nil {
+		panic(errors.New("This is an explicit error"))
+	}
+	assert.ElementsMatch(t, expectedSchemaNames, desiredSchema)
+}
+
+func TestGetDefaultCreateRequest(t *testing.T) {
+	ca := new(PostgresClusterAdapterMock)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	request := sa.GetDefaultCreateRequest()
+	assert.Equal(t, dao.DbCreateRequest{}, request)
+
+}
+
+func TestGetDefaultUserCreateRequest(t *testing.T) {
+	ca := new(PostgresClusterAdapterMock)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+	request := sa.GetDefaultUserCreateRequest()
+	assert.Equal(t, dao.UserCreateRequest{}, request)
+
+}
+
+func Test_GetMetadata(t *testing.T) {
+
+	ca := new(PostgresClusterAdapterMock)
+	ca.On("GetHost").Return(host)
+	ca.On("GetPort").Return(port)
+	ca.On("GetDatabase").Return(Database)
+
+	conn := new(MockConn)
+	rows := new(MockRows)
+
+	metadata := map[string]interface{}{
+		RolesKey: map[string]string{
+			"test_user_name": "admin",
+		},
+		RolesVersionKey: util.GetRolesVersion(),
+	}
+
+	ca.On("GetConnectionToDb", Database).Return(conn, nil)
+
+	sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+	conn.On("Query", context.Background(), getAllMetadata).Return(rows, nil)
+	conn.On("Query", context.Background(), getMetadata, "metadata").Return(rows, nil)
+	conn.On("Close", context.Background()).Return(nil)
+	rows.On("Close").Return(nil)
+
+	rows.On("Next").Return(true).Times(2)
+
+	rolesMap := metadata["roles"].(map[string]string)
+
+	for userName, role := range rolesMap {
+		rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+			arg := args.Get(0).([]interface{})
+			strArg1 := arg[0].(*map[string]interface{})
+
+			if *strArg1 == nil {
+				*strArg1 = make(map[string]interface{})
+			}
+
+			rolesMap := (*strArg1)["roles"]
+			if rolesMap == nil {
+				rolesMap = make(map[string]string)
+				(*strArg1)["roles"] = rolesMap
+			}
+
+			rolesMap.(map[string]string)[userName] = role
+			(*strArg1)[RolesVersionKey] = util.GetRolesVersion()
+		}).Times(2)
+	}
+
+	rows.On("Next").Return(false).Times(2)
+
+	_, errParse := json.Marshal(metadata)
+	if errParse != nil {
+		log.Error("Error during marshal metadata")
+		return
+	}
+
+	fmt.Printf("metadata : %v\n", metadata)
+	fmt.Printf("metadatajsonerror : %v\n", errParse)
+
+	data := sa.GetMetadata(context.Background(), Database)
+	fmt.Printf("data : %v\n", data)
+
+	assert.Equal(t, metadata, data)
+
+}
+
+func TestValidatePostgresAvailableExtensions(t *testing.T) {
+	t.Run("Returns true if all extensions are available", func(t *testing.T) {
+
+		expectedExtensionNames := []string{"ext1", "ext2"}
+		ca := new(PostgresClusterAdapterMock)
+		ca.On("GetHost").Return(host)
+		ca.On("GetPort").Return(port)
+		ca.On("GetDatabase").Return(Database)
+
+		conn := new(MockConn)
+		rows := new(MockRows)
+
+		ca.On("GetConnection").Return(conn, nil)
+		conn.On("Close", context.Background()).Return(nil)
+		conn.On("Query", context.Background(), selectPostgresAvailableExtensions).Return(rows, nil)
+
+		rows.On("Next").Return(true).Twice()
+
+		for _, exts := range expectedExtensionNames {
+			exts := exts
+			rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+				arg := args.Get(0).([]interface{})
+				strArg := arg[0].(*string)
+				*strArg = exts
+			}).Once()
+		}
+
+		rows.On("Next").Return(false).Once()
+
+		sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+		result, err := sa.ValidatePostgresAvailableExtensions(context.Background(), []string{"ext1", "ext2"})
+		fmt.Printf("available extensions : %v\n", result)
+		assert.NoError(t, err)
+		assert.True(t, result)
+	})
+
+	t.Run("Returns false if an extension is not available", func(t *testing.T) {
+		expectedExtensionNames := []string{"ext1", "ext2"}
+		ca := new(PostgresClusterAdapterMock)
+		ca.On("GetHost").Return(host)
+		ca.On("GetPort").Return(port)
+		ca.On("GetDatabase").Return(Database)
+
+		conn := new(MockConn)
+		rows := new(MockRows)
+
+		ca.On("GetConnection").Return(conn, nil)
+		conn.On("Close", context.Background()).Return(nil)
+		conn.On("Query", context.Background(), selectPostgresAvailableExtensions).Return(rows, nil)
+
+		rows.On("Next").Return(true).Once()
+
+		for _, exts := range expectedExtensionNames {
+			exts := exts
+			rows.On("Scan", mock.Anything).Return(nil).Run(func(args mock.Arguments) {
+				arg := args.Get(0).([]interface{})
+				strArg := arg[0].(*string)
+				*strArg = exts
+			}).Once()
+		}
+
+		rows.On("Next").Return(false).Once()
+
+		sa := NewServiceAdapter(ca, apiVersion, roles, features)
+
+		result, err := sa.ValidatePostgresAvailableExtensions(context.Background(), []string{"ext1", "ext2"})
+		fmt.Printf("available extensions : %v\n", result)
+
+		assert.NoError(t, err)
+		assert.False(t, result)
+	})
+}
+
+func TestCreateExtFromSlice(t *testing.T) {
+	t.Run("Creates extensions successfully", func(t *testing.T) {
+
+		ca := new(PostgresClusterAdapterMock)
+		ca.On("GetHost").Return(host)
+		ca.On("GetPort").Return(port)
+		ca.On("GetDatabase").Return(Database)
+
+		conn := new(MockConn)
+
+		ca.On("GetConnection").Return(conn, nil)
+
+		conn.On("Exec", mock.Anything, mock.Anything, mock.Anything).Return(pgconn.CommandTag(""), nil).Twice()
+		CreateExtFromSlice(context.Background(), conn, "", []string{"ext1", "ext2"})
+
+		conn.AssertCalled(t, "Exec", context.Background(), createExtensionIfNotExist("ext1"), mock.Anything)
+		conn.AssertCalled(t, "Exec", context.Background(), createExtensionIfNotExist("ext2"), mock.Anything)
+	})
+
+	t.Run("Creates extensions successfully when username is nil", func(t *testing.T) {
+
+		ca := new(PostgresClusterAdapterMock)
+		ca.On("GetHost").Return(host)
+		ca.On("GetPort").Return(port)
+		ca.On("GetDatabase").Return(Database)
+
+		conn := new(MockConn)
+		var OracleFdw = "oracle_fdw"
+		userName := "user1"
+
+		ca.On("GetConnection").Return(conn, nil)
+
+		conn.On("Exec", mock.Anything, mock.Anything, mock.Anything).Return(pgconn.CommandTag(""), nil).Times(4)
+		CreateExtFromSlice(context.Background(), conn, userName, []string{"ext1", "ext2", OracleFdw})
+
+		conn.AssertCalled(t, "Exec", context.Background(), createExtensionIfNotExist("ext1"), mock.Anything)
+		conn.AssertCalled(t, "Exec", context.Background(), createExtensionIfNotExist("ext2"), mock.Anything)
+		conn.AssertCalled(t, "Exec", context.Background(), createExtensionIfNotExist(OracleFdw), mock.Anything)
+
+		conn.AssertCalled(t, "Exec", context.Background(), grantRightsForFdw(OracleFdw, userName))
+
+	})
+
+}
diff --git a/docker-dbaas-adapter/adapter/basic/handlers.go b/docker-dbaas-adapter/adapter/basic/handlers.go
new file mode 100644
index 0000000..fa32421
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/handlers.go
@@ -0,0 +1,88 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"context"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/gofiber/fiber/v2"
+	"go.uber.org/zap"
+)
+
+func (sa ServiceAdapter) UpdatePostgreSQLSettingsHandler() func(c *fiber.Ctx) error {
+	return func(c *fiber.Ctx) error {
+		sa.log.Info("Received request to update settings")
+		dbName := c.Params("dbName")
+
+		if !validateDbIdentifierParam(context.Background(), "dbName", dbName, DbIdentifiersPattern) {
+			return sendInvalidParameterResponse(c, "dbName", dbName, DbIdentifiersPattern)
+		}
+
+		var updateSettingsRequest PostgresUpdateSettingsRequest
+		err := c.BodyParser(&updateSettingsRequest)
+		if err != nil {
+			sa.log.Error("Failed to parse request in update settings handler", zap.Error(err))
+			return c.Status(500).SendString(err.Error())
+		}
+		updateResult, err := sa.updateSettings(dbName, updateSettingsRequest)
+		if err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+
+		if updateResult.isStatusExists(Failed) {
+			return c.Status(500).JSON(updateResult)
+		}
+
+		if updateResult.isStatusExists(BadRequest) {
+			return c.Status(400).JSON(updateResult)
+		}
+
+		return c.JSON(updateResult)
+	}
+}
+
+func (sa ServiceAdapter) GetDatabaseOwnerHandler() func(c *fiber.Ctx) error {
+	ctx := context.Background()
+	logger := util.ContextLogger(ctx)
+	return func(c *fiber.Ctx) error {
+		logger.Info("Received request to get database info")
+		dbName := c.Params("dbName")
+
+		if dbName == "" || !validateDbIdentifierParam(ctx, "dbName", dbName, DbIdentifiersPattern) {
+			return sendInvalidParameterResponse(c, "dbName", dbName, DbIdentifiersPattern)
+		}
+
+		userName, err := sa.getDBInfo(ctx, dbName)
+		if err != nil {
+			return c.Status(500).SendString(fmt.Sprintf("Cannot get info for %s", dbName))
+		}
+		return c.JSON(userName)
+	}
+}
+
+func sendInvalidParameterResponse(c *fiber.Ctx, paramName string, paramValue string, pattern string) error {
+	return c.Status(400).SendString(fmt.Sprintf("Invalid '%s' param provided: %s. '%s' param must comply to the pattern %s", paramName, paramValue, paramName, pattern))
+}
+
+func (sr PostgresUpdateSettingsResult) isStatusExists(status string) bool {
+	for _, result := range sr.SettingUpdateResult {
+		if result.Status == status {
+			return true
+		}
+	}
+	return false
+}
diff --git a/docker-dbaas-adapter/adapter/basic/sql_helper.go b/docker-dbaas-adapter/adapter/basic/sql_helper.go
new file mode 100644
index 0000000..bee5cc4
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/sql_helper.go
@@ -0,0 +1,312 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"fmt"
+	"strconv"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+)
+
+const (
+	DbPrefixPattern      = `^(_|[a-z])[\w\d_]{0,29}$`
+	DbIdentifiersPattern = `^(_|[a-z])[\w\d_-]{0,62}$`
+	OracleFdw            = "oracle_fdw"
+	Dblink               = "dblink"
+)
+
+var protectedRoles = []string{"'rdsadmin'", "'cloudsqladmin'", "'azuresu'"}
+var dblinkFuncs = []string{"dblink_connect_u(text)", "dblink_connect_u(text, text)"}
+
+var (
+	crParamsKeys = []string{"OWNER", "TEMPLATE", "ENCODING", "LOCALE", "LC_COLLATE",
+		"LC_CTYPE", "TABLESPACE", "ALLOW_CONNECTIONS", "CONNECTION LIMIT", "IS_TEMPLATE"}
+	getDatabases                      = "SELECT DATNAME FROM pg_database;"
+	deleteMetaData                    = "DELETE FROM _DBAAS_METADATA"
+	insertIntoMetaTable               = "INSERT INTO _DBAAS_METADATA VALUES ($1, $2)"
+	selectPostgresAvailableExtensions = "SELECT NAME FROM PG_AVAILABLE_EXTENSIONS"
+	createMetaTable                   = "CREATE TABLE IF NOT EXISTS public._DBAAS_METADATA (key varchar(256) PRIMARY KEY, value json)"
+	getUser                           = "SELECT 1 FROM pg_roles WHERE rolname = $1;"
+	getDatabase                       = "SELECT 1 FROM pg_database WHERE datname = $1;"
+	dropConnectionsToDb               = "SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE datname = $1;"
+	terminateListenConnections        = "SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid() AND (query LIKE '%listen%' OR query LIKE '%LISTEN%') AND wait_event = 'ClientWrite' AND state_change IS NOT NULL AND (now() - pg_stat_activity.state_change) > interval '1 second';"
+	prohibitConnectionsToDb           = "UPDATE pg_database SET datallowconn = false WHERE datname = $1;"
+	prohibitConnectionsToDbExt        = "ALTER DATABASE \"%s\" WITH ALLOW_CONNECTIONS false;"
+	getDependenceDbForRole            = "SELECT distinct datname FROM pg_database WHERE has_database_privilege($1, datname, 'CREATE') or oid in (select dbid from pg_shdepend join pg_roles on pg_shdepend.refobjid = pg_roles.oid where rolname =$2);"
+	dropUserConnectionsToDb           = "SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE usename = $1;"
+	getOwnerForMetaData               = "select tableowner from pg_tables where tablename = '_dbaas_metadata';"
+	getMetadata                       = "SELECT value FROM _DBAAS_METADATA where key = $1;"
+	getAllMetadata                    = "SELECT * FROM _DBAAS_METADATA;"
+	getConnectionLimitFromTemplate1   = "select datconnlimit::text from pg_database where datname = 'template1';"
+	existsRollbackTransactions        = "SELECT EXISTS (SELECT 1 FROM pg_prepared_xacts WHERE database = $1);"
+	selectPreparedTransactions        = "SELECT gid FROM pg_prepared_xacts WHERE database = $1;"
+)
+
+func createDatabase(dbName string, parameters map[string]string) string {
+	createStr := fmt.Sprintf("CREATE DATABASE \"%s\"", escapeInputValue(dbName))
+	if len(parameters) > 0 {
+		createStr += " WITH"
+		for k, v := range parameters {
+			if k == "connection limit" {
+				intVal, _ := strconv.Atoi(v)
+				createStr += fmt.Sprintf(" %s=%d", k, intVal)
+			} else {
+				createStr += fmt.Sprintf(" %s='%s'", k, escapeInputValue(v))
+			}
+		}
+	}
+	createStr += ";"
+	return createStr
+}
+
+func createUser(userName string, password string) string {
+	return fmt.Sprintf("CREATE USER \"%s\" WITH PASSWORD '%s'", userName, password)
+}
+
+func createSchema(schema string) string {
+	return fmt.Sprintf("CREATE SCHEMA IF NOT EXISTS \"%s\"", escapeInputValue(schema))
+}
+
+func grantAllRightsOnDatabase(dbName string, userName string) string {
+	return fmt.Sprintf("GRANT ALL PRIVILEGES ON DATABASE \"%s\" TO \"%s\";", dbName, userName)
+}
+
+func grantSchemaCreate(schema, userName string) string {
+	return fmt.Sprintf("GRANT CREATE ON SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func grantSchemaUsage(schema, userName string) string {
+	return fmt.Sprintf("GRANT USAGE ON SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func revokeRights() string {
+	return "REVOKE CREATE ON SCHEMA public FROM public;"
+}
+
+func dropDatabase(dbName string) string {
+	return fmt.Sprintf("DROP DATABASE IF EXISTS \"%s\";", dbName)
+}
+
+func dropUser(userName string) string {
+	return fmt.Sprintf("DROP USER IF EXISTS \"%s\";", userName)
+}
+
+func alterOwnerMetaTable(userName string) string {
+	return fmt.Sprintf("ALTER TABLE _DBAAS_METADATA OWNER TO \"%s\"", userName)
+}
+
+func alterOwnerForTable(schema, tableName, userName string) string {
+	return fmt.Sprintf("ALTER TABLE \"%s\".\"%s\" OWNER TO \"%s\"", schema, tableName, userName)
+}
+
+func alterOwnerForSequence(schema, sequenceName, newOwner string) string {
+	return fmt.Sprintf(`ALTER SEQUENCE "%s"."%s" OWNER TO "%s"`, schema, sequenceName, newOwner)
+}
+
+func alterOwnerForLargeObject(schema, oid, userName string) string {
+	return fmt.Sprintf("ALTER LARGE OBJECT %s OWNER TO \"%s\";", oid, userName) //TODO:
+}
+
+func alterOwnerForSchema(schema, userName string) string {
+	return fmt.Sprintf("ALTER SCHEMA \"%s\" OWNER TO \"%s\"", schema, userName)
+}
+
+func alterDatabaseOwnerQuery(dbName, userName string) string {
+	return fmt.Sprintf("ALTER DATABASE \"%s\" OWNER TO \"%s\";", dbName, userName)
+}
+
+func alterProcedureOwnerQuery(schema, procedure, userName string) string {
+	return fmt.Sprintf("ALTER PROCEDURE %s OWNER TO \"%s\"", procedure, userName)
+}
+
+func alterFunctionOwnerQuery(schema, procedure, userName string) string {
+	return fmt.Sprintf("ALTER FUNCTION %s OWNER TO \"%s\"", procedure, userName)
+}
+
+func alterViewOwnerQuery(schema, view, userName string) string {
+	return fmt.Sprintf("ALTER VIEW \"%s\".\"%s\" OWNER TO \"%s\"", schema, view, userName)
+}
+
+func alterTypeOwnerQuery(schema, typeForAlter, userName string) string {
+	return fmt.Sprintf("ALTER TYPE \"%s\".\"%s\" OWNER TO \"%s\"", schema, typeForAlter, userName)
+}
+
+func createExtensionIfNotExist(extension string) string {
+	return fmt.Sprintf("CREATE EXTENSION IF NOT EXISTS \"%s\" CASCADE;", extension)
+}
+
+func grantRightsForFdw(fdwname string, userName string) string {
+	return fmt.Sprintf("GRANT ALL ON FOREIGN DATA WRAPPER \"%s\" to \"%s\";", fdwname, userName)
+}
+
+func changeUserPassword(userName string, password string) string {
+	return fmt.Sprintf("ALTER USER \"%s\" WITH PASSWORD '%s' ;", userName, password)
+}
+
+func deleteExtension(extension string) string {
+	return fmt.Sprintf("DROP EXTENSION IF EXISTS \"%s\";", extension)
+}
+
+func allowConnectionsToDb(dbName string) string {
+	return fmt.Sprintf("UPDATE pg_database SET datallowconn = true WHERE datname = '%s';", dbName)
+}
+
+func allowConnectionsToDbExt(dbName string) string {
+	return fmt.Sprintf("ALTER DATABASE \"%s\" WITH ALLOW_CONNECTIONS true;", dbName)
+}
+
+func dropOldDatabase(dbName string) string {
+	return fmt.Sprintf("DROP DATABASE IF EXISTS \"%s\";", strings.Replace(dbName, "\"", "\\\"", -1))
+}
+
+func reassignGrants(userName string, tempRole string) string {
+	return fmt.Sprintf("REASSIGN OWNED BY \"%s\" TO \"%s\";", userName, tempRole)
+}
+
+func dropOwnedObjects(userName string) string {
+	return fmt.Sprintf("DROP OWNED BY \"%s\";", userName)
+}
+
+func grantUserToAdmin(userName string, adminName string) string {
+	return fmt.Sprintf("GRANT \"%s\" TO \"%s\";", userName, adminName)
+}
+
+func escapeInputValue(value string) string {
+	result := strings.ReplaceAll(value, `"`, `""`)
+	return strings.ReplaceAll(result, "'", "''")
+}
+
+func setReadOnlyRoleGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT SELECT ON ALL TABLES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setAdminRoleSequencesGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT ALL ON ALL SEQUENCES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setReadWriteRoleGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setReadWriteRoleSequencesGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setReadOnlyRoleSequencesGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT SELECT ON ALL SEQUENCES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setAdminGrants(schema string, userName string) string {
+	return fmt.Sprintf("GRANT ALL ON ALL TABLES IN SCHEMA \"%s\" TO \"%s\";", schema, userName)
+}
+
+func setReadOnlyRoleDefaultGrants(owner, userName string) string {
+	return fmt.Sprintf("ALTER DEFAULT PRIVILEGES for role \"%s\" GRANT SELECT ON TABLES TO \"%s\";", owner, userName)
+}
+
+func setReadWriteRoleDefaultGrants(owner, userName string) string {
+	return fmt.Sprintf("ALTER DEFAULT PRIVILEGES for role \"%s\" GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO \"%s\";", owner, userName)
+}
+
+func setSchemasDefaultGrants(adminUser, userName string) string {
+	return fmt.Sprintf("ALTER DEFAULT PRIVILEGES for role \"%s\" GRANT USAGE ON SCHEMAS TO \"%s\";", adminUser, userName)
+}
+
+func setSequencesRODefaultGrants(adminUser, userName string) string {
+	return fmt.Sprintf("ALTER DEFAULT PRIVILEGES for role \"%s\" GRANT SELECT ON SEQUENCES TO \"%s\";", adminUser, userName)
+}
+
+func setSequencesRWDefaultGrants(adminUser, userName string) string {
+	return fmt.Sprintf("ALTER DEFAULT PRIVILEGES for role \"%s\" GRANT USAGE, SELECT ON SEQUENCES TO \"%s\";", adminUser, userName)
+}
+
+func setGrantAsForRole(owner, userName string) string {
+	return fmt.Sprintf("GRANT \"%s\" TO \"%s\";", owner, userName)
+}
+
+func grantConnectionToDB(dbName, userName string) string {
+	return fmt.Sprintf("GRANT CONNECT ON DATABASE \"%s\" TO \"%s\";", dbName, userName)
+}
+
+func AllowReplicationForUser(userName string) string {
+	if strings.ToUpper(util.ExternalPostgreSQLType) == "RDS" {
+		return fmt.Sprintf("GRANT rds_replication TO \"%s\";", userName)
+	}
+	return fmt.Sprintf("ALTER USER \"%s\" WITH REPLICATION;", userName)
+}
+
+func getTablesListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname = '%s' and tablename != '_dbaas_metadata'", schema)
+	return getExtPreparedQuery(baseQuery, fmt.Sprintf("tableowner not in (%s)", strings.Join(protectedRoles, ",")), external)
+}
+
+func getSequenceListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("SELECT sequencename FROM pg_sequences WHERE schemaname = '%s'", schema)
+	return getExtPreparedQuery(baseQuery, fmt.Sprintf("sequenceowner not in (%s)", strings.Join(protectedRoles, ",")), external)
+}
+
+func getLargeObjectsListQuery(external bool) string {
+	baseQuery := "SELECT oid::TEXT FROM pg_catalog.pg_largeobject_metadata"
+	if external {
+		baseQuery += " where lomowner != 10"
+	}
+	return fmt.Sprintf("%s;", baseQuery)
+}
+
+func getViewsListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("select viewname from pg_catalog.pg_views where schemaname='%s'", schema)
+	return getExtPreparedQuery(baseQuery, fmt.Sprintf("viewowner not in (%s)", strings.Join(protectedRoles, ",")), external)
+}
+
+func getProceduresListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("SELECT p.oid::regprocedure FROM pg_catalog.pg_namespace n JOIN pg_catalog.pg_proc p ON p.pronamespace = n.oid WHERE p.prokind = 'p' AND n.nspname = '%s'", schema)
+	return getExtPreparedQuery(baseQuery, "p.proowner != 10", external)
+}
+
+func getFunctionsListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("SELECT p.oid::regprocedure FROM pg_catalog.pg_namespace n JOIN pg_catalog.pg_proc p ON p.pronamespace = n.oid WHERE p.prokind <> 'p' AND n.nspname = '%s' and not (n.nspname = 'public' and p.oid::regprocedure::text='lookup(name)')", schema)
+	return getExtPreparedQuery(baseQuery, "p.proowner != 10", external)
+}
+
+func getCustomTypesListQuery(schema string, external bool) string {
+	baseQuery := fmt.Sprintf("SELECT t.typname FROM pg_type t LEFT JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE (t.typrelid = 0 OR (SELECT c.relkind = 'c' FROM pg_catalog.pg_class c WHERE c.oid = t.typrelid)) AND NOT EXISTS(SELECT 1 FROM pg_catalog.pg_type el WHERE el.oid = t.typelem AND el.typarray = t.oid) AND n.nspname = '%s'", schema)
+	return getExtPreparedQuery(baseQuery, "t.typowner != 10", external)
+}
+
+func getMetadataGrantQuery(grantee, privilege string) string {
+	return fmt.Sprintf("select 1 from information_schema.role_table_grants where table_name='_dbaas_metadata' and grantee='%s' and privilege_type='%s';", escapeInputValue(grantee), privilege)
+}
+
+func getSchemasQuery() string {
+	return "SELECT schema_name from information_schema.schemata WHERE NOT schema_name LIKE 'pg_%' AND schema_name <> 'information_schema';"
+}
+
+func getExtPreparedQuery(originalQuery, additionalCond string, external bool) string {
+	baseQuery := originalQuery
+	if external {
+		baseQuery += fmt.Sprintf(" and %s", additionalCond) // exclude external superuser owned functions
+	}
+	return fmt.Sprintf("%s;", baseQuery)
+}
+
+func setGrantOnFunction(funcName, user string) string {
+	return fmt.Sprintf("GRANT EXECUTE ON FUNCTION %s TO \"%s\";", funcName, escapeInputValue(user))
+}
+
+func rollbackPreparedByGid(idTrans string) string {
+	return fmt.Sprintf("ROLLBACK PREPARED '%s';", idTrans)
+}
diff --git a/docker-dbaas-adapter/adapter/basic/type.go b/docker-dbaas-adapter/adapter/basic/type.go
new file mode 100644
index 0000000..6bad18c
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/basic/type.go
@@ -0,0 +1,63 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package basic
+
+import (
+	"sync"
+
+	"go.uber.org/zap"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+)
+
+type ServiceAdapter struct {
+	cluster.ClusterAdapter
+	Mutex *sync.Mutex
+	*ExtensionConfig
+	dao.ApiVersion
+	roles    []string
+	features map[string]bool
+	Generator
+	log *zap.Logger
+}
+
+type ExtensionConfig struct {
+	DefaultExt        []string
+	ExtUpdateRequired bool
+}
+
+type PostgresUpdateSettingsResult struct {
+	SettingUpdateResult map[string]SettingUpdateResult `json:"settingUpdateResult"`
+}
+
+type PostgresUpdateSettingsRequest struct {
+	CurrentSettings map[string]interface{} `json:"currentSettings"`
+	NewSettings     map[string]interface{} `json:"newSettings"`
+}
+
+type SettingUpdateResult struct {
+	Status  string `json:"status"`
+	Message string `json:"message"`
+}
+
+type DbInfo struct {
+	Owner string `json:"owner"`
+}
+
+type DbResource struct {
+	SelectQuery        string
+	AlterQueryFunction func(string, string, string) string
+}
diff --git a/docker-dbaas-adapter/adapter/cluster/cluster.go b/docker-dbaas-adapter/adapter/cluster/cluster.go
new file mode 100644
index 0000000..4dd9cfc
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/cluster/cluster.go
@@ -0,0 +1,178 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package cluster
+
+import (
+	"context"
+	"fmt"
+	"net/url"
+	"time"
+
+	"github.com/jackc/pgx/v4"
+	"github.com/jackc/pgx/v4/pgxpool"
+	"go.uber.org/zap"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/jackc/pgconn"
+)
+
+const (
+	HealthUP  = "UP"
+	HealthOOS = "OUT_OF_SERVICE"
+)
+
+var (
+	log         = util.GetLogger()
+	connTimeout = time.Duration(util.GetEnvInt("PG_CONN_TIMEOUT_SEC", 20))
+)
+
+type Conn interface {
+	Query(ctx context.Context, sql string, args ...interface{}) (pgx.Rows, error)
+	Exec(ctx context.Context, sql string, arguments ...interface{}) (pgconn.CommandTag, error)
+	Close()
+	QueryRow(ctx context.Context, sql string, args ...interface{}) pgx.Row
+}
+
+type ClusterAdapter interface {
+	GetConnection(ctx context.Context) (Conn, error)
+	GetConnectionToDb(ctx context.Context, database string) (Conn, error)
+	GetConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error)
+	GetUser() string
+	GetPassword() string
+	GetHost() string
+	GetPort() int
+}
+
+type ClusterAdapterImpl struct {
+	Pool     *pgxpool.Pool
+	Host     string
+	Port     int
+	SSl      string
+	User     string
+	Password string
+	Database string
+	Health   string
+	log      *zap.Logger
+}
+
+func NewAdapter(host string, port int, username, password string, database string, ssl string) *ClusterAdapterImpl {
+	username = url.PathEscape(username)
+	password = url.PathEscape(password)
+
+	c := &ClusterAdapterImpl{
+		Host:     host,
+		Port:     port,
+		User:     username,
+		Password: password,
+		SSl:      ssl,
+		Health:   HealthUP,
+		Database: database,
+		log:      util.GetLogger(),
+	}
+	log.Debug(fmt.Sprintf("Checking connection for host=%s port=%d with database %s", host, port, database))
+	c.RequestHealth()
+	return c
+}
+
+func (ca ClusterAdapterImpl) RequestHealth() string {
+	ch := make(chan string, 1)
+	go func() {
+		ch <- ca.getHealth()
+	}()
+
+	select {
+	case healthStatus := <-ch:
+		if healthStatus == HealthOOS {
+			panic(fmt.Errorf("postgres is unavailable"))
+		}
+		ca.Health = healthStatus
+	case <-time.After(connTimeout * time.Second):
+		panic("postgres connection timeout expired")
+	}
+
+	return ca.Health
+}
+
+func (ca ClusterAdapterImpl) GetPort() int {
+	return ca.Port
+}
+
+func (ca ClusterAdapterImpl) GetUser() string {
+	return ca.User
+}
+
+func (ca ClusterAdapterImpl) GetPassword() string {
+	return ca.Password
+}
+
+func (ca ClusterAdapterImpl) GetHost() string {
+	return ca.Host
+}
+
+func (ca ClusterAdapterImpl) GetConnection(ctx context.Context) (Conn, error) {
+	return ca.GetConnectionToDb(ctx, ca.Database)
+}
+
+func (ca ClusterAdapterImpl) GetConnectionToDb(ctx context.Context, database string) (Conn, error) {
+	if database == "" {
+		database = ca.Database
+	}
+	return ca.GetConnectionToDbWithUser(ctx, database, ca.GetUser(), ca.GetPassword())
+}
+
+func (ca ClusterAdapterImpl) GetConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error) {
+	return ca.getConnectionToDbWithUser(ctx, database, username, password)
+}
+
+func (ca ClusterAdapterImpl) getConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error) {
+	conn, err := pgxpool.Connect(ctx, ca.getConnectionUrl(username, password, database))
+	if err != nil {
+		log.Error("Error occurred during connect to DB", zap.Error(err))
+		return nil, err
+	}
+	return conn, nil
+}
+
+func (ca ClusterAdapterImpl) getConnectionUrl(username string, password string, database string) string {
+	if ca.SSl == "on" {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s?%s", username, password, ca.Host, ca.GetPort(), database, "sslmode=require")
+	} else {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s", username, password, ca.Host, ca.GetPort(), database)
+	}
+}
+
+func (ca ClusterAdapterImpl) getHealth() string {
+	err := ca.executeHealthQuery()
+	if err != nil {
+		log.Error("Postgres is unavailable", zap.Error(err))
+		return HealthOOS
+	} else {
+		return HealthUP
+	}
+}
+
+func (ca ClusterAdapterImpl) executeHealthQuery() error {
+	ctx, cancel := context.WithTimeout(context.Background(), connTimeout*time.Second)
+	defer cancel()
+
+	conn, err := ca.getConnectionToDbWithUser(ctx, ca.Database, ca.User, ca.Password)
+	if err != nil {
+		return err
+	}
+	defer conn.Close()
+
+	_, err = conn.Exec(ctx, "SELECT * FROM pg_catalog.pg_tables")
+	return err
+}
diff --git a/docker-dbaas-adapter/adapter/initial/cm_manager.go b/docker-dbaas-adapter/adapter/initial/cm_manager.go
new file mode 100644
index 0000000..3707b4d
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/initial/cm_manager.go
@@ -0,0 +1,57 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package initial
+
+import (
+	"context"
+	"fmt"
+	"os"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"go.uber.org/zap"
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	v12 "k8s.io/client-go/kubernetes/typed/core/v1"
+)
+
+var log = util.GetLogger()
+
+func getCM(cmName string) *v1.ConfigMap {
+	CMs := getCMListForNamespace()
+	cm, err := CMs.Get(context.Background(), cmName, metav1.GetOptions{})
+	if err != nil {
+		log.Error(fmt.Sprintf("Couldn't get %s config map", cmName), zap.Error(err))
+	}
+	return cm
+}
+
+func updateCM(cm *v1.ConfigMap) {
+	CMs := getCMListForNamespace()
+	_, err := CMs.Update(context.Background(), cm, metav1.UpdateOptions{})
+	if err != nil {
+		log.Error("Couldn't update config map", zap.Error(err))
+	} else {
+		log.Info("ConfigMap updated", zap.String("cm", cm.Name))
+	}
+}
+
+func getCMListForNamespace() v12.ConfigMapInterface {
+	client, err := util.GetK8sClient()
+	if err != nil {
+		return nil
+	}
+	CMs := client.CoreV1().ConfigMaps(os.Getenv("CLOUD_NAMESPACE"))
+	return CMs
+}
diff --git a/docker-dbaas-adapter/adapter/initial/extensions.go b/docker-dbaas-adapter/adapter/initial/extensions.go
new file mode 100644
index 0000000..9a03c61
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/initial/extensions.go
@@ -0,0 +1,118 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package initial
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/basic"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/service"
+	"go.uber.org/zap"
+)
+
+const SelectDbUsers = "SELECT USENAME FROM pg_catalog.pg_user where USENAME <> $1;"
+
+func UpdateExtensions(adAdministration service.DbAdministration) {
+	adapter := adAdministration.(*basic.ServiceAdapter)
+
+	if !adapter.ExtUpdateRequired || len(adapter.DefaultExt) == 0 {
+		return
+	}
+
+	log.Info(fmt.Sprintf("Extensions %s will be applied to all databases", adapter.DefaultExt))
+	valid, err := adapter.ValidatePostgresAvailableExtensions(context.Background(), adapter.DefaultExt)
+	if err != nil || !valid {
+		return
+	}
+
+	databases := adapter.GetDatabases(context.Background())
+	ctx := context.Background()
+	for _, database := range databases {
+		log.Debug(fmt.Sprintf("Create extensions for %s", database))
+		createDatabaseExt(adapter, ctx, database)
+	}
+
+	updateExtensionsCM()
+}
+
+func createDatabaseExt(adapter *basic.ServiceAdapter, ctx context.Context, database string) {
+	conn, err := adapter.GetConnectionToDb(context.Background(), database)
+	if err != nil {
+		log.Warn(fmt.Sprintf("Database %s has been skipped for extensions update", database))
+		return
+	}
+	defer conn.Close()
+	log.Info(fmt.Sprintf("Create extensions for \"%s\" database", database))
+
+	username := ""
+	if util.Contains(adapter.DefaultExt, basic.OracleFdw) {
+		username = findUserName(ctx, conn, database)
+	}
+
+	basic.CreateExtFromSlice(ctx, conn, username, adapter.DefaultExt)
+}
+
+func findUserName(ctx context.Context, conn cluster.Conn, database string) string {
+	userName := ""
+	row := conn.QueryRow(ctx, SelectDbUsers)
+	err := row.Scan(&userName)
+	if err != nil {
+		log.Warn(fmt.Sprintf("Error during get owner for db %s", database), zap.Error(err))
+	}
+
+	return userName
+}
+
+func updateExtensionsCM() {
+	log.Debug("Update extensions CM")
+	names := []string{basic.ExtensionConfigNameNew, basic.ExtensionConfigNameOld}
+
+	for _, name := range names {
+		cm := getCM(name)
+		if cm == nil {
+			continue
+		}
+
+		data := cm.Data
+		if data == nil {
+			data = map[string]string{}
+		}
+
+		extensionsData := data[basic.ExtensionName]
+
+		var extensionMap map[string]interface{}
+		err := json.Unmarshal([]byte(extensionsData), &extensionMap)
+		if err != nil {
+			log.Warn(fmt.Sprintf("Failed to parse extensions file %s", basic.ExtensionPath+basic.ExtensionName), zap.Error(err))
+			extensionMap = make(map[string]interface{})
+		}
+
+		extensionMap[basic.UpdateRequiredKey] = false
+
+		updatedMap, err := json.Marshal(extensionMap)
+		if err != nil {
+			log.Warn("Failed to marshal updated result for extensions map")
+		}
+
+		data[basic.ExtensionName] = string(updatedMap)
+
+		cm.Data = data
+		updateCM(cm)
+	}
+}
diff --git a/docker-dbaas-adapter/adapter/initial/migration.go b/docker-dbaas-adapter/adapter/initial/migration.go
new file mode 100644
index 0000000..9d31a37
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/initial/migration.go
@@ -0,0 +1,150 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package initial
+
+import (
+	"context"
+	"fmt"
+	"sync"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/basic"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/service"
+	"go.uber.org/zap"
+	"golang.org/x/exp/maps"
+)
+
+const (
+	MigrationKey = "isMigrationPerformed"
+	limitPool    = 10
+)
+
+var excludedDatabases = []string{"template0", "template1", "postgres", "powa", "rdsadmin", "cloudsqladmin", "azure_maintenance", "azure_sys"}
+var wg sync.WaitGroup
+
+// PerformMigration Set replication privileges on admin user for each database
+func PerformMigration(dbAdministration service.DbAdministration) {
+	adapter := dbAdministration.(*basic.ServiceAdapter)
+
+	if isEnabled := adapter.GetFeatures()[basic.FeatureMultiUsers]; !isEnabled {
+		log.Info("Multiusers feature is disabled or api version is v1, skip migration...")
+		return
+	}
+	log.Info("Migration will be performed for all roles")
+	databases := adapter.GetDatabases(context.Background())
+	ctx := context.Background()
+	connCount := 0
+	for _, database := range databases {
+		wg.Add(1)
+		connCount++
+		go processDatabaseForMigration(ctx, adapter, database)
+		if connCount >= limitPool {
+			wg.Wait()
+			connCount = 0
+		}
+	}
+	wg.Wait()
+}
+
+func processDatabaseForMigration(ctx context.Context, adapter *basic.ServiceAdapter, database string) {
+	defer wg.Done()
+	if isExcludedDatabase(database) {
+		return
+	}
+
+	// Prepare metadata
+	originalMetadata, err := adapter.GetMetadataInternal(ctx, database)
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot get metadata for %s", database), zap.Error(err))
+		return
+	}
+	if _, ok := originalMetadata[basic.RolesVersionKey]; !ok {
+		originalMetadata[basic.RolesVersionKey] = 0.0
+	}
+
+	if originalMetadata[basic.RolesKey] == nil {
+		log.Warn(fmt.Sprintf("roles are empty for %s database, migration skipped...", database))
+		return
+	}
+
+	metadata := cloneMetadata(originalMetadata)
+
+	if isMigrationRequiredForDb(metadata) {
+		migrationPerformed := performAlteringRoles(ctx, adapter, database, metadata)
+		if migrationPerformed {
+			updateRolesVersionInMetadata(ctx, adapter, database, originalMetadata)
+		} else {
+			err := adapter.UpdateMetadataInternal(ctx, originalMetadata, database)
+			if err != nil {
+				log.Error(fmt.Sprintf("cannot save original metadata for %s", database), zap.Error(err))
+			}
+			log.Error(fmt.Sprintf("Migration is not fully performed for %s", database))
+		}
+	} else {
+		log.Info(fmt.Sprintf("Database roles migration is not required for %s", database))
+	}
+}
+
+func cloneMetadata(metadataOrig map[string]interface{}) map[string]interface{} {
+	metadata := maps.Clone(metadataOrig)
+	metadata[basic.RolesKey] = maps.Clone(metadataOrig[basic.RolesKey].(map[string]interface{}))
+	return metadata
+}
+
+func isExcludedDatabase(database string) bool {
+	for _, excDatname := range excludedDatabases {
+		if database == excDatname {
+			return true
+		}
+	}
+	return false
+}
+
+func updateRolesVersionInMetadata(ctx context.Context, adapter *basic.ServiceAdapter, database string, metadata map[string]interface{}) {
+	metadata[basic.RolesVersionKey] = util.GetRolesVersion()
+	err := adapter.UpdateMetadataInternal(ctx, metadata, database)
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot save original metadata for %s", database), zap.Error(err))
+	}
+	log.Info(fmt.Sprintf("Roles successfully updated for database %s", database))
+}
+
+func isMigrationRequiredForDb(metadata map[string]interface{}) bool {
+	if rolesVersionInterface, ok := metadata[basic.RolesVersionKey]; ok {
+		return rolesVersionInterface.(float64) < util.GetRolesVersion()
+	}
+	return true
+}
+
+func performAlteringRoles(ctx context.Context, dbAdministration *basic.ServiceAdapter, database string, metadata map[string]interface{}) bool {
+	log.Info(fmt.Sprintf("Start migration for database %s", database))
+	roles := dbAdministration.GetRolesFromMetadata(metadata)
+	validRoles, err := dbAdministration.GetValidRolesFromMetadata(ctx, database, metadata)
+	if err != nil {
+		log.Warn(fmt.Sprintf("Can't obtain roles for database %s", database))
+		return false
+	}
+
+	err = dbAdministration.GrantUsersForAllSchemas(ctx, database, validRoles)
+	if err != nil {
+		log.Warn(fmt.Sprintf("cannot perform roles grant for database %s", database), zap.Error(err))
+		return false
+	}
+	return maps.Equal(roles, validRoles)
+}
+
+func IsRoleUpdateRequired() bool {
+	return util.GetEnvBool("IS_ROLE_UPDATE_REQUIRED", false)
+}
diff --git a/docker-dbaas-adapter/adapter/main.go b/docker-dbaas-adapter/adapter/main.go
new file mode 100644
index 0000000..ac13a39
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/main.go
@@ -0,0 +1,314 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"context"
+	"encoding/json"
+	"flag"
+	"fmt"
+	"log"
+	"os"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/backup"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/basic"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/cluster"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/initial"
+	"github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter/adapter/util"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dao"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/dbaas"
+	fiber2 "github.com/Netcracker/qubership-dbaas-adapter-core/pkg/impl/fiber"
+	"github.com/Netcracker/qubership-dbaas-adapter-core/pkg/service"
+	coreUtils "github.com/Netcracker/qubership-dbaas-adapter-core/pkg/utils"
+	fiber "github.com/gofiber/fiber/v2"
+	"go.uber.org/zap"
+)
+
+const (
+	appName = "postgresql"
+	appPath = "/" + appName
+)
+
+var (
+	logger = util.GetLogger()
+
+	apiVersion = "v1"
+
+	pgHost     = flag.String("pg_host", util.GetEnv("POSTGRES_HOST", "127.0.0.1"), "Host of PostgreSQL cluster, env: POSTGRES_HOST")
+	pgPort     = flag.Int("pg_port", util.GetEnvInt("POSTGRES_PORT", 5432), "Port of PostgreSQL cluster, env: POSTGRES_PORT")
+	pgUser     = flag.String("pg_user", util.GetEnv("POSTGRES_ADMIN_USER", "postgres"), "Username of dbaas user in PostgreSQL, env: POSTGRES_ADMIN_USER")
+	pgPass     = flag.String("pg_pass", util.GetEnv("POSTGRES_ADMIN_PASSWORD", ""), "Password of dbaas user in PostgreSQL, env: POSTGRES_ADMIN_PASSWORD")
+	pgDatabase = flag.String("pg_database", util.GetEnv("POSTGRES_DATABASE", "postgres"), "PostgreSQL database, env: POSTGRES_DATABASE")
+	pgSsl      = flag.String("pg_ssl", util.GetEnv("PG_SSL", "off"), "Enable ssl connection to postgreSQL, env: PG_SSL")
+
+	pgBackupAuth          = flag.Bool("backup_auth", util.GetEnvBool("AUTH", false), "Backup auth, env: AUTH")
+	pgBackupDaemonAddress = flag.String("daemon_address", util.GetEnv("POSTGRES_BACKUP_DAEMON_ADDRESS", "http://postgres-backup-daemon:9000"), "Host of PostgreSQL cluster, env: POSTGRES_BACKUP_DAEMON_ADDRESS")
+	pgBackupKeep          = flag.String("backup_keep", util.GetEnv("POSTGRES_BACKUP_KEEP", "1 week"), "Host of PostgreSQL cluster, env: POSTGRES_BACKUP_KEEP")
+
+	readOnlyHost   = flag.String("readonly_host", util.GetEnv("READONLY_HOST", "127.0.0.1"), " ReadOnly Host of PostgreSQL cluster, env: READONLY_HOST")
+	vaultEnabled   = flag.Bool("vault_enabled", util.GetEnvBool("VAULT_ENABLED", false), "Enable vault, env: VAULT_ENABLED")
+	vaultAddress   = flag.String("vault_address", util.GetEnv("VAULT_ADDR", ""), "Vault address, env: VAULT_ADDR")
+	vaultRole      = flag.String("vault_role", util.GetEnv("VAULT_ROLE", "postgres-sa"), "Vault role, env: VAULT_ROLE")
+	vaultRotPeriod = flag.String("vault_rotation_period", util.GetEnv("VAULT_ROTATION_PERIOD", "3600"), "Vault rotation period, env: VAULT_ROTATION_PERIOD")
+	vaultDBName    = flag.String("vault_db_engine_name", util.GetEnv("VAULT_DB_ENGINE_NAME", "postgresql"), "Vault db engine name, env: VAULT_DB_ENGINE_NAME")
+	namespace      = flag.String("namespace", util.GetEnv("CLOUD_NAMESPACE", ""), "Namespace name, env: CLOUD_NAMESPACE")
+
+	isMultiUsersEnabled = flag.Bool("multi_users_enabled", util.GetEnvBool("MULTI_USERS_ENABLED", false), "Is multi Users functionality enabled, env: MULTI_USERS_ENABLED")
+
+	servePort = flag.Int("serve_port", 8080, "Port to serve requests incoming to adapter")
+	serveUser = flag.String(
+		"serve_user",
+		util.GetEnv("DBAAS_ADAPTER_API_USER", "dbaas-aggregator"),
+		"Username to authorize incoming requests, env: DBAAS_ADAPTER_API_USER",
+	)
+	servePass = flag.String(
+		"serve_pass",
+		util.GetEnv("DBAAS_ADAPTER_API_PASSWORD", "dbaas-aggregator"),
+		"Password to authorize incoming requests, env: DBAAS_ADAPTER_API_PASSWORD",
+	)
+
+	phydbid = flag.String(
+		"phydbid",
+		util.GetEnv("DBAAS_AGGREGATOR_PHYSICAL_DATABASE_IDENTIFIER", ""),
+		"Identifier to register physical database in dbaas aggregator, env DBAAS_AGGREGATOR_PHYSICAL_DATABASE_IDENTIFIER",
+	)
+
+	selfAddress = flag.String(
+		"self_address",
+		util.GetEnv("DBAAS_ADAPTER_ADDRESS", ""),
+		"Address in the form <scheme>://<host>:<port> how adapter could be reached from aggregator, env DBAAS_ADAPTER_ADDRESS",
+	)
+
+	dbaasAggregatorRegistrationAddress = flag.String(
+		"registration_address",
+		util.GetEnv("DBAAS_AGGREGATOR_REGISTRATION_ADDRESS", "http://dbaas-aggregator.dbaas:8080"),
+		"Address in the form <scheme>://<host>:<port> to reach aggregator for registration, env DBAAS_AGGREGATOR_REGISTRATION_ADDRESS",
+	)
+
+	dbaasAggregatorRegistrationUsername = flag.String(
+		"registration_username",
+		util.GetEnv("DBAAS_AGGREGATOR_REGISTRATION_USERNAME", "cluster-dba"),
+		"Username of basic auth to reach aggregator for registration, env DBAAS_AGGREGATOR_REGISTRATION_USERNAME ",
+	)
+
+	dbaasAggregatorRegistrationPassword = flag.String(
+		"registration_password",
+		util.GetEnv("DBAAS_AGGREGATOR_REGISTRATION_PASSWORD", ""),
+		"Username of basic auth to reach aggregator for registration, env DBAAS_AGGREGATOR_REGISTRATION_PASSWORD ",
+	)
+
+	labelsFileName = flag.String(
+		"labels_file_location_name",
+		"dbaas.physical_databases.registration.labels.json",
+		"File name where labels are located in json key-value format",
+	)
+
+	labelsLocationDir = flag.String(
+		"labels_file_location_dir",
+		"/app/config/",
+		"Directory with file where labels are located in json key-value format",
+	)
+
+	registrationFixedDelay = flag.Int(
+		"registration_fixed_delay",
+		util.GetEnvInt("DBAAS_AGGREGATOR_REGISTRATION_FIXED_DELAY_MS", 150000), // default 2.5 min
+		"Scheduled physical database registration fixed delay in milliseconds")
+
+	registrationRetryTime = flag.Int(
+		"registration_retry_time",
+		util.GetEnvInt("DBAAS_AGGREGATOR_REGISTRATION_RETRY_TIME_MS", 60000), // default 1 min
+		"Force physical database registration retry time in milliseconds")
+
+	registrationRetryDelay = flag.Int(
+		"registration_retry_delay",
+		util.GetEnvInt("DBAAS_AGGREGATOR_REGISTRATION_RETRY_DELAY_MS", 5000), // default 5 sec
+		"Force physical database registration retry delay between attempts in milliseconds")
+
+	supports = dao.SupportsBase{
+		Users:             true,
+		Settings:          true,
+		DescribeDatabases: false,
+		AdditionalKeys:    map[string]bool{"vault": true},
+	}
+)
+
+func checkInitMode() bool {
+	for _, arg := range os.Args {
+		if arg == "init" {
+			return true
+		}
+	}
+	return false
+}
+
+func main() {
+	flag.Parse()
+	logger.Debug("Adapter started")
+	clusterAdapter := cluster.NewAdapter(*pgHost, *pgPort, *pgUser, *pgPass, *pgDatabase, *pgSsl)
+
+	basicRegistrationAuth := dao.BasicAuth{
+		Username: *dbaasAggregatorRegistrationUsername,
+		Password: *dbaasAggregatorRegistrationPassword,
+	}
+
+	dbaasClient, err := dbaas.NewDbaasClient(*dbaasAggregatorRegistrationAddress, &basicRegistrationAuth, nil)
+
+	if dbaasClient == nil {
+		panic(fmt.Errorf("failed to establish connection to DBaaS aggregator, err: %v", err))
+	}
+
+	if err != nil {
+		logger.Error(fmt.Sprintf("Failed to get DBaaS aggregator version, err %v. Setting default API version", err))
+	}
+
+	//Getting dbaas-adpater api version
+	version, _ := dbaasClient.GetVersion()
+	if version == "v3" {
+		apiVersion = "v2"
+	} else {
+		apiVersion = "v1"
+	}
+	logger.Info(fmt.Sprintf("API version obtained: %s", apiVersion))
+
+	var dbAdminImpl service.DbAdministration = basic.NewServiceAdapter(clusterAdapter, dao.ApiVersion(apiVersion), getRoles(), getFeatures())
+
+	if checkInitMode() {
+		initial.UpdateExtensions(dbAdminImpl)
+		if initial.IsRoleUpdateRequired() {
+			initial.PerformMigration(dbAdminImpl)
+		} else {
+			logger.Info("Roles update is disabled, skip migration...")
+		}
+		return
+	}
+
+	var vaultClient *coreUtils.VaultClient
+
+	if *vaultEnabled {
+		logger.Info("Vault Integration is Enabled")
+		vaultConfig := coreUtils.VaultConfig{
+			IsVaultEnabled:  *vaultEnabled,
+			Address:         *vaultAddress,
+			VaultRole:       *vaultRole,
+			VaultRotPeriod:  *vaultRotPeriod,
+			VaultDBName:     *vaultDBName,
+			VaultAuthMethod: coreUtils.NCPrefix + util.GetEnv("CLOUD_PUBLIC_HOST", "") + "_" + *namespace,
+		}
+		vaultClient = coreUtils.NewVaultClient(vaultConfig)
+	} else {
+		vaultClient = &coreUtils.VaultClient{}
+	}
+
+	var backupAdminServiceImpl service.BackupAdministrationService
+	if *pgSsl == "on" {
+		logger.Debug("SSL is enabled")
+		if !strings.Contains(*pgBackupDaemonAddress, "https") {
+			logger.Info(fmt.Sprintf("replacing backup-daemon address with https, %s", *pgBackupDaemonAddress))
+			*pgBackupDaemonAddress = strings.ReplaceAll(*pgBackupDaemonAddress, "http", "https")
+		}
+	}
+	backupAdminServiceImpl = backup.NewServiceAdapter(clusterAdapter, *pgBackupDaemonAddress, *pgBackupKeep, *pgBackupAuth, *pgUser, *pgPass, *pgSsl)
+
+	administrationService := service.NewCoreAdministrationService(
+		*namespace,
+		*servePort,
+		dbAdminImpl,
+		logger,
+		*vaultEnabled,
+		vaultClient,
+		*readOnlyHost,
+	)
+
+	if strings.Contains(*dbaasAggregatorRegistrationAddress, "https") {
+		logger.Info("tls is enabled, will check if https presented in adapter url")
+		if !strings.Contains(*selfAddress, "https") {
+			*selfAddress = strings.ReplaceAll(*selfAddress, "http", "https")
+		}
+		*selfAddress = strings.ReplaceAll(*selfAddress, "8080", "8443")
+		logger.Info(fmt.Sprintf("replacing self address with https, %s", *selfAddress))
+	}
+	logger.Info(fmt.Sprintf("self address set for registring in aggregator, %s", *selfAddress))
+	log.Fatal(fiber2.RunFiberServer(*servePort, func(app *fiber.App, ctx context.Context) error {
+		fiber2.BuildFiberDBaaSAdapterHandlers(
+			app,
+			*serveUser,
+			*servePass,
+			appPath,
+			administrationService,
+			service.NewPhysicalRegistrationService(
+				appName,
+				logger,
+				*phydbid,
+				*selfAddress,
+				dao.BasicAuth{
+					Username: *serveUser,
+					Password: *servePass,
+				},
+				ReadLabelsFile(), //labels
+				dbaasClient,
+				*registrationFixedDelay,
+				*registrationRetryTime,
+				*registrationRetryDelay,
+				administrationService,
+				ctx,
+			),
+			backupAdminServiceImpl,
+			supports.ToMap(),
+			logger,
+			false,
+			"")
+
+		prefix := fmt.Sprintf("/api/%s/dbaas/adapter/postgresql/databases/", apiVersion)
+		app.Get(prefix+":dbName/info", dbAdminImpl.(*basic.ServiceAdapter).GetDatabaseOwnerHandler())
+		app.Put(prefix+":dbName/settings", dbAdminImpl.(*basic.ServiceAdapter).UpdatePostgreSQLSettingsHandler())
+
+		app.Get("/api/version", func(c *fiber.Ctx) error {
+			return c.SendString(apiVersion)
+		})
+
+		return nil
+	}))
+
+}
+
+func ReadLabelsFile() map[string]string {
+	file, err := os.ReadFile(*labelsLocationDir + *labelsFileName)
+	if err != nil {
+		logger.Info(fmt.Sprintf("Skipping labels file, cannot read it: %s", *labelsLocationDir+*labelsFileName))
+		return make(map[string]string)
+	}
+	var labels map[string]string
+	err = json.Unmarshal(file, &labels)
+	if err != nil {
+		logger.Warn(fmt.Sprintf("Failed to parse labels file %s", *labelsLocationDir+*labelsFileName), zap.Error(err))
+		labels = make(map[string]string)
+	}
+	logger.Info(fmt.Sprintf("Labels: %v", labels))
+	return labels
+}
+
+func getFeatures() map[string]bool {
+	IsTls := *pgSsl == "on"
+	return map[string]bool{
+		basic.FeatureMultiUsers:   *isMultiUsersEnabled && apiVersion != "v1",
+		basic.FeatureTls:          IsTls,
+		basic.FeatureNotStrictTLS: IsTls && util.IsExternalPostgreSQl(),
+	}
+}
+
+func getRoles() []string {
+	return []string{"admin", "streaming", "rw", "ro"}
+}
diff --git a/docker-dbaas-adapter/adapter/util/util.go b/docker-dbaas-adapter/adapter/util/util.go
new file mode 100644
index 0000000..e1d48f8
--- /dev/null
+++ b/docker-dbaas-adapter/adapter/util/util.go
@@ -0,0 +1,164 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package util
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"go.uber.org/zap"
+	"go.uber.org/zap/zapcore"
+	"k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/rest"
+	"os"
+	"strconv"
+)
+
+const (
+	MaxPgDBLength          = 63
+	MaxAllowedSuffixLength = 12
+
+	rolesVersion = 1.0
+)
+
+var (
+	log                    *zap.Logger
+	isDebugEnabled         = *flag.Bool("log_debug", GetEnvBool("LOG_DEBUG", false), "If debug logs is enabled, env: LOG_DEBUG")
+	ExternalPostgreSQLType = *flag.String("cloud_type", GetEnv("EXTERNAL_POSTGRESQL", ""), "A type of cloud where postgres is deployed, env: EXTERNAL_POSTGRESQL")
+)
+
+func ContextLogger(ctx context.Context) *zap.Logger {
+	//loggerCtx := ctx.Value("logger")
+	//if loggerCtx != nil {
+	//	return loggerCtx.(zap.Logger)
+	//}
+	logger := GetLogger()
+	return logger.With(zap.ByteString("request_id", []byte(fmt.Sprintf("%s", ctx.Value("request_id")))))
+}
+
+func GetLogger() *zap.Logger {
+	//if log != nil {
+	//	return log
+	//}
+
+	atom := zap.NewAtomicLevel()
+	if isDebugEnabled {
+		atom = zap.NewAtomicLevelAt(zap.DebugLevel)
+	}
+
+	encoderCfg := zap.NewProductionEncoderConfig()
+	encoderCfg.TimeKey = "timestamp"
+	encoderCfg.EncodeTime = zapcore.ISO8601TimeEncoder
+
+	logger := zap.New(zapcore.NewCore(
+		zapcore.NewJSONEncoder(encoderCfg),
+		zapcore.Lock(os.Stdout),
+		atom,
+	))
+	defer func() {
+		_ = logger.Sync()
+	}()
+	log = logger
+	return logger
+}
+
+func Contains(array []string, str string) bool {
+	for _, a := range array {
+		if a == str {
+			return true
+		}
+	}
+	return false
+}
+
+func GetStringArrayFromInterface(inter interface{}) []string {
+	if inter == nil {
+		return make([]string, 0)
+	}
+	interfaceArray := inter.([]interface{})
+	resultArray := make([]string, len(interfaceArray))
+	for i := range interfaceArray {
+		resultArray[i] = interfaceArray[i].(string)
+	}
+	return resultArray
+}
+
+func GetMapStringFromMapInterface(inter interface{}) map[string]string {
+	if inter == nil {
+		return make(map[string]string)
+	}
+	rawMap := inter.(map[string]interface{})
+	resultMap := make(map[string]string)
+	for i, v := range rawMap {
+		resultMap[i] = v.(string)
+	}
+	return resultMap
+}
+
+func GetEnv(key, fallback string) string {
+	if value, ok := os.LookupEnv(key); ok {
+		return value
+	}
+	return fallback
+}
+
+func GetEnvInt(key string, fallback int) int {
+	if value, ok := os.LookupEnv(key); ok {
+		if ivalue, err := strconv.Atoi(value); err == nil {
+			return ivalue
+		}
+	}
+	return fallback
+}
+
+func GetEnvBool(key string, fallback bool) bool {
+	if value, ok := os.LookupEnv(key); ok {
+		bvalue, err := strconv.ParseBool(value)
+		if err != nil {
+			log.Error(fmt.Sprintf("Can't parse %s boolean variable", key), zap.Error(err))
+			panic(err)
+		}
+		return bvalue
+	}
+	return fallback
+}
+
+func GetK8sClient() (*kubernetes.Clientset, error) {
+	config, err := rest.InClusterConfig()
+	if err != nil {
+		log.Error("Couldn't get k8s config", zap.Error(err))
+		return nil, err
+	}
+
+	client, err := kubernetes.NewForConfig(config)
+	if err != nil {
+		log.Error("Couldn't get k8s client", zap.Error(err))
+		return nil, err
+	}
+
+	return client, nil
+}
+
+func GetPgDBLength() int {
+	return MaxPgDBLength
+}
+
+func IsExternalPostgreSQl() bool {
+	return ExternalPostgreSQLType != ""
+}
+
+func GetRolesVersion() float64 {
+	return rolesVersion
+}
diff --git a/docker-dbaas-adapter/docker/Dockerfile b/docker-dbaas-adapter/docker/Dockerfile
new file mode 100644
index 0000000..02bb007
--- /dev/null
+++ b/docker-dbaas-adapter/docker/Dockerfile
@@ -0,0 +1,31 @@
+FROM --platform=$BUILDPLATFORM golang:1.25.3-alpine3.22 AS builder
+
+WORKDIR /workspace
+
+# Copy the Go Modules manifests
+COPY go.mod go.mod
+COPY go.sum go.sum
+
+RUN go mod download
+
+# Copy the go source
+COPY adapter/ adapter/
+
+RUN go mod tidy
+
+# Build
+ARG TARGETOS TARGETARCH
+RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o /build/_output/bin/postgresql-dbaas -gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./adapter
+
+FROM alpine:3.22
+
+ENV OPERATOR=/usr/local/bin/postgresql-dbaas \
+    USER_UID=1001
+
+# install dbaas binary
+COPY --from=builder build/_output/bin/postgresql-dbaas ${OPERATOR}
+COPY build/bin /usr/local/bin
+
+USER ${USER_UID}
+
+ENTRYPOINT ["/usr/local/bin/entrypoint"]
diff --git a/docker-dbaas-adapter/docker/bin/entrypoint b/docker-dbaas-adapter/docker/bin/entrypoint
new file mode 100755
index 0000000..457186b
--- /dev/null
+++ b/docker-dbaas-adapter/docker/bin/entrypoint
@@ -0,0 +1,3 @@
+#!/bin/sh -e
+
+exec ${OPERATOR} $@
diff --git a/docker-dbaas-adapter/go.mod b/docker-dbaas-adapter/go.mod
new file mode 100644
index 0000000..f2cc70d
--- /dev/null
+++ b/docker-dbaas-adapter/go.mod
@@ -0,0 +1,109 @@
+module github.com/Netcracker/pgskipper-dbaas-adapter/postgresql-dbaas-adapter
+
+go 1.25.3
+
+require (
+	github.com/Netcracker/qubership-dbaas-adapter-core v0.11.0
+	github.com/gofiber/fiber/v2 v2.52.9
+	github.com/google/uuid v1.6.0
+	github.com/jackc/pgconn v1.14.3
+	github.com/jackc/pgx/v4 v4.18.2
+	github.com/stretchr/testify v1.9.0
+	github.com/valyala/fasthttp v1.52.0
+	go.uber.org/zap v1.27.0
+	k8s.io/api v0.20.0
+	k8s.io/apimachinery v0.21.0
+	k8s.io/client-go v0.20.0
+)
+
+require (
+	github.com/gabriel-vasile/mimetype v1.4.8 // indirect
+	github.com/go-playground/locales v0.14.1 // indirect
+	github.com/go-playground/universal-translator v0.18.1 // indirect
+	github.com/go-playground/validator/v10 v10.27.0 // indirect
+	github.com/leodido/go-urn v1.4.0 // indirect
+)
+
+require (
+	github.com/KyleBanks/depth v1.2.1 // indirect
+	github.com/PuerkitoBio/purell v1.1.1 // indirect
+	github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 // indirect
+	github.com/andybalholm/brotli v1.1.0 // indirect
+	github.com/ansrivas/fiberprometheus/v2 v2.7.0 // indirect
+	github.com/beorn7/perks v1.0.1 // indirect
+	github.com/cenkalti/backoff/v3 v3.2.2 // indirect
+	github.com/cespare/xxhash/v2 v2.3.0 // indirect
+	github.com/davecgh/go-spew v1.1.1 // indirect
+	github.com/docker/distribution v2.8.3+incompatible // indirect
+	github.com/go-jose/go-jose/v4 v4.0.1 // indirect
+	github.com/go-logr/logr v0.4.0 // indirect
+	github.com/go-openapi/jsonpointer v0.19.5 // indirect
+	github.com/go-openapi/jsonreference v0.19.6 // indirect
+	github.com/go-openapi/spec v0.20.4 // indirect
+	github.com/go-openapi/swag v0.19.15 // indirect
+	github.com/gofiber/swagger v1.1.0 // indirect
+	github.com/gogo/protobuf v1.3.2 // indirect
+	github.com/golang/protobuf v1.5.3 // indirect
+	github.com/google/go-cmp v0.6.0 // indirect
+	github.com/google/gofuzz v1.2.0 // indirect
+	github.com/googleapis/gnostic v0.4.1 // indirect
+	github.com/hashicorp/errwrap v1.1.0 // indirect
+	github.com/hashicorp/go-cleanhttp v0.5.2 // indirect
+	github.com/hashicorp/go-multierror v1.1.1 // indirect
+	github.com/hashicorp/go-retryablehttp v0.7.6 // indirect
+	github.com/hashicorp/go-rootcerts v1.0.2 // indirect
+	github.com/hashicorp/go-secure-stdlib/parseutil v0.1.6 // indirect
+	github.com/hashicorp/go-secure-stdlib/strutil v0.1.2 // indirect
+	github.com/hashicorp/go-sockaddr v1.0.2 // indirect
+	github.com/hashicorp/hcl v1.0.0 // indirect
+	github.com/hashicorp/vault/api v1.14.0 // indirect
+	github.com/jackc/chunkreader/v2 v2.0.1 // indirect
+	github.com/jackc/pgio v1.0.0 // indirect
+	github.com/jackc/pgpassfile v1.0.0 // indirect
+	github.com/jackc/pgproto3/v2 v2.3.3 // indirect
+	github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a // indirect
+	github.com/jackc/pgtype v1.14.0 // indirect
+	github.com/jackc/puddle v1.3.0 // indirect
+	github.com/josharian/intern v1.0.0 // indirect
+	github.com/json-iterator/go v1.1.12 // indirect
+	github.com/klauspost/compress v1.17.9 // indirect
+	github.com/mailru/easyjson v0.7.6 // indirect
+	github.com/mattn/go-colorable v0.1.13 // indirect
+	github.com/mattn/go-isatty v0.0.20 // indirect
+	github.com/mattn/go-runewidth v0.0.16 // indirect
+	github.com/mitchellh/go-homedir v1.1.0 // indirect
+	github.com/mitchellh/mapstructure v1.5.0 // indirect
+	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
+	github.com/modern-go/reflect2 v1.0.2 // indirect
+	github.com/pmezard/go-difflib v1.0.0 // indirect
+	github.com/prometheus/client_golang v1.19.0 // indirect
+	github.com/prometheus/client_model v0.6.1 // indirect
+	github.com/prometheus/common v0.53.0 // indirect
+	github.com/prometheus/procfs v0.14.0 // indirect
+	github.com/rivo/uniseg v0.4.7 // indirect
+	github.com/ryanuber/go-glob v1.0.0 // indirect
+	github.com/stretchr/objx v0.5.2 // indirect
+	github.com/swaggo/files/v2 v2.0.0 // indirect
+	github.com/swaggo/swag v1.16.3 // indirect
+	github.com/valyala/bytebufferpool v1.0.0 // indirect
+	github.com/valyala/tcplisten v1.0.0 // indirect
+	go.uber.org/multierr v1.10.0 // indirect
+	golang.org/x/crypto v0.35.0 // indirect
+	golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3
+	golang.org/x/net v0.34.0 // indirect
+	golang.org/x/oauth2 v0.18.0 // indirect
+	golang.org/x/sys v0.30.0 // indirect
+	golang.org/x/term v0.29.0 // indirect
+	golang.org/x/text v0.22.0 // indirect
+	golang.org/x/time v0.3.0 // indirect
+	golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d // indirect
+	google.golang.org/appengine v1.6.7 // indirect
+	google.golang.org/protobuf v1.33.0 // indirect
+	gopkg.in/inf.v0 v0.9.1 // indirect
+	gopkg.in/yaml.v2 v2.4.0 // indirect
+	gopkg.in/yaml.v3 v3.0.1 // indirect
+	k8s.io/klog/v2 v2.8.0 // indirect
+	k8s.io/utils v0.0.0-20201110183641-67b214c5f920 // indirect
+	sigs.k8s.io/structured-merge-diff/v4 v4.1.0 // indirect
+	sigs.k8s.io/yaml v1.3.0 // indirect
+)
diff --git a/docker-dbaas-adapter/go.sum b/docker-dbaas-adapter/go.sum
new file mode 100644
index 0000000..00934a0
--- /dev/null
+++ b/docker-dbaas-adapter/go.sum
@@ -0,0 +1,745 @@
+cloud.google.com/go v0.26.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=
+cloud.google.com/go v0.34.0/go.mod h1:aQUYkXzVsufM+DwF1aE+0xfcU+56JwCaLick0ClmMTw=
+cloud.google.com/go v0.38.0/go.mod h1:990N+gfupTy94rShfmMCWGDn0LpTmnzTp2qbd1dvSRU=
+cloud.google.com/go v0.44.1/go.mod h1:iSa0KzasP4Uvy3f1mN/7PiObzGgflwredwwASm/v6AU=
+cloud.google.com/go v0.44.2/go.mod h1:60680Gw3Yr4ikxnPRS/oxxkBccT6SA1yMk63TGekxKY=
+cloud.google.com/go v0.45.1/go.mod h1:RpBamKRgapWJb87xiFSdk4g1CME7QZg3uwTez+TSTjc=
+cloud.google.com/go v0.46.3/go.mod h1:a6bKKbmY7er1mI7TEI4lsAkts/mkhTSZK8w33B4RAg0=
+cloud.google.com/go v0.50.0/go.mod h1:r9sluTvynVuxRIOHXQEHMFffphuXHOMZMycpNR5e6To=
+cloud.google.com/go v0.52.0/go.mod h1:pXajvRH/6o3+F9jDHZWQ5PbGhn+o8w9qiu/CffaVdO4=
+cloud.google.com/go v0.53.0/go.mod h1:fp/UouUEsRkN6ryDKNW/Upv/JBKnv6WDthjR6+vze6M=
+cloud.google.com/go v0.54.0/go.mod h1:1rq2OEkV3YMf6n/9ZvGWI3GWw0VoqH/1x2nd8Is/bPc=
+cloud.google.com/go/bigquery v1.0.1/go.mod h1:i/xbL2UlR5RvWAURpBYZTtm/cXjCha9lbfbpx4poX+o=
+cloud.google.com/go/bigquery v1.3.0/go.mod h1:PjpwJnslEMmckchkHFfq+HTD2DmtT67aNFKH1/VBDHE=
+cloud.google.com/go/bigquery v1.4.0/go.mod h1:S8dzgnTigyfTmLBfrtrhyYhwRxG72rYxvftPBK2Dvzc=
+cloud.google.com/go/datastore v1.0.0/go.mod h1:LXYbyblFSglQ5pkeyhO+Qmw7ukd3C+pD7TKLgZqpHYE=
+cloud.google.com/go/datastore v1.1.0/go.mod h1:umbIZjpQpHh4hmRpGhH4tLFup+FVzqBi1b3c64qFpCk=
+cloud.google.com/go/pubsub v1.0.1/go.mod h1:R0Gpsv3s54REJCy4fxDixWD93lHJMoZTyQ2kNxGRt3I=
+cloud.google.com/go/pubsub v1.1.0/go.mod h1:EwwdRX2sKPjnvnqCa270oGRyludottCI76h+R3AArQw=
+cloud.google.com/go/pubsub v1.2.0/go.mod h1:jhfEVHT8odbXTkndysNHCcx0awwzvfOlguIAii9o8iA=
+cloud.google.com/go/storage v1.0.0/go.mod h1:IhtSnM/ZTZV8YYJWCY8RULGVqBDmpoyjwiyrjsg+URw=
+cloud.google.com/go/storage v1.5.0/go.mod h1:tpKbwo567HUNpVclU5sGELwQWBDZ8gh0ZeosJ0Rtdos=
+cloud.google.com/go/storage v1.6.0/go.mod h1:N7U0C8pVQ/+NIKOBQyamJIeKQKkZ+mxpohlUTyfDhBk=
+dmitri.shuralyov.com/gpu/mtl v0.0.0-20190408044501-666a987793e9/go.mod h1:H6x//7gZCb22OMCxBHrMx7a5I7Hp++hsVxbQ4BYO7hU=
+github.com/Azure/go-autorest v14.2.0+incompatible/go.mod h1:r+4oMnoxhatjLLJ6zxSWATqVooLgysK6ZNox3g/xq24=
+github.com/Azure/go-autorest/autorest v0.11.1/go.mod h1:JFgpikqFJ/MleTTxwepExTKnFUKKszPS8UavbQYUMuw=
+github.com/Azure/go-autorest/autorest/adal v0.9.0/go.mod h1:/c022QCutn2P7uY+/oQWWNcK9YU+MH96NgK+jErpbcg=
+github.com/Azure/go-autorest/autorest/adal v0.9.5/go.mod h1:B7KF7jKIeC9Mct5spmyCB/A8CG/sEz1vwIRGv/bbw7A=
+github.com/Azure/go-autorest/autorest/date v0.3.0/go.mod h1:BI0uouVdmngYNUzGWeSYnokU+TrmwEsOqdt8Y6sso74=
+github.com/Azure/go-autorest/autorest/mocks v0.4.0/go.mod h1:LTp+uSrOhSkaKrUy935gNZuuIPPVsHlr9DSOxSayd+k=
+github.com/Azure/go-autorest/autorest/mocks v0.4.1/go.mod h1:LTp+uSrOhSkaKrUy935gNZuuIPPVsHlr9DSOxSayd+k=
+github.com/Azure/go-autorest/logger v0.2.0/go.mod h1:T9E3cAhj2VqvPOtCYAvby9aBXkZmbF5NWuPV8+WeEW8=
+github.com/Azure/go-autorest/tracing v0.6.0/go.mod h1:+vhtPC754Xsa23ID7GlGsrdKBpUA79WCAKPPZVC2DeU=
+github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=
+github.com/BurntSushi/xgb v0.0.0-20160522181843-27f122750802/go.mod h1:IVnqGOEym/WlBOVXweHU+Q+/VP0lqqI8lqeDx9IjBqo=
+github.com/KyleBanks/depth v1.2.1 h1:5h8fQADFrWtarTdtDudMmGsC7GPbOAu6RVB3ffsVFHc=
+github.com/KyleBanks/depth v1.2.1/go.mod h1:jzSb9d0L43HxTQfT+oSA1EEp2q+ne2uh6XgeJcm8brE=
+github.com/Masterminds/semver/v3 v3.1.1/go.mod h1:VPu/7SZ7ePZ3QOrcuXROw5FAcLl4a0cBrbBpGY/8hQs=
+github.com/NYTimes/gziphandler v0.0.0-20170623195520-56545f4a5d46/go.mod h1:3wb06e3pkSAbeQ52E9H9iFoQsEEwGN64994WTCIhntQ=
+github.com/Netcracker/qubership-dbaas-adapter-core v0.11.0 h1:tBdeneu9r0vwaGP7jhegfQVeXHORySJP8Vlf2M6XMWg=
+github.com/Netcracker/qubership-dbaas-adapter-core v0.11.0/go.mod h1:WwqayO1puRMdzoB6EbIpHWqhkYi4JT7v4pCVM6h15D8=
+github.com/PuerkitoBio/purell v1.1.1 h1:WEQqlqaGbrPkxLJWfBwQmfEAE1Z7ONdDLqrN38tNFfI=
+github.com/PuerkitoBio/purell v1.1.1/go.mod h1:c11w/QuzBsJSee3cPx9rAFu61PvFxuPbtSwDGJws/X0=
+github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578 h1:d+Bc7a5rLufV/sSk/8dngufqelfh6jnri85riMAaF/M=
+github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578/go.mod h1:uGdkoq3SwY9Y+13GIhn11/XLaGBb4BfwItxLd5jeuXE=
+github.com/andybalholm/brotli v1.1.0 h1:eLKJA0d02Lf0mVpIDgYnqXcUn0GqVmEFny3VuID1U3M=
+github.com/andybalholm/brotli v1.1.0/go.mod h1:sms7XGricyQI9K10gOSf56VKKWS4oLer58Q+mhRPtnY=
+github.com/ansrivas/fiberprometheus/v2 v2.7.0 h1:09XiSzG0J7aZp7RviklngdWdDbSybKjhuWAstp003Gg=
+github.com/ansrivas/fiberprometheus/v2 v2.7.0/go.mod h1:hSJdO65lfnWW70Qn9uGdXXsUUSkckbhuw5r/KesygpU=
+github.com/armon/go-radix v0.0.0-20180808171621-7fddfc383310/go.mod h1:ufUuZ+zHj4x4TnLV4JWEpy2hxWSpsRywHrMgIH9cCH8=
+github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a/go.mod h1:lB+ZfQJz7igIIfQNfa7Ml4HSf2uFQQRzpGGRXenZAgY=
+github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=
+github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
+github.com/bgentry/speakeasy v0.1.0/go.mod h1:+zsyZBPWlz7T6j88CTgSN5bM796AkVf0kBD4zp0CCIs=
+github.com/cenkalti/backoff/v3 v3.2.2 h1:cfUAAO3yvKMYKPrvhDuHSwQnhZNk/RMHKdZqKTxfm6M=
+github.com/cenkalti/backoff/v3 v3.2.2/go.mod h1:cIeZDE3IrqwwJl6VUwCN6trj1oXrTS4rc0ij+ULvLYs=
+github.com/census-instrumentation/opencensus-proto v0.2.1/go.mod h1:f6KPmirojxKA12rnyqOA5BBL4O983OfeGPqjHWSTneU=
+github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
+github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
+github.com/chzyer/logex v1.1.10/go.mod h1:+Ywpsq7O8HXn0nuIou7OrIPyXbp3wmkHB+jjWRnGsAI=
+github.com/chzyer/readline v0.0.0-20180603132655-2972be24d48e/go.mod h1:nSuG5e5PlCu98SY8svDHJxuZscDgtXS6KTTbou5AhLI=
+github.com/chzyer/test v0.0.0-20180213035817-a1ea475d72b1/go.mod h1:Q3SI9o4m/ZMnBNeIyt5eFwwo7qiLfzFZmjNmxjkiQlU=
+github.com/client9/misspell v0.3.4/go.mod h1:qj6jICC3Q7zFZvVWo7KLAzC3yx5G7kyvSDkc90ppPyw=
+github.com/cockroachdb/apd v1.1.0 h1:3LFP3629v+1aKXU5Q37mxmRxX/pIu1nijXydLShEq5I=
+github.com/cockroachdb/apd v1.1.0/go.mod h1:8Sl8LxpKi29FqWXR16WEFZRNSz3SoPzUzeMeY4+DwBQ=
+github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
+github.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
+github.com/creack/pty v1.1.7/go.mod h1:lj5s0c3V2DBrqTV7llrYr5NG6My20zk30Fl46Y7DoTY=
+github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/dgrijalva/jwt-go v3.2.0+incompatible/go.mod h1:E3ru+11k8xSBh+hMPgOLZmtrrCbhqsmaPHjLKYnJCaQ=
+github.com/docker/distribution v2.8.3+incompatible h1:AtKxIZ36LoNK51+Z6RpzLpddBirtxJnzDrHLEKxTAYk=
+github.com/docker/distribution v2.8.3+incompatible/go.mod h1:J2gT2udsDAN96Uj4KfcMRqY0/ypR+oyYUYmja8H+y+w=
+github.com/docker/spdystream v0.0.0-20160310174837-449fdfce4d96/go.mod h1:Qh8CwZgvJUkLughtfhJv5dyTYa91l1fOUCrgjqmcifM=
+github.com/docopt/docopt-go v0.0.0-20180111231733-ee0de3bc6815/go.mod h1:WwZ+bS3ebgob9U8Nd0kOddGdZWjyMGR8Wziv+TBNwSE=
+github.com/elazarl/goproxy v0.0.0-20180725130230-947c36da3153/go.mod h1:/Zj4wYkgs4iZTTu3o/KG3Itv/qCCa8VVMlb3i9OVuzc=
+github.com/emicklei/go-restful v0.0.0-20170410110728-ff4f55a20633/go.mod h1:otzb+WCGbkyDHkqmQmT5YD2WR4BBwUdeQoFo8l/7tVs=
+github.com/envoyproxy/go-control-plane v0.9.1-0.20191026205805-5f8ba28d4473/go.mod h1:YTl/9mNaCwkRvm6d1a2C3ymFceY/DCBVvsKhRF0iEA4=
+github.com/envoyproxy/protoc-gen-validate v0.1.0/go.mod h1:iSmxcyjqTsJpI2R4NaDN7+kN2VEUnK/pcBlmesArF7c=
+github.com/evanphx/json-patch v4.9.0+incompatible/go.mod h1:50XU6AFN0ol/bzJsmQLiYLvXMP4fmwYFNcr97nuDLSk=
+github.com/fatih/color v1.7.0/go.mod h1:Zm6kSWBoL9eyXnKyktHP6abPY2pDugNf5KwzbycvMj4=
+github.com/fatih/color v1.16.0 h1:zmkK9Ngbjj+K0yRhTVONQh1p/HknKYSlNT+vZCzyokM=
+github.com/fatih/color v1.16.0/go.mod h1:fL2Sau1YI5c0pdGEVCbKQbLXB6edEj1ZgiY4NijnWvE=
+github.com/form3tech-oss/jwt-go v3.2.2+incompatible/go.mod h1:pbq4aXjuKjdthFRnoDwaVPLA+WlJuPGy+QneDUgJi2k=
+github.com/fsnotify/fsnotify v1.4.7/go.mod h1:jwhsz4b93w/PPRr/qN1Yymfu8t87LnFCMoQvtojpjFo=
+github.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=
+github.com/gabriel-vasile/mimetype v1.4.8 h1:FfZ3gj38NjllZIeJAmMhr+qKL8Wu+nOoI3GqacKw1NM=
+github.com/gabriel-vasile/mimetype v1.4.8/go.mod h1:ByKUIKGjh1ODkGM1asKUbQZOLGrPjydw3hYPU2YU9t8=
+github.com/ghodss/yaml v0.0.0-20150909031657-73d445a93680/go.mod h1:4dBDuWmgqj2HViK6kFavaiC9ZROes6MMH2rRYeMEF04=
+github.com/go-gl/glfw v0.0.0-20190409004039-e6da0acd62b1/go.mod h1:vR7hzQXu2zJy9AVAgeJqvqgH9Q5CA+iKCZ2gyEVpxRU=
+github.com/go-gl/glfw/v3.3/glfw v0.0.0-20191125211704-12ad95a8df72/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=
+github.com/go-gl/glfw/v3.3/glfw v0.0.0-20200222043503-6f7a984d4dc4/go.mod h1:tQ2UAYgL5IevRw8kRxooKSPJfGvJ9fJQFa0TUsXzTg8=
+github.com/go-jose/go-jose/v4 v4.0.1 h1:QVEPDE3OluqXBQZDcnNvQrInro2h0e4eqNbnZSWqS6U=
+github.com/go-jose/go-jose/v4 v4.0.1/go.mod h1:WVf9LFMHh/QVrmqrOfqun0C45tMe3RoiKJMPvgWwLfY=
+github.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=
+github.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=
+github.com/go-logr/logr v0.1.0/go.mod h1:ixOQHD9gLJUVQQ2ZOR7zLEifBX6tGkNJF4QyIY7sIas=
+github.com/go-logr/logr v0.2.0/go.mod h1:z6/tIYblkpsD+a4lm/fGIIU9mZ+XfAiaFtq7xTgseGU=
+github.com/go-logr/logr v0.4.0 h1:K7/B1jt6fIBQVd4Owv2MqGQClcgf0R266+7C/QjRcLc=
+github.com/go-logr/logr v0.4.0/go.mod h1:z6/tIYblkpsD+a4lm/fGIIU9mZ+XfAiaFtq7xTgseGU=
+github.com/go-openapi/jsonpointer v0.19.2/go.mod h1:3akKfEdA7DF1sugOqz1dVQHBcuDBPKZGEoHC/NkiQRg=
+github.com/go-openapi/jsonpointer v0.19.3/go.mod h1:Pl9vOtqEWErmShwVjC8pYs9cog34VGT37dQOVbmoatg=
+github.com/go-openapi/jsonpointer v0.19.5 h1:gZr+CIYByUqjcgeLXnQu2gHYQC9o73G2XUeOFYEICuY=
+github.com/go-openapi/jsonpointer v0.19.5/go.mod h1:Pl9vOtqEWErmShwVjC8pYs9cog34VGT37dQOVbmoatg=
+github.com/go-openapi/jsonreference v0.19.2/go.mod h1:jMjeRr2HHw6nAVajTXJ4eiUwohSTlpa0o73RUL1owJc=
+github.com/go-openapi/jsonreference v0.19.3/go.mod h1:rjx6GuL8TTa9VaixXglHmQmIL98+wF9xc8zWvFonSJ8=
+github.com/go-openapi/jsonreference v0.19.6 h1:UBIxjkht+AWIgYzCDSv2GN+E/togfwXUJFRTWhl2Jjs=
+github.com/go-openapi/jsonreference v0.19.6/go.mod h1:diGHMEHg2IqXZGKxqyvWdfWU/aim5Dprw5bqpKkTvns=
+github.com/go-openapi/spec v0.19.3/go.mod h1:FpwSN1ksY1eteniUU7X0N/BgJ7a4WvBFVA8Lj9mJglo=
+github.com/go-openapi/spec v0.20.4 h1:O8hJrt0UMnhHcluhIdUgCLRWyM2x7QkBXRvOs7m+O1M=
+github.com/go-openapi/spec v0.20.4/go.mod h1:faYFR1CvsJZ0mNsmsphTMSoRrNV3TEDoAM7FOEWeq8I=
+github.com/go-openapi/swag v0.19.2/go.mod h1:POnQmlKehdgb5mhVOsnJFsivZCEZ/vjK9gh66Z9tfKk=
+github.com/go-openapi/swag v0.19.5/go.mod h1:POnQmlKehdgb5mhVOsnJFsivZCEZ/vjK9gh66Z9tfKk=
+github.com/go-openapi/swag v0.19.15 h1:D2NRCBzS9/pEY3gP9Nl8aDqGUcPFrwG2p+CNFrLyrCM=
+github.com/go-openapi/swag v0.19.15/go.mod h1:QYRuS/SOXUCsnplDa677K7+DxSOj6IPNl/eQntq43wQ=
+github.com/go-playground/assert/v2 v2.2.0 h1:JvknZsQTYeFEAhQwI4qEt9cyV5ONwRHC+lYKSsYSR8s=
+github.com/go-playground/assert/v2 v2.2.0/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=
+github.com/go-playground/locales v0.14.1 h1:EWaQ/wswjilfKLTECiXz7Rh+3BjFhfDFKv/oXslEjJA=
+github.com/go-playground/locales v0.14.1/go.mod h1:hxrqLVvrK65+Rwrd5Fc6F2O76J/NuW9t0sjnWqG1slY=
+github.com/go-playground/universal-translator v0.18.1 h1:Bcnm0ZwsGyWbCzImXv+pAJnYK9S473LQFuzCbDbfSFY=
+github.com/go-playground/universal-translator v0.18.1/go.mod h1:xekY+UJKNuX9WP91TpwSH2VMlDf28Uj24BCp08ZFTUY=
+github.com/go-playground/validator/v10 v10.27.0 h1:w8+XrWVMhGkxOaaowyKH35gFydVHOvC0/uWoy2Fzwn4=
+github.com/go-playground/validator/v10 v10.27.0/go.mod h1:I5QpIEbmr8On7W0TktmJAumgzX4CA1XNl4ZmDuVHKKo=
+github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=
+github.com/go-test/deep v1.0.2 h1:onZX1rnHT3Wv6cqNgYyFOOlgVKJrksuCMCRvJStbMYw=
+github.com/go-test/deep v1.0.2/go.mod h1:wGDj63lr65AM2AQyKZd/NYHGb0R+1RLqB8NKt3aSFNA=
+github.com/gofiber/fiber/v2 v2.52.9 h1:YjKl5DOiyP3j0mO61u3NTmK7or8GzzWzCFzkboyP5cw=
+github.com/gofiber/fiber/v2 v2.52.9/go.mod h1:YEcBbO/FB+5M1IZNBP9FO3J9281zgPAreiI1oqg8nDw=
+github.com/gofiber/swagger v1.1.0 h1:ff3rg1fB+Rp5JN/N8jfxTiZtMKe/9tB9QDc79fPiJKQ=
+github.com/gofiber/swagger v1.1.0/go.mod h1:pRZL0Np35sd+lTODTE5The0G+TMHfNY+oC4hM2/i5m8=
+github.com/gofrs/uuid v4.0.0+incompatible h1:1SD/1F5pU8p29ybwgQSwpQk+mwdRrXCYuPhW6m+TnJw=
+github.com/gofrs/uuid v4.0.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=
+github.com/gogo/protobuf v1.3.1/go.mod h1:SlYgWuQ5SjCEi6WLHjHCa1yvBfUnHcTbrrZtXPKa29o=
+github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
+github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
+github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b/go.mod h1:SBH7ygxi8pfUlaOkMMuAQtPIUF8ecWP5IEl/CR7VP2Q=
+github.com/golang/groupcache v0.0.0-20190702054246-869f871628b6/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
+github.com/golang/groupcache v0.0.0-20191227052852-215e87163ea7/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
+github.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
+github.com/golang/mock v1.1.1/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=
+github.com/golang/mock v1.2.0/go.mod h1:oTYuIxOrZwtPieC+H1uAHpcLFnEyAGVDL/k47Jfbm0A=
+github.com/golang/mock v1.3.1/go.mod h1:sBzyDLLjw3U8JLTeZvSv8jJB+tU5PVekmnlKIyFUx0Y=
+github.com/golang/mock v1.4.0/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=
+github.com/golang/mock v1.4.1/go.mod h1:UOMv5ysSaYNkG+OFQykRIcU/QvvxJf3p21QfJ2Bt3cw=
+github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.3.1/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.3.2/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.3.3/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=
+github.com/golang/protobuf v1.3.4/go.mod h1:vzj43D7+SQXF/4pzW/hwtAqwc6iTitCiVSaWz5lYuqw=
+github.com/golang/protobuf v1.4.0-rc.1/go.mod h1:ceaxUfeHdC40wWswd/P6IGgMaK3YpKi5j83Wpe3EHw8=
+github.com/golang/protobuf v1.4.0-rc.1.0.20200221234624-67d41d38c208/go.mod h1:xKAWHe0F5eneWXFV3EuXVDTCmh+JuBKY0li0aMyXATA=
+github.com/golang/protobuf v1.4.0-rc.2/go.mod h1:LlEzMj4AhA7rCAGe4KMBDvJI+AwstrUpVNzEA03Pprs=
+github.com/golang/protobuf v1.4.0-rc.4.0.20200313231945-b860323f09d0/go.mod h1:WU3c8KckQ9AFe+yFwt9sWVRKCVIyN9cPHBJSNnbL67w=
+github.com/golang/protobuf v1.4.0/go.mod h1:jodUvKwWbYaEsadDk5Fwe5c77LiNKVO9IDvqG2KuDX0=
+github.com/golang/protobuf v1.4.1/go.mod h1:U8fpvMrcmy5pZrNK1lt4xCsGvpyWQ/VVv6QDs8UjoX8=
+github.com/golang/protobuf v1.4.3/go.mod h1:oDoupMAO8OvCJWAcko0GGGIgR6R6ocIYbsSw735rRwI=
+github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
+github.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=
+github.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
+github.com/google/btree v0.0.0-20180813153112-4030bb1f1f0c/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
+github.com/google/btree v1.0.0/go.mod h1:lNA+9X1NB3Zf8V7Ke586lFgjr2dZNuvo3lPJSGZ5JPQ=
+github.com/google/go-cmp v0.2.0/go.mod h1:oXzfMopK8JAjlY9xF4vHSVASa0yLyX7SntLO5aqRK0M=
+github.com/google/go-cmp v0.3.0/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
+github.com/google/go-cmp v0.3.1/go.mod h1:8QqcDgzrUqlUb/G2PQTWiueGozuR1884gddMywk6iLU=
+github.com/google/go-cmp v0.4.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.0/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.2/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
+github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/gofuzz v1.1.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
+github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/martian v2.1.0+incompatible/go.mod h1:9I4somxYTbIHy5NJKHRl3wXiIaQGbYVAs8BPL6v8lEs=
+github.com/google/pprof v0.0.0-20181206194817-3ea8567a2e57/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=
+github.com/google/pprof v0.0.0-20190515194954-54271f7e092f/go.mod h1:zfwlbNMJ+OItoe0UupaVj+oy1omPYYDuagoSzA8v9mc=
+github.com/google/pprof v0.0.0-20191218002539-d4f498aebedc/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=
+github.com/google/pprof v0.0.0-20200212024743-f11f1df84d12/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=
+github.com/google/pprof v0.0.0-20200229191704-1ebb73c60ed3/go.mod h1:ZgVRPoUq/hfqzAqh7sHMqb3I9Rq5C59dIz2SbBwJ4eM=
+github.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=
+github.com/google/uuid v1.1.1/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/google/uuid v1.1.2/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
+github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/googleapis/gax-go/v2 v2.0.4/go.mod h1:0Wqv26UfaUD9n4G6kQubkQ+KchISgw+vpHVxEJEs9eg=
+github.com/googleapis/gax-go/v2 v2.0.5/go.mod h1:DWXyrwAJ9X0FpwwEdw+IPEYBICEFu5mhpdKc/us6bOk=
+github.com/googleapis/gnostic v0.4.1 h1:DLJCy1n/vrD4HPjOvYcT8aYQXpPIzoRZONaYwyycI+I=
+github.com/googleapis/gnostic v0.4.1/go.mod h1:LRhVm6pbyptWbWbuZ38d1eyptfvIytN3ir6b65WBswg=
+github.com/gorilla/websocket v1.4.2/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
+github.com/gregjones/httpcache v0.0.0-20180305231024-9cad4c3443a7/go.mod h1:FecbI9+v66THATjSRHfNgh1IVFe/9kFxbXtjV0ctIMA=
+github.com/hashicorp/errwrap v1.0.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=
+github.com/hashicorp/errwrap v1.1.0 h1:OxrOeh75EUXMY8TBjag2fzXGZ40LB6IKw45YeGUDY2I=
+github.com/hashicorp/errwrap v1.1.0/go.mod h1:YH+1FKiLXxHSkmPseP+kNlulaMuP3n2brvKWEqk/Jc4=
+github.com/hashicorp/go-cleanhttp v0.5.2 h1:035FKYIWjmULyFRBKPs8TBQoi0x6d9G4xc9neXJWAZQ=
+github.com/hashicorp/go-cleanhttp v0.5.2/go.mod h1:kO/YDlP8L1346E6Sodw+PrpBSV4/SoxCXGY6BqNFT48=
+github.com/hashicorp/go-hclog v1.6.3 h1:Qr2kF+eVWjTiYmU7Y31tYlP1h0q/X3Nl3tPGdaB11/k=
+github.com/hashicorp/go-hclog v1.6.3/go.mod h1:W4Qnvbt70Wk/zYJryRzDRU/4r0kIg0PVHBcfoyhpF5M=
+github.com/hashicorp/go-multierror v1.0.0/go.mod h1:dHtQlpGsu+cZNNAkkCN/P3hoUDHhCYQXV3UM06sGGrk=
+github.com/hashicorp/go-multierror v1.1.1 h1:H5DkEtf6CXdFp0N0Em5UCwQpXMWke8IA0+lD48awMYo=
+github.com/hashicorp/go-multierror v1.1.1/go.mod h1:iw975J/qwKPdAO1clOe2L8331t/9/fmwbPZ6JB6eMoM=
+github.com/hashicorp/go-retryablehttp v0.7.6 h1:TwRYfx2z2C4cLbXmT8I5PgP/xmuqASDyiVuGYfs9GZM=
+github.com/hashicorp/go-retryablehttp v0.7.6/go.mod h1:pkQpWZeYWskR+D1tR2O5OcBFOxfA7DoAO6xtkuQnHTk=
+github.com/hashicorp/go-rootcerts v1.0.2 h1:jzhAVGtqPKbwpyCPELlgNWhE1znq+qwJtW5Oi2viEzc=
+github.com/hashicorp/go-rootcerts v1.0.2/go.mod h1:pqUvnprVnM5bf7AOirdbb01K4ccR319Vf4pU3K5EGc8=
+github.com/hashicorp/go-secure-stdlib/parseutil v0.1.6 h1:om4Al8Oy7kCm/B86rLCLah4Dt5Aa0Fr5rYBG60OzwHQ=
+github.com/hashicorp/go-secure-stdlib/parseutil v0.1.6/go.mod h1:QmrqtbKuxxSWTN3ETMPuB+VtEiBJ/A9XhoYGv8E1uD8=
+github.com/hashicorp/go-secure-stdlib/strutil v0.1.1/go.mod h1:gKOamz3EwoIoJq7mlMIRBpVTAUn8qPCrEclOKKWhD3U=
+github.com/hashicorp/go-secure-stdlib/strutil v0.1.2 h1:kes8mmyCpxJsI7FTwtzRqEy9CdjCtrXrXGuOpxEA7Ts=
+github.com/hashicorp/go-secure-stdlib/strutil v0.1.2/go.mod h1:Gou2R9+il93BqX25LAKCLuM+y9U2T4hlwvT1yprcna4=
+github.com/hashicorp/go-sockaddr v1.0.2 h1:ztczhD1jLxIRjVejw8gFomI1BQZOe2WoVOu0SyteCQc=
+github.com/hashicorp/go-sockaddr v1.0.2/go.mod h1:rB4wwRAUzs07qva3c5SdrY/NEtAUjGlgmH/UkBUC97A=
+github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=
+github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=
+github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=
+github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
+github.com/hashicorp/vault/api v1.14.0 h1:Ah3CFLixD5jmjusOgm8grfN9M0d+Y8fVR2SW0K6pJLU=
+github.com/hashicorp/vault/api v1.14.0/go.mod h1:pV9YLxBGSz+cItFDd8Ii4G17waWOQ32zVjMWHe/cOqk=
+github.com/hpcloud/tail v1.0.0/go.mod h1:ab1qPbhIpdTxEkNHXyeSf5vhxWSCs/tWer42PpOxQnU=
+github.com/ianlancetaylor/demangle v0.0.0-20181102032728-5e5cf60278f6/go.mod h1:aSSvb/t6k1mPoxDqO4vJh6VOCGPwU4O0C2/Eqndh1Sc=
+github.com/imdario/mergo v0.3.5/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=
+github.com/jackc/chunkreader v1.0.0/go.mod h1:RT6O25fNZIuasFJRyZ4R/Y2BbhasbmZXF9QQ7T3kePo=
+github.com/jackc/chunkreader/v2 v2.0.0/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=
+github.com/jackc/chunkreader/v2 v2.0.1 h1:i+RDz65UE+mmpjTfyz0MoVTnzeYxroil2G82ki7MGG8=
+github.com/jackc/chunkreader/v2 v2.0.1/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=
+github.com/jackc/pgconn v0.0.0-20190420214824-7e0022ef6ba3/go.mod h1:jkELnwuX+w9qN5YIfX0fl88Ehu4XC3keFuOJJk9pcnA=
+github.com/jackc/pgconn v0.0.0-20190824142844-760dd75542eb/go.mod h1:lLjNuW/+OfW9/pnVKPazfWOgNfH2aPem8YQ7ilXGvJE=
+github.com/jackc/pgconn v0.0.0-20190831204454-2fabfa3c18b7/go.mod h1:ZJKsE/KZfsUgOEh9hBm+xYTstcNHg7UPMVJqRfQxq4s=
+github.com/jackc/pgconn v1.8.0/go.mod h1:1C2Pb36bGIP9QHGBYCjnyhqu7Rv3sGshaQUvmfGIB/o=
+github.com/jackc/pgconn v1.9.0/go.mod h1:YctiPyvzfU11JFxoXokUOOKQXQmDMoJL9vJzHH8/2JY=
+github.com/jackc/pgconn v1.9.1-0.20210724152538-d89c8390a530/go.mod h1:4z2w8XhRbP1hYxkpTuBjTS3ne3J48K83+u0zoyvg2pI=
+github.com/jackc/pgconn v1.14.3 h1:bVoTr12EGANZz66nZPkMInAV/KHD2TxH9npjXXgiB3w=
+github.com/jackc/pgconn v1.14.3/go.mod h1:RZbme4uasqzybK2RK5c65VsHxoyaml09lx3tXOcO/VM=
+github.com/jackc/pgio v1.0.0 h1:g12B9UwVnzGhueNavwioyEEpAmqMe1E/BN9ES+8ovkE=
+github.com/jackc/pgio v1.0.0/go.mod h1:oP+2QK2wFfUWgr+gxjoBH9KGBb31Eio69xUb0w5bYf8=
+github.com/jackc/pgmock v0.0.0-20190831213851-13a1b77aafa2/go.mod h1:fGZlG77KXmcq05nJLRkk0+p82V8B8Dw8KN2/V9c/OAE=
+github.com/jackc/pgmock v0.0.0-20201204152224-4fe30f7445fd/go.mod h1:hrBW0Enj2AZTNpt/7Y5rr2xe/9Mn757Wtb2xeBzPv2c=
+github.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65 h1:DadwsjnMwFjfWc9y5Wi/+Zz7xoE5ALHsRQlOctkOiHc=
+github.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65/go.mod h1:5R2h2EEX+qri8jOWMbJCtaPWkrrNc7OHwsp2TCqp7ak=
+github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
+github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
+github.com/jackc/pgproto3 v1.1.0/go.mod h1:eR5FA3leWg7p9aeAqi37XOTgTIbkABlvcPB3E5rlc78=
+github.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190420180111-c116219b62db/go.mod h1:bhq50y+xrl9n5mRYyCBFKkpRVTLYJVWeCc+mEAI3yXA=
+github.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190609003834-432c2951c711/go.mod h1:uH0AWtUmuShn0bcesswc4aBTWGvw0cAxIJp+6OB//Wg=
+github.com/jackc/pgproto3/v2 v2.0.0-rc3/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=
+github.com/jackc/pgproto3/v2 v2.0.0-rc3.0.20190831210041-4c03ce451f29/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=
+github.com/jackc/pgproto3/v2 v2.0.6/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgproto3/v2 v2.1.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgproto3/v2 v2.3.3 h1:1HLSx5H+tXR9pW3in3zaztoEwQYRC9SQaYUHjTSUOag=
+github.com/jackc/pgproto3/v2 v2.3.3/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b/go.mod h1:vsD4gTJCa9TptPL8sPkXrLZ+hDuNrZCnj29CQpr4X1E=
+github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a h1:bbPeKD0xmW/Y25WS6cokEszi5g+S0QxI/d45PkRi7Nk=
+github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
+github.com/jackc/pgtype v0.0.0-20190421001408-4ed0de4755e0/go.mod h1:hdSHsc1V01CGwFsrv11mJRHWJ6aifDLfdV3aVjFF0zg=
+github.com/jackc/pgtype v0.0.0-20190824184912-ab885b375b90/go.mod h1:KcahbBH1nCMSo2DXpzsoWOAfFkdEtEJpPbVLq8eE+mc=
+github.com/jackc/pgtype v0.0.0-20190828014616-a8802b16cc59/go.mod h1:MWlu30kVJrUS8lot6TQqcg7mtthZ9T0EoIBFiJcmcyw=
+github.com/jackc/pgtype v1.8.1-0.20210724151600-32e20a603178/go.mod h1:C516IlIV9NKqfsMCXTdChteoXmwgUceqaLfjg2e3NlM=
+github.com/jackc/pgtype v1.14.0 h1:y+xUdabmyMkJLyApYuPj38mW+aAIqCe5uuBB51rH3Vw=
+github.com/jackc/pgtype v1.14.0/go.mod h1:LUMuVrfsFfdKGLw+AFFVv6KtHOFMwRgDDzBt76IqCA4=
+github.com/jackc/pgx/v4 v4.0.0-20190420224344-cc3461e65d96/go.mod h1:mdxmSJJuR08CZQyj1PVQBHy9XOp5p8/SHH6a0psbY9Y=
+github.com/jackc/pgx/v4 v4.0.0-20190421002000-1b8f0016e912/go.mod h1:no/Y67Jkk/9WuGR0JG/JseM9irFbnEPbuWV2EELPNuM=
+github.com/jackc/pgx/v4 v4.0.0-pre1.0.20190824185557-6972a5742186/go.mod h1:X+GQnOEnf1dqHGpw7JmHqHc1NxDoalibchSk9/RWuDc=
+github.com/jackc/pgx/v4 v4.12.1-0.20210724153913-640aa07df17c/go.mod h1:1QD0+tgSXP7iUjYm9C1NxKhny7lq6ee99u/z+IHFcgs=
+github.com/jackc/pgx/v4 v4.18.2 h1:xVpYkNR5pk5bMCZGfClbO962UIqVABcAGt7ha1s/FeU=
+github.com/jackc/pgx/v4 v4.18.2/go.mod h1:Ey4Oru5tH5sB6tV7hDmfWFahwF15Eb7DNXlRKx2CkVw=
+github.com/jackc/puddle v0.0.0-20190413234325-e4ced69a3a2b/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jackc/puddle v0.0.0-20190608224051-11cab39313c9/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jackc/puddle v1.1.3/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jackc/puddle v1.3.0 h1:eHK/5clGOatcjX3oWGBO/MpxpbHzSwud5EWTSCI+MX0=
+github.com/jackc/puddle v1.3.0/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jarcoal/httpmock v1.3.1 h1:iUx3whfZWVf3jT01hQTO/Eo5sAYtB2/rqaUuOtpInww=
+github.com/jarcoal/httpmock v1.3.1/go.mod h1:3yb8rc4BI7TCBhFY8ng0gjuLKJNquuDNiPaZjnENuYg=
+github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
+github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
+github.com/json-iterator/go v1.1.6/go.mod h1:+SdeFBvtyEkXs7REEP0seUULqWtbJapLOCVDaaPEHmU=
+github.com/json-iterator/go v1.1.10/go.mod h1:KdQUCv79m/52Kvf8AW2vK1V8akMuk1QjK/uOdHXbAo4=
+github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
+github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
+github.com/jstemmer/go-junit-report v0.0.0-20190106144839-af01ea7f8024/go.mod h1:6v2b51hI/fHJwM22ozAgKL4VKDeJcHhJFhtBdhmNjmU=
+github.com/jstemmer/go-junit-report v0.9.1/go.mod h1:Brl9GWCQeLvo8nXZwPNNblvFj/XSXhF0NWZEnDohbsk=
+github.com/kisielk/errcheck v1.2.0/go.mod h1:/BMXB+zMLi60iA8Vv6Ksmxu/1UDYcXs4uQLJ+jE2L00=
+github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
+github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
+github.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=
+github.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=
+github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
+github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
+github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
+github.com/kr/pretty v0.2.0/go.mod h1:ipq/a2n7PKx3OHsz4KJII5eveXtPO4qwEXGdVfWzfnI=
+github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
+github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
+github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
+github.com/kr/pty v1.1.5/go.mod h1:9r2w37qlBe7rQ6e1fg1S/9xpWHSnaqNdHD3WcMdbPDA=
+github.com/kr/pty v1.1.8/go.mod h1:O1sed60cT9XZ5uDucP5qwvh+TE3NnUj51EiZO/lmSfw=
+github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
+github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
+github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
+github.com/leodido/go-urn v1.4.0 h1:WT9HwE9SGECu3lg4d/dIA+jxlljEa1/ffXKmRjqdmIQ=
+github.com/leodido/go-urn v1.4.0/go.mod h1:bvxc+MVxLKB4z00jd1z+Dvzr47oO32F/QSNjSBOlFxI=
+github.com/lib/pq v1.0.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.1.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.2.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.10.2 h1:AqzbZs4ZoCBp+GtejcpCpcxM3zlSMx29dXbUSeVtJb8=
+github.com/lib/pq v1.10.2/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
+github.com/mailru/easyjson v0.0.0-20190614124828-94de47d64c63/go.mod h1:C1wdFJiN94OJF2b5HbByQZoLdCWB1Yqtg26g4irojpc=
+github.com/mailru/easyjson v0.0.0-20190626092158-b2ccc519800e/go.mod h1:C1wdFJiN94OJF2b5HbByQZoLdCWB1Yqtg26g4irojpc=
+github.com/mailru/easyjson v0.7.6 h1:8yTIVnZgCoiM1TgqoeTl+LfU5Jg6/xL3QhGQnimLYnA=
+github.com/mailru/easyjson v0.7.6/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
+github.com/mattn/go-colorable v0.0.9/go.mod h1:9vuHe8Xs5qXnSaW/c/ABM9alt+Vo+STaOChaDxuIBZU=
+github.com/mattn/go-colorable v0.1.1/go.mod h1:FuOcm+DKB9mbwrcAfNl7/TZVBZ6rcnceauSikq3lYCQ=
+github.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=
+github.com/mattn/go-colorable v0.1.13 h1:fFA4WZxdEF4tXPZVKMLwD8oUnCTTo08duU7wxecdEvA=
+github.com/mattn/go-colorable v0.1.13/go.mod h1:7S9/ev0klgBDR4GtXTXX8a3vIGJpMovkB8vQcUbaXHg=
+github.com/mattn/go-isatty v0.0.3/go.mod h1:M+lRXTBqGeGNdLjl/ufCoiOlB5xdOkqRJdNxMWT7Zi4=
+github.com/mattn/go-isatty v0.0.5/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
+github.com/mattn/go-isatty v0.0.7/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
+github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=
+github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=
+github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
+github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
+github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
+github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
+github.com/mitchellh/cli v1.0.0/go.mod h1:hNIlj7HEI86fIcpObd7a0FcrxTWetlwJDGcceTlRvqc=
+github.com/mitchellh/go-homedir v1.1.0 h1:lukF9ziXFxDFPkA1vsr5zpc1XuPDn/wFntq5mG+4E0Y=
+github.com/mitchellh/go-homedir v1.1.0/go.mod h1:SfyaCUpYCn1Vlf4IUYiD9fPX4A5wJrkLzIz1N1q0pr0=
+github.com/mitchellh/go-wordwrap v1.0.0/go.mod h1:ZXFpozHsX6DPmq2I0TCekCxypsnAUbP2oI0UX1GXzOo=
+github.com/mitchellh/mapstructure v1.1.2/go.mod h1:FVVH3fgwuzCH5S8UJGiWEs2h04kUh9fWfEaFds41c1Y=
+github.com/mitchellh/mapstructure v1.4.1/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=
+github.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=
+github.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=
+github.com/moby/spdystream v0.2.0/go.mod h1:f7i0iNDQJ059oMTcWxx8MA/zKFIuD/lY+0GqbN2Wy8c=
+github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/reflect2 v0.0.0-20180701023420-4b7aa43c6742/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=
+github.com/modern-go/reflect2 v1.0.1/go.mod h1:bx2lNnkwVCuqBIxFjflWJWanXIb3RllmbCylyMrvgv0=
+github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
+github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
+github.com/munnerz/goautoneg v0.0.0-20120707110453-a547fc61f48d/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
+github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f/go.mod h1:ZdcZmHo+o7JKHSa8/e818NopupXU1YMK5fe1lsApnBw=
+github.com/niemeyer/pretty v0.0.0-20200227124842-a10e7caefd8e/go.mod h1:zD1mROLANZcx1PVRCS0qkT7pwLkGfwJo4zjcN/Tysno=
+github.com/onsi/ginkgo v0.0.0-20170829012221-11459a886d9c/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=
+github.com/onsi/ginkgo v1.6.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=
+github.com/onsi/ginkgo v1.11.0/go.mod h1:lLunBs/Ym6LB5Z9jYTR76FiuTmxDTDusOGeTQH+WWjE=
+github.com/onsi/gomega v0.0.0-20170829124025-dcabb60a477c/go.mod h1:C1qb7wdrVGGVU+Z6iS04AVkA3Q65CEZX59MT0QO5uiA=
+github.com/onsi/gomega v1.7.0/go.mod h1:ex+gbHU/CVuBBDIJjb2X0qEXbFg53c61hWP/1CpauHY=
+github.com/peterbourgon/diskv v2.0.1+incompatible/go.mod h1:uqqh8zWWbv1HBMNONnaR/tNboyR3/BZd58JJSHlUSCU=
+github.com/philhofer/fwd v1.1.3-0.20240916144458-20a13a1f6b7c h1:dAMKvw0MlJT1GshSTtih8C2gDs04w8dReiOGXrGLNoY=
+github.com/philhofer/fwd v1.1.3-0.20240916144458-20a13a1f6b7c/go.mod h1:RqIHx9QI14HlwKwm98g9Re5prTQ6LdeRQn+gXJFxsJM=
+github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/posener/complete v1.1.1/go.mod h1:em0nMJCgc9GFtwrmVmEMR/ZL6WyhyjMBndrE9hABlRI=
+github.com/prometheus/client_golang v1.19.0 h1:ygXvpU1AoN1MhdzckN+PyD9QJOSD4x7kmXYlnfbA6JU=
+github.com/prometheus/client_golang v1.19.0/go.mod h1:ZRM9uEAypZakd+q/x7+gmsvXdURP+DABIEIjnmDdp+k=
+github.com/prometheus/client_model v0.0.0-20190812154241-14fe0d1b01d4/go.mod h1:xMI15A0UPsDsEKsMN9yxemIoYk6Tm2C1GtYGdfGttqA=
+github.com/prometheus/client_model v0.6.1 h1:ZKSh/rekM+n3CeS952MLRAdFwIKqeY8b62p8ais2e9E=
+github.com/prometheus/client_model v0.6.1/go.mod h1:OrxVMOVHjw3lKMa8+x6HeMGkHMQyHDk9E3jmP2AmGiY=
+github.com/prometheus/common v0.53.0 h1:U2pL9w9nmJwJDa4qqLQ3ZaePJ6ZTwt7cMD3AG3+aLCE=
+github.com/prometheus/common v0.53.0/go.mod h1:BrxBKv3FWBIGXw89Mg1AeBq7FSyRzXWI3l3e7W3RN5U=
+github.com/prometheus/procfs v0.14.0 h1:Lw4VdGGoKEZilJsayHf0B+9YgLGREba2C6xr+Fdfq6s=
+github.com/prometheus/procfs v0.14.0/go.mod h1:XL+Iwz8k8ZabyZfMFHPiilCniixqQarAy5Mu67pHlNQ=
+github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
+github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
+github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
+github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=
+github.com/rogpeppe/go-internal v1.10.0 h1:TMyTOH3F/DB16zRVcYyreMH6GnZZrwQVAoYjRBZyWFQ=
+github.com/rogpeppe/go-internal v1.10.0/go.mod h1:UQnix2H7Ngw/k4C5ijL5+65zddjncjaFoBhdsK/akog=
+github.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=
+github.com/rs/zerolog v1.13.0/go.mod h1:YbFCdg8HfsridGWAh22vktObvhZbQsZXe4/zB0OKkWU=
+github.com/rs/zerolog v1.15.0/go.mod h1:xYTKnLHcpfU2225ny5qZjxnj9NvkumZYjJHlAThCjNc=
+github.com/ryanuber/columnize v2.1.0+incompatible/go.mod h1:sm1tb6uqfes/u+d4ooFouqFdy9/2g9QGwK3SQygK0Ts=
+github.com/ryanuber/go-glob v1.0.0 h1:iQh3xXAumdQ+4Ufa5b25cRpC5TYKlno6hsv6Cb3pkBk=
+github.com/ryanuber/go-glob v1.0.0/go.mod h1:807d1WSdnB0XRJzKNil9Om6lcp/3a0v4qIHxIXzX/Yc=
+github.com/satori/go.uuid v1.2.0/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=
+github.com/shopspring/decimal v0.0.0-20180709203117-cd690d0c9e24/go.mod h1:M+9NzErvs504Cn4c5DxATwIqPbtswREoFCre64PpcG4=
+github.com/shopspring/decimal v1.2.0 h1:abSATXmQEYyShuxI4/vyW3tV1MrKAJzCZ/0zLUXYbsQ=
+github.com/shopspring/decimal v1.2.0/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=
+github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=
+github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=
+github.com/spf13/afero v1.2.2/go.mod h1:9ZxEEn6pIJ8Rxe320qSDBk6AsU0r9pR7Q4OcevTdifk=
+github.com/spf13/pflag v0.0.0-20170130214245-9ff6c6923cff/go.mod h1:DYY7MBk1bdzusC3SYhjObp+wFpr4gzcvqqNjLnInEg4=
+github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
+github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=
+github.com/stretchr/objx v0.5.2 h1:xuMeJ0Sdp5ZMRXx/aWO6RZxdr3beISkG5/G/aIRr3pY=
+github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
+github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=
+github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=
+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=
+github.com/stretchr/testify v1.6.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=
+github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
+github.com/swaggo/files/v2 v2.0.0 h1:hmAt8Dkynw7Ssz46F6pn8ok6YmGZqHSVLZ+HQM7i0kw=
+github.com/swaggo/files/v2 v2.0.0/go.mod h1:24kk2Y9NYEJ5lHuCra6iVwkMjIekMCaFq/0JQj66kyM=
+github.com/swaggo/swag v1.16.3 h1:PnCYjPCah8FK4I26l2F/KQ4yz3sILcVUN3cTlBFA9Pg=
+github.com/swaggo/swag v1.16.3/go.mod h1:DImHIuOFXKpMFAQjcC7FG4m3Dg4+QuUgUzJmKjI/gRk=
+github.com/tinylib/msgp v1.2.5 h1:WeQg1whrXRFiZusidTQqzETkRpGjFjcIhW6uqWH09po=
+github.com/tinylib/msgp v1.2.5/go.mod h1:ykjzy2wzgrlvpDCRc4LA8UXy6D8bzMSuAF3WD57Gok0=
+github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=
+github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=
+github.com/valyala/fasthttp v1.52.0 h1:wqBQpxH71XW0e2g+Og4dzQM8pk34aFYlA1Ga8db7gU0=
+github.com/valyala/fasthttp v1.52.0/go.mod h1:hf5C4QnVMkNXMspnsUlfM3WitlgYflyhHYoKol/szxQ=
+github.com/valyala/tcplisten v1.0.0 h1:rBHj/Xf+E1tRGZyWIWwJDiRY0zc1Js+CV5DqwacVSA8=
+github.com/valyala/tcplisten v1.0.0/go.mod h1:T0xQ8SeCZGxckz9qRXTfG43PvQ/mcWh7FwZEA7Ioqkc=
+github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+github.com/zenazn/goji v0.9.0/go.mod h1:7S9M489iMyHBNxwZnk9/EHS098H4/F6TATF2mIxtB1Q=
+go.opencensus.io v0.21.0/go.mod h1:mSImk1erAIZhrmZN+AvHh14ztQfjbGwt4TtuofqLduU=
+go.opencensus.io v0.22.0/go.mod h1:+kGneAE2xo2IficOXnaByMWTGM9T73dGwxeWcUqIpI8=
+go.opencensus.io v0.22.2/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=
+go.opencensus.io v0.22.3/go.mod h1:yxeiOL68Rb0Xd1ddK5vPZ/oVn4vY4Ynel7k9FzqtOIw=
+go.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
+go.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
+go.uber.org/atomic v1.5.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=
+go.uber.org/atomic v1.6.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=
+go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
+go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
+go.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=
+go.uber.org/multierr v1.3.0/go.mod h1:VgVr7evmIr6uPjLBxg28wmKNXyqE9akIJ5XnfpiKl+4=
+go.uber.org/multierr v1.5.0/go.mod h1:FeouvMocqHpRaaGuG9EjoKcStLC43Zu/fmqdUMPcKYU=
+go.uber.org/multierr v1.10.0 h1:S0h4aNzvfcFsC3dRF1jLoaov7oRaKqRGC/pUEJ2yvPQ=
+go.uber.org/multierr v1.10.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
+go.uber.org/tools v0.0.0-20190618225709-2cfd321de3ee/go.mod h1:vJERXedbb3MVM5f9Ejo0C68/HhF8uaILCdgjnY+goOA=
+go.uber.org/zap v1.9.1/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
+go.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
+go.uber.org/zap v1.13.0/go.mod h1:zwrFLgMcdUuIBviXEYEH1YKNaOBnKXsx2IPda5bBwHM=
+go.uber.org/zap v1.27.0 h1:aJMhYGrd5QSmlpLMr2MftRKl7t8J8PTZPA732ud/XR8=
+go.uber.org/zap v1.27.0/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
+golang.org/x/crypto v0.0.0-20190411191339-88737f569e3a/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=
+golang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20190605123033-f99c8df09eb5/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20190611184440-5c40567a22f8/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20190820162420-60c769a6c586/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.0.0-20201002170205-7f63de1d35b0/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.0.0-20201203163018-be400aefbc4c/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=
+golang.org/x/crypto v0.0.0-20210616213533-5ff15b29337e/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
+golang.org/x/crypto v0.0.0-20210711020723-a769d52b0f97/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
+golang.org/x/crypto v0.35.0 h1:b15kiHdrGCHrP6LvwaQ3c03kgNhhiMgvlhxHQhmg2Xs=
+golang.org/x/crypto v0.35.0/go.mod h1:dy7dXNW32cAb/6/PRuTNsix8T+vJAqvuIy5Bli/x0YQ=
+golang.org/x/exp v0.0.0-20190121172915-509febef88a4/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=
+golang.org/x/exp v0.0.0-20190306152737-a1d7652674e8/go.mod h1:CJ0aWSM057203Lf6IL+f9T1iT9GByDxfZKAQTCR3kQA=
+golang.org/x/exp v0.0.0-20190510132918-efd6b22b2522/go.mod h1:ZjyILWgesfNpC6sMxTJOJm9Kp84zZh5NQWvqDGG3Qr8=
+golang.org/x/exp v0.0.0-20190829153037-c13cbed26979/go.mod h1:86+5VVa7VpoJ4kLfm080zCjGlMRFzhUhsZKEZO7MGek=
+golang.org/x/exp v0.0.0-20191030013958-a1ab85dbe136/go.mod h1:JXzH8nQsPlswgeRAPE3MuO9GYsAcnJvJ4vnMwN/5qkY=
+golang.org/x/exp v0.0.0-20191129062945-2f5052295587/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=
+golang.org/x/exp v0.0.0-20191227195350-da58074b4299/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=
+golang.org/x/exp v0.0.0-20200119233911-0405dc783f0a/go.mod h1:2RIsYlXP63K8oxa1u096TMicItID8zy7Y6sNkU49FU4=
+golang.org/x/exp v0.0.0-20200207192155-f17229e696bd/go.mod h1:J/WKrq2StrnmMY6+EHIKF9dgMWnmCNThgcyBT1FY9mM=
+golang.org/x/exp v0.0.0-20200224162631-6cc2880d07d6/go.mod h1:3jZMyOhIsHpP37uCMkUooju7aAi5cS1Q23tOzKc+0MU=
+golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3 h1:hNQpMuAJe5CtcUqCXaWga3FHu+kQvCqcsoVaQgSV60o=
+golang.org/x/exp v0.0.0-20240112132812-db7319d0e0e3/go.mod h1:idGWGoKP1toJGkd5/ig9ZLuPcZBC3ewk7SzmH0uou08=
+golang.org/x/image v0.0.0-20190227222117-0694c2d4d067/go.mod h1:kZ7UVZpmo3dzQBMxlp+ypCbDeSB+sBbTgSJuh5dn5js=
+golang.org/x/image v0.0.0-20190802002840-cff245a6509b/go.mod h1:FeLwcggjj3mMvU+oOTbSwawSJRM1uh48EjtB4UJZlP0=
+golang.org/x/lint v0.0.0-20181026193005-c67002cb31c3/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
+golang.org/x/lint v0.0.0-20190227174305-5b3e6a55c961/go.mod h1:wehouNa3lNwaWXcvxsM5YxQ5yQlVC4a0KAMCusXpPoU=
+golang.org/x/lint v0.0.0-20190301231843-5614ed5bae6f/go.mod h1:UVdnD1Gm6xHRNCYTkRU2/jEulfH38KcIWyp/GAMgvoE=
+golang.org/x/lint v0.0.0-20190313153728-d0100b6bd8b3/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/lint v0.0.0-20190409202823-959b441ac422/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/lint v0.0.0-20190909230951-414d861bb4ac/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/lint v0.0.0-20191125180803-fdd1cda4f05f/go.mod h1:5qLYkcX4OjUUV8bRuDixDT3tpyyb+LUpUlRWLxfhWrs=
+golang.org/x/lint v0.0.0-20200130185559-910be7a94367/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=
+golang.org/x/lint v0.0.0-20200302205851-738671d3881b/go.mod h1:3xt1FjdF8hUf6vQPIChWIBhFzV8gjjsPE/fR3IyQdNY=
+golang.org/x/mobile v0.0.0-20190312151609-d3739f865fa6/go.mod h1:z+o9i4GpDbdi3rU15maQ/Ox0txvL9dWGYEHz965HBQE=
+golang.org/x/mobile v0.0.0-20190719004257-d2bd2a29d028/go.mod h1:E/iHnbuqvinMTCcRqshq8CkpyQDoeVncDDYHnLhea+o=
+golang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=
+golang.org/x/mod v0.1.0/go.mod h1:0QHyrYULN0/3qlju5TqG8bIK38QM8yzMo5ekMj3DlcY=
+golang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=
+golang.org/x/mod v0.1.1-0.20191107180719-034126e5016b/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=
+golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/mod v0.17.0 h1:zY54UmvipHiNd+pm+m0x9KhZ9hl1/7QNMyxXbc6ICqA=
+golang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
+golang.org/x/net v0.0.0-20180724234803-3673e40ba225/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20180826012351-8a410e7b638d/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20180906233101-161cd47e91fd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20190108225652-1e06a53dbb7e/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20190213061140-3a22650c66bd/go.mod h1:mL1N/T3taQHkDXs73rZJwtUhF3w3ftmwwsq0BUmARs4=
+golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190501004415-9ce7a6920f09/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190503192946-f4e77d36d62c/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190603091049-60506f45cf65/go.mod h1:HSz+uSET+XFnRR8LxR5pz3Of3rY3CfYBVs4xY44aLks=
+golang.org/x/net v0.0.0-20190613194153-d28f0bde5980/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20190724013045-ca1201d0de80/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20190813141303-74dc4d7220e7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20190827160401-ba9fcec4b297/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20191209160850-c0dbc17a3553/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200114155413-6afb5195e5aa/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200202094626-16171245cfb2/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200222125558-5a598a2470a0/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200301022130-244492dfa37a/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200324143707-d3edc9973b7e/go.mod h1:qpuaurCH72eLCgpAm/N6yyVIVM9cpaDIP3A8BGJEC5A=
+golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
+golang.org/x/net v0.0.0-20201110031124-69a78807bb2b/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
+golang.org/x/net v0.0.0-20210224082022-3d97a244fca7/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
+golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
+golang.org/x/net v0.0.0-20210421230115-4e50805a0758/go.mod h1:72T/g9IO56b78aLF+1Kcs5dz7/ng1VjMUvfKvpfy+jM=
+golang.org/x/net v0.34.0 h1:Mb7Mrk043xzHgnRM88suvJFwzVrRfHEHJEl5/71CKw0=
+golang.org/x/net v0.34.0/go.mod h1:di0qlW3YNM5oh6GqDGQr92MyTozJPmybPK4Ev/Gm31k=
+golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be/go.mod h1:N/0e6XlmueqKjAGxoOufVs8QHGRruUQn6yWY3a++T0U=
+golang.org/x/oauth2 v0.0.0-20190226205417-e64efc72b421/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
+golang.org/x/oauth2 v0.0.0-20190604053449-0f29369cfe45/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
+golang.org/x/oauth2 v0.0.0-20191202225959-858c2ad4c8b6/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
+golang.org/x/oauth2 v0.0.0-20200107190931-bf48bf16ab8d/go.mod h1:gOpvHmFTYa4IltrdGE7lF6nIHvwfUNPOp7c8zoXwtLw=
+golang.org/x/oauth2 v0.18.0 h1:09qnuIAgzdx1XplqJvW6CQqMCtGZykZWcXzPMPUusvI=
+golang.org/x/oauth2 v0.18.0/go.mod h1:Wf7knwG0MPoWIMMBgFlEaSUDaKskp0dCfrlJRJXbBi8=
+golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20181108010431-42b317875d0f/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190227155943-e225da77a7e6/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.11.0 h1:GGz8+XQP4FvTTrjZPzNKTMFtSXH80RAzG+5ghFPgK9w=
+golang.org/x/sync v0.11.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
+golang.org/x/sys v0.0.0-20180823144017-11551d06cbcc/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20180830151530-49385e6e1522/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20180909124046-d0be0721c37e/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190312061237-fead79001313/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190502145724-3ef323f4f1fd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190507160741-ecd444e8653b/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190606165138-5da285871e9c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190616124812-15dcb6c0061f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190624142023-c5567b49c5d0/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190726091711-fc99dfbffb4e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190813064441-fde4db37ae7a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191001151750-bb3f8db39f24/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191005200804-aed5e4c7ecf9/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191204072324-ce4227a45e2e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191228213918-04cbcbbfeed8/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200113162924-86b910548bc1/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200122134326-e047566fdf82/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200202164722-d101bd2416d5/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200212091648-12a6c2dcc1e4/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200302150141-5c8b2ff67527/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200323222414-85ca7c5b95cd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20201112073958-5cba982894dd/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20210225134936-a50acf3fe073/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20210420072515-93ed5bcd2bfe/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.30.0 h1:QjkSwP/36a20jFYWkSue1YwXzLmsV5Gfq7Eiy72C1uc=
+golang.org/x/sys v0.30.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
+golang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=
+golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
+golang.org/x/term v0.29.0 h1:L6pJp37ocefwRRtYPKSWOWzOtWSxVajvz2ldH/xi3iU=
+golang.org/x/term v0.29.0/go.mod h1:6bl4lRlvVuDgSf3179VpIxBF0o10JUpXWOnI7nErv7s=
+golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.1-0.20180807135948-17ff2d5776d2/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=
+golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.3.4/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=
+golang.org/x/text v0.22.0 h1:bofq7m3/HAFvbF51jz3Q9wLg3jkvSPuiZu/pD1XwgtM=
+golang.org/x/text v0.22.0/go.mod h1:YRoo4H8PVmsu+E3Ou7cqLVH8oXWIHVoX0jqUWALQhfY=
+golang.org/x/time v0.0.0-20181108054448-85acf8d2951c/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/time v0.0.0-20190308202827-9d24e82272b4/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/time v0.0.0-20191024005414-555d28b269f0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/time v0.0.0-20200630173020-3af7569d3a1e/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/time v0.3.0 h1:rg5rLMjNzMS1RkNLzCG38eapWhnYLFYXDXj2gOlr8j4=
+golang.org/x/time v0.3.0/go.mod h1:tRJNPiyCQ0inRvYxbN9jk5I+vvW/OXSQhTDSoE431IQ=
+golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20181030221726-6c7e314b6563/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20190114222345-bf090417da8b/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20190226205152-f727befe758c/go.mod h1:9Yl7xja0Znq3iFh3HoIrodX9oNMXvdceNzlUR8zjMvY=
+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
+golang.org/x/tools v0.0.0-20190312151545-0bb0c0a6e846/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
+golang.org/x/tools v0.0.0-20190312170243-e65039ee4138/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
+golang.org/x/tools v0.0.0-20190425150028-36563e24a262/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20190425163242-31fd60d6bfdc/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20190506145303-2d16b83fe98c/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20190524140312-2c0ae7006135/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20190606124116-d0a3d012864b/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=
+golang.org/x/tools v0.0.0-20190614205625-5aca471b1d59/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=
+golang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=
+golang.org/x/tools v0.0.0-20190628153133-6cdbf07be9d0/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=
+golang.org/x/tools v0.0.0-20190816200558-6889da9d5479/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20190823170909-c4a336ef6a2f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20190911174233-4f2ddba30aff/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191012152004-8de300cfc20a/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191029041327-9cc4af7d6b2c/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191029190741-b9c20aec41a5/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191113191852-77e3bb0ad9e7/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191115202509-3a792d9c32b2/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191125144606-a911d9008d1f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191130070609-6e064ea0cf2d/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191216173652-a0e659d51361/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20191227053925-7b8e75db28f4/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200103221440-774c71fcf114/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200117161641-43d50277825c/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200122220014-bf1340f18c4a/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200130002326-2f3ba24bd6e7/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200204074204-1cc6d1ef6c74/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200207183749-b753a1ba74fa/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200212150539-ea181f53ac56/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200224181240-023911ca70b2/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/tools v0.0.0-20200304193943-95d2e580d8eb/go.mod h1:o4KQGtdN14AW+yjsvvwRTJJuXz8XRtIHtEnmAXLyFUw=
+golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
+golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
+golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d h1:vU5i/LfpvrRCpgM/VPfJLg5KjxD3E+hfT1SH+d9zLwg=
+golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=
+golang.org/x/xerrors v0.0.0-20190410155217-1f06c39b4373/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20190513163551-3ee3066db522/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+google.golang.org/api v0.4.0/go.mod h1:8k5glujaEP+g9n7WNsDg8QP6cUVNI86fCNMcbazEtwE=
+google.golang.org/api v0.7.0/go.mod h1:WtwebWUNSVBH/HAw79HIFXZNqEvBhG+Ra+ax0hx3E3M=
+google.golang.org/api v0.8.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=
+google.golang.org/api v0.9.0/go.mod h1:o4eAsZoiT+ibD93RtjEohWalFOjRDx6CVaqeizhEnKg=
+google.golang.org/api v0.13.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=
+google.golang.org/api v0.14.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=
+google.golang.org/api v0.15.0/go.mod h1:iLdEw5Ide6rF15KTC1Kkl0iskquN2gFfn9o9XIsbkAI=
+google.golang.org/api v0.17.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=
+google.golang.org/api v0.18.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=
+google.golang.org/api v0.20.0/go.mod h1:BwFmGc8tA3vsd7r/7kR8DY7iEEGSU04BFxCo5jP/sfE=
+google.golang.org/appengine v1.1.0/go.mod h1:EbEs0AVv82hx2wNQdGPgUI5lhzA/G0D9YwlJXL52JkM=
+google.golang.org/appengine v1.4.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=
+google.golang.org/appengine v1.5.0/go.mod h1:xpcJRLb0r/rnEns0DIKYYv+WjYCduHsrkT7/EB5XEv4=
+google.golang.org/appengine v1.6.1/go.mod h1:i06prIuMbXzDqacNJfV5OdTW448YApPu5ww/cMBSeb0=
+google.golang.org/appengine v1.6.5/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=
+google.golang.org/appengine v1.6.7 h1:FZR1q0exgwxzPzp/aF+VccGrSfxfPpkBqjIIEq3ru6c=
+google.golang.org/appengine v1.6.7/go.mod h1:8WjMMxjGQR8xUklV/ARdw2HLXBOI7O7uCIDZVag1xfc=
+google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8/go.mod h1:JiN7NxoALGmiZfu7CAH4rXhgtRTLTxftemlI0sWmxmc=
+google.golang.org/genproto v0.0.0-20190307195333-5fe7a883aa19/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
+google.golang.org/genproto v0.0.0-20190418145605-e7d98fc518a7/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
+google.golang.org/genproto v0.0.0-20190425155659-357c62f0e4bb/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
+google.golang.org/genproto v0.0.0-20190502173448-54afdca5d873/go.mod h1:VzzqZJRnGkLBvHegQrXjBqPurQTc5/KpmUdxsrq26oE=
+google.golang.org/genproto v0.0.0-20190801165951-fa694d86fc64/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=
+google.golang.org/genproto v0.0.0-20190819201941-24fa4b261c55/go.mod h1:DMBHOl98Agz4BDEuKkezgsaosCRResVns1a3J2ZsMNc=
+google.golang.org/genproto v0.0.0-20190911173649-1774047e7e51/go.mod h1:IbNlFCBrqXvoKpeg0TB2l7cyZUmoaFKYIwrEpbDKLA8=
+google.golang.org/genproto v0.0.0-20191108220845-16a3f7862a1a/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20191115194625-c23dd37a84c9/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20191216164720-4f79533eabd1/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20191230161307-f3c370f40bfb/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20200115191322-ca5a22157cba/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20200122232147-0452cf42e150/go.mod h1:n3cpQtvxv34hfy77yVDNjmbRyujviMdxYliBSkLhpCc=
+google.golang.org/genproto v0.0.0-20200204135345-fa8e72b47b90/go.mod h1:GmwEX6Z4W5gMy59cAlVYjN9JhxgbQH6Gn+gFDQe2lzA=
+google.golang.org/genproto v0.0.0-20200212174721-66ed5ce911ce/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=
+google.golang.org/genproto v0.0.0-20200224152610-e50cd9704f63/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=
+google.golang.org/genproto v0.0.0-20200305110556-506484158171/go.mod h1:55QSHmfGQM9UVYDPBsyGGes0y52j32PQ3BqQfXhyH3c=
+google.golang.org/genproto v0.0.0-20200526211855-cb27e3aa2013/go.mod h1:NbSheEEYHJ7i3ixzK3sjbqSGDJWnxyFXZblF3eUsNvo=
+google.golang.org/grpc v1.19.0/go.mod h1:mqu4LbDTu4XGKhr4mRzUsmM4RtVoemTSY81AxZiDr8c=
+google.golang.org/grpc v1.20.1/go.mod h1:10oTOabMzJvdu6/UiuZezV6QK5dSlG84ov/aaiqXj38=
+google.golang.org/grpc v1.21.1/go.mod h1:oYelfM1adQP15Ek0mdvEgi9Df8B9CZIaU1084ijfRaM=
+google.golang.org/grpc v1.23.0/go.mod h1:Y5yQAOtifL1yxbo5wqy6BxZv8vAUGQwXBOALyacEbxg=
+google.golang.org/grpc v1.26.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
+google.golang.org/grpc v1.27.0/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
+google.golang.org/grpc v1.27.1/go.mod h1:qbnxyOmOxrQa7FizSgH+ReBfzJrCY1pSN7KXBS8abTk=
+google.golang.org/protobuf v0.0.0-20200109180630-ec00e32a8dfd/go.mod h1:DFci5gLYBciE7Vtevhsrf46CRTquxDuWsQurQQe4oz8=
+google.golang.org/protobuf v0.0.0-20200221191635-4d8936d0db64/go.mod h1:kwYJMbMJ01Woi6D6+Kah6886xMZcty6N08ah7+eCXa0=
+google.golang.org/protobuf v0.0.0-20200228230310-ab0ca4ff8a60/go.mod h1:cfTl7dwQJ+fmap5saPgwCLgHXTUD7jkjRqWcaiX5VyM=
+google.golang.org/protobuf v1.20.1-0.20200309200217-e05f789c0967/go.mod h1:A+miEFZTKqfCUM6K7xSMQL9OKL/b6hQv+e19PK+JZNE=
+google.golang.org/protobuf v1.21.0/go.mod h1:47Nbq4nVaFHyn7ilMalzfO3qCViNmqZ2kzikPIcrTAo=
+google.golang.org/protobuf v1.22.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+google.golang.org/protobuf v1.23.0/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+google.golang.org/protobuf v1.23.1-0.20200526195155-81db48ad09cc/go.mod h1:EGpADcykh3NcUnDUJcl1+ZksZNG86OlYog2l/sGQquU=
+google.golang.org/protobuf v1.25.0/go.mod h1:9JNX74DMeImyA3h4bdi1ymwjUzf21/xIlbajtzgsN7c=
+google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
+google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
+google.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=
+google.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20200227125254-8fa46927fb4f/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=
+gopkg.in/fsnotify.v1 v1.4.7/go.mod h1:Tz8NjZHkW78fSQdbUxIjBTcgA1z1m8ZHf0WmKUhAMys=
+gopkg.in/inconshreveable/log15.v2 v2.0.0-20180818164646-67afb5ed74ec/go.mod h1:aPpfJ7XW+gOuirDoZ8gHhLh3kZ1B08FtV2bbmy7Jv3s=
+gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
+gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
+gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=
+gopkg.in/yaml.v2 v2.2.1/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
+gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
+gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.0-20200615113413-eeeca48fe776/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+honnef.co/go/tools v0.0.0-20190102054323-c2f93a96b099/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+honnef.co/go/tools v0.0.0-20190106161140-3f1c8253044a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+honnef.co/go/tools v0.0.0-20190418001031-e561f6794a2a/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+honnef.co/go/tools v0.0.0-20190523083050-ea95bdfd59fc/go.mod h1:rf3lG4BRIbNafJWhAfAdb/ePZxsR/4RtNHQocxwk9r4=
+honnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=
+honnef.co/go/tools v0.0.1-2020.1.3/go.mod h1:X/FiERA/W4tHapMX5mGpAtMSVEeEUOyHaw9vFzvIQ3k=
+k8s.io/api v0.20.0 h1:WwrYoZNM1W1aQEbyl8HNG+oWGzLpZQBlcerS9BQw9yI=
+k8s.io/api v0.20.0/go.mod h1:HyLC5l5eoS/ygQYl1BXBgFzWNlkHiAuyNAbevIn+FKg=
+k8s.io/apimachinery v0.20.0/go.mod h1:WlLqWAHZGg07AeltaI0MV5uk1Omp8xaN0JGLY6gkRpU=
+k8s.io/apimachinery v0.21.0 h1:3Fx+41if+IRavNcKOz09FwEXDBG6ORh6iMsTSelhkMA=
+k8s.io/apimachinery v0.21.0/go.mod h1:jbreFvJo3ov9rj7eWT7+sYiRx+qZuCYXwWT1bcDswPY=
+k8s.io/client-go v0.20.0 h1:Xlax8PKbZsjX4gFvNtt4F5MoJ1V5prDvCuoq9B7iax0=
+k8s.io/client-go v0.20.0/go.mod h1:4KWh/g+Ocd8KkCwKF8vUNnmqgv+EVnQDK4MBF4oB5tY=
+k8s.io/gengo v0.0.0-20200413195148-3a45101e95ac/go.mod h1:ezvh/TsK7cY6rbqRK0oQQ8IAqLxYwwyPxAX1Pzy0ii0=
+k8s.io/klog/v2 v2.0.0/go.mod h1:PBfzABfn139FHAV07az/IF9Wp1bkk3vpT2XSJ76fSDE=
+k8s.io/klog/v2 v2.4.0/go.mod h1:Od+F08eJP+W3HUb4pSrPpgp9DGU4GzlpG/TmITuYh/Y=
+k8s.io/klog/v2 v2.8.0 h1:Q3gmuM9hKEjefWFFYF0Mat+YyFJvsUyYuwyNNJ5C9Ts=
+k8s.io/klog/v2 v2.8.0/go.mod h1:hy9LJ/NvuK+iVyP4Ehqva4HxZG/oXyIS3n3Jmire4Ec=
+k8s.io/kube-openapi v0.0.0-20201113171705-d219536bb9fd/go.mod h1:WOJ3KddDSol4tAGcJo0Tvi+dK12EcqSLqcWsryKMpfM=
+k8s.io/kube-openapi v0.0.0-20210305001622-591a79e4bda7/go.mod h1:wXW5VT87nVfh/iLV8FpR2uDvrFyomxbtb1KivDbvPTE=
+k8s.io/utils v0.0.0-20201110183641-67b214c5f920 h1:CbnUZsM497iRC5QMVkHwyl8s2tB3g7yaSHkYPkpgelw=
+k8s.io/utils v0.0.0-20201110183641-67b214c5f920/go.mod h1:jPW/WVKK9YHAvNhRxK0md/EJ228hCsBRufyofKtW8HA=
+rsc.io/binaryregexp v0.2.0/go.mod h1:qTv7/COck+e2FymRvadv62gMdZztPaShugOCi3I+8D8=
+rsc.io/quote/v3 v3.1.0/go.mod h1:yEA65RcK8LyAZtP9Kv3t0HmxON59tX3rD+tICJqUlj0=
+rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA=
+sigs.k8s.io/structured-merge-diff/v4 v4.0.2/go.mod h1:bJZC9H9iH24zzfZ/41RGcq60oK1F7G282QMXDPYydCw=
+sigs.k8s.io/structured-merge-diff/v4 v4.1.0 h1:C4r9BgJ98vrKnnVCjwCSXcWjWe0NKcUQkmzDXZXGwH8=
+sigs.k8s.io/structured-merge-diff/v4 v4.1.0/go.mod h1:bJZC9H9iH24zzfZ/41RGcq60oK1F7G282QMXDPYydCw=
+sigs.k8s.io/yaml v1.1.0/go.mod h1:UJmg0vDUVViEyp3mgSv9WPwZCDxu4rQW1olrI1uml+o=
+sigs.k8s.io/yaml v1.2.0/go.mod h1:yfXDCHCao9+ENCvLSE62v9VSji2MKu5jeNfTrofGhJc=
+sigs.k8s.io/yaml v1.3.0 h1:a2VclLzOGrwOHDiV8EfBGhvjHvP46CtW5j6POvhYGGo=
+sigs.k8s.io/yaml v1.3.0/go.mod h1:GeOyir5tyXNByN85N/dRIT9es5UQNerPYEKK56eTBm8=
diff --git a/docker-monitoring-agent/.dockerignore b/docker-monitoring-agent/.dockerignore
new file mode 100644
index 0000000..96c0ecc
--- /dev/null
+++ b/docker-monitoring-agent/.dockerignore
@@ -0,0 +1 @@
+docs/
\ No newline at end of file
diff --git a/docker-monitoring-agent/.gitignore b/docker-monitoring-agent/.gitignore
new file mode 100644
index 0000000..af2026b
--- /dev/null
+++ b/docker-monitoring-agent/.gitignore
@@ -0,0 +1,3 @@
+# Build output.
+target
+/_output/*
diff --git a/docker-monitoring-agent/CODE-OF-CONDUCT.md b/docker-monitoring-agent/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-monitoring-agent/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-monitoring-agent/CONTRIBUTING.md b/docker-monitoring-agent/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-monitoring-agent/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-monitoring-agent/Dockerfile b/docker-monitoring-agent/Dockerfile
new file mode 100644
index 0000000..78b8d77
--- /dev/null
+++ b/docker-monitoring-agent/Dockerfile
@@ -0,0 +1,28 @@
+FROM --platform=$BUILDPLATFORM golang:1.25.3-alpine3.22 AS builder
+
+ENV GO111MODULE=on
+
+# Copy the go source
+COPY ./ /workspace
+
+WORKDIR /workspace
+
+RUN go mod tidy
+
+# Build
+RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o ./_output/bin/metrics ./collector/main.go
+
+FROM alpine:3.20 AS base
+COPY --from=builder /workspace/_output/bin/metrics /monitor/
+
+RUN apk add --upgrade --no-cache curl
+
+RUN chmod +x -R /monitor/ && ln -s /lib /lib64
+
+VOLUME /tmp
+
+EXPOSE 8000 9273
+
+CMD ["/monitor/metrics"]
+
+USER 1001
\ No newline at end of file
diff --git a/docker-monitoring-agent/LICENSE b/docker-monitoring-agent/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-monitoring-agent/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-monitoring-agent/README.md b/docker-monitoring-agent/README.md
new file mode 100644
index 0000000..5cebd4f
--- /dev/null
+++ b/docker-monitoring-agent/README.md
@@ -0,0 +1 @@
+# pgskipper-monitoring-agent
\ No newline at end of file
diff --git a/docker-monitoring-agent/SECURITY.md b/docker-monitoring-agent/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-monitoring-agent/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-monitoring-agent/collector/main.go b/docker-monitoring-agent/collector/main.go
new file mode 100755
index 0000000..f6e033f
--- /dev/null
+++ b/docker-monitoring-agent/collector/main.go
@@ -0,0 +1,134 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"flag"
+	"fmt"
+	"log"
+	"net/http"
+	_ "net/http/pprof"
+	"strconv"
+	"strings"
+	"sync"
+	"time"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/initiate"
+	pgScraper "github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/metrics"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/rest"
+)
+
+var (
+	addr = flag.String("localhost", ":9273",
+		"The address to listen on for HTTP requests.")
+	timeout = util.GetEnv("METRIC_COLLECTION_INTERVAL", "60")
+
+	metrics      = make([]string, 0)
+	metricsMutex sync.RWMutex
+	metricsReady bool
+)
+
+func collectMetrics(scraper *pgScraper.Scraper, dr bool) {
+	log.Printf("Start collecting metrics")
+
+	scraper.CollectCommonMetrics()
+	scraper.CollectMetrics()
+	scraper.CollectBackupMetrics()
+	if dr {
+		scraper.CollectDRMetrics()
+	}
+
+	mode := util.GetEnv("METRICS_PROFILE", "prod")
+	if mode == "dev" {
+		scraper.CollectPerformanceMetrics()
+	}
+
+	scraper.CollectMetricsFromCM()
+	metricsMutex.Lock()
+	metrics = scraper.PrintMetrics()
+	metricsReady = true
+	metricsMutex.Unlock()
+}
+
+func Metrics(w http.ResponseWriter, r *http.Request) {
+	log.Printf("metrics gets requested")
+	metricsMutex.RLock()
+	defer metricsMutex.RUnlock()
+	fmt.Fprintf(w, "%s", strings.Join(metrics, "\n"))
+}
+
+func LivenessProbe(w http.ResponseWriter, r *http.Request) {
+	w.WriteHeader(http.StatusOK)
+	fmt.Fprintf(w, "OK")
+}
+
+func ReadinessProbe(w http.ResponseWriter, r *http.Request) {
+	metricsMutex.RLock()
+	ready := metricsReady
+	metricsMutex.RUnlock()
+
+	if ready {
+		w.WriteHeader(http.StatusOK)
+		fmt.Fprintf(w, "OK")
+	} else {
+		w.WriteHeader(http.StatusServiceUnavailable)
+		fmt.Fprintf(w, "Not ready")
+	}
+}
+
+func HttpHandler() {
+	// Registering our handler functions, and creating paths.
+	http.HandleFunc("/metrics", Metrics)
+	http.HandleFunc("/healthz", LivenessProbe)
+	http.HandleFunc("/ready", ReadinessProbe)
+	log.Println("Started on port", *addr)
+	// Spinning up the server.
+	err := http.ListenAndServe(*addr, nil)
+	if err != nil {
+		log.Fatal(err)
+	}
+}
+
+func main() {
+	log.Printf("Init Metric collector")
+	initiate.InitMetricCollector()
+	log.Println("Starting http server")
+	go HttpHandler()
+
+	scrapeTimeout, _ := strconv.Atoi(timeout)
+	protocol, port := util.GetProtocol()
+	config, err := rest.InClusterConfig()
+	if err != nil {
+		panic(err.Error())
+	}
+	client, err := kubernetes.NewForConfig(config)
+	if err != nil {
+		panic(err.Error())
+	}
+
+	dr := util.IsSiteManagerEnabled()
+	if dr {
+		log.Println("DR mode is enabled")
+	}
+
+	for {
+		scraper := pgScraper.GetScraper(client, util.GetHttpClient(), protocol, port)
+		collectMetrics(scraper, dr)
+		log.Printf("Timeout %v", scrapeTimeout)
+		time.Sleep(time.Duration(scrapeTimeout) * time.Second)
+	}
+}
diff --git a/docker-monitoring-agent/collector/pkg/gauges/gauges.go b/docker-monitoring-agent/collector/pkg/gauges/gauges.go
new file mode 100755
index 0000000..ce10212
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/gauges/gauges.go
@@ -0,0 +1,31 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package gauges
+
+import (
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+)
+
+func DefaultLabels() map[string]string {
+	ns := util.GetEnv("NAMESPACE", "postgres-service")
+	return map[string]string{
+		"namespace":    ns,
+		"pod_name":     util.GetEnv("HOSTNAME", ""),
+		"selector":     "health",
+		"service_name": "pg-common-collector",
+		"host":         postgres.PgHost + "." + ns,
+	}
+}
diff --git a/docker-monitoring-agent/collector/pkg/initiate/initiate.go b/docker-monitoring-agent/collector/pkg/initiate/initiate.go
new file mode 100644
index 0000000..7cbf564
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/initiate/initiate.go
@@ -0,0 +1,113 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package initiate
+
+import (
+	"context"
+	b64 "encoding/base64"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"go.uber.org/zap"
+	corev1 "k8s.io/api/core/v1"
+	"k8s.io/apimachinery/pkg/types"
+)
+
+var (
+	logger = util.GetLogger()
+	ctx    = context.Background()
+)
+
+func InitMetricCollector() {
+
+	logger.Info("Will run preparation scripts")
+
+	clusterName := util.GetEnv("PGCLUSTER", "patroni")
+	monitoringRole := util.GetEnv("MONITORING_USER", "monitoring-user")
+	monitoringPassword := util.GetEnv("MONITORING_PASSWORD", "monitoring_password")
+	pgHost := util.GetEnv("POSTGRES_HOST", "pg-patroni")
+	pgPort := util.GetEnvInt("POSTGRES_PORT", 5432)
+
+	queries := append(make([]string, 0),
+		"DROP TABLE if exists monitor_test",
+		fmt.Sprintf("DO $$BEGIN IF NOT EXISTS (SELECT FROM pg_roles WHERE rolname = '%s') THEN CREATE ROLE \"%s\" WITH LOGIN PASSWORD '%s'; END IF; END$$", monitoringRole, monitoringRole, monitoringPassword),
+		fmt.Sprintf("ALTER ROLE \"%s\" with login password '%s'", monitoringRole, monitoringPassword),
+		fmt.Sprintf("GRANT pg_monitor to \"%s\"", monitoringRole),
+		fmt.Sprintf("GRANT pg_read_all_data to \"%s\"", monitoringRole),
+		"CREATE EXTENSION if not exists pg_stat_statements",
+		fmt.Sprintf("GRANT CREATE ON SCHEMA public TO \"%s\"", monitoringRole),
+		"DROP SEQUENCE if exists monitor_test_seq")
+
+	securedViews := append(make([]string, 0),
+		"pg_stat_replication", "pg_stat_statements", "pg_database", "pg_stat_activity",
+	)
+
+	user, password := getPGCredentials(clusterName)
+	pc := postgres.NewConnectorForUser(pgHost, pgPort, user, password)
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		panic("Error while init metric collector. Can not connect to postgres database")
+	}
+	defer pc.CloseConnection(ctx)
+	for _, query := range queries {
+		if _, err := pc.Exec(ctx, query); err != nil {
+			logger.Error("Error while init metric collector. Can not execute query", zap.Error(err))
+		}
+	}
+	for _, view := range securedViews {
+		query := fmt.Sprintf("DROP FUNCTION IF EXISTS func_%s()", view)
+		if _, err := pc.Exec(ctx, query); err != nil {
+			logger.Error(fmt.Sprintf("Error while init metric collector. Can not execute query: %s", query), zap.Error(err))
+		}
+	}
+	logger.Info("metric collector init completed")
+
+}
+
+func getPGCredentials(clusterName string) (user, password string) {
+
+	namespace := util.GetEnv("NAMESPACE", "postgres-service")
+	user = util.GetEnv("PG_ROOT_USER", "")
+	password = util.GetEnv("PG_ROOT_PASSWORD", "")
+
+	if user != "" || password != "" {
+		return user, password
+	}
+
+	k8sClient, err := util.CreateClient()
+	if err != nil {
+		panic("Error while init metric collector. Can not create k8s client")
+	}
+
+	secretName := "postgres-credentials"
+	foundSrv := &corev1.Secret{}
+	err = k8sClient.Get(context.TODO(), types.NamespacedName{
+		Name: secretName, Namespace: namespace,
+	}, foundSrv)
+	if err != nil {
+		panic(fmt.Sprintf("Error while init metric collector. Can not read secret: %s", secretName))
+	}
+
+	pwd := foundSrv.Data["password"]
+	usr, ok := foundSrv.Data["user"]
+	if !ok {
+		usr = foundSrv.Data["username"]
+	}
+	pwdEnc := b64.StdEncoding.EncodeToString(pwd)
+	usrEnc := b64.StdEncoding.EncodeToString(usr)
+
+	return usrEnc, pwdEnc
+}
diff --git a/docker-monitoring-agent/collector/pkg/k8s/client.go b/docker-monitoring-agent/collector/pkg/k8s/client.go
new file mode 100644
index 0000000..0efb444
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/k8s/client.go
@@ -0,0 +1,119 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package k8s
+
+import (
+	"context"
+	"fmt"
+	"os"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"go.uber.org/zap"
+	appsv1 "k8s.io/api/apps/v1"
+	corev1 "k8s.io/api/core/v1"
+	crclient "sigs.k8s.io/controller-runtime/pkg/client"
+)
+
+var (
+	logger    = util.GetLogger()
+	k8sClient crclient.Client
+	namespace = os.Getenv("NAMESPACE")
+)
+
+type PatroniNode struct {
+	Name string
+	IP   string
+}
+
+func init() {
+	var err error
+	k8sClient, err = util.CreateClient()
+	if err != nil {
+		logger.Error("Can't create k8s client")
+		panic(err)
+	}
+}
+
+func GetPodsByLabel(ctx context.Context, selectors map[string]string) (corev1.PodList, error) {
+	podList := &corev1.PodList{}
+	listOpts := []crclient.ListOption{
+		crclient.InNamespace(namespace),
+		crclient.MatchingLabels(selectors),
+	}
+	if err := k8sClient.List(ctx, podList, listOpts...); err != nil {
+		logger.Error("Can not get pods by label", zap.Error(err))
+		return *podList, err
+	}
+	return *podList, nil
+}
+
+func GetDeploymentsByLabel(ctx context.Context) (appsv1.DeploymentList, error) {
+	deploymentList := &appsv1.DeploymentList{}
+	listOpts := []crclient.ListOption{
+		crclient.InNamespace(namespace),
+	}
+	if err := k8sClient.List(ctx, deploymentList, listOpts...); err != nil {
+		logger.Error("Can not get pods by label", zap.Error(err))
+		return *deploymentList, err
+	}
+	return *deploymentList, nil
+}
+
+func GetDeploymentInfo(deploymentName string) *appsv1.Deployment {
+	dcInfo, _ := GetDeploymentsByLabel(context.Background())
+	for _, deployment := range dcInfo.Items {
+		if deployment.Name == deploymentName {
+			logger.Debug(fmt.Sprintf("deployment with name %s\n%s", deploymentName, deploymentName))
+			return &deployment
+		}
+	}
+	return nil
+}
+
+func GetPatroniNodes(ctx context.Context, pgCluster string) []PatroniNode {
+	result := make([]PatroniNode, 0)
+
+	selectors := map[string]string{
+		"app":       pgCluster,
+		"pgcluster": pgCluster,
+	}
+	pods, err := GetPodsByLabel(ctx, selectors)
+	if err != nil {
+		return result
+	}
+
+	for _, pod := range pods.Items {
+		nodeName := ""
+		for _, env := range pod.Spec.Containers[0].Env {
+			if env.Name == "POD_IDENTITY" {
+				nodeName = env.Value
+				result = append(result, PatroniNode{Name: nodeName, IP: pod.Status.PodIP})
+			}
+		}
+	}
+	return result
+}
+
+func GetConfigMaps() *corev1.ConfigMapList {
+
+	configMapList := &corev1.ConfigMapList{}
+	listOps := &crclient.ListOptions{
+		Namespace: namespace,
+	}
+	if err := k8sClient.List(context.Background(), configMapList, listOps); err != nil {
+		logger.Error("Error while listing config maps", zap.Error(err))
+	}
+	return configMapList
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/common_metrics.go b/docker-monitoring-agent/collector/pkg/metrics/common_metrics.go
new file mode 100644
index 0000000..138cb81
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/common_metrics.go
@@ -0,0 +1,106 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"context"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/k8s"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	"go.uber.org/zap"
+)
+
+func (s *Scraper) CollectCommonMetrics() {
+	logger.Info("Common metrics collection started")
+	defer s.HandleMetricCollectorStatus()
+	ctx := context.Background()
+	if pgType == TypePatroni {
+		logger.Debug("Pg type is Patroni")
+		nodes := k8s.GetPatroniNodes(ctx, clusterName)
+		for _, node := range nodes {
+			logger.Debug(fmt.Sprintf("Collection of common metrics for node %s started", node.Name))
+			s.collectMetricsFromNode(ctx, node)
+		}
+	} else {
+		logger.Debug("Pg type is External")
+		pc := postgres.NewConnector()
+		s.collectMetrics(ctx, pc, "External")
+	}
+	logger.Info("Common metrics collection finished")
+}
+
+func (s *Scraper) collectMetricsFromNode(ctx context.Context, node k8s.PatroniNode) {
+	pc := postgres.NewConnector()
+	pc.SetHost(node.IP)
+	s.collectMetrics(ctx, pc, node.Name)
+}
+
+func (s *Scraper) collectMetrics(ctx context.Context, pc *postgres.PostgresConnector, nodeName string) {
+	Log.Debug("Collect Common PG Metrics")
+	var pgVersion int
+	metrics := getCommonMetrics()
+	labels := gauges.DefaultLabels()
+	if len(nodeName) > 0 {
+		labels["pg_node"] = nodeName
+	}
+
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		return
+	}
+	defer pc.CloseConnection(ctx)
+
+	pgVersion = s.pgMajorVersion
+
+	if pgVersion >= 15 {
+		delete(metrics, "pg_is_in_backup")
+	}
+
+	s.metrics = append(s.metrics, NewMetric("postgres_version").withLabels(labels).setValue(s.pgFullVersion))
+
+	for metricName, query := range metrics {
+		value, err := pc.GetValue(ctx, query)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Can't collect metric %s", metricName), zap.Error(err))
+			continue
+		}
+		s.metrics = append(s.metrics, NewMetric(prepareCommonMetricName(metricName)).withLabels(labels).setValue(value))
+	}
+	Log.Debug("Collect Common PG Metrics Completed")
+}
+
+func prepareCommonMetricName(metricName string) string {
+	return fmt.Sprintf("ma_pg_metrics_%s", metricName)
+}
+
+func getCommonMetrics() map[string]string {
+	return map[string]string{
+		"running":                  "select 1",
+		"db_count":                 "select count(*) from pg_stat_database",
+		"current_connections":      "select count(*) from pg_stat_activity",
+		"locks":                    "select count(*) from pg_locks",
+		"locks_not_granted":        "select count(*) from pg_locks where NOT GRANTED",
+		"replication_connections":  "select count(*) from pg_stat_replication",
+		"replication_slots_count":  "select count(*) from pg_replication_slots",
+		"xlog_location":            "SELECT pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery() THEN pg_last_wal_replay_lsn() ELSE pg_current_wal_lsn() END, '0/0')::bigint",
+		"pg_is_in_recovery":        "SELECT case pg_is_in_recovery when 't' then 1 else 0 end pg_is_in_recovery from pg_is_in_recovery()",
+		"pg_is_in_backup":          "SELECT case pg_is_in_backup when 't' then 1 else 0 end pg_is_in_backup from pg_is_in_backup()",
+		"xact_sum":                 "select sum(xact_commit + xact_rollback) :: bigint from pg_stat_database",
+		"postgres_max_connections": "SELECT setting::int FROM pg_settings WHERE name = 'max_connections'",
+		"query_max_time":           "SELECT COALESCE(trunc(extract(epoch from (now() - min(query_start)))), 0) FROM pg_stat_activity WHERE state='active' AND usename != 'replicator' AND wait_event_type ||'.'|| wait_event != 'Client.WalSenderWaitForWAL'",
+	}
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/dr_metrics.go b/docker-monitoring-agent/collector/pkg/metrics/dr_metrics.go
new file mode 100644
index 0000000..a3f7717
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/dr_metrics.go
@@ -0,0 +1,178 @@
+// // Copyright 2024-2025 NetCracker Technology Corporation
+// //
+// // Licensed under the Apache License, Version 2.0 (the "License");
+// // you may not use this file except in compliance with the License.
+// // You may obtain a copy of the License at
+// //
+// //     http://www.apache.org/licenses/LICENSE-2.0
+// //
+// // Unless required by applicable law or agreed to in writing, software
+// // distributed under the License is distributed on an "AS IS" BASIS,
+// // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// // See the License for the specific language governing permissions and
+// // limitations under the License.
+
+package metrics
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"strconv"
+	"time"
+
+	"strings"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/k8s"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"go.uber.org/zap"
+)
+
+const (
+	replicaInfoQuery = "SELECT usename, application_name, client_addr::text, pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn)::text AS lag_in_bytes," +
+		"COALESCE(replay_lag, '0') as reply_lag FROM pg_stat_replication where usename = 'replicator' and application_name like 'pg-%s-node%%';"
+)
+
+type ReplicaInfo struct {
+	Usename         string  `json:"usename"`
+	ApplicationName string  `json:"application_name"`
+	IP              string  `json:"client_addr"`
+	LagInBytes      string  `json:"lag_in_bytes"`
+	LagInMs         float64 `json:"replay_lag"`
+}
+
+type ClusterInfo struct {
+	Members []Member `json:"members"`
+}
+
+type Member struct {
+	Role  string `json:"role"`
+	State string `json:"state"`
+}
+
+func (s *Scraper) CollectDRMetrics() {
+	logger.Info("DR metrics collection started")
+	defer s.HandleMetricCollectorStatus()
+	ctx := context.Background()
+
+	isActive, err := s.IsCurrentSiteActive(ctx)
+	if err != nil {
+		logger.Error("Error, while getting cluster status", zap.Error(err))
+		return
+	}
+
+	if !isActive {
+		logger.Info("Current site is standby, skipping dr metrics collection ...")
+		return
+	}
+
+	patroniPodsIP := getPatroniPodsIP(ctx)
+	standbyInfo, err := getStandbyInfo(ctx, patroniPodsIP)
+	if err != nil {
+		logger.Error("Error, while getting standby info", zap.Error(err))
+		return
+	}
+
+	s.metrics = append(s.metrics, NewMetric("ma_pg_standby_leader_count").withLabels(gauges.DefaultLabels()).setValue(len(standbyInfo)))
+
+	for _, replica := range standbyInfo {
+		labels := gauges.DefaultLabels()
+		labels["replica_ip"] = replica.IP
+		s.metrics = append(s.metrics, NewMetric("ma_pg_standby_replication_lag_in_bytes").withLabels(labels).setValue(replica.LagInBytes))
+		s.metrics = append(s.metrics, NewMetric("ma_pg_standby_replication_lag_in_ms").withLabels(labels).setValue(replica.LagInMs))
+	}
+
+	logger.Info("DR metrics collection finished")
+}
+
+func getPatroniPodsIP(ctx context.Context) []string {
+	nodes := k8s.GetPatroniNodes(ctx, clusterName)
+	ips := make([]string, 0)
+	for _, node := range nodes {
+		ips = append(ips, node.IP)
+	}
+	return ips
+}
+
+func getStandbyInfo(ctx context.Context, patroniPodsIP []string) ([]ReplicaInfo, error) {
+	standbyInfo := make([]ReplicaInfo, 0)
+
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		return nil, err
+	}
+
+	rows, err := pc.Query(ctx, fmt.Sprintf(replicaInfoQuery, clusterName))
+	if err != nil {
+		return nil, err
+	}
+	defer rows.Close()
+
+	for rows.Next() {
+		var usename string
+		var applicationName string
+		var ip string
+		var lagBytesPt *string
+		var lag time.Duration
+		err = rows.Scan(&usename, &applicationName, &ip, &lagBytesPt, &lag)
+		if err != nil {
+			logger.Error("Error, while getting replica info", zap.Error(err))
+			return nil, err
+		}
+
+		isReplica := false
+		for _, patroniIP := range patroniPodsIP {
+			if strings.Contains(ip, patroniIP) {
+				isReplica = true
+				break
+			}
+		}
+
+		if !isReplica {
+			lagInBytes := ""
+			if lagBytesPt != nil {
+				lagInBytes = *lagBytesPt
+			}
+			standbyInfo = append(standbyInfo, ReplicaInfo{
+				Usename:         usename,
+				ApplicationName: applicationName,
+				IP:              ip,
+				LagInBytes:      lagInBytes,
+				LagInMs:         float64(lag.Milliseconds()),
+			})
+		}
+	}
+	return standbyInfo, nil
+}
+
+func (s *Scraper) IsCurrentSiteActive(ctx context.Context) (bool, error) {
+	var response = ClusterInfo{}
+	protocol, _ := util.GetProtocol()
+	url := fmt.Sprintf("%spg-%s-api:8008/cluster", protocol, clusterName)
+
+	status, body, err := util.ProcessHttpRequest(s.httpClient, url, s.token)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Cannot collect backup status metric. url %v", url))
+		return false, err
+	}
+	code := strings.Fields(status)[0]
+	statusCode, err := strconv.Atoi(code)
+	if statusCode != 200 || err != nil {
+		logger.Warn(fmt.Sprintf("Cannot collect cluster status for dr metrics. Error code %v", statusCode))
+		logger.Warn(fmt.Sprintf("Error: %v", err))
+		return false, err
+	}
+	err = json.Unmarshal(body, &response)
+	if err != nil {
+		Log.Error(fmt.Sprintf("Process cluster info Unmarshal Error: %s", err))
+		logger.Error(fmt.Sprintf("Error: %v", err))
+		return false, err
+	}
+	for _, member := range response.Members {
+		if member.Role == "leader" && member.State == "running" {
+			return true, nil
+		}
+	}
+	return false, nil
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/generic.go b/docker-monitoring-agent/collector/pkg/metrics/generic.go
new file mode 100644
index 0000000..6d85083
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/generic.go
@@ -0,0 +1,36 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"log"
+	"os"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+)
+
+const TypePatroni = "patroni"
+
+var (
+	logger      = util.GetLogger()
+	pgType      = util.GetEnv("EXT_DB_TYPE", TypePatroni)
+	namespace   = util.GetEnv("NAMESPACE", "")
+	clusterName = util.GetEnv("PGCLUSTER", "patroni")
+)
+
+func init() {
+	log.SetFlags(0)
+	log.SetOutput(os.Stdout)
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/metrics.go b/docker-monitoring-agent/collector/pkg/metrics/metrics.go
new file mode 100644
index 0000000..5a5e35a
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/metrics.go
@@ -0,0 +1,105 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"fmt"
+	"sort"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+)
+
+var (
+	Metrics9 = map[string]string{
+		"replication_data": "select usename, application_name, client_addr, state, sync_priority, sync_state, " +
+			"pg_xlog_location_diff(sent_location, replay_location)::bigint as sent_replay_lag, " +
+			"pg_xlog_location_diff(pg_current_xlog_location(), sent_location)::bigint as sent_lag " +
+			"from pg_stat_replication WHERE usename = 'replicator'",
+		"replication_slots": "select slot_name, pg_xlog_location_diff(CASE WHEN pg_is_in_recovery() " +
+			"THEN pg_last_xlog_replay_location() ELSE pg_current_xlog_location() END, restart_lsn)::bigint as restart_lsn_lag, " +
+			"pg_xlog_location_diff(CASE WHEN pg_is_in_recovery() THEN pg_last_xlog_replay_location() " +
+			"ELSE pg_current_xlog_location() END, confirmed_flush_lsn)::bigint as confirmed_flush_lsn_lag " +
+			"from pg_replication_slots",
+		"xlog_location": "SELECT pg_xlog_location_diff(CASE WHEN pg_is_in_recovery() " +
+			"THEN pg_last_xlog_replay_location() ELSE pg_current_xlog_location() END, '0/0')::bigint",
+		"pg_xlog": "/var/lib/pgsql/data/postgresql_%s/pg_xlog",
+	}
+
+	Metrics10 = map[string]string{
+		"replication_data": "select usename, application_name, client_addr, state, sync_priority, sync_state, " +
+			"pg_wal_lsn_diff(sent_lsn, replay_lsn)::bigint as sent_replay_lag, (case pg_is_in_recovery() when 't' " +
+			"then pg_wal_lsn_diff(pg_last_wal_receive_lsn(), sent_lsn) else pg_wal_lsn_diff(pg_current_wal_lsn(), " +
+			"sent_lsn)end)::bigint as sent_lag from pg_stat_replication WHERE usename = 'replicator' ",
+		"replication_slots": "select slot_name, pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery() THEN pg_last_wal_replay_lsn() " +
+			"ELSE pg_current_wal_lsn() END, restart_lsn)::bigint as restart_lsn_lag, pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery() " +
+			"THEN pg_last_wal_replay_lsn() ELSE pg_current_wal_lsn() END, confirmed_flush_lsn)::bigint as confirmed_flush_lsn_lag " +
+			"from pg_replication_slots",
+		"xlog_location": "SELECT pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery() THEN pg_last_wal_replay_lsn() " +
+			"ELSE pg_current_wal_lsn() END, '0/0')::bigint",
+		"pg_xlog": "/var/lib/pgsql/data/postgresql_%s/pg_wal",
+	}
+
+	ArchiveDataQuery = "SELECT (select setting from pg_settings where name='archive_mode'), " +
+		"COALESCE(EXTRACT(EPOCH FROM (now() - last_archived_time)), 0.0) as delay, archived_count, failed_count FROM pg_stat_archiver"
+)
+
+func GetMetricsTypeByVersion(name string, version int) string {
+	logger.Debug(fmt.Sprintf("Getting metrics of type: %s for pgsql version: %v", name, version))
+	if version < 10 {
+		return Metrics9[name]
+	} else {
+		return Metrics10[name]
+	}
+}
+
+type Metric struct {
+	name   string
+	value  float64
+	labels map[string]string
+}
+
+func NewMetric(name string) *Metric {
+	return &Metric{name: name, labels: map[string]string{}}
+}
+
+func (m *Metric) getValue() float64 {
+	return m.value
+}
+
+func (m *Metric) setValue(value any) *Metric {
+
+	if val, err := util.GetFloatValue(value, float64(-1)); err != nil {
+		Log.Warn(fmt.Sprintf("Can't set value '%v' for metric '%s'", value, m.name))
+	} else {
+		m.value = val
+	}
+	return m
+}
+
+func (m *Metric) withLabels(labels map[string]string) *Metric {
+	m.labels = labels
+	return m
+}
+
+type labels map[string]string
+
+func (l labels) sort() (keys []string) {
+	keys = make([]string, 0, len(l))
+	for k := range l {
+		keys = append(keys, k)
+	}
+	sort.Strings(keys)
+	return
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/performance_metrics.go b/docker-monitoring-agent/collector/pkg/metrics/performance_metrics.go
new file mode 100644
index 0000000..efea5a4
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/performance_metrics.go
@@ -0,0 +1,419 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"context"
+	"fmt"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"go.uber.org/zap"
+)
+
+var (
+	queryLength   = util.GetEnvInt("QUERY_LENGTH", 200)
+	metricFields  = []string{"calls", "total_time", "min_time", "max_time", "mean_time"}
+	procDatabases = []string{"template0", "template1", "postgres",
+		"cloudsqladmin",
+		"azure_maintenance", "azure_sys", "rdsadmin"}
+)
+
+var (
+	qStatQuery = fmt.Sprintf(`
+	SELECT queryid, datname, round(sum(calls)) AS calls,
+	round(sum(total_exec_time) / 1000) AS total_time,
+	round(min(min_exec_time) / 1000) AS min_time,
+	round(max(max_exec_time) / 1000) AS max_time,
+	round(sum(mean_exec_time) / 1000) AS mean_time,
+	rolname,
+	lower(left(query, %d)) AS short_query
+	FROM pg_stat_statements as pg_stat_statements
+	JOIN pg_database ON pg_stat_statements.dbid = pg_database.oid
+	JOIN pg_roles ON pg_stat_statements.userid = pg_roles.oid
+	WHERE calls > 50 and datname not in ('postgres', 'template0', 'template1')
+	GROUP BY queryid, short_query, datname, mean_exec_time, calls, rolname;`,
+		queryLength)
+	dbQuery    = `select datname from pg_database`
+	dbaasQuery = `select current_database() as datname,
+	value#>>'{classifier,namespace}' as db_namespace,
+	value#>>'{classifier,microserviceName}' as microservice,
+	value#>>'{classifier,tenantId}' as tenant_id, 1 as name
+	  from _dbaas_metadata where key='metadata'`
+	tablesStatQuery = `
+    SELECT relid, relname,
+        pg_relation_size(relid) AS "t_size",
+        pg_indexes_size(relid) AS "idx_size",
+        pg_total_relation_size(relid) AS "tot_size",
+        coalesce(nullif(n_live_tup,0),seq_tup_read) as "live_tuples"
+    FROM pg_stat_all_tables
+    WHERE schemaname NOT IN ('pg_catalog', 'information_schema') 
+        AND schemaname !~ '^pg_toast'`
+	commonPerfMetricsQueries = map[string]string{
+		"count_not_idle": ` SELECT count(*) cnt, datname, usename FROM pg_stat_activity
+				 WHERE state <> 'idle' and datname <> '' GROUP BY datname, usename, state
+				 ORDER BY cnt desc;
+			 `,
+		"connection_by_database": ` SELECT count(*) cnt, datname FROM pg_stat_activity
+				WHERE datname <> '' GROUP BY datname ORDER BY cnt DESC;
+			`,
+		"connection_by_role": ` SELECT count(*) cnt, usename, rolconnlimit as limit 
+				FROM pg_stat_activity as pg_stat_activity
+				JOIN pg_roles ON pg_stat_activity.usename = pg_roles.rolname
+				WHERE datname <> '' GROUP BY usename, rolconnlimit ORDER BY cnt DESC;
+			`,
+		"connection_by_role_with_limit": ` SELECT role_cnt, t1.usename, CASE
+                WHEN rolconnlimit = -1 THEN 'No data'
+                ELSE rolconnlimit::text
+                END AS limit, not_idle_cnt
+                FROM(SELECT count(*) role_cnt, usename, rolconnlimit
+                FROM pg_stat_activity as pg_stat_activity
+                JOIN pg_roles ON pg_stat_activity.usename = pg_roles.rolname
+                WHERE datname <> '' GROUP BY usename, rolconnlimit ORDER BY role_cnt DESC) t1
+                FULL OUTER JOIN  (SELECT count(*) not_idle_cnt, usename FROM pg_stat_activity
+                WHERE state <> 'idle' and datname <> '' GROUP BY datname, usename, state
+                ORDER BY not_idle_cnt desc) t2 on (t1.usename = t2.usename) order by role_cnt desc;
+			`,
+		"size_by_database": ` SELECT pg_database_size(datname) as value, 
+				datname from pg_database order by pg_database_size(datname) desc;
+			`,
+	}
+
+	largeObjectQuery = `SELECT count(distinct loid) AS object_count,
+    count(*) AS total_chunks, sum(octet_length(data)) AS total_size_bytes
+	FROM pg_largeobject;`
+
+	countStatQuery = `SELECT max_connections, used, res_for_super as reserved_for_superuser,
+	(max_connections - used-res_for_super) reserved_for_reg_users
+	FROM (SELECT count(*) used FROM pg_stat_activity) t1,
+	(SELECT setting::int res_for_super FROM pg_settings
+	WHERE name=$$superuser_reserved_connections$$) t2,
+	(SELECT setting::int max_connections FROM pg_settings
+	WHERE name=$$max_connections$$) t3;
+`
+)
+
+func (s *Scraper) CollectPerformanceMetrics() {
+	logger.Info("Performance metrics collection started")
+	defer s.HandleMetricCollectorStatus()
+	ctx := context.Background()
+	pc := postgres.NewConnector()
+
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		return
+	}
+	defer pc.CloseConnection(ctx)
+
+	s.collectQueriesMetrics(ctx, pc)
+	s.collectMetricsPerDB(ctx, pc)
+	s.collectTempFileMetrics(ctx, pc)
+	s.collectLargeObjectMetrics(ctx, pc)
+	s.collectCommonPerfMetrics(ctx, pc)
+	s.collectCountStatMetrics(ctx, pc)
+	logger.Info("Performance metrics collection finished")
+}
+
+func (s *Scraper) collectLargeObjectMetrics(ctx context.Context, pg *postgres.PostgresConnector) {
+	logger.Info("Large object metrics per DB collection started")
+
+	databases, err := getDatabasesList(ctx, pg)
+	if err != nil {
+		logger.Error("Failed to retrieve databases list for large object metrics")
+		return
+	}
+
+	defer func() {
+		_ = pg.EstablishConnForDB(ctx, postgres.PgDatabase)
+	}()
+
+	logger.Debug(fmt.Sprintf("Will Collect large object Stats for next dbs %v", databases))
+	for _, db := range databases {
+		logger.Debug(fmt.Sprintf("Collecting large object metrics for DB: %s", db))
+		err = pg.EstablishConnForDB(ctx, db)
+		if err != nil {
+			logger.Warn(fmt.Sprintf("Skipping DB %s due to connection issue", db))
+			continue
+		}
+
+		columns, rows := getData(ctx, pg, largeObjectQuery)
+		if len(rows) == 0 {
+			logger.Info(fmt.Sprintf("No large objects found in DB: %s", db))
+			continue
+		}
+
+		for _, row := range rows {
+			labels := gauges.DefaultLabels()
+			labels["datname"] = db
+
+			for _, column := range columns {
+				value := fmt.Sprintf("%v", row[column])
+				metricName := fmt.Sprintf("ma_pg_large_object_%s", column)
+				s.metrics = append(s.metrics, NewMetric(metricName).withLabels(labels).setValue(value))
+			}
+		}
+	}
+
+	logger.Info("Large object metrics per DB collection finished")
+}
+
+func (s *Scraper) collectTempFileMetrics(ctx context.Context, pg *postgres.PostgresConnector) {
+	logger.Info("Temp file metrics collection started")
+
+	tempMetricsQueries := map[string]string{
+		"temp_file_size_by_db": ` SELECT now() AS time, datname, 
+		temp_bytes / 1024 / 1024 AS temp_size_mb FROM pg_stat_database
+		WHERE temp_bytes > 0 AND datname IS NOT NULL ORDER BY temp_size_mb DESC; 
+		`,
+		"temp_file_count_by_db": ` SELECT datname, temp_files AS temp_file_count
+		FROM pg_stat_database WHERE temp_files > 0 AND datname IS NOT NULL ORDER BY temp_file_count DESC;
+		`,
+		"temp_file_size_by_query": ` SELECT query, calls AS total_calls,
+		(temp_blks_read + temp_blks_written) * current_setting('block_size')::int / 1024 / 1024 AS temp_size_mb
+		FROM pg_stat_statements WHERE (temp_blks_read > 0 OR temp_blks_written > 0)
+		ORDER BY temp_size_mb DESC LIMIT 10;
+		`,
+		"active_temp_writes": ` SELECT pid, usename, datname, query, state FROM pg_stat_activity
+		WHERE state = 'active' AND datname IS NOT NULL;
+		`,
+	}
+
+	for metricName, query := range tempMetricsQueries {
+		columns, rows := getData(ctx, pg, query)
+		for _, row := range rows {
+			labels := gauges.DefaultLabels()
+			var value string
+
+			if metricName == "active_temp_writes" {
+				value = fmt.Sprintf("%d", len(rows))
+				for _, column := range columns {
+					labels[column] = fmt.Sprintf("%v", row[column])
+				}
+			} else {
+				for _, column := range columns {
+					rValue := fmt.Sprintf("%v", row[column])
+
+					if column == "temp_size_mb" || column == "temp_file_count" || column == "total_calls" {
+						value = rValue
+					} else {
+						labels[column] = rValue
+					}
+				}
+			}
+
+			if value != "" {
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s", metricName)).withLabels(labels).setValue(value))
+			} else {
+				logger.Warn(fmt.Sprintf("No numeric value found for metric '%s'", metricName))
+			}
+		}
+	}
+
+	logger.Info("Temp file metrics collection finished")
+}
+
+func (s *Scraper) collectQueriesMetrics(ctx context.Context, pc *postgres.PostgresConnector) {
+	columns, rows := getData(ctx, pc, qStatQuery)
+	for _, row := range rows {
+		for _, metric := range columns {
+			if util.Contains(metricFields, metric) {
+				mValue := row[metric]
+				labels := gauges.DefaultLabels()
+				labels["metric_name"] = metric
+				for _, column := range columns {
+					if !util.Contains(metricFields, column) {
+						rValue := row[column]
+						if column == "short_query" || column == "query" {
+							escaper := strings.NewReplacer(
+								`\`, `\\`,
+								`"`, `\"`,
+								"\n", `\n`,
+								"\r", `\n`,
+							)
+							rValue = escaper.Replace(fmt.Sprintf("%s", rValue))
+						}
+						labels[column] = fmt.Sprintf("%v", rValue)
+					}
+				}
+				s.metrics = append(s.metrics, NewMetric("ma_pg_queries_stat").withLabels(labels).setValue(mValue))
+			}
+		}
+	}
+}
+
+func (s *Scraper) collectMetricsPerDB(ctx context.Context, pg *postgres.PostgresConnector) {
+	logger.Info("Tables statistic metrics collection started")
+	databases, err := getDatabasesList(ctx, pg)
+	if err != nil {
+		return
+	}
+	defer func() {
+		_ = pg.EstablishConnForDB(ctx, postgres.PgDatabase)
+	}()
+	logger.Debug(fmt.Sprintf("Will Collect Table Stats for next dbs %v", databases))
+	for _, db := range databases {
+		logger.Debug(fmt.Sprintf("Start table stat collection for db: %s", db))
+
+		// select database
+		err = pg.EstablishConnForDB(ctx, db)
+		if err != nil {
+			logger.Error(fmt.Sprintf("Can't establish connection with %s, skipping", db))
+			continue
+		}
+		s.collectTableMetrics(ctx, pg)
+		s.collectDBaaSData(ctx, pg)
+	}
+	logger.Info("Tables statistic metrics collection finished")
+}
+
+func (s *Scraper) collectDBaaSData(ctx context.Context, pg *postgres.PostgresConnector) {
+	columns, rows := getData(ctx, pg, dbaasQuery)
+
+	for _, row := range rows {
+		labels := gauges.DefaultLabels()
+		labels["datname"] = pg.GetDatabase()
+		value := ""
+		for _, column := range columns {
+			rValue := row[column]
+			if column == "name" { //todo why name is value ???
+				value = fmt.Sprintf("%v", rValue)
+			} else {
+				labels[column] = fmt.Sprintf("%v", rValue)
+			}
+		}
+		s.metrics = append(s.metrics, NewMetric("ma_pg_dbaas").withLabels(labels).setValue(value))
+	}
+}
+
+func (s *Scraper) collectTableMetrics(ctx context.Context, pg *postgres.PostgresConnector) {
+	columns, rows := getData(ctx, pg, tablesStatQuery)
+
+	for _, row := range rows {
+		labels := gauges.DefaultLabels()
+		labels["datname"] = pg.GetDatabase()
+		var tableSizeValue, liveTuplesValue, totSizeValue, idxSizeValue string
+		for _, column := range columns {
+			rValue := fmt.Sprintf("%v", row[column])
+			switch column {
+			case "t_size":
+				tableSizeValue = rValue
+			case "live_tuples":
+				liveTuplesValue = rValue
+			case "tot_size":
+				totSizeValue = rValue
+			case "idx_size":
+				idxSizeValue = rValue
+			default:
+				labels[column] = rValue
+			}
+		}
+		s.metrics = append(s.metrics, NewMetric("ma_pg_table_size_by_db").withLabels(labels).setValue(tableSizeValue))
+		s.metrics = append(s.metrics, NewMetric("ma_pg_tuples_by_db").withLabels(labels).setValue(liveTuplesValue))
+		s.metrics = append(s.metrics, NewMetric("ma_pg_tot_size_by_db").withLabels(labels).setValue(totSizeValue))
+		s.metrics = append(s.metrics, NewMetric("ma_pg_idx_size_by_db").withLabels(labels).setValue(idxSizeValue))
+	}
+}
+
+func (s *Scraper) collectCommonPerfMetrics(ctx context.Context, pg *postgres.PostgresConnector) {
+	logger.Info("Common additional performance metrics collection started")
+	for metric, query := range commonPerfMetricsQueries {
+		columns, rows := getData(ctx, pg, query)
+		for _, row := range rows {
+			labels := gauges.DefaultLabels()
+			value := ""
+			for i, column := range columns {
+				if i == 0 {
+					value = fmt.Sprintf("%v", row[column])
+				} else {
+					labels[column] = fmt.Sprintf("%v", row[column])
+				}
+			}
+			s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s", metric)).withLabels(labels).setValue(value))
+		}
+	}
+
+	logger.Debug("Common additional performance metrics collection finished")
+}
+
+func (s *Scraper) collectCountStatMetrics(ctx context.Context, pc *postgres.PostgresConnector) {
+	columns, rows := getData(ctx, pc, countStatQuery)
+	for _, row := range rows {
+		for _, metric := range columns {
+			mValue := row[metric]
+			s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_connection_count_stat_%s", metric)).withLabels(gauges.DefaultLabels()).setValue(mValue))
+		}
+	}
+}
+
+func getDatabasesList(ctx context.Context, pg *postgres.PostgresConnector) ([]string, error) {
+	databases := []string{}
+	rows, err := pg.Query(ctx, dbQuery)
+	if err != nil {
+		logger.Error("Can't get databases list")
+		return nil, err
+	}
+	defer rows.Close()
+
+	for rows.Next() {
+		datname := new(string)
+		err = rows.Scan(datname)
+		if err != nil {
+			logger.Error("Can't scan database name")
+			return nil, err
+		}
+
+		if !util.Contains(procDatabases, *datname) {
+			databases = append(databases, *datname)
+		}
+	}
+	return databases, nil
+}
+
+func getData(ctx context.Context, pg *postgres.PostgresConnector, query string) ([]string, []Row) {
+	resultRows := []Row{}
+	rows, err := pg.Query(ctx, query)
+	if err != nil {
+		// Failing DBaaS query for non-DBaas databases is valid
+		if query != dbaasQuery {
+			logger.Error(fmt.Sprintf("Cannot execute query %s", query), zap.Error(err))
+		}
+		return nil, nil
+	}
+	defer rows.Close()
+
+	fields := []string{}
+	values := make([]interface{}, len(rows.FieldDescriptions()))
+	valuesPoint := make([]interface{}, len(rows.FieldDescriptions()))
+	for i, field := range rows.FieldDescriptions() {
+		fields = append(fields, field.Name)
+		valuesPoint[i] = &values[i]
+	}
+
+	for rows.Next() {
+		resultMap := map[string]interface{}{}
+		err = rows.Scan(valuesPoint...)
+		if err != nil {
+			logger.Error("Cannot scan row")
+			continue
+		}
+		for i, fName := range fields {
+			resultMap[fName] = values[i]
+		}
+		resultRows = append(resultRows, Row(resultMap))
+	}
+	return fields, resultRows
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/scrape.go b/docker-monitoring-agent/collector/pkg/metrics/scrape.go
new file mode 100644
index 0000000..5f86179
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/scrape.go
@@ -0,0 +1,777 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"bytes"
+	"context"
+	"encoding/json"
+	"fmt"
+	"math"
+	"net/http"
+	"strconv"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+	k8sClient "github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/k8s"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"go.uber.org/zap"
+	v1 "k8s.io/api/core/v1"
+	"k8s.io/client-go/kubernetes"
+	"k8s.io/utils/strings/slices"
+)
+
+var (
+	pc                      = postgres.NewConnector()
+	ctx                     = context.Background()
+	DefaultClusterPgNodeQty = util.GetEnv("POSTGRES_NODES_QTY", "2")
+	Log                     = util.GetLogger()
+	shellMetrics            = map[string]string{
+		"du": "du /var/lib/pgsql/data/ --max-depth=2 --exclude=/var/lib/pgsql/data/lost+found",
+		"df": "df /var/lib/pgsql/data"}
+	backupStatus = map[string](int){"SUCCESSFUL": 1, "IN PROGRESS": 2, "FAILED": 3, "CANCELED": 4}
+)
+
+type Scraper struct {
+	client         *kubernetes.Clientset
+	protocol       string
+	httpClient     *http.Client
+	port           string
+	token          string
+	metrics        []*Metric
+	pgMajorVersion int
+	pgFullVersion  float64
+}
+
+type BackupResponse map[string]([]BackupInfo)
+
+type BackupInfo struct {
+	BackupID       string `json:"backupId"`
+	Namespace      string `json:"namespace"`
+	Status         string `json:"status"`
+	ExpirationDate string `json:"expirationDate"`
+	Created        string `json:"created"`
+}
+
+func GetScraper(client *kubernetes.Clientset, httpClient *http.Client, protocol, port string) *Scraper {
+
+	scraper := &Scraper{
+		client:         client,
+		protocol:       protocol,
+		httpClient:     httpClient,
+		port:           port,
+		token:          util.GetToken(),
+		metrics:        make([]*Metric, 0),
+		pgMajorVersion: getVersionOfPGSQLServer(),
+		pgFullVersion:  getFullVersionOfPGSQLServer(),
+	}
+
+	return scraper
+}
+func (s *Scraper) CollectMetrics() {
+	logger.Info("Start to collect metrics")
+	s.metrics = append(s.metrics, NewMetric("ma_status").withLabels(gauges.DefaultLabels()).setValue(0))
+	defer s.HandleMetricCollectorStatus()
+	startTime := time.Now()
+	s.metrics = append(s.metrics, NewMetric("ma_collector_start").withLabels(gauges.DefaultLabels()).setValue(startTime.Nanosecond()))
+	s.metrics = append(s.metrics, NewMetric("ma_collector_status").withLabels(gauges.DefaultLabels()).setValue(0))
+
+	s.collectPodMetrics()
+	labels := gauges.DefaultLabels()
+	labels["host"] = util.GetEnv("HOSTNAME", "")
+	s.metrics = append(s.metrics, NewMetric("ma_collector_duration").withLabels(labels).setValue(time.Now().Nanosecond()-startTime.Nanosecond()))
+	logger.Info("Start to collect metrics finished")
+
+}
+
+func (s *Scraper) HandleMetricCollectorStatus() {
+	logger.Info("Start Handle Metric Collector Status")
+
+	if r := recover(); r != nil {
+		logger.Error(fmt.Sprintf("Metric Collector stopped with error. Set Status 'PROBLEM'. Recovered in: %v", r))
+		status := 6
+		s.metrics = append(s.metrics, NewMetric("ma_status").withLabels(gauges.DefaultLabels()).setValue(status))
+	}
+}
+
+func (s *Scraper) PrintMetrics() []string {
+	logger.Info("Print gathered metrics to response")
+	metrics := make([]string, 0)
+	for _, metric := range s.metrics {
+		result := fmt.Sprintf("%s{%s} %v", metric.name, mapToString(metric.labels), metric.value)
+		logger.Debug(fmt.Sprintf("%v", result))
+		metrics = append(metrics, result)
+	}
+	s.metrics = make([]*Metric, 0)
+	return metrics
+}
+
+func mapToString(m labels) string {
+	b := new(bytes.Buffer)
+	for _, key := range m.sort() {
+		fmt.Fprintf(b, "%s=\"%s\",", key, m[key])
+	}
+	str := b.String()
+	return str[:len(str)-1]
+}
+
+func (s *Scraper) CollectBackupMetrics() {
+	logger.Info("Collect backup metrics")
+	var response = BackupResponse{}
+	protocol, _ := util.GetProtocol()
+	url := fmt.Sprintf("%s%s", protocol, "postgres-backup-daemon:9000/backup/info")
+
+	status, body, err := util.ProcessHttpRequest(s.httpClient, url, s.token)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Cannot collect backup status metric. url %v", url))
+		return
+	}
+	code := strings.Fields(status)[0]
+	statusCode, err := strconv.Atoi(code)
+	if statusCode >= 400 || err != nil {
+		logger.Warn(fmt.Sprintf("Cannot collect backup status metric. Error code %v", statusCode))
+		logger.Warn(fmt.Sprintf("Error: %v", err))
+		return
+	}
+	err = json.Unmarshal(body, &response)
+	if err != nil {
+		Log.Error(fmt.Sprintf("Process Config Map Unmarshal Error: %s", err))
+		logger.Error(fmt.Sprintf("Error: %v", err))
+		return
+	}
+	infos := response["granular"]
+	for _, info := range infos {
+		labels := gauges.DefaultLabels()
+		labels["backupId"] = info.BackupID
+		labels["space"] = info.Namespace
+		labels["expirationDate"] = info.ExpirationDate
+		labels["created"] = info.Created
+		status, ok := backupStatus[strings.ToUpper(info.Status)]
+		if !ok {
+			Log.Warn(fmt.Sprintf("Backup status %s in undefined for %s", info.Status, info.BackupID))
+			status = -1 //UNDEFINED
+		}
+		s.metrics = append(s.metrics, NewMetric("ma_granular_backups_info").withLabels(labels).setValue(status))
+	}
+}
+
+func (s *Scraper) collectPodMetrics() {
+
+	podData, _ := k8sClient.GetPodsByLabel(context.Background(), map[string]string{"app": clusterName, "pgcluster": clusterName})
+	Log.Debug(fmt.Sprintf("pod info: %v", podData))
+
+	podItems := podData.Items
+	podIdentities := make([]string, 0)
+
+	for _, pod := range podItems {
+		phase := pod.Status.Phase
+		deploymentInfo := pod.Status.ContainerStatuses
+		if len(deploymentInfo) == 0 {
+			Log.Info(fmt.Sprintf("\"Skipping pod with info: %v", pod))
+			continue
+		}
+		Log.Debug(fmt.Sprintf("Deployment information returned %v", deploymentInfo))
+		deploymentName := deploymentInfo[0].Name
+		Log.Debug(fmt.Sprintf("Deployment Name returned from array %s", deploymentName))
+		unavailableReplicas := s.getUnavailableReplicasFromDeployment(deploymentName)
+		if phase == "Running" && unavailableReplicas != 1 { //TODO why unavailableReplicas != 1 ? may be < 1 ?
+			podIdentity := s.collectGenericMetricsForPod(pod)
+			podIdentities = append(podIdentities, podIdentity)
+		} else {
+			podName := pod.Name
+			reason := pod.Status.Reason
+			Log.Info(fmt.Sprintf("skipping pod: %s phase: %s reason: %s", podName, phase, reason))
+		}
+	}
+	s.collectClusterStatusMetrics(podIdentities, podItems)
+}
+
+func (s *Scraper) gatherClasterInfo(podItems []v1.Pod) (masters []v1.Pod, replicaNames []string, masterName string, workingNodes int) {
+	logger.Debug("Process data from pods - get working nodes count")
+	workingNodes = 0
+	masters = make([]v1.Pod, 0)
+	replicaNames = make([]string, 0)
+	masterName = ""
+	for _, pod := range podItems {
+		if util.GetPodStatus(pod) == "running" {
+			workingNodes++
+		}
+		role := util.SafeGet(pod.Annotations["status"], append(make([]interface{}, 0), "role"), "")
+		if role == "master" {
+			masters = append(masters, pod)
+			masterName = pod.Name
+		} else if role == "replica" {
+			replicaNames = append(replicaNames, pod.Name)
+		}
+	}
+	return masters, replicaNames, masterName, workingNodes
+}
+
+func (s *Scraper) collectClusterStatusMetrics(podIdentities []string, podItems []v1.Pod) {
+	masters, replicaNames, masterName, workingNodes := s.gatherClasterInfo(podItems)
+
+	logger.Debug("Process data from pods - collect info about master")
+	logger.Debug(fmt.Sprintf("Process master pod: %s", masterName))
+	leaderRole := util.GetLeaderPod(s.httpClient, s.token, clusterName)
+	isClusterActive := 1
+	if util.SafeGet(leaderRole, append(make([]interface{}, 0), "name"), "").(string) == masterName &&
+		util.SafeGet(leaderRole, append(make([]interface{}, 0), "role"), "").(string) == "standby_leader" {
+		isClusterActive = 0
+	}
+
+	errorStatus := "None"
+	errorMessage := "None"
+	errorDescription := "None"
+	var clusterState string
+	clusterStatusCode := 0
+
+	if len(masters) > 0 {
+		logger.Debug("Master found. Starting smoke tests")
+		master := masters[0]
+		s.enrichMasterMetrics(master, replicaNames, isClusterActive)
+
+		podIdentity := util.GetPodIdentity(master)
+
+		for _, metric := range s.metrics {
+			if metric.name == "ma_pg_metrics_xlog_location" && metric.labels["pg_node"] == podIdentity {
+
+				util.StoreLastMasterAppearance(metric.getValue())
+			}
+		}
+		logger.Debug(fmt.Sprintf("Master Status: %v", master.Status))
+
+	} else {
+		errorStatus, errorMessage, errorDescription = s.calculatePGClusterStatusForMissingMaster(podItems)
+	}
+	actualNodes := 0
+	for _, podIdentity := range podIdentities {
+		logger.Debug(fmt.Sprintf("Check Actual podIdentities : %v", podIdentities))
+		if s.isXlogLocationActual(podIdentity) {
+			actualNodes++
+		}
+	}
+
+	masterWritable := s.getMetricValue(fmt.Sprintf("ma_pg_%s_cluster_master_writable", clusterName))
+
+	if masterWritable == nil {
+		masterWritable = float64(0)
+	}
+
+	// check standby cluster settings here and proceed with OK to prevent metrics rewriting.
+	// It's expected that standby_leader is not writable.
+	pgNodesQty, _ := strconv.Atoi(DefaultClusterPgNodeQty)
+	logger.Debug(fmt.Sprintf("Cluster Status: masterWritable: %v, isClusterActive: %v, workingNodes: %v, DefaultClusterPgNodeQty: %v, actualNodes: %v", masterWritable, isClusterActive, workingNodes, DefaultClusterPgNodeQty, actualNodes))
+	if (masterWritable == 1 || isClusterActive == 1) && workingNodes >= pgNodesQty && actualNodes >= pgNodesQty {
+		clusterState = "OK"
+		clusterStatusCode = 0
+	} else if masterWritable == 1 || isClusterActive == 0 {
+		clusterState = "DEGRADED"
+		clusterStatusCode = 6
+		if workingNodes != pgNodesQty {
+			errorStatus = "WARNING"
+			errorMessage = "One or more replicas does not have running postgresql."
+			errorDescription = "Supposed action: Check logs and follow troubleshooting guide."
+		} else {
+			errorStatus = "WARNING"
+			errorMessage = "One or more replicas cannot start replication."
+			errorDescription = "Supposed action: Check logs and follow troubleshooting guide."
+		}
+	} else {
+		clusterState = "ERROR"
+		clusterStatusCode = 10
+		if len(masters) > 0 {
+			errorStatus, errorMessage, errorDescription = s.calculatePGClusterStatusForMissingMaster(podItems)
+		}
+	}
+
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_endpoints_cluster_nodes", clusterName)).withLabels(gauges.DefaultLabels()).setValue(len(podItems)))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_working_nodes", clusterName)).withLabels(gauges.DefaultLabels()).setValue(workingNodes))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_actual_nodes", clusterName)).withLabels(gauges.DefaultLabels()).setValue(workingNodes))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_master_writable", clusterName)).withLabels(gauges.DefaultLabels()).setValue(masterWritable))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_status", clusterName)).withLabels(gauges.DefaultLabels()).setValue(clusterStatusCode))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_active", clusterName)).withLabels(gauges.DefaultLabels()).setValue(isClusterActive))
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_nodes", clusterName)).withLabels(gauges.DefaultLabels()).setValue(len(podItems)))
+
+	// log cluster status
+	logger.Debug(fmt.Sprintf("Cluster Info \n Status: %s\n Message: %s\n Description: %s\n State: %s", errorStatus, errorMessage, errorDescription, clusterState))
+}
+
+func (s *Scraper) isXlogLocationActual(podIdentity string) bool {
+	lastMasterXlogLocation := util.GetPodValue("master", "last_master_xlog_location", "0")
+	isRunning := float64(0)
+	currentXlogLocation := float64(0)
+
+	for _, metric := range s.metrics {
+		if metric.name == "ma_pg_pg_metrics_running" {
+			isRunning = metric.getValue()
+		}
+		if metric.name == "ma_pg_metrics_xlog_location" && metric.labels["pg_node"] == podIdentity {
+			currentXlogLocation = metric.getValue()
+		}
+	}
+	if isRunning == 1 {
+		lastXlogLocationStr := fmt.Sprintf("%f", currentXlogLocation)
+		lastXlogLocation := util.GetPodValue(podIdentity, "xlog_location", lastXlogLocationStr)
+		currentXlogLocationStr := fmt.Sprintf("%f", currentXlogLocation)
+		util.StorePodValue(podIdentity, "xlog_location", currentXlogLocationStr)
+		logger.Debug(fmt.Sprintf("For %s: current_xlog_location=%s, last_xlog_locatioqn=%s, last_master_xlog_location=%s",
+			podIdentity, currentXlogLocationStr, lastXlogLocation, lastMasterXlogLocation))
+		lastMasterXlogLocationFloat, _ := strconv.ParseFloat(lastMasterXlogLocation, 64)
+		lastXlogLocationFloat, _ := strconv.ParseFloat(lastXlogLocation, 64)
+		delta1 := currentXlogLocation - lastXlogLocationFloat
+		delta2 := currentXlogLocation - lastMasterXlogLocationFloat
+		if delta2 < 0 && delta1 == 0 {
+			logger.Debug(fmt.Sprintf("Node %s has zero progress for xlog_location while master is ahead of node. "+
+				"Calculated metrics: delta1: %f, delta2: %F", podIdentity, delta1, delta2))
+			return false
+		}
+	}
+	return true
+}
+
+func (s *Scraper) calculatePGClusterStatusForMissingMaster(podItems []v1.Pod) (string, string, string) {
+	logger.Warn("Master not found or in read only mode. Calculating cluster status.")
+
+	errorStatus := "None"
+	errorMessage := "None"
+	errorDescription := "None"
+
+	if len(podItems) < 1 {
+		logger.Warn("Can not calculate metrics for missing master.")
+		return errorStatus, errorMessage, errorDescription
+	}
+	pod := podItems[0]
+
+	RTO, _ := strconv.ParseFloat(util.GetEnvValueFromPod(pod, "PATRONI_TTL", "60"), 64)
+	RPO, _ := strconv.ParseFloat(util.GetEnvValueFromPod(pod, "PATRONI_MAXIMUM_LAG_ON_FAILOVER", "1048576"), 64)
+	podIdentity := util.GetPodIdentity(pod)
+	logger.Info(fmt.Sprintf("RTO: %f, RPO: %f", RTO, RPO))
+	lma, lmxl := util.GetLatestMasterAppearance()
+	maxLagLocation := float64(0)
+	if len(podItems) > 0 {
+		for _, metric := range s.metrics {
+			if metric.name == "ma_pg_metrics_xlog_location" && metric.labels["pg_node"] == podIdentity {
+				maxLagLocation = math.Max(maxLagLocation, metric.getValue())
+			}
+		}
+	}
+	if lma != "" {
+		timeSinceMasterDisappear := float64(time.Now().Second()) - maxLagLocation
+		lmxlFloat, _ := strconv.ParseFloat(lmxl, 64)
+		minimumLag := lmxlFloat - maxLagLocation
+		if timeSinceMasterDisappear < RTO && minimumLag > RPO {
+			errorStatus = "WARNING"
+			errorMessage = "No replica to promote but service is down less than RTO"
+			errorDescription = fmt.Sprintf("Supposed action: check master state and restore if possible. Stats:"+
+				" [ last_master_appearance: %s, ast_master_xlog_location: %s, max_xlog_location: %f ]", lma, lmxl, maxLagLocation)
+		}
+		if timeSinceMasterDisappear > RTO && minimumLag > RPO {
+			errorStatus = "CRITICAL"
+			errorMessage = "No replica to promote and service is down more than RTO"
+			errorDescription = fmt.Sprintf("Supposed action: try to restore master manually if possible or perform failover. Stats:"+
+				" [ last_master_appearance: %s, ast_master_xlog_location: %s, max_xlog_location: %f ]", lma, lmxl, maxLagLocation)
+		}
+
+	} else {
+		errorStatus = "CRITICAL"
+		errorMessage = "monitoring does not have record if master ever existed."
+		errorDescription = fmt.Sprintf("Supposed action: Check cluster state. Ignore this message if cluster is staring up. Stats:"+
+			" [ last_master_appearance: %s, ast_master_xlog_location: %s, max_xlog_location: %f ]", lma, lmxl, maxLagLocation)
+	}
+	return errorStatus, errorMessage, errorDescription
+}
+
+func (s *Scraper) enrichMasterMetrics(master v1.Pod, replicaNames []string, active int) {
+
+	logger.Debug(fmt.Sprintf("Enrich metrics for master pod: %s", master.Name))
+
+	isMasterRunning := util.SafeGet(master.Annotations["status"], append(make([]interface{}, 0), "state"), "").(string)
+	if strings.EqualFold(isMasterRunning, "running") {
+		if active == 1 {
+			logger.Debug("Starting smoke tests")
+			s.performSmokeTest()
+			// print metrics from smoke tests here
+		} else {
+			logger.Debug("Cluster is in standby mode. Skip smoke tests")
+		}
+		s.collectReplicationData(replicaNames)
+		s.collectArchiveData()
+	} else {
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_passed").withLabels(gauges.DefaultLabels()).setValue(0))
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_master_writable", clusterName)).withLabels(gauges.DefaultLabels()).setValue(0))
+	}
+	for _, metric := range s.metrics {
+
+		if metric.name == "ma_pg_endpoints_cluster_smoketest_passed" {
+			passed := metric.getValue()
+			s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_cluster_master_writable", clusterName)).withLabels(gauges.DefaultLabels()).setValue(passed))
+		}
+	}
+	s.collectReplicationLag()
+
+}
+func (s *Scraper) performSmokeTest() {
+	// using already rewrote method for smoke tests
+	s.GetEndpointsStatus()
+}
+
+func (s *Scraper) collectReplicationLag() {
+
+	url := fmt.Sprintf("http://pg-%s-api:8008/cluster", clusterName)
+
+	status, response, err := util.ProcessHttpRequest(s.httpClient, url, s.token)
+	if err != nil {
+		logger.Error(err.Error())
+	}
+	statusCode, _ := strconv.Atoi(status)
+	if statusCode >= 400 {
+		logger.Error(fmt.Sprintf("Cannot collect replication lag data. Error code: %v", statusCode))
+		return
+	}
+	var res map[string]interface{}
+	err = json.Unmarshal(response, &res)
+	if err != nil {
+		logger.Error("Can not collect replication lag metric. Response parsing error")
+	} else {
+		members := util.SafeGet(res, append(make([]interface{}, 0), "members"), make(map[string]interface{}, 0)).([]interface{})
+		for _, member := range members {
+			role := util.SafeGet(member, append(make([]interface{}, 0), "role"), "").(string)
+			if role == "leader" || role == "standby_leader" {
+				continue
+			}
+			state := util.SafeGet(member, append(make([]interface{}, 0), "state"), "").(string)
+			if state == "running" || state == "streaming" {
+				lag := util.SafeGet(member, append(make([]interface{}, 0), "lag"), "")
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_replication_lag", clusterName)).withLabels(gauges.DefaultLabels()).setValue(lag))
+			}
+		}
+	}
+}
+
+func (s *Scraper) collectReplicationData(replicaNames []string) {
+
+	logger.Debug("Collect data about replication")
+
+	if s.pgMajorVersion > 0 {
+		err := pc.EstablishConn(ctx)
+		if err != nil {
+			Log.Error("Can not establish connection to postgresql to collect replication data")
+		} else {
+			defer pc.CloseConnection(ctx)
+			_, rows := pc.GetData(ctx, GetMetricsTypeByVersion("replication_data", s.pgMajorVersion))
+			replicasMatch := len(replicaNames) == len(rows)
+			replicationStates := make([]string, 0)
+			for _, row := range rows {
+
+				appName := strings.Replace(postgres.GetStringValue(row, "application_name", "empty"), " ", "_", -1)
+				sentReplayLag, _ := util.GetFloatValue(row["sent_replay_lag"], float64(0))
+				sentLag, _ := util.GetFloatValue(row["sent_lag"], float64(0))
+
+				sentLagMetricName := fmt.Sprintf("ma_pg_%s_replication_state_sent_lag", clusterName)
+				sentReplayLagmetricName := fmt.Sprintf("ma_pg_%s_replication_state_sent_replay_lag", clusterName)
+
+				labels := gauges.DefaultLabels()
+				labels["hostname"] = appName
+
+				s.metrics = append(s.metrics, NewMetric(sentLagMetricName).withLabels(labels).setValue(sentLag))
+				s.metrics = append(s.metrics, NewMetric(sentReplayLagmetricName).withLabels(labels).setValue(sentReplayLag))
+
+				replicationStates = append(replicationStates, appName)
+			}
+			if util.GetEnv("SITE_MANAGER", "") == "on" {
+				for _, name := range replicaNames {
+					if slices.Contains(replicationStates, name) && replicasMatch {
+						smReplicationState := 0
+						metricName := fmt.Sprintf("ma_pg_%s_replication_state_sm_replication_state", clusterName)
+						s.metrics = append(s.metrics, NewMetric(metricName).withLabels(gauges.DefaultLabels()).setValue(smReplicationState))
+					}
+				}
+			}
+		}
+	}
+}
+
+func (s *Scraper) collectArchiveData() {
+
+	logger.Debug("Collect data of wal archive")
+	baseName := fmt.Sprintf("ma_pg_%s_pg_metrics_archive", clusterName)
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		Log.Error("Can not establish connection to postgresql to collect archive data")
+	} else {
+		defer pc.CloseConnection(ctx)
+		_, rows := pc.GetData(ctx, ArchiveDataQuery)
+		for _, row := range rows {
+			mode := postgres.GetStringValue(row, "setting", "off")
+			if mode == "on" {
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "mode")).withLabels(gauges.DefaultLabels()).setValue(1))
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "mode_prom")).withLabels(gauges.DefaultLabels()).setValue(1))
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "archived_count")).withLabels(gauges.DefaultLabels()).setValue(row["archived_count"]))
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "failed_count")).withLabels(gauges.DefaultLabels()).setValue(row["failed_count"]))
+				delay, err := util.GetFloatValue(row["delay"], 0.0)
+				if err != nil {
+					logger.Error(fmt.Sprintf("Error parsing float value: %v for column extract, returning default value: %v", row["delay"], 0.0), zap.Error(err))
+				}
+				if delay != 0.0 {
+					s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "delay")).withLabels(gauges.DefaultLabels()).setValue(delay))
+				}
+			} else {
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "mode")).withLabels(gauges.DefaultLabels()).setValue(0))
+				s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("%s_%s", baseName, "mode_prom")).withLabels(gauges.DefaultLabels()).setValue(0))
+			}
+		}
+	}
+}
+
+func (s *Scraper) getUnavailableReplicasFromDeployment(deploymentName string) int32 {
+	dcInfo := k8sClient.GetDeploymentInfo(deploymentName)
+	if dcInfo != nil {
+		status := &dcInfo.Status
+		unavailableReplicas := &status.UnavailableReplicas
+		return *unavailableReplicas
+	}
+	return 0
+}
+
+func (s *Scraper) collectGenericMetricsForPod(pod v1.Pod) string {
+
+	podId := pod.Name
+	podIp := pod.Status.PodIP
+	Log.Debug(fmt.Sprintf("Start metric collection for %s [%s]", podId, podIp))
+
+	podIdentity := util.GetPodIdentity(pod)
+
+	Log.Debug(fmt.Sprintf("pod_identity: %s", podIdentity))
+
+	podLabels := map[string]string{
+		"namespace": namespace,
+		"name":      podId,
+		"ip":        podIp,
+		"role":      util.DetermineRole(pod),
+		"status":    string(pod.Status.Phase),
+		"startedAt": pod.Status.StartTime.String(),
+		"pg_node":   podIdentity,
+	}
+
+	// #scrapemetric
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_pod", clusterName)).withLabels(podLabels).setValue(0))
+
+	state := util.GetPodStatus(pod)
+	Log.Debug(fmt.Sprintf("state: %s", state))
+
+	if state == "running" {
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_patroni_status", clusterName)).withLabels(podLabels).setValue(1))
+	} else {
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_patroni_status", clusterName)).withLabels(podLabels).setValue(0))
+	}
+
+	if s.pgMajorVersion > 0 {
+
+		pgConnector := postgres.NewConnectorForUser(podIp, postgres.PgPort, postgres.PgUser, postgres.PgPass)
+
+		s.collectPGMetrics(podIdentity, pgConnector)
+		s.collectShellMetrics(pod, podIdentity, s.pgMajorVersion)
+	}
+
+	return podIdentity
+}
+
+func getFullVersionOfPGSQLServer() float64 {
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		Log.Debug("Can not establish connection to postgresql to check version")
+		return 0
+	}
+	defer pc.CloseConnection(ctx)
+	if version, err := pc.GetValue(ctx, "SHOW SERVER_VERSION"); err == nil {
+		if result, err := strconv.ParseFloat(strings.Split(version.(string), " ")[0], 64); err == nil {
+			return result
+		} else {
+			Log.Debug("Can not read postgresql version")
+			return 0
+		}
+	} else {
+		Log.Debug("Can not read postgresql version")
+		return 0
+	}
+}
+
+func getVersionOfPGSQLServer() int {
+	result := getFullVersionOfPGSQLServer()
+	return int(result)
+}
+
+func (s *Scraper) collectPGMetrics(podIdentity string, pc *postgres.PostgresConnector) {
+
+	startTime := time.Now()
+
+	// common pg metrics will be collected in  collector/pkg/metrics/common_metrics.go:collectMetrics
+	// disable to avoid metrics duplication
+	//s.collectCommonPGMetrics(podIdentity, pc)
+	s.collectReplicationSlotsData(podIdentity, pc)
+
+	Log.Debug(fmt.Sprintf("Pg metrics loading time %v", time.Since(startTime)))
+
+}
+
+func (s *Scraper) collectReplicationSlotsData(podIdentity string, pgConnector *postgres.PostgresConnector) {
+
+	Log.Debug(fmt.Sprintf("Collecting replication slots data for pgsql server %v", s.pgMajorVersion))
+
+	err := pgConnector.EstablishConn(ctx)
+	if err != nil {
+		Log.Error("Can not establish connection to postgresql to collect replication slots data")
+	} else {
+		defer pgConnector.CloseConnection(ctx)
+		labels := gauges.DefaultLabels()
+		labels["pg_node"] = podIdentity
+		_, rows := pgConnector.GetData(ctx, GetMetricsTypeByVersion("replication_slots", s.pgMajorVersion))
+		for _, row := range rows {
+			serviceName := strings.Replace(row["slot_name"].(string), " ", "_", -1)
+			if val, ok := row["restart_lsn_lag"]; ok {
+				if restartLsnLagValue, err := util.GetFloatValue(val, float64(0)); err != nil {
+					Log.Warn(fmt.Sprintf("Can not parse replication_slots metric with value %v", row["restart_lsn_lag"]))
+				} else {
+					metricName := fmt.Sprintf("ma_pg_%s_pg_metrics_replication_slots_%s_restart_lsn_lag", clusterName, serviceName)
+					s.metrics = append(s.metrics, NewMetric(metricName).withLabels(labels).setValue(restartLsnLagValue))
+				}
+			}
+
+			if confirmedFlushLsnLagValue, err := util.GetFloatValue(row["confirmed_flush_lsn_lag"], float64(0)); err != nil {
+				Log.Warn(fmt.Sprintf("Can not parse replication_slots metric with value %v", row["confirmed_flush_lsn_lag"]))
+			} else {
+				metricName := fmt.Sprintf("ma_pg_%s_pg_metrics_replication_slots_%s_confirmed_flush_lsn_lag", clusterName, serviceName)
+				s.metrics = append(s.metrics, NewMetric(metricName).withLabels(labels).setValue(confirmedFlushLsnLagValue))
+			}
+		}
+	}
+	Log.Debug(fmt.Sprintf("Collecting replication slots data for pgsql server %v Completed", s.pgMajorVersion))
+
+}
+
+// TODO: there are can be several metrics with the same name
+func (s *Scraper) getMetricValue(metricName string) any {
+	for _, metric := range s.metrics {
+		if metric.name == fmt.Sprint(metricName) {
+			return metric.getValue()
+		}
+	}
+	return nil
+}
+
+func (s *Scraper) collectShellMetrics(pod v1.Pod, podIdentity string, pgVersion int) {
+
+	startTime := time.Now()
+	s.collectDiskMetricsOnPod(pod, podIdentity, pgVersion)
+	logger.Debug(fmt.Sprintf("Shell metrics loading time: %v", int(time.Since(startTime).Milliseconds())))
+}
+
+func GetContainerNameForPatroniPod(pod v1.Pod) string {
+	for _, c := range pod.Spec.Containers {
+		if strings.HasPrefix(c.Name, "pg-patroni") {
+			return c.Name
+		}
+	}
+	if len(pod.Spec.Containers) > 0 {
+		return pod.Spec.Containers[0].Name
+	}
+	return ""
+}
+
+func (s *Scraper) collectDiskMetricsOnPod(pod v1.Pod, podIdentity string, pgVersion int) {
+	logger.Debug("Collect disk metrics on pod")
+	containerName := GetContainerNameForPatroniPod(pod)
+	for metric, command := range shellMetrics {
+		res, _, err := util.ExecCmdOnPod(s.client, pod.Name, pod.Namespace, command, containerName)
+		if err != nil {
+			return
+		}
+		if metric == "du" {
+			s.duProcessor(res, podIdentity, pgVersion)
+		}
+		if metric == "df" {
+			s.dfProcessor(res, podIdentity)
+		}
+	}
+}
+
+func (s *Scraper) duProcessor(result string, podIdentity string, pgVersion int) {
+
+	logger.Debug(fmt.Sprintf("Run du Processor for node: %s", podIdentity))
+	var values = map[string]string{}
+	for _, line := range strings.Split(strings.TrimSuffix(result, "\n"), "\n") {
+
+		str := strings.Fields(line)
+		values[str[1]] = str[0]
+
+	}
+	labels := gauges.DefaultLabels()
+	labels["pg_node"] = podIdentity
+
+	total, ok := values["/var/lib/pgsql/data/"]
+	if ok {
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_du_%s", clusterName, "total")).withLabels(labels).setValue(total))
+	}
+	pg_xlog, ok := values[fmt.Sprintf(GetMetricsTypeByVersion("pg_xlog", pgVersion), podIdentity)]
+	if ok {
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_du_%s", clusterName, "pg_xlog")).withLabels(labels).setValue(pg_xlog))
+	}
+	base, ok := values[fmt.Sprintf("/var/lib/pgsql/data/postgresql_%s/base", podIdentity)]
+	if ok {
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_du_%s", clusterName, "base")).withLabels(labels).setValue(base))
+	}
+
+	totalFloat, err := util.GetFloatValue(total, float64(0))
+	if err != nil {
+		Log.Warn(fmt.Sprintf("Can't parse disk etric value. Error: %v", err))
+		return
+	}
+	pg_xlogFloat, err := util.GetFloatValue(pg_xlog, float64(0))
+	if err != nil {
+		Log.Warn(fmt.Sprintf("Can't parse disk etric value. Error: %v", err))
+		return
+	}
+	baseFloat, err := util.GetFloatValue(base, float64(0))
+	if err != nil {
+		Log.Warn(fmt.Sprintf("Can't parse disk etric value. Error: %v", err))
+		return
+	}
+
+	other := totalFloat - pg_xlogFloat - baseFloat
+
+	s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_du_other", clusterName)).withLabels(labels).setValue(other))
+}
+
+func (s *Scraper) dfProcessor(result string, podIdentity string) {
+	logger.Debug(fmt.Sprintf("Run df Processor for node: %s result \n%s", podIdentity, result))
+
+	for i, line := range strings.Split(strings.TrimSuffix(result, "\n"), "\n") {
+		// skip 1st info line from command output
+		if i == 0 {
+			continue
+		}
+		values := strings.Fields(line)
+		avail := values[3]
+		pcent := values[4]
+		pcent = pcent[:len(pcent)-1]
+
+		labels := gauges.DefaultLabels()
+		labels["pg_node"] = podIdentity
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_df_avail", clusterName)).withLabels(labels).setValue(avail))
+		s.metrics = append(s.metrics, NewMetric(fmt.Sprintf("ma_pg_%s_metrics_df_pcent", clusterName)).withLabels(labels).setValue(pcent))
+	}
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/smoketest.go b/docker-monitoring-agent/collector/pkg/metrics/smoketest.go
new file mode 100644
index 0000000..05a2c32
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/smoketest.go
@@ -0,0 +1,145 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"context"
+	"fmt"
+	"time"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/postgres"
+	uuid "github.com/google/uuid"
+	"go.uber.org/zap"
+)
+
+func (s *Scraper) GetEndpointsStatus() {
+	logger.Info("Endpoints metric collection started")
+	ctx := context.Background()
+	pc := postgres.NewConnector()
+	err := pc.EstablishConn(ctx)
+	if err != nil {
+		return
+	}
+	defer pc.CloseConnection(ctx)
+	labels := gauges.DefaultLabels()
+
+	status, err := pc.GetValue(ctx, "select 1;")
+	if err == nil {
+		s.performSmoketests(ctx, pc)
+	} else {
+
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_passed").withLabels(labels).setValue(0))
+		insertLabels := gauges.DefaultLabels()
+		insertLabels["action"] = "insert"
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_check").withLabels(insertLabels).setValue(0))
+		selectLabels := gauges.DefaultLabels()
+		selectLabels["action"] = "select"
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_check").withLabels(selectLabels).setValue(0))
+		updateLabels := gauges.DefaultLabels()
+		updateLabels["action"] = "update"
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_check").withLabels(updateLabels).setValue(0))
+		deleteLabels := gauges.DefaultLabels()
+		deleteLabels["action"] = "delete"
+		s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_check").withLabels(labels).setValue(0))
+	}
+
+	delete(labels, "action")
+	labels["url"] = pc.GetHost()
+	s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_running").withLabels(labels).setValue(status))
+	logger.Info("Endpoints metric collection finished")
+}
+
+func (s *Scraper) performSmoketests(ctx context.Context, pc *postgres.PostgresConnector) {
+	type param struct {
+		query string
+		err   string
+	}
+	var params = []param{
+		{query: "create table if not exists monitor_test (id bigint primary key not null, value text not null);",
+			err: "cannot execute create"},
+		{query: "create sequence if not exists monitor_test_seq start 10001;",
+			err: "cannot create sequence for smoketest"}}
+	for _, p := range params {
+		_, err := pc.Exec(ctx, p.query)
+		if err != nil {
+			logger.Error(p.err, zap.Error(err))
+			return
+		}
+	}
+	seqVal, err := pc.GetValue(ctx, "SELECT nextval('monitor_test_seq');")
+	if err != nil {
+		logger.Error("cannot select nextval('monitor_test_seq')", zap.Error(err))
+		return
+	}
+	if seqVal.(int64) == int64(2147483647) {
+		_, err = pc.Exec(ctx, "delete from monitor_test;")
+		if err != nil {
+			logger.Error("cannot delete from monitor_test", zap.Error(err))
+			return
+		}
+		_, err = pc.Exec(ctx, "SELECT setval(monitor_test_seq, 10001);")
+		if err != nil {
+			logger.Error("cannot setval for monitor_test_seq", zap.Error(err))
+			return
+		}
+	}
+
+	newUUID := getUUID4()
+	secondUUID := getUUID4()
+
+	resultInsert := s.performSingleSmokeTest(ctx, pc, "insert", newUUID, fmt.Sprintf("insert into monitor_test values (%v, %v) returning id;", newUUID, secondUUID))
+
+	passed := 0
+	if resultInsert {
+		resultSelect := s.performSingleSmokeTest(ctx, pc, "select", newUUID, fmt.Sprintf("select id from monitor_test where id=%v;", newUUID))
+		resultUpdate := s.performSingleSmokeTest(ctx, pc, "update", newUUID, fmt.Sprintf("update monitor_test set value='%v' where id=%v returning id;", secondUUID, newUUID))
+		resultDelete := s.performSingleSmokeTest(ctx, pc, "delete", newUUID, fmt.Sprintf("delete from monitor_test where id=%v returning id;", newUUID))
+
+		logger.Debug(fmt.Sprintf("resultInsert: %v resultSelect: %v resultUpdate %v resultDelete %v", resultInsert, resultSelect, resultUpdate, resultDelete))
+		if resultInsert && resultSelect && resultUpdate && resultDelete {
+			passed = 1
+		}
+	}
+	s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_passed").withLabels(gauges.DefaultLabels()).setValue(passed))
+}
+
+func (s *Scraper) performSingleSmokeTest(ctx context.Context, pc *postgres.PostgresConnector, action string, expectedOutput int64, query string, args ...interface{}) bool {
+	resultCode := 0
+	startTime := time.Now()
+	output, err := pc.GetValue(ctx, query, args...)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Error during process action %s", action), zap.Error(err))
+		resultCode = -1
+	}
+	if output.(int64) == expectedOutput {
+		resultCode = 1
+	}
+	execTime := int(time.Since(startTime).Milliseconds())
+	labels := gauges.DefaultLabels()
+	labels["action"] = action
+	s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_check").withLabels(labels).setValue(resultCode))
+	s.metrics = append(s.metrics, NewMetric("ma_pg_endpoints_cluster_smoketest_time_ms").withLabels(labels).setValue(execTime))
+	if resultCode != 1 && output != nil {
+		s.metrics = append(s.metrics, NewMetric("ma_pg_smoketest_output").withLabels(labels).setValue(output))
+	}
+	return resultCode == 1
+}
+
+func getUUID4() int64 {
+	uuid := uuid.New()
+	return int64(uuid.ID())
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/type.go b/docker-monitoring-agent/collector/pkg/metrics/type.go
new file mode 100644
index 0000000..4fad5ec
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/type.go
@@ -0,0 +1,65 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import "github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+
+type Row map[string]interface{}
+
+type SortedMap struct {
+	keys         []string
+	mappedValues map[string]interface{}
+}
+
+type SortedMapIterator struct {
+	pos int
+	sm  SortedMap
+}
+
+func (sm *SortedMap) Put(key string, value interface{}) {
+	if sm.keys == nil {
+		sm.keys = make([]string, 0)
+	}
+	if sm.mappedValues == nil {
+		sm.mappedValues = make(map[string]interface{})
+	}
+	if !util.Contains(sm.keys, key) {
+		sm.keys = append(sm.keys, key)
+
+	}
+	sm.mappedValues[key] = value
+}
+
+func (sm *SortedMap) Iterator() SortedMapIterator {
+	return SortedMapIterator{sm: *sm}
+}
+
+func (smi *SortedMapIterator) Next() bool {
+	result := smi.pos < len(smi.sm.keys)
+	if result {
+		smi.pos = smi.pos + 1
+	}
+	return result
+}
+
+func (smi *SortedMapIterator) GetPair() (string, interface{}) {
+	index := smi.pos - 1
+	if index < 0 {
+		return "", nil
+	}
+	key := smi.sm.keys[smi.pos-1]
+	value := smi.sm.mappedValues[key]
+	return key, value
+}
diff --git a/docker-monitoring-agent/collector/pkg/metrics/ura_collector.go b/docker-monitoring-agent/collector/pkg/metrics/ura_collector.go
new file mode 100644
index 0000000..1d0ab53
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/metrics/ura_collector.go
@@ -0,0 +1,258 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package metrics
+
+import (
+	"encoding/json"
+	"fmt"
+	"strconv"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/gauges"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/k8s"
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	dto "github.com/prometheus/client_model/go"
+	"github.com/prometheus/common/expfmt"
+	"go.uber.org/zap"
+)
+
+var (
+	mapping = map[string]int{
+		"up":         0,
+		"successful": 0,
+		"ok":         0,
+		"warning":    3,
+		"planned":    4,
+		"inprogress": 5,
+		"problem":    6,
+		"off":        0,
+		"on":         1,
+		"crit":       10,
+		"fatal":      10,
+		"failed":     10,
+		"down":       14,
+		"unknown":    -1,
+	}
+)
+
+func (s *Scraper) CollectMetricsFromCM() {
+	logger.Debug("Start Collect Services metrics from CMs")
+	defer s.HandleMetricCollectorStatus()
+
+	cms := k8s.GetConfigMaps()
+	for _, configMap := range cms.Items {
+
+		if len(configMap.Name) > 16 {
+			andsWith := configMap.Name[len(configMap.Name)-16:]
+			if andsWith == "collector-config" {
+				logger.Info(fmt.Sprintf("start to process cm: %s", configMap.Name))
+				s.processConfigMap(configMap.Data)
+			}
+		}
+	}
+}
+
+func (s *Scraper) processUrlCollect(url string) []byte {
+
+	logger.Debug(fmt.Sprintf("Process url: %s to fetch metrics", url))
+
+	status, body, err := util.ProcessHttpRequest(s.httpClient, url, s.token)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Cannot collect service status metric. url %v", url), zap.Error(err))
+		return []byte("")
+	}
+	statusCode, _ := strconv.Atoi(status)
+	if statusCode >= 400 {
+		logger.Warn(fmt.Sprintf("Cannot collect service status metric. Error code %v", statusCode))
+	}
+	return body
+
+}
+
+func parseMF(data string) (map[string]*dto.MetricFamily, error) {
+
+	var parser expfmt.TextParser
+	mf, err := parser.TextToMetricFamilies(strings.NewReader(data))
+	if err != nil {
+		return nil, err
+	}
+	return mf, nil
+}
+
+func (s *Scraper) getPrometheusMetrics(url string, module map[string]interface{}) {
+	logger.Debug(fmt.Sprintf("Process prometheus metrics with url: %s", url))
+	data := s.processUrlCollect(url)
+	if len(data) == 0 {
+		Log.Warn("Returned Empty Data, skipping.")
+		return
+	}
+	families, err := parseMF(string(data))
+	if err != nil {
+		Log.Error(fmt.Sprintf("Parce prometheus metrics error: %s", err))
+		return
+	} else {
+		labels := gauges.DefaultLabels()
+		labels["pod_name"] = s.getPodName(module)
+		labels["service_name"] = module["parameters"].(map[string]interface{})["service_name"].(string)
+		for _, family := range families {
+			for _, metric := range family.Metric {
+				if metric.Gauge == nil {
+					continue
+				}
+				m := NewMetric(fmt.Sprintf("ma_%s", *family.Name)).withLabels(labels).setValue(*metric.Gauge.Value)
+
+				s.metrics = append(s.metrics, m)
+			}
+		}
+	}
+}
+
+func (s *Scraper) processConfigMap(data map[string]string) {
+
+	for _, v := range data {
+		var res []map[string]interface{}
+		err := json.Unmarshal([]byte(v), &res)
+		if err != nil {
+			Log.Error(fmt.Sprintf("Process Config Map Unmarshal Error: %s", err))
+			continue
+		}
+		for _, module := range res {
+			if parameters, ok := module["parameters"].(map[string]interface{}); ok {
+				paramType, ok := parameters["type"]
+				if !ok {
+					paramType = "url"
+				}
+				metricType, ok := parameters["metrics_type"]
+				if !ok {
+					metricType = "json"
+				}
+				err = handleSourceOfMetric(s, parameters, paramType, metricType, module)
+				if err != nil {
+					continue
+				}
+			}
+
+		}
+	}
+}
+
+func handleSourceOfMetric(s *Scraper, parameters map[string]interface{}, paramType interface{}, metricType interface{}, module map[string]interface{}) error {
+	var jsonData map[string]interface{}
+	if paramType == "url" {
+		url := parseSSLMode(parameters["url"].(string))
+		if metricType == "json" {
+			byteData := s.processUrlCollect(url)
+			if len(byteData) != 0 {
+				err := json.Unmarshal(byteData, &jsonData)
+				if err != nil {
+					Log.Error(fmt.Sprintf("Process Config Map Unmarshal Error: %s", err))
+					return err
+				}
+				s.handleJsonData(jsonData, module)
+				return nil
+			}
+		} else {
+			s.getPrometheusMetrics(url, module)
+			return nil
+		}
+	}
+	return nil
+}
+
+func parseSSLMode(url string) string {
+	if util.GetEnv("PGSSLMODE", "prefer") == "require" {
+		return strings.Replace(url, "http", "https", -1)
+	}
+	return url
+}
+
+func linearizeJson(obj interface{}) map[string]interface{} {
+
+	res := make(map[string]interface{})
+	walk(obj, "", res)
+	return res
+}
+
+func walk(obj interface{}, prefix string, res map[string]interface{}) {
+	propKey := ""
+	switch o := obj.(type) {
+	default:
+		res[prefix] = obj
+	case map[string]interface{}:
+		for k, v := range o {
+
+			if prefix == "" {
+				propKey = k
+			} else {
+				propKey = fmt.Sprintf("%s_%s", prefix, k)
+			}
+			walk(v, propKey, res)
+		}
+	case []interface{}:
+		for i, item := range o {
+			if prefix == "" {
+				propKey = strconv.Itoa(i)
+			} else {
+				propKey = fmt.Sprintf("%s_%v", prefix, i)
+			}
+			walk(item, propKey, res)
+		}
+	}
+}
+
+func mapStatuses(lines map[string]interface{}) {
+
+	for k, v := range lines {
+		if k == "status" || (len(k) > 7 && k[len(k)-7:] == "_status") {
+			for status, value := range mapping {
+				if strings.ToLower(v.(string)) == status {
+					lines[k] = value
+					break
+				} else {
+					lines[k] = mapping["unknown"]
+				}
+			}
+		}
+	}
+
+}
+
+func (s *Scraper) handleJsonData(json, module map[string]interface{}) {
+
+	podName := s.getPodName(module)
+	labels := gauges.DefaultLabels()
+	labels["selector"] = module["parameters"].(map[string]interface{})["output_selector"].(string)
+	labels["service_name"] = module["parameters"].(map[string]interface{})["service_name"].(string)
+	labels["pod_name"] = podName
+
+	points := linearizeJson(json)
+	mapStatuses(points)
+	for metricName, value := range points {
+
+		mName := fmt.Sprintf("ma_%s", strings.Replace(metricName, ".", "_", -1))
+		metric := NewMetric(mName).withLabels(labels).setValue(value)
+		s.metrics = append(s.metrics, metric)
+	}
+}
+
+func (s *Scraper) getPodName(module map[string]interface{}) string {
+
+	lableKey := module["parameters"].(map[string]interface{})["selector"].(map[string]interface{})["key"].(string)
+	lableValue := module["parameters"].(map[string]interface{})["selector"].(map[string]interface{})["value"].(string)
+	commonLabels := map[string]string{lableKey: lableValue}
+
+	podList, _ := util.GetPodList(s.client, namespace, commonLabels)
+	return podList.Items[0].Name
+}
diff --git a/docker-monitoring-agent/collector/pkg/postgres/client.go b/docker-monitoring-agent/collector/pkg/postgres/client.go
new file mode 100644
index 0000000..71a1a5b
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/postgres/client.go
@@ -0,0 +1,197 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package postgres
+
+import (
+	"context"
+	"fmt"
+	"net/url"
+	"strconv"
+
+	"github.com/Netcracker/pgskipper-monitoring-agent/collector/pkg/util"
+	"github.com/jackc/pgx/v5"
+	"github.com/jackc/pgx/v5/pgxpool"
+	"go.uber.org/zap"
+)
+
+var logger = util.GetLogger()
+
+var (
+	PgHost     = util.GetEnv("POSTGRES_HOST", "pg-patroni")
+	PgPort     = util.GetEnvInt("POSTGRES_PORT", 5432)
+	PgUser     = util.GetEnv("MONITORING_USER", "monitoring-user")
+	PgPass     = util.GetEnv("MONITORING_PASSWORD", "monitoring_password")
+	PgDatabase = util.GetEnv("POSTGRES_DATABASE", "postgres")
+	PgSsl      = util.GetEnv("PGSSLMODE", "prefer")
+)
+
+type Row map[string]interface{}
+
+type PostgresConnector struct {
+	host     string
+	port     int
+	database string
+	user     string
+	password string
+	ssl      bool
+	conn     *pgxpool.Pool
+}
+
+func NewConnector() *PostgresConnector {
+
+	return NewConnectorForUser(PgHost, PgPort, PgUser, PgPass)
+}
+
+func NewConnectorForUser(host string, port int, user, pass string) *PostgresConnector {
+
+	ssl := false
+	if PgSsl == "require" {
+		ssl = true
+	}
+
+	return &PostgresConnector{
+		host:     host,
+		port:     port,
+		database: PgDatabase,
+		user:     user,
+		password: pass,
+		ssl:      ssl,
+	}
+}
+
+func (pc *PostgresConnector) EstablishConn(ctx context.Context) error {
+	return pc.EstablishConnForDB(ctx, pc.database)
+}
+
+func (pc *PostgresConnector) EstablishConnForDB(ctx context.Context, database string) error {
+	pc.CloseConnection(ctx)
+	pc.database = database
+	conn, err := pc.getConnectionToDb(ctx)
+	if err != nil {
+		return err
+	}
+	pc.conn = conn
+	return nil
+}
+
+func (pc *PostgresConnector) CloseConnection(ctx context.Context) {
+	if pc.conn != nil {
+		pc.conn.Close()
+	}
+	pc.conn = nil
+}
+
+func (pc *PostgresConnector) GetHost() string {
+	return pc.host
+}
+
+func (pc *PostgresConnector) GetDatabase() string {
+	return pc.database
+}
+
+func (pc *PostgresConnector) SetHost(host string) {
+	pc.host = host
+}
+
+func (pc *PostgresConnector) getConnectionToDb(ctx context.Context) (*pgxpool.Pool, error) {
+	conn, err := pgxpool.New(ctx, pc.getConnectionUrl())
+	if err != nil {
+		logger.Error("Error occurred during connect to DB", zap.Error(err))
+		return nil, err
+	}
+
+	return conn, nil
+}
+
+func (pc *PostgresConnector) getConnectionUrl() string {
+	username := url.PathEscape(pc.user)
+	password := url.PathEscape(pc.password)
+	if pc.ssl {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s?%s", username, password, pc.host, pc.port, pc.database, "sslmode=require")
+	} else {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s", username, password, pc.host, pc.port, pc.database)
+	}
+}
+
+func (pc *PostgresConnector) GetValue(ctx context.Context, query string, args ...interface{}) (result interface{}, err error) {
+	row := pc.conn.QueryRow(ctx, query, args...)
+	err = row.Scan(&result)
+	if err != nil {
+		logger.Error(fmt.Sprintf("cannot execute %s", query), zap.Error(err))
+		return result, err
+	}
+	return result, nil
+}
+
+func (pc *PostgresConnector) Exec(ctx context.Context, query string, args ...interface{}) (int, error) {
+	cT, err := pc.conn.Exec(ctx, query, args...)
+	return int(cT.RowsAffected()), err
+}
+
+func (pc *PostgresConnector) Query(ctx context.Context, query string, args ...interface{}) (pgx.Rows, error) {
+	if pc.conn == nil {
+		logger.Error("Postgres connection is nil")
+	}
+	rows, err := pc.conn.Query(ctx, query, args...)
+	return rows, err
+}
+
+func (pc *PostgresConnector) GetData(ctx context.Context, query string) ([]string, []Row) {
+	resultRows := []Row{}
+	rows, err := pc.Query(ctx, query)
+	if err != nil {
+		logger.Error(fmt.Sprintf("Cannot execute query %s", query), zap.Error(err))
+		return nil, nil
+	}
+	defer rows.Close()
+
+	fields := []string{}
+	values := make([]interface{}, len(rows.FieldDescriptions()))
+	valuesPoint := make([]interface{}, len(rows.FieldDescriptions()))
+	for i, field := range rows.FieldDescriptions() {
+		fields = append(fields, field.Name)
+		valuesPoint[i] = &values[i]
+	}
+
+	for rows.Next() {
+		resultMap := map[string]interface{}{}
+		err = rows.Scan(valuesPoint...)
+		if err != nil {
+			logger.Error("Cannot scan row")
+			continue
+		}
+		for i, fName := range fields {
+			resultMap[fName] = values[i]
+		}
+		resultRows = append(resultRows, Row(resultMap))
+	}
+	return fields, resultRows
+}
+
+func GetStringValue(row Row, name, defaultValue string) string {
+	if value, ok := row[name]; ok {
+		return value.(string)
+	}
+	return defaultValue
+}
+
+func GetFloatValue(row Row, name string, defaultValue float64) float64 {
+	if value, ok := row[name]; ok {
+		if value, err := strconv.ParseFloat(fmt.Sprintf("%v", value), 64); err != nil {
+			return value
+		}
+	}
+	return defaultValue
+}
diff --git a/docker-monitoring-agent/collector/pkg/util/util.go b/docker-monitoring-agent/collector/pkg/util/util.go
new file mode 100644
index 0000000..ae572b6
--- /dev/null
+++ b/docker-monitoring-agent/collector/pkg/util/util.go
@@ -0,0 +1,556 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package util
+
+import (
+	"bytes"
+	"context"
+	"crypto/tls"
+	"crypto/x509"
+	"encoding/json"
+	"errors"
+	"fmt"
+	"io"
+	"io/fs"
+	"log"
+	"math"
+	"math/big"
+	"net/http"
+	"os"
+	"strconv"
+	"strings"
+	"time"
+
+	"github.com/jackc/pgx/v5/pgtype"
+	"github.com/twmb/franz-go/pkg/kgo"
+	"github.com/twmb/franz-go/pkg/kmsg"
+	"go.uber.org/zap"
+	"go.uber.org/zap/zapcore"
+	corev1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	k8sLabels "k8s.io/apimachinery/pkg/labels"
+	"k8s.io/apimachinery/pkg/runtime"
+	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
+	"k8s.io/client-go/kubernetes"
+	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
+	"k8s.io/client-go/rest"
+	"k8s.io/client-go/tools/remotecommand"
+	crclient "sigs.k8s.io/controller-runtime/pkg/client"
+	"sigs.k8s.io/controller-runtime/pkg/client/config"
+)
+
+var Log = GetLogger()
+
+var (
+	tmpDir       = "/tmp"
+	debugEnabled = GetEnv("DEBUG_ENABLED", "false")
+)
+
+const certificatesFolder = "/certs"
+
+func GetLogger() *zap.Logger {
+	cfg := zap.NewProductionConfig()
+	cfg.Encoding = "console"
+	cfg.EncoderConfig.TimeKey = "timestamp"
+	cfg.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
+	cfg.EncoderConfig.CallerKey = ""
+	cfg.EncoderConfig.EncodeLevel = CustomLevelEncoder
+	if debugEnabled == "true" {
+		cfg.Level = zap.NewAtomicLevelAt(zap.DebugLevel)
+	}
+	cfg.OutputPaths = []string{
+		"/proc/1/fd/1",
+	}
+	logger, err := cfg.Build()
+	if err != nil {
+		log.Printf("Cannot create logger: %s", err.Error())
+	}
+
+	defer func() {
+		_ = logger.Sync()
+	}()
+	return logger
+}
+
+func GetProtocol() (string, string) {
+	if GetEnv("TLS", "false") == "true" {
+		return "https://", "8530"
+	}
+	return "http://", "8529"
+
+}
+
+func GetToken() string {
+	tokenByte, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
+	if err != nil {
+		log.Fatal(err)
+	}
+	token := string(tokenByte[:])
+	return token
+}
+
+func GetPodInfo(client *http.Client, token, remoteHost, remotePort, project, selector, selectorValue string) map[string]interface{} {
+
+	startTime := time.Now()
+	hostUrl := GetHostUrl(remoteHost, remotePort)
+	podsDataUrl := fmt.Sprintf("%s/api/v1/namespaces/%s/pods?labelSelector=%s=%s", hostUrl, project, selector, selectorValue)
+	Log.Debug(fmt.Sprintf("ProcessHttpRequest: client: %v, podsDataUrl: %s: token: %s", *client, podsDataUrl, token))
+	status, response, err := ProcessHttpRequest(client, podsDataUrl, token)
+	Log.Debug(fmt.Sprintf("Load pods info time for %s=%s: %v", selector, selectorValue, time.Since(startTime)))
+	statusCode, _ := strconv.Atoi(status)
+	if err != nil {
+		Log.Error(fmt.Sprintf("Load pods info time for %s=%s: Error: %s", selector, selectorValue, err))
+	}
+	if statusCode >= 400 {
+		panic(fmt.Sprintf("Cannot collect pods data with code %v", statusCode))
+	}
+	var res map[string]interface{}
+	err = json.Unmarshal(response, &res)
+	if err != nil {
+		Log.Error(fmt.Sprintf("Load pods info Unmarshal Error: %s", err))
+		return res
+	}
+	Log.Debug(fmt.Sprintf("Collected info about %s=%s pods: %s", selector, selectorValue, res))
+	return res
+
+}
+
+func GetLeaderPod(client *http.Client, token, clusterName string) map[string]interface{} {
+
+	url := fmt.Sprintf("http://pg-%s-api:8008/cluster", clusterName)
+
+	status, response, err := ProcessHttpRequest(client, url, token)
+	if err != nil {
+		Log.Warn(fmt.Sprintf("Error, while http request to find leader patroni pod: %s", err))
+	}
+	statusCode, _ := strconv.Atoi(status)
+	if statusCode >= 400 {
+		panic(fmt.Sprintf("Cannot collect pods data with code %v", statusCode))
+	}
+	var res map[string]interface{}
+	err = json.Unmarshal(response, &res)
+	if err != nil {
+		//TODO: log here
+		return res
+	}
+	members := SafeGet(res, append(make([]interface{}, 0), "members"), make(map[string]interface{}, 0))
+	for _, member := range members.([]interface{}) {
+		role := SafeGet(member, append(make([]interface{}, 0), "role"), "").(string)
+		if role == "leader" || role == "standby_leader" {
+			return member.(map[string]interface{})
+		}
+	}
+	Log.Warn("Can not find leader of patroni cluster")
+	return res
+}
+
+func ProcessHttpRequest(client *http.Client, url string, token string) (string, []byte, error) {
+	req, err := http.NewRequest("GET", url, nil)
+	if err != nil {
+		return "", nil, err
+	}
+	var bearer = "bearer " + token
+	req.Header.Set("Authorization", bearer)
+	resp, err := client.Do(req)
+	if err != nil {
+		return "", nil, err
+	}
+	defer resp.Body.Close()
+	body, err := io.ReadAll(resp.Body)
+	if err != nil {
+		return "", nil, err
+	}
+	return resp.Status, body, nil
+}
+
+func GetHostUrl(remoteHost, remotePort string) string {
+	//if isIpv4(remoteHost) {
+	return fmt.Sprintf("https://%s:%s", remoteHost, remotePort)
+	//} else {
+	//	return fmt.Sprintf("https://[%s]:%s", remoteHost, remotePort)
+	//}
+}
+
+func GetServiceList(client *kubernetes.Clientset, namespace string, selector string) (*corev1.ServiceList, error) {
+	opts := metav1.ListOptions{
+		LabelSelector: selector,
+	}
+	serviceList, err := client.CoreV1().Services(namespace).List(context.TODO(), opts)
+	if len(serviceList.Items) == 0 {
+		Log.Info(fmt.Sprintf("Service with label %s not found", selector))
+		return nil, err
+	} else if err != nil {
+		return nil, err
+	}
+	Log.Info(fmt.Sprintf("Found Service with label %s", selector))
+	return serviceList, nil
+}
+
+func GetPodList(client *kubernetes.Clientset, namespace string, selector map[string]string) (*corev1.PodList, error) {
+	opts := metav1.ListOptions{
+		LabelSelector: k8sLabels.SelectorFromSet(selector).String(),
+	}
+	podList, err := client.CoreV1().Pods(namespace).List(context.TODO(), opts)
+	if len(podList.Items) == 0 {
+		Log.Debug(fmt.Sprintf("Pod with label %s not found", selector))
+		return nil, err
+	} else if err != nil {
+		return nil, err
+	}
+	Log.Debug(fmt.Sprintf("Found Pod with label %s", selector))
+	return podList, nil
+}
+
+func GetKafkaMetadata(ctx context.Context, client *kgo.Client) (*kmsg.MetadataResponse, error) {
+	req := kmsg.NewMetadataRequest()
+	req.Topics = nil
+
+	var err error
+	var metadata *kmsg.MetadataResponse
+	if metadata, err = req.RequestWith(ctx, client); err == nil {
+		return metadata, nil
+	}
+	return nil, err
+
+}
+
+func GetHttpClient() *http.Client {
+	var client *http.Client
+	var tlsClientConfig *tls.Config
+	if GetEnv("TLS", "false") == "true" {
+		certsDir, err := os.ReadDir(certificatesFolder)
+		infos := make([]fs.FileInfo, 0, len(certsDir))
+		for _, entry := range certsDir {
+			info, err := entry.Info()
+			if err != nil {
+				log.Printf("Cannot certificate from file '%s'. Maybe deleted or moved.", entry.Name())
+			}
+			infos = append(infos, info)
+		}
+		if err != nil || len(infos) == 0 {
+			log.Printf("Cannot load TLS certificates from path '%s'. InsecureSkipVerify is used.", certificatesFolder)
+
+		} else {
+			certs := x509.NewCertPool()
+			for _, cert := range infos {
+				if isNotDir(cert) {
+					pemData, err := os.ReadFile(fmt.Sprintf("%s/%s", certificatesFolder, cert.Name()))
+					if err != nil {
+						log.Panicf(fmt.Sprintf("Failed to read certificate '%s'", cert.Name()), zap.Error(err))
+					}
+					certs.AppendCertsFromPEM(pemData)
+					log.Printf("Trusted certificate '%s' was added to client", cert.Name())
+				}
+			}
+			tlsClientConfig = &tls.Config{RootCAs: certs}
+			transport := &http.Transport{TLSClientConfig: tlsClientConfig}
+			client = &http.Client{Transport: transport, Timeout: 10 * time.Second}
+			return client
+		}
+	}
+	client = &http.Client{Timeout: 10 * time.Second}
+	return client
+}
+
+func isNotDir(info os.FileInfo) bool {
+	return !info.IsDir() && !strings.HasPrefix(info.Name(), "..")
+}
+
+func GetEnv(key, fallback string) string {
+	if value, ok := os.LookupEnv(key); ok {
+		return value
+	}
+	return fallback
+}
+
+func GetEnvInt(key string, fallback int) int {
+	if value, ok := os.LookupEnv(key); ok {
+		if ivalue, err := strconv.Atoi(value); err == nil {
+			return ivalue
+		}
+	}
+	return fallback
+}
+
+func GetEnvBool(key string, fallback bool) bool {
+	if value, ok := os.LookupEnv(key); ok {
+		bvalue, err := strconv.ParseBool(value)
+		if err != nil {
+			Log.Error(fmt.Sprintf("Can't parse %s boolean variable", key), zap.Error(err))
+		}
+		return bvalue
+	}
+	return fallback
+}
+
+func DetermineRole(pod corev1.Pod) string {
+	podLabels := pod.Labels
+	pgType, ok := podLabels["pgtype"]
+	if ok {
+		return pgType
+	}
+	return "replica"
+
+}
+
+func SafeGet(data interface{}, path []interface{}, defaultValue interface{}) interface{} {
+	var result interface{}
+	if data == nil {
+		result = defaultValue
+	}
+	for _, pathElement := range path {
+		switch v := data.(type) {
+		case map[string]interface{}:
+			if path, ok := pathElement.(string); ok {
+				if cur, ok := v[path]; ok {
+					result = cur
+				} else {
+					result = defaultValue
+				}
+			} else {
+				Log.Debug(fmt.Sprintf("SafeGet: type is map[string]interface{}. Not Valid Path element Data: %s, PathElement: %s", v, pathElement))
+				result = defaultValue
+			}
+		case []interface{}:
+			if path, ok := pathElement.(int); ok {
+				if len(v) > path {
+					result = v[path]
+				} else {
+					result = defaultValue
+				}
+			} else {
+				result = defaultValue
+			}
+		case string:
+			var res map[string]interface{}
+			err := json.Unmarshal([]byte(v), &res)
+			if err != nil {
+				Log.Error(fmt.Sprintf("SafeGet: Can not convert current string element to json: %s", v))
+				return v
+			}
+			return SafeGet(res, append(make([]interface{}, 0), pathElement), "")
+		default:
+			result = data
+		}
+	}
+	return result
+}
+
+func Contains(slice []string, value string) bool {
+	for _, v := range slice {
+		if v == value {
+			return true
+		}
+	}
+	return false
+}
+
+func GetLatestMasterAppearance() (string, string) {
+
+	lma := GetPodValue("master", "last_master_appearance", "")
+	lmxl := GetPodValue("master", "last_master_xlog_location", "0")
+	return lma, lmxl
+}
+
+func StoreLastMasterAppearance(xlogLocation float64) {
+
+	StorePodValue("master", "last_master_appearance", strconv.Itoa(time.Now().Second()))
+
+	exValue := GetPodValue("master", "last_master_xlog_location", "0")
+	val, err := strconv.ParseFloat(exValue, 64)
+	if err != nil {
+		Log.Error("Error while storing master appearance", zap.Error(err))
+	}
+	res := math.Max(val, xlogLocation)
+
+	StorePodValue("master", "last_master_xlog_location", fmt.Sprintf("%f", res))
+}
+
+func StorePodValue(pod, key, value string) {
+
+	fName := fmt.Sprintf("%s/%s.%s.tmp", tmpDir, pod, key)
+
+	Log.Debug(fmt.Sprintf("Store pod key: %s, value: %s to file: %s", key, value, fName))
+
+	mydata := []byte(value)
+	if err := os.WriteFile(fName, mydata, 0777); err != nil {
+		Log.Error(fmt.Sprintf("Error while saving pod value: %s", value), zap.Error(err))
+	}
+
+}
+func GetPodValue(pod, key, defaultValue string) string {
+
+	fName := fmt.Sprintf("%s/%s.%s.tmp", tmpDir, pod, key)
+
+	Log.Debug(fmt.Sprintf("Get pod key: %s from file: %s", key, fName))
+
+	data, err := os.ReadFile(fName)
+	if err != nil {
+		Log.Warn(fmt.Sprintf("Error while reading pod value of %s", key))
+		Log.Debug(fmt.Sprintf("Error while reading pod value of %s", key), zap.Error(err))
+		return defaultValue
+	}
+	return string(data)
+}
+
+func GetPodStatus(pod corev1.Pod) string {
+	state := ""
+	status := SafeGet(pod.Annotations["status"], append(make([]interface{}, 0), "state"), "")
+	if status == "" {
+		status = pod.Status.ContainerStatuses[0].State
+		Log.Debug(fmt.Sprintf("pod status: %s", status))
+		for _, st := range status.([]string) {
+			state = st
+		}
+	} else {
+		Log.Debug(fmt.Sprintf("pod status: %s", status))
+		state = fmt.Sprintf("%s", status)
+	}
+	return state
+}
+
+func CustomLevelEncoder(level zapcore.Level, enc zapcore.PrimitiveArrayEncoder) {
+	enc.AppendString("[" + level.CapitalString() + "]")
+}
+
+func CreateClient() (crclient.Client, error) {
+	clientConfig, err := config.GetConfig()
+	if err != nil {
+		return nil, err
+	}
+	scheme := runtime.NewScheme()
+	utilruntime.Must(clientgoscheme.AddToScheme(scheme))
+
+	client, err := crclient.New(clientConfig, crclient.Options{Scheme: scheme})
+	if err != nil {
+		return nil, err
+	}
+	return client, nil
+}
+
+func GetFloatValue(value any, defaultValue float64) (float64, error) {
+	var result float64
+	switch v := value.(type) {
+	default:
+		return defaultValue, fmt.Errorf("unexpected type %T. Can not parse value", v)
+	case int:
+		result = float64(v)
+	case int32:
+		result = float64(v)
+	case int64:
+		result = float64(v)
+	case float64:
+		result = v
+	case string:
+		if v == "" || strings.EqualFold(v, "None") {
+			return defaultValue, errors.New("unexpected empty value")
+		}
+		if strings.EqualFold(v, "true") {
+			result = float64(1)
+		} else if strings.EqualFold(v, "false") {
+			result = float64(0)
+		} else {
+			if val, err := strconv.ParseFloat(v, 64); err != nil {
+				return defaultValue, fmt.Errorf("can't parse value '%v'", v)
+			} else {
+				result = val
+			}
+		}
+	case bool:
+		if v {
+			result = float64(1)
+		} else {
+			result = float64(0)
+		}
+	case pgtype.Numeric:
+		if v.Valid {
+			result, _ = new(big.Float).SetInt(v.Int).Float64()
+		} else {
+			result = float64(0)
+		}
+	}
+	return result, nil
+}
+
+func ExecCmdOnPod(client *kubernetes.Clientset, podName string, namespace string, command string, container string) (string, string, error) {
+
+	Log.Debug(fmt.Sprintf("Executing shell command: %s  on pod %s", command, podName))
+	config, err := rest.InClusterConfig()
+	if err != nil {
+		panic(err.Error())
+	}
+
+	buf := &bytes.Buffer{}
+	errBuf := &bytes.Buffer{}
+	request := client.CoreV1().RESTClient().
+		Post().
+		Namespace(namespace).
+		Resource("pods").
+		Name(podName).
+		SubResource("exec").
+		VersionedParams(&corev1.PodExecOptions{
+			Command:   []string{"/bin/sh", "-c", command},
+			Container: container,
+			Stdin:     false,
+			Stdout:    true,
+			Stderr:    true,
+			TTY:       true,
+		}, clientgoscheme.ParameterCodec)
+	exec, err := remotecommand.NewSPDYExecutor(config, "POST", request.URL())
+	if err != nil {
+		Log.Error(fmt.Sprintf("Executing shell command: Error: \n%v\nerrBuf: %v", err, errBuf))
+		return "", "", fmt.Errorf("%w Failed executing command %s on %v/%v", err, command, namespace, podName)
+	}
+	err = exec.StreamWithContext(context.TODO(), remotecommand.StreamOptions{
+		Stdout: buf,
+		Stderr: errBuf,
+	})
+	if err != nil {
+		Log.Error(fmt.Sprintf("Executing shell command: Error: \n%v\nerrBuf: %v", err, errBuf))
+		return "", "", fmt.Errorf("%w Failed executing command %s on %v/%v", err, command, namespace, podName)
+	}
+	return buf.String(), errBuf.String(), nil
+}
+
+func GetPodIdentity(pod corev1.Pod) string {
+	podIdentity := GetEnvValueFromPod(pod, "POD_IDENTITY", "node")
+
+	podId := pod.Name
+	podIp := pod.Status.PodIP
+	if podIdentity == "node" {
+		if podIdentity[0:10] == "postgresql" {
+			podIdentity = "node1"
+		} else {
+			num, _ := strconv.Atoi(podId[len(podIp)-1:])
+			podIdentity = "node" + strconv.Itoa(num+1)
+		}
+	}
+	return podIdentity
+}
+
+func GetEnvValueFromPod(pod corev1.Pod, envName, defaultValue string) string {
+
+	envs := pod.Spec.Containers[0].Env
+	for _, env := range envs {
+		if env.Name == envName {
+			return env.Value
+		}
+	}
+	return defaultValue
+}
+
+func IsSiteManagerEnabled() bool {
+	return GetEnv("SITE_MANAGER", "off") == "on"
+}
diff --git a/docker-monitoring-agent/go.mod b/docker-monitoring-agent/go.mod
new file mode 100644
index 0000000..e9321cc
--- /dev/null
+++ b/docker-monitoring-agent/go.mod
@@ -0,0 +1,70 @@
+module github.com/Netcracker/pgskipper-monitoring-agent
+
+go 1.25.3
+
+require (
+	github.com/google/uuid v1.6.0
+	github.com/jackc/pgx/v5 v5.6.0
+	github.com/prometheus/client_model v0.6.1
+	github.com/prometheus/common v0.55.0
+	github.com/twmb/franz-go v1.17.1
+	github.com/twmb/franz-go/pkg/kmsg v1.8.0
+	go.uber.org/zap v1.27.0
+	k8s.io/api v0.30.3
+	k8s.io/apimachinery v0.30.3
+	k8s.io/client-go v0.30.3
+	k8s.io/utils v0.0.0-20240711033017-18e509b52bc8
+	sigs.k8s.io/controller-runtime v0.18.4
+)
+
+require (
+	github.com/gorilla/websocket v1.5.3 // indirect
+	github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f // indirect
+)
+
+require (
+	github.com/davecgh/go-spew v1.1.1 // indirect
+	github.com/emicklei/go-restful/v3 v3.12.1 // indirect
+	github.com/evanphx/json-patch/v5 v5.9.0 // indirect
+	github.com/go-logr/logr v1.4.2 // indirect; indirectprometheus
+	github.com/go-openapi/jsonpointer v0.21.0 // indirect
+	github.com/go-openapi/jsonreference v0.21.0 // indirect
+	github.com/go-openapi/swag v0.23.0 // indirect
+	github.com/gogo/protobuf v1.3.2 // indirect
+	github.com/golang/protobuf v1.5.4 // indirect
+	github.com/google/gnostic-models v0.6.8 // indirect
+	github.com/google/gofuzz v1.2.0 // indirect
+	github.com/imdario/mergo v0.3.16 // indirect
+	github.com/jackc/pgpassfile v1.0.0 // indirect
+	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
+	github.com/jackc/puddle/v2 v2.2.1 // indirect
+	github.com/josharian/intern v1.0.0 // indirect
+	github.com/json-iterator/go v1.1.12 // indirect
+	github.com/klauspost/compress v1.17.9 // indirect
+	github.com/mailru/easyjson v0.7.7 // indirect
+	github.com/moby/spdystream v0.5.0 // indirect
+	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
+	github.com/modern-go/reflect2 v1.0.2 // indirect
+	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
+	github.com/pierrec/lz4/v4 v4.1.21 // indirect
+	github.com/pkg/errors v0.9.1 // indirect
+	github.com/spf13/pflag v1.0.5 // indirect
+	go.uber.org/multierr v1.11.0 // indirect
+	golang.org/x/crypto v0.43.0 // indirect
+	golang.org/x/net v0.45.0 // indirect
+	golang.org/x/oauth2 v0.27.0 // indirect
+	golang.org/x/sync v0.17.0 // indirect
+	golang.org/x/sys v0.37.0 // indirect
+	golang.org/x/term v0.36.0 // indirect
+	golang.org/x/text v0.30.0 // indirect
+	golang.org/x/time v0.6.0 // indirect
+	google.golang.org/protobuf v1.34.2 // indirect
+	gopkg.in/inf.v0 v0.9.1 // indirect
+	gopkg.in/yaml.v2 v2.4.0 // indirect
+	gopkg.in/yaml.v3 v3.0.1 // indirect
+	k8s.io/klog/v2 v2.130.1 // indirect
+	k8s.io/kube-openapi v0.0.0-20240730131305-7a9a4e85957e // indirect
+	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
+	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
+	sigs.k8s.io/yaml v1.4.0 // indirect
+)
diff --git a/docker-monitoring-agent/go.sum b/docker-monitoring-agent/go.sum
new file mode 100644
index 0000000..532a01a
--- /dev/null
+++ b/docker-monitoring-agent/go.sum
@@ -0,0 +1,192 @@
+github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5 h1:0CwZNZbxp69SHPdPJAN/hZIm0C4OItdklCFmMRWYpio=
+github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5/go.mod h1:wHh0iHkYZB8zMSxRWpUBQtwG5a7fFgvEO+odwuTv2gs=
+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/emicklei/go-restful/v3 v3.12.1 h1:PJMDIM/ak7btuL8Ex0iYET9hxM3CI2sjZtzpL63nKAU=
+github.com/emicklei/go-restful/v3 v3.12.1/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
+github.com/evanphx/json-patch v4.12.0+incompatible h1:4onqiflcdA9EOZ4RxV643DvftH5pOlLGNtQ5lPWQu84=
+github.com/evanphx/json-patch v4.12.0+incompatible/go.mod h1:50XU6AFN0ol/bzJsmQLiYLvXMP4fmwYFNcr97nuDLSk=
+github.com/evanphx/json-patch/v5 v5.9.0 h1:kcBlZQbplgElYIlo/n1hJbls2z/1awpXxpRi0/FOJfg=
+github.com/evanphx/json-patch/v5 v5.9.0/go.mod h1:VNkHZ/282BpEyt/tObQO8s5CMPmYYq14uClGH4abBuQ=
+github.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=
+github.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
+github.com/go-logr/zapr v1.3.0 h1:XGdV8XW8zdwFiwOA2Dryh1gj2KRQyOOoNmBy4EplIcQ=
+github.com/go-logr/zapr v1.3.0/go.mod h1:YKepepNBd1u/oyhd/yQmtjVXmm9uML4IXUgMOwR8/Gg=
+github.com/go-openapi/jsonpointer v0.21.0 h1:YgdVicSA9vH5RiHs9TZW5oyafXZFc6+2Vc1rr/O9oNQ=
+github.com/go-openapi/jsonpointer v0.21.0/go.mod h1:IUyH9l/+uyhIYQ/PXVA41Rexl+kOkAPDdXEYns6fzUY=
+github.com/go-openapi/jsonreference v0.21.0 h1:Rs+Y7hSXT83Jacb7kFyjn4ijOuVGSvOdF2+tg1TRrwQ=
+github.com/go-openapi/jsonreference v0.21.0/go.mod h1:LmZmgsrTkVg9LG4EaHeY8cBDslNPMo06cago5JNLkm4=
+github.com/go-openapi/swag v0.23.0 h1:vsEVJDUo2hPJ2tu0/Xc+4noaxyEffXNIs3cOULZ+GrE=
+github.com/go-openapi/swag v0.23.0/go.mod h1:esZ8ITTYEsH1V2trKHjAN8Ai7xHb8RV+YSZ577vPjgQ=
+github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 h1:tfuBGBXKqDEevZMzYi5KSi8KkcZtzBcTgAUUtapy0OI=
+github.com/go-task/slim-sprig/v3 v3.0.0 h1:sUs3vkvUymDpBKi3qH1YSqBQk9+9D/8M2mN1vB6EwHI=
+github.com/go-task/slim-sprig/v3 v3.0.0/go.mod h1:W848ghGpv3Qj3dhTPRyJypKRiqCdHZiAzKg9hl15HA8=
+github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
+github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
+github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
+github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
+github.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=
+github.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=
+github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
+github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
+github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
+github.com/google/pprof v0.0.0-20240424215950-a892ee059fd6 h1:k7nVchz72niMH6YLQNvHSdIE7iqsQxK1P41mySCvssg=
+github.com/google/pprof v0.0.0-20240424215950-a892ee059fd6/go.mod h1:kf6iHlnVGwgKolg33glAes7Yg/8iWP8ukqeldJSO7jw=
+github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
+github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/gorilla/websocket v1.5.3 h1:saDtZ6Pbx/0u+bgYQ3q96pZgCzfhKXGPqt7kZ72aNNg=
+github.com/gorilla/websocket v1.5.3/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
+github.com/imdario/mergo v0.3.16 h1:wwQJbIsHYGMUyLSPrEq1CT16AhnhNJQ51+4fdHUnCl4=
+github.com/imdario/mergo v0.3.16/go.mod h1:WBLT9ZmE3lPoWsEzCh9LPo3TiwVN+ZKEjmz+hD27ysY=
+github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
+github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
+github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
+github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
+github.com/jackc/pgx/v5 v5.6.0 h1:SWJzexBzPL5jb0GEsrPMLIsi/3jOo7RHlzTjcAeDrPY=
+github.com/jackc/pgx/v5 v5.6.0/go.mod h1:DNZ/vlrUnhWCoFGxHAG8U2ljioxukquj7utPDgtQdTw=
+github.com/jackc/puddle/v2 v2.2.1 h1:RhxXJtFG022u4ibrCSMSiu5aOq1i77R3OHKNJj77OAk=
+github.com/jackc/puddle/v2 v2.2.1/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
+github.com/josharian/intern v1.0.0 h1:vlS4z54oSdjm0bgjRigI+G1HpF+tI+9rE5LLzOg8HmY=
+github.com/josharian/intern v1.0.0/go.mod h1:5DoeVV0s6jJacbCEi61lwdGj/aVlrQvzHFFd8Hwg//Y=
+github.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=
+github.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=
+github.com/kisielk/errcheck v1.5.0/go.mod h1:pFxgyoBC7bSaBwPgfKdkLd5X25qrDl4LWUI2bnpBCr8=
+github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
+github.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=
+github.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=
+github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
+github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
+github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
+github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
+github.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=
+github.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
+github.com/moby/spdystream v0.5.0 h1:7r0J1Si3QO/kjRitvSLVVFUjxMEb/YLj6S9FF62JBCU=
+github.com/moby/spdystream v0.5.0/go.mod h1:xBAYlnt/ay+11ShkdFKNAG7LsyK/tmNBVvVOwrfMgdI=
+github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=
+github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
+github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=
+github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
+github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f h1:y5//uYreIhSUg3J1GEMiLbxo1LJaP8RfCpH6pymGZus=
+github.com/mxk/go-flowrate v0.0.0-20140419014527-cca7078d478f/go.mod h1:ZdcZmHo+o7JKHSa8/e818NopupXU1YMK5fe1lsApnBw=
+github.com/onsi/ginkgo/v2 v2.19.0 h1:9Cnnf7UHo57Hy3k6/m5k3dRfGTMXGvxhHFvkDTCTpvA=
+github.com/onsi/ginkgo/v2 v2.19.0/go.mod h1:rlwLi9PilAFJ8jCg9UE1QP6VBpd6/xj3SRC0d6TU0To=
+github.com/onsi/gomega v1.33.1 h1:dsYjIxxSR755MDmKVsaFQTE22ChNBcuuTWgkUDSubOk=
+github.com/onsi/gomega v1.33.1/go.mod h1:U4R44UsT+9eLIaYRB2a5qajjtQYn0hauxvRm16AVYg0=
+github.com/pierrec/lz4/v4 v4.1.21 h1:yOVMLb6qSIDP67pl/5F7RepeKYu/VmTyEXvuMI5d9mQ=
+github.com/pierrec/lz4/v4 v4.1.21/go.mod h1:gZWDp/Ze/IJXGXf23ltt2EXimqmTUXEy0GFuRQyBid4=
+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/prometheus/client_model v0.6.1 h1:ZKSh/rekM+n3CeS952MLRAdFwIKqeY8b62p8ais2e9E=
+github.com/prometheus/client_model v0.6.1/go.mod h1:OrxVMOVHjw3lKMa8+x6HeMGkHMQyHDk9E3jmP2AmGiY=
+github.com/prometheus/common v0.55.0 h1:KEi6DK7lXW/m7Ig5i47x0vRzuBsHuvJdi5ee6Y3G1dc=
+github.com/prometheus/common v0.55.0/go.mod h1:2SECS4xJG1kd8XF9IcM1gMX6510RAEL65zxzNImwdc8=
+github.com/rogpeppe/go-internal v1.11.0 h1:cWPaGQEPrBb5/AsnsZesgZZ9yb1OQ+GOISoDNXVBh4M=
+github.com/rogpeppe/go-internal v1.11.0/go.mod h1:ddIwULY96R17DhadqLgMfk9H9tvdUzkipdSkR5nkCZA=
+github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
+github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
+github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=
+github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
+github.com/twmb/franz-go v1.17.1 h1:0LwPsbbJeJ9R91DPUHSEd4su82WJWcTY1Zzbgbg4CeQ=
+github.com/twmb/franz-go v1.17.1/go.mod h1:NreRdJ2F7dziDY/m6VyspWd6sNxHKXdMZI42UfQ3GXM=
+github.com/twmb/franz-go/pkg/kmsg v1.8.0 h1:lAQB9Z3aMrIP9qF9288XcFf/ccaSxEitNA1CDTEIeTA=
+github.com/twmb/franz-go/pkg/kmsg v1.8.0/go.mod h1:HzYEb8G3uu5XevZbtU0dVbkphaKTHk0X68N5ka4q6mU=
+github.com/yuin/goldmark v1.1.27/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+github.com/yuin/goldmark v1.2.1/go.mod h1:3hX8gzYuyVAZsxl0MRgGTJEmQBFcNTphYh9decYSb74=
+go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
+go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
+go.uber.org/multierr v1.11.0 h1:blXXJkSxSSfBVBlC76pxqeO+LN3aDfLQo+309xJstO0=
+go.uber.org/multierr v1.11.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
+go.uber.org/zap v1.27.0 h1:aJMhYGrd5QSmlpLMr2MftRKl7t8J8PTZPA732ud/XR8=
+go.uber.org/zap v1.27.0/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
+golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.43.0 h1:dduJYIi3A3KOfdGOHX8AVZ/jGiyPa3IbBozJ5kNuE04=
+golang.org/x/crypto v0.43.0/go.mod h1:BFbav4mRNlXJL4wNeejLpWxB7wMbc79PdRGhWKncxR0=
+golang.org/x/exp v0.0.0-20220722155223-a9213eeb770e h1:+WEEuIdZHnUeJJmEUjyYC2gfUMj69yZXw17EnHg/otA=
+golang.org/x/exp v0.0.0-20220722155223-a9213eeb770e/go.mod h1:Kr81I6Kryrl9sr8s2FK3vxD90NdsKWRuOIl2O4CvYbA=
+golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
+golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
+golang.org/x/net v0.45.0 h1:RLBg5JKixCy82FtLJpeNlVM0nrSqpCRYzVU1n8kj0tM=
+golang.org/x/net v0.45.0/go.mod h1:ECOoLqd5U3Lhyeyo/QDCEVQ4sNgYsqvCZ722XogGieY=
+golang.org/x/oauth2 v0.27.0 h1:da9Vo7/tDv5RH/7nZDz1eMGS/q1Vv1N/7FCrBhI9I3M=
+golang.org/x/oauth2 v0.27.0/go.mod h1:onh5ek6nERTohokkhCD/y2cV4Do3fxFHFuAejCkRWT8=
+golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.17.0 h1:l60nONMj9l5drqw6jlhIELNv9I0A4OFgRsG9k2oT9Ug=
+golang.org/x/sync v0.17.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.37.0 h1:fdNQudmxPjkdUTPnLn5mdQv7Zwvbvpaxqs831goi9kQ=
+golang.org/x/sys v0.37.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
+golang.org/x/term v0.36.0 h1:zMPR+aF8gfksFprF/Nc/rd1wRS1EI6nDBGyWAvDzx2Q=
+golang.org/x/term v0.36.0/go.mod h1:Qu394IJq6V6dCBRgwqshf3mPF85AqzYEzofzRdZkWss=
+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.30.0 h1:yznKA/E9zq54KzlzBEAWn1NXSQ8DIp/NYMy88xJjl4k=
+golang.org/x/text v0.30.0/go.mod h1:yDdHFIX9t+tORqspjENWgzaCVXgk0yYnYuSZ8UzzBVM=
+golang.org/x/time v0.6.0 h1:eTDhh4ZXt5Qf0augr54TN6suAUudPcawVZeIAPU7D4U=
+golang.org/x/time v0.6.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=
+golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
+golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
+golang.org/x/tools v0.37.0 h1:DVSRzp7FwePZW356yEAChSdNcQo6Nsp+fex1SUW09lE=
+golang.org/x/tools v0.37.0/go.mod h1:MBN5QPQtLMHVdvsbtarmTNukZDdgwdwlO5qGacAzF0w=
+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+google.golang.org/protobuf v1.34.2 h1:6xV6lTsCfpGD21XK49h7MhtcApnLqkfYgPcdHftf6hg=
+google.golang.org/protobuf v1.34.2/go.mod h1:qYOHts0dSfpeUzUFpOMr/WGzszTmLH+DiWniOlNbLDw=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
+gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
+gopkg.in/yaml.v2 v2.2.8/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
+gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
+gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+k8s.io/api v0.30.3 h1:ImHwK9DCsPA9uoU3rVh4QHAHHK5dTSv1nxJUapx8hoQ=
+k8s.io/api v0.30.3/go.mod h1:GPc8jlzoe5JG3pb0KJCSLX5oAFIW3/qNJITlDj8BH04=
+k8s.io/apiextensions-apiserver v0.30.1 h1:4fAJZ9985BmpJG6PkoxVRpXv9vmPUOVzl614xarePws=
+k8s.io/apiextensions-apiserver v0.30.1/go.mod h1:R4GuSrlhgq43oRY9sF2IToFh7PVlF1JjfWdoG3pixk4=
+k8s.io/apimachinery v0.30.3 h1:q1laaWCmrszyQuSQCfNB8cFgCuDAoPszKY4ucAjDwHc=
+k8s.io/apimachinery v0.30.3/go.mod h1:iexa2somDaxdnj7bha06bhb43Zpa6eWH8N8dbqVjTUc=
+k8s.io/client-go v0.30.3 h1:bHrJu3xQZNXIi8/MoxYtZBBWQQXwy16zqJwloXXfD3k=
+k8s.io/client-go v0.30.3/go.mod h1:8d4pf8vYu665/kUbsxWAQ/JDBNWqfFeZnvFiVdmx89U=
+k8s.io/klog/v2 v2.130.1 h1:n9Xl7H1Xvksem4KFG4PYbdQCQxqc/tTUyrgXaOhHSzk=
+k8s.io/klog/v2 v2.130.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=
+k8s.io/kube-openapi v0.0.0-20240730131305-7a9a4e85957e h1:OnKkExfhk4yxMqvBSPzUfhv3zQ96FWJ+UOZzLrAFyAo=
+k8s.io/kube-openapi v0.0.0-20240730131305-7a9a4e85957e/go.mod h1:0CVn9SVo8PeW5/JgsBZZIFmmTk5noOM8WXf2e1tCihE=
+k8s.io/utils v0.0.0-20240711033017-18e509b52bc8 h1:pUdcCO1Lk/tbT5ztQWOBi5HBgbBP1J8+AsQnQCKsi8A=
+k8s.io/utils v0.0.0-20240711033017-18e509b52bc8/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
+sigs.k8s.io/controller-runtime v0.18.4 h1:87+guW1zhvuPLh1PHybKdYFLU0YJp4FhJRmiHvm5BZw=
+sigs.k8s.io/controller-runtime v0.18.4/go.mod h1:TVoGrfdpbA9VRFaRnKgk9P5/atA0pMwq+f+msb9M8Sg=
+sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=
+sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=
+sigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=
+sigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=
+sigs.k8s.io/yaml v1.4.0 h1:Mk1wCc2gy/F0THH0TAp1QYyJNzRm2KCLy3o5ASXVI5E=
+sigs.k8s.io/yaml v1.4.0/go.mod h1:Ejl7/uTz7PSA4eKMyQCUTnhZYNmLIl+5c2lQPGR2BPY=
diff --git a/docker-monitoring-agent/scripts/cloud_sql_metric_utils.py b/docker-monitoring-agent/scripts/cloud_sql_metric_utils.py
new file mode 100644
index 0000000..5b5d678
--- /dev/null
+++ b/docker-monitoring-agent/scripts/cloud_sql_metric_utils.py
@@ -0,0 +1,136 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from google.cloud import monitoring_v3
+from googleapiclient import discovery
+import time
+import logging
+import os
+
+SA_PATH = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+
+
+class CloudSqlMetricApiCaller:
+    def __init__(self):
+        self.client = monitoring_v3.MetricServiceClient()
+        self.log = logging.getLogger('MetricRequestEndpoint')
+        self.project = os.getenv("EXT_DB_PROJECT_NAME")
+        self.instance = self.get_cloud_sql_instance()
+        self.metric_dict = {
+            "df.pcent": "database/disk/utilization",
+            "du.base": "database/disk/bytes_used",
+            "cpu.pcent": "database/cpu/utilization",
+            "cpu.reserved_cores": "database/cpu/reserved_cores",
+            "memory.usage": "database/memory/usage",
+            "memory.quota": "database/memory/quota",
+            "current_connections": "database/postgresql/num_backends",
+            "running": "database/up"
+        }
+
+    def get_cloud_sql_instance(self):
+        import googleapiclient
+        instance_from_env = os.getenv("EXT_DB_INSTANCE_NAME")
+        service = discovery.build('sqladmin', 'v1beta4')
+        request = service.instances().get(project=self.project, instance=instance_from_env)
+        instance_exists = True
+        try:
+            request.execute()
+        except googleapiclient.errors.HttpError as err:
+            if err.resp.status == 404:
+                self.log.warning(f"Requested resource {instance_from_env} not found\n"
+                                 f"Will try to find instance with namespace label")
+                instance_exists = False
+        if instance_exists:
+            return instance_from_env
+        else:
+            request = service.instances().list(project=self.project)
+            response = request.execute()
+            cur_region = self.get_current_region()
+            for database_instance in response.get('items', []):
+                region = database_instance["region"]
+                labels = database_instance["settings"]["userLabels"]
+                if labels.get("namespace", "") == os.environ['NAMESPACE'] \
+                        and region == cur_region:
+                    instance_name = database_instance["name"]
+                    self.log.info(f"Will collect metrics from next instance: {instance_name}")
+                    return instance_name
+                else:
+                    continue
+
+    @staticmethod
+    def get_current_region():
+        from kubernetes import config
+        from kubernetes.client.apis import core_v1_api
+        config.load_incluster_config()
+        api = core_v1_api.CoreV1Api()
+        namespace = open(SA_PATH).read()
+        cm = api.read_namespaced_config_map("cloud-sql-configuration", namespace)
+        return cm.data["region"]
+
+    def set_interval(self):
+        now = time.time()
+        seconds = int(now)
+        nanos = int((now - seconds) * 10 ** 9)
+        interval = monitoring_v3.TimeInterval(
+            {
+                "end_time": {"seconds": seconds, "nanos": nanos},
+                "start_time": {"seconds": (seconds - 210), "nanos": nanos},
+            }
+        )
+        return interval
+
+    def collect_all_metrics(self):
+        interval = self.set_interval()
+        tmp_metric = {}
+        for key, endpoint in self.metric_dict.items():
+            tmp_metric[key] = self.list_time_series(endpoint, interval)
+        return tmp_metric
+
+    def collect_status_metric(self):
+        interval = self.set_interval()
+        tmp_metric = {"cluster.status": self.list_time_series("database/state", interval)}
+        return tmp_metric
+
+    def map_cluster_state(self, value):
+        if value == "RUNNING":
+            return 0
+        elif value == "FAILED":
+            return 10
+        else:
+            return 6
+
+    def list_time_series(self, metric_type, interval):
+        project_name = "projects/" + self.project
+        result = self.client.list_time_series(
+            request={
+                "name": project_name,
+                "filter": f'metric.type = "cloudsql.googleapis.com/{metric_type}" AND resource.labels.database_id = "{self.project}:{self.instance}"',
+                "interval": interval,
+            }
+        )
+        for time_series in result._response._pb.time_series:
+            if time_series.value_type == 1:
+                for point in time_series.points:
+                    return point.value.bool_value
+            elif time_series.value_type == 2:
+                for point in time_series.points:
+                    return point.value.int64_value
+            elif time_series.value_type == 3:
+                for point in time_series.points:
+                    return point.value.double_value
+            elif time_series.value_type == 4:
+                for point in time_series.points:
+                    if metric_type == "database/state":
+                        return self.map_cluster_state(point.value.string_value)
+                    return point.value.string_value
diff --git a/docker-monitoring-agent/scripts/collector_utils.py b/docker-monitoring-agent/scripts/collector_utils.py
new file mode 100644
index 0000000..5fd1430
--- /dev/null
+++ b/docker-monitoring-agent/scripts/collector_utils.py
@@ -0,0 +1,700 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import decimal
+import logging
+import os
+import subprocess
+import time
+import traceback
+import uuid
+import json
+from multiprocessing.pool import ThreadPool
+
+import psycopg2
+
+from m_utils import safe_get, close_connection, get_version_of_pgsql_server, \
+    get_connection_to_pod
+from metrics_query_utils import get_metrics_type_by_version, \
+    get_common_pg_metrics_by_version
+from pod_utils import determine_role, get_env_value_from_pod
+
+from kubernetes.stream import stream
+from kubernetes.client.apis import core_v1_api
+
+
+import pickle
+
+from metric import Metric, default_tags
+
+DEFAULT_STATUS_MAPPING = dict(up=0, successful=0, ok=0, warning=3, warn=3, planned=4, inprogress=5, problem=6, crit=10, fatal=10, failed=10, down=14, unknown=-1)
+
+tmp_dir = '/tmp'
+
+logger = logging.getLogger("metric-collector")
+
+shell_pg_metrics = {
+    "check": "echo 1",
+    "df": "df /var/lib/pgsql/data",
+    # "df.pcent":     "df --output=pcent /var/lib/pgsql/data | grep -v 'Use' | cut -d '%' -f1",
+    # "df.avail":     "df --output=avail /var/lib/pgsql/data | grep -v 'Avail'",
+    "du": "du /var/lib/pgsql/data/ --max-depth=2 --exclude=/var/lib/pgsql/data/lost+found"
+    # "mem.rss":      "ps -u postgres -o pid,rss:8 | awk 'NR>1 {A+=$2} END{print A}'",
+    # "loadavg":      "cat /proc/loadavg | cut -d\  -f1"
+}
+
+
+def spm_du_postprocessor(pod_info, result, version):
+    """
+    :param pod_info:
+    :type pod_info: dict
+    :param result:
+    :type result: string
+    :param version: pgsql version
+    :type version: string
+    :return:
+    """
+    pod_identity = get_env_value_from_pod(pod_info, "POD_IDENTITY", "node")
+    lines = list([p for p in result.decode().split("\n") if p.strip()])
+    sizes = dict(reversed(k.split("\t")) for k in lines)
+    total = int(sizes["/var/lib/pgsql/data/"])
+    pg_xlog = int(sizes[get_metrics_type_by_version("pg_xlog", version).format(pod_identity)])
+    base = int(sizes["/var/lib/pgsql/data/postgresql_{}/base".format(pod_identity)])
+    other = total - pg_xlog - base
+    return {"base": base, "pg_xlog": pg_xlog, "other": other}
+
+
+def spm_df_postprocessor(pod_info, result, version):
+    """
+    Filesystem     1K-blocks     Used Available Use% Mounted on
+    /dev/vda1       83872132 65756268  18115864  79% /var/lib/pgsql/data
+    :param pod_info:
+    :type pod_info: dict
+    :param result:
+    :type result: string
+    :param version: pgsql version
+    :type version: string
+    :return:
+    """
+    data = result.decode().split('\n')[1]
+    metrics = data.split()
+    return {"pcent": int(metrics[4][:-1]), "avail": int(metrics[3])}
+
+
+shell_pg_metrics_postprocessors = {
+    "du": spm_du_postprocessor,
+    "df": spm_df_postprocessor,
+}
+
+
+def collect_generic_metrics_for_pod(pod, shell_metrics_collector_executors=3):
+    """
+    Collect metrics from postgres and through shell
+    Returns dict with pod metrics {"pod": {}, "pg_metrics": {}, "metrics": {}}.
+    Metrics determined by common_pg_metrics and shell_pg_metrics lists.
+    :type pod dict
+    :type shell_metrics_collector_executors int
+    """
+    pod_id = safe_get(pod, ["metadata", "name"], None)
+    pod_ip = safe_get(pod, ["status", "podIP"], "None")
+    logger.debug("Start metric collection for {} [{}]".format(pod_id, pod_ip))
+
+    pod_identity = get_env_value_from_pod(pod, "POD_IDENTITY", "node")
+
+    if pod_identity == "node":
+        if pod_id.startswith('postgresql'):
+            pod_identity = "node1"
+        else:
+            pod_identity = "node" + str(int(pod_id.strip()[-1]) + 1)
+
+    logger.info("pod_identity: {}".format(pod_identity))
+
+    conn_string = get_connection_to_pod(pod_ip)
+
+    pod_description = {
+        "pod": {
+            "name": pod_id,
+            "ip": pod_ip,
+            "role": determine_role(pod),
+            "status": safe_get(pod, ["status", "phase"], "None"),
+            "startedAt": safe_get(pod, ["status", "startTime"], "None")
+        }
+    }
+
+    # Calculates Patroni Member status by annotation on pod. It is temporary metric to identify problem with incrorrect Patroni status during DR managemnt opertaions
+    try:
+        state = ''
+        status = safe_get(pod, ["metadata", "annotations", "status"])
+        if not status:
+            # Get status from containerStatuses if annotations doesn't contain status
+            status = safe_get(pod, ["status", "containerStatuses"])
+            logger.debug("pod status: {}".format(status))
+            for k in status[0].get('state', {}):
+                state = k
+        else:
+            json_status = json.loads(status)
+            state = json_status['state']
+
+        logger.debug("state: {}".format(state))
+        if state == "running":
+            pod_description["patroni_status"] = 1
+        else:
+            pod_description["patroni_status"] = 0
+    except Exception:
+        pod_description["patroni_status"] = 0
+        logging.exception("Cannot get Patroni status from {}".format(pod_id))
+
+    version = None
+    try:
+        version = get_version_of_pgsql_server(conn_string)
+    except Exception:
+        logging.exception("Cannot get version from {}".format(pod_id))
+
+    if version:
+        appropriate_pg_metrics = get_common_pg_metrics_by_version(version)
+        pod_description["pg_metrics"] = __collect_pg_metrics(conn_string, appropriate_pg_metrics, pod_identity)
+        pod_description["metrics"] = __collect_shell_metrics(pod_id, pod, shell_pg_metrics,
+                                                             shell_pg_metrics_postprocessors,
+                                                             version, shell_metrics_collector_executors)
+
+    logger.debug("Collected generic metrics: {}".format(pod_description))
+    return pod_identity, pod_description
+
+
+def perform_smoketests(conn_string):
+    logger.debug("Start smoketests")
+    conn = None
+    cursor = None
+    smoketest = []
+    try:
+        conn = psycopg2.connect(conn_string)
+
+        # prepare table for smoketests
+        cursor = conn.cursor()
+        cursor.execute("create table if not exists monitor_test (id int primary key not null, value text not null);")
+        cursor.execute("SELECT to_regclass('monitor_test_seq');")
+        monitor_test_seq_exist = cursor.fetchone()[0]
+
+        if not monitor_test_seq_exist:
+            logger.info("creating missing sequence for tests")
+            cursor.execute("create sequence monitor_test_seq start 10001;")
+        cursor.execute("SELECT nextval('monitor_test_seq');")
+
+        new_id = int(cursor.fetchone()[0])
+        # in case if we reach maximum for pg type integer
+        # (~ 300 years for 10 sec cycle x2 checks)
+        # see https://www.postgresql.org/docs/9.6/static/datatype-numeric.html
+        if new_id == 2147483647:
+            cursor.execute("delete from monitor_test;")
+            cursor.execute("SELECT setval(monitor_test_seq, 10001);")
+            new_id = int(cursor.fetchone()[0])
+        conn.commit()
+        cursor.close()
+        conn.close()
+
+        new_uuid = str(uuid.uuid4()).lower()
+        second_uuid = str(uuid.uuid4()).lower()
+
+        insert_result, exec_time, output = __perform_single_smoketest(conn_string,
+                                   "insert into monitor_test values ({}, '{}')".format(new_id, new_uuid),
+                                   "insert 0 1", "insert", smoketest)
+
+        if insert_result == 1:
+            select_result, exec_time, output = __perform_single_smoketest(conn_string,
+                                       "select * from monitor_test where id={}".format(new_id),
+                                       "select 1", "select", smoketest)
+            update_result, exec_time, output = __perform_single_smoketest(conn_string,
+                                       "update monitor_test set value='{}' where id={}".format(second_uuid, new_id),
+                                       'update 1', "update", smoketest)
+            delete_result, exec_time, output = __perform_single_smoketest(conn_string,
+                                       "delete from monitor_test where id={}".format(new_id),
+                                       'delete 1', "delete", smoketest)
+
+        if insert_result == 1 and select_result == 1 and update_result == 1 and delete_result == 1:
+            passed = 1
+        else:
+            passed = 0
+        smoketest.append(Metric(metric_name="endpoints.cluster.smoketest.passed", tags=default_tags.copy(), fields={"value": passed}))
+
+    except Exception:
+        logger.exception("Cannot collect metrics. Most likely cannot connect to postgres")
+    finally:
+        close_connection(cursor, conn)
+
+    return smoketest
+
+
+def collect_replication_lag(pgcluster):
+    import requests
+    response = requests.get("http://pg-{}-api:8008/cluster".format(pgcluster))
+    if response:
+        json_response = response.json()
+        for member in json_response["members"]:
+            if member["role"] in {"leader", "standby_leader"}:
+                continue
+            if member["state"] == "running":
+                tags = {"namespace": os.environ['NAMESPACE'], "hostname": member["name"]}
+                tag = ",".join(["{}={}".format(t, tags[t]) for t in tags])
+                print("ma.pg.patroni.replication_lag,{0} value={1}".format(tag, member["lag"]))
+    else:
+        logger.warning("were not able to collect replication lag from patroni", response.text)
+
+
+def get_leader_pod(pgcluster):
+    import requests
+    response = requests.get("http://pg-{}-api:8008/cluster".format(pgcluster))
+    if response:
+        json_response = response.json()
+        for member in json_response["members"]:
+            if member["role"] in {"leader", "standby_leader"}:
+                return member
+    else:
+        logger.warning("Can not find leader of patroni cluster", response.text)
+
+
+def collect_replication_data(conn_string):
+    logger.debug("Collect data about replication")
+    conn = None
+    cursor = None
+    replication = {}
+    try:
+        conn = psycopg2.connect(conn_string)
+
+        # get pgsql server version
+        version = None
+        try:
+            version = get_version_of_pgsql_server(conn_string)
+        except Exception:
+            logging.exception("Cannot get version")
+        if version:
+            # collect replication info
+            cursor = conn.cursor()
+            cursor.execute(get_metrics_type_by_version("replication_data", version))
+            for row in cursor:
+                replication[row[1].replace(" ", "_")] = {
+                    "usename": row[0] if row[0] else "empty",
+                    "client_addr": row[2] if row[2] else "empty",
+                    "state": row[3] if row[3] else "empty",
+                    "sync_priority": row[4] if row[4] else 0,
+                    "sync_state": row[5] if row[5] else "empty",
+                    "sent_replay_lag": row[6] if row[6] else 0,
+                    "sent_lag": row[7] if row[7] else 0,
+                }
+    except Exception:
+        logger.exception("Cannot collect metrics. Most likely cannot connect to postgres")
+    finally:
+        close_connection(cursor, conn)
+    return replication
+
+
+def collect_archive_data(conn_string):
+    logger.debug("Collect data about wal archive")
+    conn = None
+    cursor = None
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        cursor.execute("""SELECT 
+(select setting from pg_settings where name='archive_mode'), 
+EXTRACT(EPOCH FROM (now()-last_archived_time)), 
+archived_count, 
+failed_count FROM pg_stat_archiver""")
+        for row in cursor:
+            result = {
+                "mode": row[0],
+                # this is a for backward compatibility
+                "mode_prom": 0 if row[0] == "off" else 1
+            }
+            if row[0] == "on":
+                if row[1]:  # assuming we just started and no record was archived yet
+                    result["delay"] = row[1]
+                result["archived_count"] = row[2]
+                result["failed_count"] = row[3]
+            return result
+    except Exception:
+        logger.exception("Cannot collect metrics. Most likely cannot connect to postgres")
+    finally:
+        close_connection(cursor, conn)
+
+
+def enrich_master_metrics(conn_string, master, pgcluster, active, replica_names):
+    """
+    Performs smoketests on master and calculates writable attribute.
+    Writes result in master
+    """
+    logger.debug("Enrich metrics for master {}".format(master))
+    if safe_get(master, ["pg_metrics", "running"]) == 1:
+        smoke_test = []
+        if active:
+            logger.debug("Starting smoke tests")
+            smoke_test = perform_smoketests(conn_string)
+        else:
+            logger.debug("Cluster is in standby mode. Skip smoke tests")
+        master["pg_metrics"]["replication"] = collect_replication_data(conn_string)
+        master["pg_metrics"]["archive"] = collect_archive_data(conn_string)
+    else:
+        smoke_test = [(Metric(metric_name="endpoints.cluster.smoketest.passed", tags=default_tags.copy(), fields={"value": 0}))]
+        master["pg_metrics"]["replication"] = []
+        master["pg_metrics"]["archive"] = {}
+    passed = 0
+    for metric in smoke_test:
+        if metric.metric_name == "endpoints.cluster.smoketest.passed":
+            passed = metric.get_field_value("value", 0)
+    master["pg_metrics"]["writable"] = True if passed is 1 else False
+
+    enrich_prometheus_metrics(master, replica_names)
+    collect_replication_lag(pgcluster)
+
+
+def enrich_prometheus_metrics(master_metrics, replica_names):
+    replication_state = master_metrics["pg_metrics"]["replication"]
+    for key, value in replication_state.items():
+        sent_lag = value.get("sent_lag", 0)
+        sent_replay_lag = value.get("sent_replay_lag", 0)
+        tags = {
+            "namespace": os.environ['NAMESPACE'],
+            "hostname": key,
+        }
+        tag = ",".join(["{}={}".format(t, tags[t]) for t in tags])
+        print("ma.pg.patroni.replication_state.sent_lag,{0} value={1}".format(tag, sent_lag))
+        print("ma.pg.patroni.replication_state.sent_replay_lag,{0} value={1}".format(tag, sent_replay_lag))
+
+    if os.getenv("SITE_MANAGER") == "on":
+        sm_replication_state = 1
+        tags = {
+            "namespace": os.environ['NAMESPACE'],
+        }
+        tag = ",".join(["{}={}".format(t, tags[t]) for t in tags])
+        for name in replica_names:
+            if name in replication_state and len(replica_names) == len(replication_state):
+                sm_replication_state = 0
+            print("ma.pg.patroni.replication_state.sm_replication_state,{0} value={1}".format(tag, sm_replication_state))
+
+    archive = master_metrics["pg_metrics"]["archive"]
+    points = linearizeJson(archive)
+    tags = default_tags.copy()
+    tag = ",".join(["{}={}".format(t, tags[t]) for t in tags])
+    for key in points:
+        value = get_influxdb_value(key, points[key])
+        print("ma.pg.patroni.pg_metrics_archive.{0},{1} value={2}".format(key, tag, value))
+
+
+def __collect_single_pg_metric(conn_string, query):
+    conn = None
+    cursor = None
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        logger.debug("Executing query {}".format(query))
+        cursor.execute(query)
+        return cursor.fetchone()[0]
+    finally:
+        close_connection(cursor, conn)
+
+
+def collect_pg_metrics(conn_string, requests):
+    pg_metrics = {}
+    pool = ThreadPool(processes=len(requests))
+    tasks = {}
+    for (key, value) in list(requests.items()):
+        tasks[key] = pool.apply_async(__collect_single_pg_metric, (conn_string, value))
+
+    for (key, value) in list(tasks.items()):
+        try:
+            result = value.get(timeout=5)
+            pg_metrics[key] = result
+        except Exception as e:
+            logger.exception('Cannot collect metric  "{}"'.format(key))
+
+    pool.close()
+    return pg_metrics
+
+
+def __collect_pg_metrics(conn_string, requests, pod_identity):
+    """
+    This method executes SELECT requests defined in requests dict and returns dict with results
+    :type conn_string str
+    :type requests dict
+    :rtype dict
+    """
+    start_time = time.time()
+    pg_metrics = collect_pg_metrics(conn_string, requests)
+
+    pg_metrics["xact_sum"] = safe_get(pg_metrics, ["xact_sum"], 0)
+    pg_metrics["replication_slots"] = __collect_replication_slots_data(conn_string)
+
+    logger.debug("Pg metrics loading time: {}".format((time.time() - start_time)))
+    return pg_metrics
+
+
+def __collect_replication_slots_data(conn_string):
+    logger.debug("Collect data about replication slots")
+    conn = None
+    cursor = None
+    replication = {}
+    try:
+        conn = psycopg2.connect(conn_string)
+
+        # get pgsql server version
+        version = None
+        try:
+            version = get_version_of_pgsql_server(conn_string)
+        except Exception:
+            logging.exception("Cannot get version")
+        if version:
+            logger.debug("Collecting replication slots data for pgsql server: %s" % version)
+            # collect replication info
+            cursor = conn.cursor()
+            cursor.execute(get_metrics_type_by_version("replication_slots", version))
+            for row in cursor:
+                replication[row[0].replace(" ", "_")] = {
+                    "restart_lsn_lag": row[1] if row[1] else 0,
+                    "confirmed_flush_lsn_lag": row[2] if row[2] else 0
+                }
+    except Exception:
+        logger.exception("Cannot collect metrics. Most likely cannot connect to postgres")
+    finally:
+        close_connection(cursor, conn)
+    return replication
+
+
+def execute_query(conn_string, query, collect_data=True):
+    conn = None
+    cursor = None
+    try:
+        conn = psycopg2.connect(conn_string)
+        conn.autocommit = True
+        cursor = conn.cursor()
+
+        try:
+            cursor.execute(query)
+            if collect_data:
+                data = cursor.fetchone()
+                return data[0] if data else None
+        except Exception as e:
+            logger.exception('Cannot check execute  "{}"'.format(query))
+    except Exception:
+        logger.exception("Cannot execute query {}. Most likely cannot connect to postgres".format(query))
+    finally:
+        close_connection(cursor, conn)
+    return None
+
+
+def exec_in_pod (name, command):
+    api = core_v1_api.CoreV1Api()
+    resp = stream(api.connect_get_namespaced_pod_exec, name, os.environ.get('NAMESPACE'),
+                  command=command.split(" "),
+                  stderr=True, stdin=False,
+                  stdout=True, tty=False)
+    return resp
+
+
+def __collect_single_shell_metric(pod_id, pod_info, command, pg_version, postprocessor):
+    oc_exec_timeout = int(os.environ.get('METRIC_COLLECTOR_OC_EXEC_TIMEOUT', 10))
+    logger.debug("Executing command {}".format(command))
+    try:
+        result = exec_in_pod(pod_id, command)
+        command_result = result.encode()
+
+        if postprocessor:
+            return postprocessor(pod_info, command_result, pg_version)
+        else:
+            try:
+                return float(command_result)
+            except ValueError:
+                return command_result
+    except subprocess.TimeoutExpired:
+        logger.exception("Timeout exception raised for cmd: {}".format(command))
+
+
+def __collect_shell_metrics(pod_id, pod_info, metrics, postprocessors, pg_version, executors=3):
+    """
+    This method executes SELECT requests defined in requests dict and returns dict with results
+    :type pod_id str
+    :type metrics dict
+    :rtype dict
+    """
+    start_time = time.time()
+    shell_metrics = {}
+
+    pool = ThreadPool(processes=executors)
+    tasks = {}
+    for (key, value) in list(metrics.items()):
+        tasks[key] = pool.apply_async(__collect_single_shell_metric,
+                                      (pod_id, pod_info, value, pg_version, postprocessors.get(key)))
+
+    for (key, value) in list(tasks.items()):
+        try:
+            shell_metrics[key] = value.get(timeout=int(os.environ.get('METRIC_COLLECTOR_OC_EXEC_TIMEOUT', 5)))
+        except Exception as e:
+            logger.exception('Cannot collect metric  "{}"'.format(key))
+
+    pool.close()
+
+    logger.debug("Shell metrics loading time: {}".format((time.time() - start_time)))
+    return shell_metrics
+
+
+def __perform_single_smoketest(conn_string, query, expected_status, prefix, smoketests):
+    """
+    Performs query and checks if status message contains required message
+    """
+    logger.debug("Execute '{}'".format(query))
+    exec_time = -1
+    result = 0
+    output = None
+    conn = None
+    cursor = None
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        # todo[anin] collect time from pg itself
+        start_time = time.time()
+        cursor.execute(query)
+        cc = cursor.statusmessage
+        exec_time = 1000 * (time.time() - start_time)
+        if expected_status in cc.lower():
+            result = 1
+            output = cc
+        else:
+            result = -1
+
+        conn.commit()
+    except Exception:
+        logger.exception("Exception during smoketest execution")
+        output = traceback.format_exc()
+        result = -1
+    finally:
+        close_connection(cursor, conn)
+    tags = default_tags.copy()
+    tags.update({"action": prefix})
+
+    smoketests.append(Metric(metric_name="endpoints.cluster.smoketest.check", tags=tags, fields={"value": result}))
+    smoketests.append(Metric(metric_name="endpoints.cluster.smoketest.time_ms", tags=tags, fields={"value": exec_time}))
+    if result != 1:
+        smoketests.append(Metric(metric_name="endpoints.cluster.smoketest.output", tags=tags, fields={"value": output}))
+    return result, exec_time, output
+
+
+def linearizeJson(obj):
+    def __walk(obj, prefix, res):
+        if isinstance(obj, dict):
+            for key in obj:
+                propKey = key if prefix is None else "%s.%s" % (prefix, key)
+                __walk(obj[key], propKey, res)
+
+        elif isinstance(obj, list):
+            for i, item in enumerate(obj):
+                propKey = str(i) if prefix is None else "%s.%d" % (prefix, i)
+                __walk(item, propKey, res)
+
+        else:
+            res[prefix] = obj
+
+    res = {}
+    __walk(obj, None, res)
+    return res
+
+
+def get_influxdb_value(key, value):
+    if type(value) == int:
+        rvalue = value
+    elif type(value) == bool:
+        if value == True:
+            rvalue = 1
+        else:
+            rvalue = 0
+    elif type(value) == float:
+        rvalue = value
+    elif type(value) == decimal.Decimal:
+        rvalue = value
+    elif not value:
+        rvalue = ""
+    elif value == "None":
+        rvalue = '"None"'
+    else:
+        # some time type function detect number as string. Why? i don't know
+        try:
+            val = int(value)
+            rvalue = val
+        except ValueError:
+            rvalue = '"' + str(value) + '"'
+    # fix for existing cloud db metric
+    # if key in ['storage.size', 'storage.archive_size', 'storage.last.metrics.size',
+    #            'storage.lastSuccessful.metrics.size']:
+    #     rvalue = '"' + str(value) + '"'
+    # fix in case of last backup is failed,
+    # skipping exception metric because its not used in dashboard
+    if key == "storage.last.metrics.exception":
+        rvalue = "\"Exception happend during backup\""
+    return rvalue
+
+
+def mapStatuses(obj, mapping=DEFAULT_STATUS_MAPPING):
+    res = {}
+    for key in obj:
+        if (key == 'status' or key.endswith('.status')) and isinstance(obj[key], (bytes, str)):
+            if obj[key].lower() in mapping:
+                res[key] = mapping[obj[key].lower().replace(" ","")]
+            else:
+                logger.debug("Unknown status for mapping: %s" % obj[key])
+                res[key] = DEFAULT_STATUS_MAPPING['unknown']
+        else:
+            res[key] = obj[key]
+
+    return res
+
+
+def storePodValue(pod, key, value):
+    global tmp_dir
+    fh = open("{}/{}.{}.tmp".format(tmp_dir, pod, key), 'wb')
+    pickle.dump(value, fh)
+
+
+def getPodValue(pod, key, default=None):
+    global tmp_dir
+    filename = "{}/{}.{}.tmp".format(tmp_dir, pod, key)
+    if os.path.exists(filename):
+        fh = open(filename, 'rb')
+        value=pickle.load(fh)
+    else:
+        return default
+    return value
+
+
+def get_host():
+    cluster_name = os.getenv('PGCLUSTER', 'patroni')
+    namespace = os.getenv('NAMESPACE', 'postgres-service')
+    host = os.getenv('POSTGRES_HOST', f'pg-{cluster_name}.{namespace}')
+    return host
+
+
+def get_port():
+    return int(os.getenv('POSTGRES_PORT', 5432))
+
+
+def get_region():
+    return os.getenv('REGION', '')
+
+
+def prepare_node_metrics(pod_identity, pod_description):
+    node_metrics = []
+    tags = default_tags.copy()
+    tags.update({"pg_node": pod_identity})
+
+    points = linearizeJson(pod_description)
+    for key in points:
+        value = get_influxdb_value(key, points[key])
+        node_metrics.append(Metric(metric_name=key, tags=tags, fields={"value": value}))
+    return node_metrics
diff --git a/docker-monitoring-agent/scripts/gpdb.py b/docker-monitoring-agent/scripts/gpdb.py
new file mode 100644
index 0000000..6426916
--- /dev/null
+++ b/docker-monitoring-agent/scripts/gpdb.py
@@ -0,0 +1,301 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+from multiprocessing.pool import ThreadPool
+
+import psycopg2
+import time
+
+from smart_getenv import getenv
+from collector_utils import perform_smoketests, collect_pg_metrics, collect_replication_data
+from metrics_query_utils import get_common_pg_metrics_by_version
+from kubernetes import client
+from kubernetes.stream import stream
+from kubernetes.client.rest import ApiException
+from math import ceil
+from typing import List
+
+from metric import Metric, default_tags
+
+logger = logging.getLogger(__name__)
+
+
+class State:
+    DOWN = 14
+    OK = 0
+    WARNING = 3
+    ERROR = 10
+    DEGRADED = 6
+
+
+class Queries:
+    # queries for tracking cluster state
+    HOSTS_STATUS = "select hostname, status, content, role, preferred_role, mode FROM gp_segment_configuration"
+    # distributed query test. One row should be returned for each primary segment.
+    DISTRIBUTED_QUERY_TEST = "SELECT COUNT(*) FROM (SELECT GP_SEGMENT_ID, COUNT(*) " \
+                             "FROM GP_DIST_RANDOM('pg_class') GROUP BY 1) AS T;"
+    # If this query fails the active master may be down.
+    STANDBY_MASTER_REPLICATION = "SELECT state FROM pg_stat_replication;"
+
+
+class OpenshiftHelper:
+
+    SA_NAMESPACE_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/namespace'
+
+    def __init__(self):
+        self._os_workspace = open(self.SA_NAMESPACE_PATH).read()
+        self._core_api = client.CoreV1Api()
+
+    def get_pods_by_label(self, label):
+        pods = self._core_api.list_namespaced_pod(namespace=self._os_workspace, label_selector=label)
+        return [pod.metadata.name for pod in pods.items]
+
+    def execute_in_pod(self, pod, command):
+        try:
+            resp = stream(self._core_api.connect_get_namespaced_pod_exec, pod, self._os_workspace, command=command,
+                          stderr=False, stdin=False, stdout=True, tty=False)
+            return resp.strip()
+        except ApiException as e:
+            logger.error("Exception when calling CoreV1Api -> execute_in_pod: ", e)
+        return None
+
+
+class Member:
+    def __init__(self, hostname, status, content, role, preferred_role, mode):
+        self.hostname = hostname
+        self.status = status
+        self.content = content
+        self.role = role
+        self.preferred_role = preferred_role
+        self.mode = mode
+
+    def is_running_and_master(self):
+        return self.content == -1 and self.status == 'u'
+
+    def is_down_and_master(self):
+        return self.content == -1 and self.status != 'u'
+
+    def is_running_and_segment(self):
+        return self.content != -1 and self.status == 'u'
+
+    def is_down_and_segment(self):
+        return self.content != -1 and self.status != 'u'
+
+    def is_primary_master(self):
+        return self.content == -1 and self.status == 'u' and self.role == 'p'
+
+    def is_standby_master(self):
+        return self.content == -1 and self.status == 'u' and self.role == 'm'
+
+    def is_primary_segment(self):
+        return self.content != -1 and self.status == 'u' and self.role == 'p'
+
+    @property
+    def is_up(self):
+        return 1 if self.status == 'u' else 0
+
+    @property
+    def is_optimal_role(self):
+        return self.preferred_role == self.role
+
+    @property
+    def is_resync(self):
+        return self.mode == 'r' and self.content != -1
+
+    @property
+    def is_tracking(self):
+        return self.mode == 'c' and self.content != -1
+
+
+class Cluster:
+    def __init__(self, members: List[Member]):
+        self.members = members
+
+    @property
+    def down_segments(self):
+        return list(filter(lambda x: x.is_down_and_segment(), self.members))
+
+    @property
+    def down_masters(self):
+        return list(filter(lambda x: x.is_down_and_master(), self.members))
+
+    @property
+    def cluster_status(self):
+        cluster_state = State.OK
+        if self.down_segments:
+            cluster_state = State.WARNING
+        if self.down_masters:
+            cluster_state = State.ERROR
+        if not self.standby_leader:
+            cluster_state = State.DEGRADED
+        return cluster_state
+
+    @property
+    def primary_segments(self):
+        return list(filter(lambda x: x.is_primary_segment(), self.members))
+
+    @property
+    def standby_leader(self):
+        return list(filter(lambda x: x.is_standby_master(), self.members))
+
+    @property
+    def __str__(self):
+        return [member.__str__() for member in self.members]
+
+
+class GpdbCollector:
+    def __init__(self, cluster_name):
+        self._cluster_name = cluster_name
+        self._conn_properties = dict(
+            host=f'pg-{cluster_name}', port=5432, password=getenv('MONITORING_PASSWORD', type=str, default=""),
+            user=getenv('MONITORING_USER', type=str, default=""), dbname="postgres",
+            connect_timeout=10
+        )
+        self._start_time = time.time()
+        self._collection_state = State.OK
+        self._oshift_helper = OpenshiftHelper()
+        self._metrics = {}
+        self._metrics_list = []
+
+    @property
+    def conn_link(self):
+        return ' '.join("%s=%r" % (key, val) for (key, val) in self._conn_properties.items())
+
+    def get_db_connection(self):
+        try:
+            connection = psycopg2.connect(**self._conn_properties)
+            connection.autocommit = True
+        except Exception as e:
+            logger.error("Can not connect to PostgreSQL", e)
+            return None
+        return connection
+
+    def _collect_cluster_state(self):
+        logger.info("Collecting cluster state")
+        cluster_metrics = {}
+
+        conn = self.get_db_connection()
+        if not conn:
+            # DB is down, marking state as DOWN
+            cluster_state = State.DOWN
+        else:
+            # DB is accessible, check cluster state
+            members = list()
+            running_pods = self._oshift_helper.get_pods_by_label("service=gpdb")
+            with conn.cursor() as cur:
+                cur.execute(Queries.HOSTS_STATUS)
+                for hostname, status, content, role, preferred_role, mode in cur:
+                    hostname = hostname.split(".")[0]
+                    member = Member(hostname, status, content, role, preferred_role, mode)
+                    members.append(member)
+                cur.execute(Queries.STANDBY_MASTER_REPLICATION)
+                if cur.fetchone():
+                    cluster_metrics.update({"master_replication_state": State.OK})
+                else:
+                    cluster_metrics.update({"master_replication_state": State.DOWN})
+                try:
+                    cur.execute(Queries.DISTRIBUTED_QUERY_TEST)
+                    dist_query_test = int(cur.fetchone()[0])
+                except psycopg2.OperationalError as e:
+                    logger.error("DISTRIBUTED_QUERY_TEST is failed", e)
+                    dist_query_test = 0
+
+            cluster = Cluster(members)
+            members_metrics = {x.hostname: x.is_up and x.hostname in running_pods for x in members}
+            cluster_metrics.update({"members": members_metrics})
+
+            optimal_roles = {x.hostname: x.is_optimal_role for x in members}
+            cluster_metrics.update({"optimal_roles": optimal_roles})
+
+            syncing = {x.hostname: x.is_resync for x in members}
+            cluster_metrics.update({"syncing": syncing})
+
+            tracking = {x.hostname: x.is_tracking for x in members}
+            cluster_metrics.update({"tracking": tracking})
+
+            primary_segments = len(cluster.primary_segments)
+            cluster_metrics.update({"dist_query_state": State.ERROR if dist_query_test != primary_segments else State.OK})
+
+            cluster_state = cluster.cluster_status
+        for role in ["leader", "standby"]:
+            role_pod = self._oshift_helper.get_pods_by_label(f"role={role}")
+            role_ = role_pod[0] if role_pod else "None"
+            role_idx = role_pod[0].split("-")[2] if role_pod else 0
+            cluster_metrics.update({role: role_})
+            cluster_metrics.update({role + "_idx": role_idx})
+            cluster_metrics.update({role + "_count": 1})
+
+        cluster_metrics.update({"status": cluster_state})
+        self._metrics.update({"cluster": cluster_metrics})
+
+    def _collect_common_pg_metrics(self):
+        logger.info("Collecting common PostgreSQL metrics")
+        # lets hardcode version of pgsql in case of gpdb
+        common_pg_metrics = get_common_pg_metrics_by_version([9, 4])
+        pg_metrics = collect_pg_metrics(self.conn_link, common_pg_metrics)
+        pg_metrics["replication_data"] = collect_replication_data(self.conn_link)
+        self._metrics.update({"pg_metrics": pg_metrics})
+
+    def collect(self):
+        try:
+            self._collect_cluster_state()
+            self._perform_smoke_tests()
+            self._collect_common_pg_metrics()
+            self._collect_storage_metrics()
+        except Exception as e:
+            self._collection_state = State.ERROR
+            logger.error("Collection of GPDB metrics are failed", e)
+        logger.info("Collection of GPDB metrics suceeded")
+
+    def convert(self):
+        return {"gpdb": self._metrics, "status": self._collection_state,
+                "collector": {"duration": self._start_time - time.time()}}, self._metrics_list
+
+    def _perform_smoke_tests(self):
+        logger.info("Performing smoketests")
+        smoke_test = perform_smoketests(self.conn_link)
+        passed = 0
+        for metric in smoke_test:
+            if metric.metric_name == "endpoints.cluster.smoketest.passed":
+                passed = metric.fields.split("=")[1]
+        smoke_test.append(Metric(metric_name="endpoints.cluster.running", tags=default_tags, fields={"value": passed}))
+        self._metrics_list.extend(smoke_test)
+
+    @staticmethod
+    def collect_storage_metric(pod):
+        helper = OpenshiftHelper()
+        # output in bytes
+        du_command = ['/bin/bash', '-c', "du /data/ --max-depth=1 --exclude=/data/lost+found | tail -n 1 | cut -f1"]
+        du_res = ceil(int(helper.execute_in_pod(pod=pod, command=du_command)) / 1024)
+        df_command = ['/bin/bash', '-c', "df --output=pcent /data/ | grep -v 'Use' | cut -d '%' -f1"]
+        df_res = helper.execute_in_pod(pod=pod, command=df_command)
+        storage_metrics = {
+            "du": du_res,
+            "df": df_res
+        }
+        return {pod: storage_metrics}
+
+    def _collect_storage_metrics(self):
+        logger.info("Collecting storage metrics")
+        gpdb_pods = self._oshift_helper.get_pods_by_label(label="service=gpdb")
+        # collect
+        pool = ThreadPool(len(gpdb_pods))
+        results = pool.map(self.collect_storage_metric, gpdb_pods)  # Run exec calls on pods in parallel
+        pool.close()
+        pool.join()
+        # TODO fix this
+        final_res = {}
+        for x in results: final_res.update(x)
+        self._metrics.update({"storage": final_res})
diff --git a/docker-monitoring-agent/scripts/m_utils.py b/docker-monitoring-agent/scripts/m_utils.py
new file mode 100644
index 0000000..54c3d2f
--- /dev/null
+++ b/docker-monitoring-agent/scripts/m_utils.py
@@ -0,0 +1,100 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+import os
+
+import collector_utils
+
+logger = logging.getLogger("metric-collector")
+
+
+def safe_get(data, path, default=None):
+    """
+    This method tries to get element from map|array tree by provided path ["key", "second_key"].
+    If element exists returns element otherwise return default.
+    :type data dict
+    :type data List
+    :type path List
+    :type default any
+    """
+    if data:
+        current = data
+        for path_element in path:
+            if isinstance(path_element, (bytes, str)):
+                if path_element in current:
+                    current = current[path_element]
+                else:
+                    return default
+            elif isinstance(path_element, int):
+                if len(current) > path_element:
+                    current = current[path_element]
+                else:
+                    return default
+            else:
+                return default
+        return current
+    else:
+        return default
+
+
+def deep_merge(target, update):
+    for key in update:
+        if key in target:
+            if isinstance(target[key], dict) and isinstance(update[key], dict):
+                deep_merge(target[key], update[key])
+            elif target[key] != update[key]:
+                raise Exception(
+                    'Conflict occurred for {} and {}.'.format(target[key],
+                                                              update[key]))
+        else:
+            target[key] = update[key]
+    return target
+
+
+def get_version_of_pgsql_server(conn_string):
+    result = collector_utils.execute_query(conn_string, 'SHOW SERVER_VERSION;')
+    return list(map(int, result.split(' ')[0].split('.')))
+
+
+def close_connection(cursor, conn):
+    # see http://initd.org/psycopg/docs/cursor.html#cursor.closed
+    if cursor and not cursor.closed:
+        try:
+            cursor.close()
+        except Exception as cursor_close_exc:
+            logger.exception("Error while closing cursor")
+    # see http://initd.org/psycopg/docs/connection.html#connection.closed
+    if conn and conn.closed == 0:
+        try:
+            conn.close()
+        except Exception as conn_close_exc:
+            logger.exception("Error while closing connection")
+
+
+def get_monitoring_user():
+    return os.environ.get("MONITORING_USER", "monitoring_role")
+
+
+def get_monitoring_password():
+    return os.environ.get("MONITORING_PASSWORD", "monitoring_password")
+
+
+def get_connection_to_pod(pod_ip):
+    conn_string = f"host='{pod_ip}' dbname='postgres' " \
+                  f"user='{get_monitoring_user()}' " \
+                  f"password='{get_monitoring_password()}' " \
+                  "connect_timeout=3 options='-c " \
+                  "statement_timeout=3500'"
+    return conn_string
diff --git a/docker-monitoring-agent/scripts/metric-collector.py b/docker-monitoring-agent/scripts/metric-collector.py
new file mode 100755
index 0000000..824c499
--- /dev/null
+++ b/docker-monitoring-agent/scripts/metric-collector.py
@@ -0,0 +1,459 @@
+#!/usr/bin/env python
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+import argparse
+import logging
+import os
+import threading
+import time
+
+from collector_utils import collect_generic_metrics_for_pod, enrich_master_metrics, execute_query, perform_smoketests, \
+    get_common_pg_metrics_by_version, __collect_pg_metrics, prepare_node_metrics, get_leader_pod
+from collector_utils import linearizeJson, get_influxdb_value, mapStatuses, getPodValue, storePodValue, collect_replication_data
+from collector_utils import get_port, get_host, get_region
+from m_utils import safe_get, get_monitoring_user, get_monitoring_password, get_version_of_pgsql_server
+from oc_utils import get_pods_info, get_configmap, get_dc_info, get_number_of_replicas_from_statefulset, get_number_of_replicas_from_deployment, oc_login, get_deployment_info , unavailable_replicas_from_deployment
+from pod_utils import get_env_value_from_any_pod
+from kubernetes import config
+from gpdb import GpdbCollector
+# todo[anin] read actual value
+from metric import convert_metrics, Metric, default_tags
+
+DEFAULT_CLUSTER_PG_NODE_QTY = 2
+
+
+def get_rto(pod_data):
+    return get_env_value_from_any_pod(pod_data, "PATRONI_TTL", 60)
+
+
+def get_rpo(pod_data):
+    return get_env_value_from_any_pod(pod_data, "PATRONI_MAXIMUM_LAG_ON_FAILOVER", 1048576)
+
+
+def get_latest_master_appearance():
+    return getPodValue('master','last_master_appearance',None), getPodValue('master','last_master_xlog_location',0)
+
+
+def store_last_master_appearance(master):
+    storePodValue('master','last_master_appearance',time.time())
+    storePodValue('master','last_master_xlog_location',max(getPodValue('master','last_master_xlog_location',0), safe_get(master, ["pg_metrics", "xlog_location"], 0)))
+
+
+def collect_metrics(remote_host, remote_port,
+                    oc_project, pgcluster):
+    temp_metrics = {
+        "collector": {"start": time.time()},
+        "pg.{}".format(pgcluster): {
+            "nodes": {},
+            "cluster": {},
+            "endpoints": {},
+            "patroni_dcs": "kubernetes"
+        },
+        "status": 0
+    }
+    metrics = []
+    collection_start_time = time.time()
+    try:
+        with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:
+            token = f.read()
+
+        pod_data = get_pods_info(remote_host, remote_port, oc_project, token, "pgcluster", pgcluster)
+        for pod in safe_get(pod_data, ["items"], []):
+            phase = safe_get(pod, ["status", "phase"], [])
+            deployment_info = safe_get(pod, ["status", "containerStatuses"], [])
+            if len(deployment_info) == 0:
+                logger.info("Skipping pod with info: {}".format(pod))
+                continue
+            logger.debug("Deployment information returned {}" .format(deployment_info))
+            deployment_name = deployment_info[0].get('name',{})
+            logger.debug("Deployment Name returned from array {}" .format(deployment_name))
+            unavailable_replicas = unavailable_replicas_from_deployment(remote_host, remote_port, oc_project, token, deployment_name)
+            logger.debug("unavailable replica count {}" .format(unavailable_replicas))
+            if phase == "Running" and unavailable_replicas != 1:
+                (pod_identity, pod_description) = collect_generic_metrics_for_pod(pod, shell_metrics_collector_executors)
+                metrics.extend(prepare_node_metrics(pod_identity, pod_description))
+                temp_metrics["pg.{}".format(pgcluster)]["nodes"][pod_identity] = pod_description
+            else:
+                pod_name = safe_get(pod, ["metadata", "name"], [])
+                reason = safe_get(pod, ["status", "reason"], {})
+                logger.info("skipping pod: {} message: {} reason: {}".format(pod_name, phase, reason))
+
+        temp_metrics["pg.{}".format(pgcluster)]["patroni_dcs"] = "kubernetes"
+        temp_metrics["pg.{}".format(pgcluster)]["cluster"] = __get_pg_cluster_status(pgcluster, pod_data, temp_metrics)
+        __get_endpoints_status(pgcluster, safe_get(temp_metrics, ["pg.{}".format(pgcluster), "cluster", "active"]))
+        temp_metrics["collector"]["status"] = "OK"
+
+        # remove old nodes metrics format. It's used in __get_pg_cluster_status to prepare cluster metrics
+        del temp_metrics["pg.{}".format(pgcluster)]["nodes"]
+
+        status_code = temp_metrics["pg.{}".format(pgcluster)]["cluster"]["status"]
+        # this is a mapping for common app health dashboard.
+        # 1 -> UP; 0 -> Down, other codes are just warnings
+        cluster_status_health_mapping = {0: 1, 10: 0}
+        app_health_code = cluster_status_health_mapping.get(status_code, status_code)
+        # just print this metric so output telegraf plugin will take it
+        print("app_health value={0}".format(app_health_code))
+    except Exception:
+        logger.exception("Cannot collect data")
+        temp_metrics["collector"]["status"] = "ERROR"
+    temp_metrics["collector"]["duration"] = time.time() - collection_start_time
+    return temp_metrics, metrics
+
+
+def __is_xlog_location_actual(pod_identity, pod_metrics):
+    last_master_xlog_location=getPodValue('master','last_master_xlog_location',0)
+    pg_metrics = safe_get(pod_metrics, ["pg_metrics"])
+    if safe_get(pg_metrics, ["running"], None) == 1:
+        current_xlog_location = safe_get(pg_metrics, ["xlog_location"], 0)
+        last_xlog_location = int(getPodValue(pod_identity, "xlog_location", current_xlog_location))
+        storePodValue(pod_identity, "xlog_location", current_xlog_location)
+        logger.debug("For {}: current_xlog_location={}, last_xlog_locatioqn={}, last_master_xlog_location={}"
+                     .format(pod_identity, current_xlog_location, last_xlog_location, last_master_xlog_location))
+        delta1 = current_xlog_location - last_xlog_location
+        delta2 = current_xlog_location - last_master_xlog_location
+        if delta2 < 0 and delta1 == 0:
+            logger.debug("Node {} has zero progress for xlog_location while master is ahead of node. "
+                         "Calculated metrics: delta1: {}, delta2: {}".format(pod_identity, delta1, delta2))
+            return False
+    return True
+
+
+def __calculate_pg_cluster_status_for_missing_master(pod_data, nodes):
+    """
+    This method checks if cluster ready for auto failover without master.
+    Cluster can be ready for failover, not ready but downtime less than RTO and not ready but downtime more than RTO
+    :param pod_data:
+    :param nodes:
+    :return:
+    """
+    logger.warn("Master not found or in read only mode. Calculating cluster status.")
+    error_status = "None"
+    error_message = "None"
+    error_description = "None"
+
+    RTO = float(get_rto(pod_data))
+    RPO = float(get_rpo(pod_data))
+    logger.info("RTO: {}, RPO: {}".format(RTO, RPO))
+
+    (lma, lmxl) = get_latest_master_appearance()
+    max_xlog_location = 0 if len(nodes) == 0 else \
+        max([safe_get(x, ["pg_metrics", "xlog_location"], 0) for x in nodes])
+    if lma:
+        time_since_master_disappear = time.time() - lma
+        minimum_lag = lmxl - max_xlog_location
+        logger.info("last_master_appearance: {}, last_master_xlog_location: {}, max_xlog_location: {}"
+                    .format(lma, lmxl, max_xlog_location))
+        if time_since_master_disappear < RTO and minimum_lag > RPO:
+            error_status = "WARNING"
+            error_message = "No replica to promote but service is down less than RTO"
+            error_description = "Supposed action: check master state and restore if possible" \
+                                "Stats: [ " \
+                                "last_master_appearance: {}, " \
+                                "last_master_xlog_location: {}, " \
+                                "max_xlog_location: {} ]" \
+                .format(lma, lmxl, max_xlog_location)
+        if time_since_master_disappear > RTO and minimum_lag > RPO:
+            error_status = "CRITICAL"
+            error_message = "No replica to promote and service is down more than RTO"
+            error_description = "Supposed action: try to restore master manually if possible or perform failover. " \
+                                "Stats: [ " \
+                                "last_master_appearance: {}, " \
+                                "last_master_xlog_location: {}, " \
+                                "max_xlog_location: {} ]" \
+                .format(lma, lmxl, max_xlog_location)
+    else:
+        error_status = "CRITICAL"
+        error_message = "Monitoring does not have record if master ever existed."
+        error_description = "Supposed action: Check cluster state. Ignore this message if cluster is staring up" \
+                            "Stats: [ " \
+                            "last_master_appearance: {}, " \
+                            "last_master_xlog_location: {}, " \
+                            "max_xlog_location: {} ]" \
+            .format(lma, lmxl, max_xlog_location)
+    return error_status, error_message, error_description
+
+
+def __get_pg_cluster_status(pgcluster, pod_data, temp_metrics):
+    pod_identities = list(safe_get(temp_metrics, ["pg.{}".format(pgcluster), "nodes"], {}).keys())
+    nodes = list(safe_get(temp_metrics, ["pg.{}".format(pgcluster), "nodes"], {}).values())
+    logger.debug("Process data from pods - get working nodes count")
+    working_nodes = len([x for x in nodes if safe_get(x, ["pg_metrics", "running"]) == 1])
+    logger.debug("Process data from pods - collect info about master")
+    masters = [x for x in nodes if safe_get(x, ["pod", "role"]) == "master"]
+    replicas = [x for x in nodes if safe_get(x, ["pod", "role"]) == "replica"]
+    replica_names = [x["pod"]["name"] for x in replicas]
+    master_present = len(masters) > 0
+    master = masters[0] if master_present else None
+    logger.debug("Process master pod {}", master)
+    master_name = safe_get(master, ["pod", "name"], default="")
+    leader_role = get_leader_pod(pgcluster)
+    standby_cluster = True if (safe_get(leader_role, ["name"], default="") == master_name) \
+                              and (safe_get(leader_role, ["role"], default="") == "standby_leader") else False
+
+    error_status = "None"
+    error_message = "None"
+    error_description = "None"
+    if master:
+        logger.debug("Master found. Starting smoketests")
+        monitoring_user = get_monitoring_user()
+        monitoring_password = get_monitoring_password()
+        logger.info("Starting smoketests on behalf of {} user"
+                    .format(monitoring_user))
+
+        # todo[dmsh] add port
+        conn_string = f"host='{get_host()}' dbname='postgres' " \
+                      f"user='{monitoring_user}' " \
+                      f"password='{monitoring_password}' " \
+                      "connect_timeout=3 options='-c " \
+                      "statement_timeout=3000'"
+
+        enrich_master_metrics(conn_string, master, pgcluster, not standby_cluster, replica_names)
+        store_last_master_appearance(master)
+    else:
+        error_status, error_message, error_description = __calculate_pg_cluster_status_for_missing_master(pod_data,
+                                                                                                          nodes)
+
+    actual_nodes = len([x for x in pod_identities if __is_xlog_location_actual(x, safe_get(temp_metrics,
+                                                                                           ["pg.{}".format(pgcluster),
+                                                                                            "nodes", x], {}))])
+    master_writable = (safe_get(master, ["pg_metrics", "writable"], False))
+    # check standby cluster settings here and proceed with OK to prevent metrics rewriting.
+    # It's expected that standby_leader is not writable.
+    if (master_writable or standby_cluster) and working_nodes == DEFAULT_CLUSTER_PG_NODE_QTY and actual_nodes == DEFAULT_CLUSTER_PG_NODE_QTY:
+        cluster_state = "OK"
+        cluster_status_code = 0
+    elif master_writable or standby_cluster:
+        cluster_state = "DEGRADED"
+        cluster_status_code = 6
+        if working_nodes != DEFAULT_CLUSTER_PG_NODE_QTY:
+            error_status = "WARNING"
+            error_message = "One or more replicas does not have running postgresql."
+            error_description = "Supposed action: Check logs and follow troubleshooting guide."
+        else:
+            error_status = "WARNING"
+            error_message = "One or more replicas cannot start replication."
+            error_description = "Supposed action: Check logs and follow troubleshooting guide."
+    else:
+        cluster_state = "ERROR"
+        cluster_status_code = 10
+        if master:
+            error_status, error_message, error_description = __calculate_pg_cluster_status_for_missing_master(pod_data,
+                                                                                                              nodes)
+
+    cluster_status = {
+        "nodes": len(nodes),
+        "working_nodes": working_nodes,
+        "actual_nodes": actual_nodes,
+        "master_writable": master_writable,
+        "state": cluster_state,
+        "status": cluster_status_code,
+        "active": not standby_cluster,
+        "error_status": error_status,
+        "error_message": error_message,
+        "error_description": error_description,
+    }
+    if safe_get(master, ["pod"]):
+        cluster_status["master"] = safe_get(master, ["pod"])
+
+    logger.debug("Cluster status: {}".format(cluster_status))
+    return cluster_status
+
+
+def __get_endpoints_status(pgcluster, active):
+    logger.debug("Start endpoint check")
+
+    # todo[anin] configuration point
+    cluster_address = get_host()
+    logger.debug("Start check for {}".format(cluster_address))
+    cluster_metrics = []
+    monitoring_user = get_monitoring_user()
+    monitoring_password = get_monitoring_password()
+
+    # todo[dmsh] add port parameter
+    cluster_conn_string = \
+        f"host='{cluster_address}' dbname='postgres' " \
+        f"user='{monitoring_user}' " \
+        f"password='{monitoring_password}' " \
+        "connect_timeout=3 options='-c statement_timeout=3000'"
+
+    cluster_endpoint_status = execute_query(cluster_conn_string, "select 1")
+    if cluster_endpoint_status == 1:
+        if active:
+            logger.debug("Endpoint received connection. Starting smoke tests")
+            cluster_metrics = perform_smoketests(cluster_conn_string)
+        else:
+            logger.debug("Endpoint received connection. Cluster is in standby mode. Skip smoke tests")
+
+    else:
+        logger.debug("Cannot connect to endpoint")
+        _tags = default_tags.copy()
+        cluster_metrics.append(Metric(metric_name="endpoints.cluster.smoketest.passed", tags=_tags, fields={"value": 0}))
+        _tags.update({"action": "insert"})
+        cluster_metrics.append(Metric(metric_name="endpoints.cluster.smoketest.check", tags=_tags, fields={"value": 0}))
+        _tags.update({"action": "select"})
+        cluster_metrics.append(Metric(metric_name="endpoints.cluster.smoketest.check", tags=_tags, fields={"value": 0}))
+        _tags.update({"action": "update"})
+        cluster_metrics.append(Metric(metric_name="endpoints.cluster.smoketest.check", tags=_tags, fields={"value": 0}))
+        _tags.update({"action": "delete"})
+        cluster_metrics.append(Metric(metric_name="endpoints.cluster.smoketest.check", tags=_tags, fields={"value": 0}))
+
+    cluster_metrics.append(Metric(metric_name="endpoints.cluster.running", tags=default_tags, fields={"value": cluster_endpoint_status or 0}))
+    cluster_metrics.append(Metric(metric_name="endpoints.cluster.url", tags=default_tags, fields={"value": "\"{}\"".format(cluster_address)}))
+
+    convert_metrics(pgcluster, cluster_metrics)
+
+    return cluster_metrics
+
+def is_last_collection_expired(soft=False):
+    last_collection_time=getPodValue('general','last_collection_time',None)
+    return last_collection_time is None or time.time() - last_collection_time > (2 if soft else 1) * collection_timeout
+
+
+def collect_metrics_sync():
+    return collect_metrics(remote_host, remote_port, oc_project, pgcluster)
+
+
+def async_metric_collector():
+    logger.info("Started collector thread")
+    last_collection_data = getPodValue('general', 'last_collection_data', None)
+    last_collection_time = getPodValue('general', 'last_collection_time', None)
+    while True:
+        try:
+            if is_last_collection_expired():
+                with collection_lock:
+                    if is_last_collection_expired():
+                        try:
+                            logger.info("Start metric collection")
+                            last_collection_time = time.time()
+                            storePodValue('general','last_collection_time',last_collection_time)
+                            last_collection_data = collect_metrics_sync()
+                            storePodValue('general','last_collection_data',last_collection_data)
+                        except Exception as e:
+                            logger.exception("Cannot collect metrics")
+                            last_collection_data = "{}"
+                            storePodValue('general', 'last_collection_data', last_collection_data)
+                            last_collection_time = None
+                            storePodValue('general', 'last_collection_time', last_collection_time)
+            if last_collection_time:
+                sleep_time = collection_timeout - (time.time() - last_collection_time)
+                if sleep_time > 0:
+                    logger.debug("Looks like metrics is up to date. Sleep {} sec".format(sleep_time))
+                    time.sleep(sleep_time)
+                else:
+                    logger.debug("Looks like last check had taken too long. sleep_time is {} sec".format(sleep_time))
+        except Exception as e:
+            logger.exception("Something happen in main update loop")
+
+
+def collect_gpdb_metrics(pgcluster):
+    gpdb_collector = GpdbCollector(pgcluster)
+    gpdb_collector.collect()
+    return gpdb_collector.convert()
+
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser(description='metric collector for PGSQL')
+    parser.add_argument('--host', dest='host', default='localhost', help='address of openshift console')
+    parser.add_argument('--port', dest='port', default='8443', help='port of openshift console')
+    parser.add_argument('--project', dest='project', default=None, help='name of openshift project')
+    parser.add_argument('--pgcluster', dest='pgcluster', default="common", help='name of pgcluster')
+    parser.add_argument('--loglevel', dest='loglevel', default="info",
+                        choices=["info", "debug", None], help='log level. info/debug')
+    parser.add_argument('--shell-metrics-collector-executors', dest='shell_metrics_collector_executors', default=3,
+                        help='Defines amount of subprocesses which can collect shell metrics')
+
+    args = parser.parse_args()
+
+    remote_host = args.host
+    remote_port = int(args.port)
+    oc_project = args.project
+    pgcluster = args.pgcluster
+    loglevel = args.loglevel
+    shell_metrics_collector_executors = int(args.shell_metrics_collector_executors)
+
+    logger_level = logging.INFO
+    if loglevel and loglevel == "debug":
+        logger_level = logging.DEBUG
+
+    logging.basicConfig(
+        filename='/proc/1/fd/1',
+        filemode='w',
+        level=logger_level,
+        format='[%(asctime)s,%(msecs)03d][%(levelname)-5s][category=%(name)s]'
+               '[pid=%(process)d] %(message)s',
+        datefmt='%Y-%m-%dT%H:%M:%S'
+    )
+    logger = logging.getLogger("metric-collector")
+
+    collection_timeout = 30
+    collection_lock = threading.RLock()
+
+    with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:
+        token = f.read()
+
+    # load config from cluster (work only inside kubernetes)
+    # for local debug purposes use:
+    # oc_login(remote_host, remote_port, oc_project, token)
+    config.load_incluster_config()
+    metrics = []
+
+    if pgcluster == "gpdb":
+        logger.info("Type of the cluster is set to GPDB, "
+                    "start to collect Greenplum DB metrics")
+        data, metrics_list = collect_gpdb_metrics(pgcluster)
+        metrics.extend(metrics_list)
+    elif get_deployment_info(remote_host, remote_port, oc_project, token, "postgresql"):
+            DEFAULT_CLUSTER_PG_NODE_QTY = get_number_of_replicas_from_deployment(remote_host, remote_port, oc_project,
+                                                                         token, "postgresql")
+            logger.info("setting DEFAULT_CLUSTER_PG_NODE_QTY as {}, as a number of replicas from Deployment"
+                        .format(DEFAULT_CLUSTER_PG_NODE_QTY))
+            data, metrics = collect_metrics_sync()  # get metrics here
+    else:
+        dc_data = get_dc_info(remote_host, remote_port, oc_project, token)
+        dcs = safe_get(dc_data, ["items"], [])
+        patroni_nodes_dcs = [x for x in dcs if
+                             safe_get(x, ["metadata", "name"], "").startswith("pg-{}-node".format(pgcluster))]
+        patroni_nodes_dcs_num = len(list(patroni_nodes_dcs))
+        # check if dcs number not equals 0
+        if patroni_nodes_dcs_num != 0:
+            DEFAULT_CLUSTER_PG_NODE_QTY = patroni_nodes_dcs_num
+            logger.info("setting DEFAULT_CLUSTER_PG_NODE_QTY as {}, as a number of DCs".format(DEFAULT_CLUSTER_PG_NODE_QTY))
+        else:
+            # if dcs are not exists, try to collect number of replicas from statefulset
+            DEFAULT_CLUSTER_PG_NODE_QTY = get_number_of_replicas_from_statefulset(remote_host, remote_port, oc_project,
+                                                                                  token, "patroni")
+            logger.info("setting DEFAULT_CLUSTER_PG_NODE_QTY as {}, as a number of replicas from Statefulset"
+                        .format(DEFAULT_CLUSTER_PG_NODE_QTY))
+        data, node_metrics = collect_metrics_sync()
+        metrics.extend(node_metrics)
+
+    tags = default_tags.copy()
+
+    points = linearizeJson(data)
+    points = mapStatuses(points)
+
+    tag = ",".join(["{}={}".format(t, tags[t]) for t in tags])
+
+    convert_metrics("patroni", metrics)
+
+    for key in sorted(points):
+        value = get_influxdb_value(key, points[key])
+        print("ma.{},{} value={}".format(key, tag, value))
+        if loglevel == "debug":
+            logger.info(("ma.{},{} value={}".format(key, tag, value)))
+
+else:
+    raise Exception("Please run as main script. For more details about script parameters try --help option.")
diff --git a/docker-monitoring-agent/scripts/metric.py b/docker-monitoring-agent/scripts/metric.py
new file mode 100644
index 0000000..87f34e5
--- /dev/null
+++ b/docker-monitoring-agent/scripts/metric.py
@@ -0,0 +1,50 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+
+default_tags = {
+    "namespace": os.environ['NAMESPACE'],
+    "pod_name": os.environ['HOSTNAME'],
+    "selector": "health",
+    "service_name": "pg-common-collector",
+}
+
+
+class Metric:
+    """
+    This is a simple wrapper class for metrics
+    """
+
+    def __init__(self, metric_name, tags, fields):
+        self.metric_name = metric_name
+        self._tags = tags.copy()
+        self._fields = fields.copy()
+
+    @property
+    def fields(self):
+        return ",".join(
+            ["{}={}".format(t, self._fields[t]) for t in self._fields])
+
+    @property
+    def tags(self):
+        return ",".join(["{}={}".format(t, self._tags[t]) for t in self._tags])
+
+    def get_field_value(self, field, default):
+        return self._fields.get(field, default)
+
+
+def convert_metrics(cluster_name, metrics):
+    for metric in metrics:
+        print(f"ma.pg.{cluster_name}.{metric.metric_name},{metric.tags} {metric.fields}")
diff --git a/docker-monitoring-agent/scripts/metrics_query_utils.py b/docker-monitoring-agent/scripts/metrics_query_utils.py
new file mode 100644
index 0000000..f7a7c31
--- /dev/null
+++ b/docker-monitoring-agent/scripts/metrics_query_utils.py
@@ -0,0 +1,97 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+
+logger = logging.getLogger("metric-collector")
+
+METRICS_10 = {
+    "replication_data": """select 
+                        usename, application_name, client_addr, state, sync_priority, sync_state,
+                        pg_wal_lsn_diff(sent_lsn, replay_lsn)::bigint as sent_replay_lag,
+                        (case pg_is_in_recovery() when 't' then pg_wal_lsn_diff(pg_last_wal_receive_lsn(), sent_lsn) else pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn)end)::bigint as sent_lag
+                        from pg_stat_replication WHERE usename = 'replicator'
+                        """,
+    "replication_slots": """select 
+                        slot_name,
+                        pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_wal_replay_lsn()
+                                                        ELSE pg_current_wal_lsn()
+                                                   END, restart_lsn)::bigint as restart_lsn_lag,
+                        pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_wal_replay_lsn()
+                                                        ELSE pg_current_wal_lsn()
+                                                   END, confirmed_flush_lsn)::bigint as confirmed_flush_lsn_lag
+                        from pg_replication_slots""",
+    "xlog_location":    """SELECT pg_wal_lsn_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_wal_replay_lsn()
+                                                        ELSE pg_current_wal_lsn()
+                                                   END, '0/0')::bigint
+                                                   """,
+    "pg_xlog":          "/var/lib/pgsql/data/postgresql_{}/pg_wal"
+}
+
+METRICS_9 = {
+    "replication_data": """select 
+                        usename, application_name, client_addr, state, sync_priority, sync_state,
+                        pg_xlog_location_diff(sent_location, replay_location)::bigint as sent_replay_lag,
+                        pg_xlog_location_diff(pg_current_xlog_location(), sent_location)::bigint as sent_lag
+                        from pg_stat_replication WHERE usename = 'replicator'
+                        """,
+    "replication_slots": """select 
+                        slot_name,
+                        pg_xlog_location_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_xlog_replay_location()
+                                                        ELSE pg_current_xlog_location()
+                                                   END, restart_lsn)::bigint as restart_lsn_lag,
+                        pg_xlog_location_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_xlog_replay_location()
+                                                        ELSE pg_current_xlog_location()
+                                                   END, confirmed_flush_lsn)::bigint as confirmed_flush_lsn_lag
+                        from pg_replication_slots""",
+    "xlog_location":    """SELECT pg_xlog_location_diff(CASE WHEN pg_is_in_recovery()
+                                                        THEN pg_last_xlog_replay_location()
+                                                        ELSE pg_current_xlog_location()
+                                                   END, '0/0')::bigint""",
+    "pg_xlog":          "/var/lib/pgsql/data/postgresql_{}/pg_xlog"
+}
+
+
+def get_common_pg_metrics_by_version(version):
+    return {
+        "running":                  "select 1",
+        "db_count":                 "select count(*) from pg_stat_database",
+        "current_connections":      "select count(*) from pg_stat_activity",
+        "locks":                    "select count(*) from pg_locks",
+        "locks_not_granted":        "select count(*) from pg_locks where NOT GRANTED",
+        "replication_connections":  "select count(*) from pg_stat_replication",
+        "replication_slots_count":  "select count(*) from pg_replication_slots",
+        "xlog_location":            get_metrics_type_by_version("xlog_location", version),
+        "pg_is_in_recovery":        "SELECT pg_is_in_recovery()",
+        "xact_sum":                 "select sum(xact_commit + xact_rollback) :: bigint from pg_stat_database",
+        "query_max_time":           "SELECT COALESCE(trunc(extract(epoch from (now() - min(query_start)))), 0) FROM pg_stat_activity WHERE state='active' AND usename != 'replicator' AND wait_event_type ||'.'|| wait_event != 'Client.WalSenderWaitForWAL'",
+    }
+
+
+def __is_version_below_10(version):
+    return version < [10, 0]
+
+
+def get_metrics_type_by_version(type, version):
+    logger.debug("Getting metrics of type: {} for pgsql version: {}".format(type, version))
+    version_check = __is_version_below_10(version)
+    if version_check:
+        return METRICS_9[type]
+    else:
+        return METRICS_10[type]
diff --git a/docker-monitoring-agent/scripts/monitoring/additional_metrics_wrapper.sh b/docker-monitoring-agent/scripts/monitoring/additional_metrics_wrapper.sh
new file mode 100755
index 0000000..1a2a4ac
--- /dev/null
+++ b/docker-monitoring-agent/scripts/monitoring/additional_metrics_wrapper.sh
@@ -0,0 +1,16 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+[ "$METRICS_PROFILE" == dev ] && /monitor/metrics --perf_metrics=true
\ No newline at end of file
diff --git a/docker-monitoring-agent/scripts/monitoring/pgpool.py b/docker-monitoring-agent/scripts/monitoring/pgpool.py
new file mode 100644
index 0000000..ff9ffe6
--- /dev/null
+++ b/docker-monitoring-agent/scripts/monitoring/pgpool.py
@@ -0,0 +1,72 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import psycopg2
+
+from scripts.collector_utils import get_host, get_port
+
+
+def main():
+    monitoring_pw = os.environ.get("MONITORING_PASSWORD", "monitoring_role")
+    monitoring_role = os.environ.get("MONITORING_USER", "monitoring_role")
+
+    conn_properties = dict(
+        host=get_host(), port=get_port(), user=monitoring_role,
+        password=monitoring_pw, database='postgres'
+    )
+    result_data = {"status": "UP"}
+
+    nodes = {}
+    pool_nodes_info = []
+
+    try:
+        pool_nodes_info = get_pool_nodes(get_connection_properties(monitoring_role, monitoring_pw))
+    except Exception as e:
+
+        return
+
+    for node in pool_nodes_info:
+        if node[3] != 'up':
+            result_data.update({"status": "PROBLEM"})
+        nodes.update({
+            node[1]: {
+                "status": node[3],
+                "select_count": node[6]
+            }})
+
+    result_data.update({"pg-pool": nodes})
+
+    print(result_data)
+
+
+
+def get_connection_properties(user, password):
+    conn_string = f"host='{get_host()}' dbname='postgres' " \
+                  f"user='{user}' " \
+                  f"password='{password}' " \
+                  f"port='{get_port()}' "
+    return conn_string
+
+
+
+def get_pool_nodes(conn_properties):
+    with psycopg2.connect(**conn_properties) as conn:
+        with conn.cursor() as cur:
+            cur.execute('show pool_nodes;')
+            return cur.fetchall()
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-monitoring-agent/scripts/oc_utils.py b/docker-monitoring-agent/scripts/oc_utils.py
new file mode 100644
index 0000000..9217585
--- /dev/null
+++ b/docker-monitoring-agent/scripts/oc_utils.py
@@ -0,0 +1,185 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import base64
+import json
+import logging
+
+import re
+import requests
+import time
+
+from kubernetes.client import configuration
+
+logger = logging.getLogger("metric-collector")
+
+
+def is_ipv4(host):
+    p = re.compile("^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
+    return p.match(host)
+
+
+def get_host_url(remote_host, remote_port):
+    if is_ipv4(remote_host):
+        return "https://{}:{}".format(remote_host, remote_port)
+    else:
+        return "https://[{}]:{}".format(remote_host, remote_port)
+
+
+def get_configmap(remote_host, remote_port, oc_project, token, name):
+    start_time = time.time()
+    pod_data_url = "{}/api/v1/namespaces/{}/configmaps/{}" \
+        .format(get_host_url(remote_host, remote_port), oc_project, name)
+    response = requests.get(
+        pod_data_url,
+        headers={"Authorization": "Bearer {}".format(token)},
+        verify="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")
+    logger.debug("Load configmap {} info time: {}".format(name, (time.time() - start_time)))
+    if response.status_code == 404:
+        return None
+    if response.status_code >= 400:
+        raise Exception("Cannot get data for configmap {}".format(name))
+    cm_data = json.loads(response.text)
+    logger.debug("Collected info about configmap {}: {}".format(name, cm_data))
+    return cm_data
+
+
+def get_dc_info(remote_host, remote_port, oc_project, token):
+    start_time = time.time()
+    import os
+    if os.environ.get("PATRONI_ENTITY_TYPE"):
+        api_url = "{}/apis/apps/v1/namespaces/{}/deployments"
+    else:
+        api_url = "{}/oapi/v1/namespaces/{}/deploymentconfigs"
+    dc_data_url = api_url.format(get_host_url(remote_host, remote_port), oc_project)
+    response = requests.get(dc_data_url,
+                            headers={"Authorization": "Bearer {}".format(token)},
+                            verify="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")
+    logger.debug("Load dc info time: {}".format((time.time() - start_time)))
+    if response.status_code >= 400:
+        raise Exception("Cannot collect dc data with code {}: {}".format(response.status_code, response.text))
+    dcs_data = json.loads(response.text)
+    logger.debug("Collected info about dc: {}".format(dcs_data))
+    return dcs_data
+
+
+def get_pods_info(remote_host, remote_port, oc_project, token, selector, selector_value):
+    start_time = time.time()
+    get_pods_data_url = "{}/api/v1/namespaces/{}/pods?labelSelector={}={}" \
+        .format(get_host_url(remote_host, remote_port), oc_project, selector, selector_value)
+    response = requests.get(
+        get_pods_data_url,
+
+        headers={"Authorization": "Bearer {}".format(token)},
+        verify="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")
+    logger.debug("Load pods info time for {}={}: {}".format(selector, selector_value, (time.time() - start_time)))
+    if response.status_code >= 400:
+        raise Exception("Cannot collect pods data with code {}: {}".format(response.status_code, response.text))
+    pods_data = json.loads(response.text)
+    logger.debug("Collected info about {}={} pods: {}".format(selector, selector_value, pods_data))
+    return pods_data
+
+
+def get_auth_token(remote_host, remote_port, oc_username, oc_password):
+    auth_url = "{}/oauth/authorize?client_id=openshift-challenging-client&response_type=token" \
+        .format(get_host_url(remote_host, remote_port))
+    response = requests.get(
+        auth_url,
+        headers={
+            "Authorization": "Basic {}".format(base64.b64encode("{}:{}".format(oc_username, oc_password))),
+            "X-CSRF-Token": "1",
+        },
+        verify=False,
+        allow_redirects=False
+    )
+    location_header = response.headers["Location"]
+    m = re.search('access_token=([a-zA-Z0-9-_]+)', location_header)
+    token = m.group(1)
+    return token
+
+
+def get_configmaps(remote_host, remote_port, oc_project, token):
+    start_time = time.time()
+    api_data_url = "{}/api/v1/namespaces/{}/configmaps/" \
+        .format(get_host_url(remote_host, remote_port), oc_project)
+    response = requests.get(
+        api_data_url,
+        headers={"Authorization": "Bearer {}".format(token)},
+        verify="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt")
+    logger.debug("Load configmaps info time: {}".format((time.time() - start_time)))
+    if response.status_code == 404:
+        return None
+    if response.status_code >= 400:
+        raise Exception("Cannot get data for configmaps ")
+    cm_data = json.loads(response.text)
+    logger.debug("Collected info about configmaps: {}".format(cm_data))
+    return cm_data
+
+
+def get_statefulset_info(remote_host, remote_port, oc_project, token, statefulset_name):
+    api_data_url = "{}/apis/apps/v1/namespaces/{}/statefulsets/{}" \
+        .format(get_host_url(remote_host, remote_port), oc_project, statefulset_name)
+    response = requests.get(
+        api_data_url,
+        headers={"Authorization": "Bearer {}".format(token)},
+        verify="/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
+    )
+    if response.status_code == 404:
+        return None
+    if response.status_code >= 400:
+        raise Exception("Cannot get data for statefulset")
+    cm_data = json.loads(response.text)
+    logger.debug("Collected info about statefulset: {}".format(cm_data))
+    return cm_data
+
+
+def get_deployment_info(remote_host, remote_port, oc_project, token, deployment_name):
+    deployments = get_dc_info(remote_host, remote_port, oc_project, token)
+    deployment = next(filter(lambda d: d['metadata']['name'] == deployment_name, deployments.get('items', [])), None)
+    logger.debug("deployment with name {}\n{}".format(deployment_name, deployment))
+    return deployment if deployment else None
+
+
+def get_number_of_replicas_from_statefulset(remote_host, remote_port, oc_project, token, statefulset_name):
+    statefulset_info = get_statefulset_info(remote_host, remote_port, oc_project, token, statefulset_name)
+    return statefulset_info["spec"]["replicas"]
+
+
+def get_number_of_replicas_from_deployment(remote_host, remote_port, oc_project, token, deployment_name):
+    dc_info = get_deployment_info(remote_host, remote_port, oc_project, token, deployment_name)
+    if dc_info:
+        return dc_info.get('spec', {}).get('replicas', 0)
+    else:
+        return 0
+
+def unavailable_replicas_from_deployment(remote_host, remote_port, oc_project, token, deployment_name):
+    dc_info = get_deployment_info(remote_host, remote_port, oc_project, token, deployment_name)
+    if dc_info:
+        return dc_info.get('status', {}).get('unavailableReplicas', 0)
+    else:
+        return 0
+
+def oc_login(remote_host, remote_port, oc_project, token):
+    os_config = configuration.Configuration()
+    os_config.verify_ssl = False
+    os_config.assert_hostname = False
+    os_config.host = get_host_url(remote_host, remote_port)
+    os_config.api_key = {"authorization": "Bearer " + token}
+    configuration.Configuration.set_default(os_config)
+
+
+def load_file(path):
+    with open(path, 'r') as f:
+        file = f.read()
+    return file
\ No newline at end of file
diff --git a/docker-monitoring-agent/scripts/pod_utils.py b/docker-monitoring-agent/scripts/pod_utils.py
new file mode 100644
index 0000000..bad484a
--- /dev/null
+++ b/docker-monitoring-agent/scripts/pod_utils.py
@@ -0,0 +1,58 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+from m_utils import safe_get
+
+logger = logging.getLogger("metric-collector")
+
+
+def determine_role(pod):
+    """
+    Return role of pod. Role determined by pgtype label on pod
+    :type pod dict
+    """
+    pod_labels = safe_get(pod, ["metadata", "labels"], {})
+    if "pgtype" in pod_labels:
+        return "master" if pod_labels["pgtype"] == "master" else "replica"
+    return "replica"
+
+
+def get_container_image_from_any_pod(pods_data):
+    """
+    Get first pod from list and tries to get image parameter from first container.
+    Returns image or None
+    """
+    pod = safe_get(pods_data, ["items", 0], None)
+    if pod:
+        return safe_get(pod, ["spec", "containers", 0, "image"], None)
+    return None
+
+
+def get_env_value_from_pod(pod, name, default):
+    """
+    Get first pod from list and tries to get specified env variable.
+    Returns variable value or default
+    """
+    return safe_get(
+        [x for x in safe_get(pod, ["spec", "containers", 0, "env"], []) if safe_get(x, ["name"]) == name],
+        [0, "value"], default)
+
+
+def get_env_value_from_any_pod(pods_data, name, default):
+    """
+    Get first pod from list and tries to get specified env variable.
+    Returns variable value or default
+    """
+    return get_env_value_from_pod(safe_get(pods_data, ["items", 0], {}), name, default)
diff --git a/docker-monitoring-agent/scripts/telegraf/preparation_script.py b/docker-monitoring-agent/scripts/telegraf/preparation_script.py
new file mode 100644
index 0000000..6fa939c
--- /dev/null
+++ b/docker-monitoring-agent/scripts/telegraf/preparation_script.py
@@ -0,0 +1,122 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import logging
+
+SA_PATH = "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+
+cluster_name = os.environ.get("PGCLUSTER", "common")
+pg_port = os.environ.get("PGPORT", 5432)
+monitoring_role = '"' + os.environ.get("MONITORING_USER", "monitoring_role") + '"'
+monitoring_pw = os.environ.get("MONITORING_PASSWORD", "monitoring_password")
+
+queries = [
+    "DROP TABLE if exists monitor_test",
+    f"CREATE ROLE {monitoring_role} with login password '{monitoring_pw}'",
+    f"ALTER ROLE {monitoring_role} with login password '{monitoring_pw}'",
+    f"GRANT pg_monitor to {monitoring_role}",
+    f"GRANT pg_read_all_data to {monitoring_role}",
+    "CREATE EXTENSION if not exists pg_stat_statements",
+    f"GRANT CREATE ON SCHEMA public TO {monitoring_role}",
+    "DROP SEQUENCE if exists monitor_test_seq"
+]
+
+secured_views = ['pg_stat_replication', 'pg_stat_statements', 'pg_database', 'pg_stat_activity']
+
+func_template = """
+    DROP FUNCTION IF EXISTS func_{0}();
+"""
+
+logging.basicConfig(
+    filename='/proc/1/fd/1',
+    filemode='w',
+    level=logging.INFO,
+    format='[%(asctime)s,%(msecs)03d][%(levelname)-5s][category=%(name)s]'
+           '[pid=%(process)d] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+
+logger = logging.getLogger()
+
+
+def main():
+    logger.info("Will run preparation scripts")
+    pg_user, pg_password = get_postgres_credentials()
+    import psycopg2
+    conn = psycopg2.connect(get_connection_properties(pg_user, pg_password))
+    conn.autocommit = True
+    with conn.cursor() as cursor:
+        def execute_silently(query_):
+            logger.debug("Executing next query: {}".format(query_))
+            try:
+                cursor.execute(query_)
+            except psycopg2.Error:
+                logger.exception(
+                    "Exception happened during execution of the query")
+
+        list(map(execute_silently, queries))
+        list(map(execute_silently, [func_template.format(view) for view in secured_views]))
+    conn.close()
+    logger.info("Run of preparation scripts has been completed")
+
+
+def get_postgres_credentials():
+    from kubernetes import config
+    from kubernetes.client.apis import core_v1_api
+    from kubernetes.client.rest import ApiException
+    config.load_incluster_config()
+    api = core_v1_api.CoreV1Api()
+    namespace = open(SA_PATH).read()
+    try:
+        user = os.getenv("PG_ROOT_USER")
+        password = os.getenv("PG_ROOT_PASSWORD")
+        if user or password:
+            return user, password
+        secret_name = os.environ.get("POSTGRESQL_CREDENTIALS", f"{cluster_name}-pg-root-credentials")
+        api_response = api.read_namespaced_secret(secret_name, namespace)
+        import base64
+        data = api_response.data
+        password = base64.b64decode(data.get("password")).decode('utf-8')
+        user_data = data.get("user")
+        if not user_data:
+            user_data = data.get("username")
+        user = base64.b64decode(user_data).decode('utf-8')
+        return user, password
+    except ApiException as exc:
+        logger.error(exc)
+        raise exc
+
+
+def get_connection_properties(user, password):
+    conn_string = f"host='{get_host()}' dbname='postgres' " \
+                  f"user='{user}' " \
+                  f"password='{password}' " \
+                  f"port='{get_port()}' "
+    return conn_string
+
+
+def get_host():
+    cluster_name = os.getenv('PGCLUSTER', 'patroni')
+    namespace = os.getenv('NAMESPACE', 'postgres-service')
+    host = os.getenv('POSTGRES_HOST', f'pg-{cluster_name}.{namespace}')
+    return host
+
+
+def get_port():
+    return int(os.getenv('POSTGRES_PORT', 5432))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-monitoring-agent/scripts/telegraf/telegraf.conf b/docker-monitoring-agent/scripts/telegraf/telegraf.conf
new file mode 100644
index 0000000..91e9548
--- /dev/null
+++ b/docker-monitoring-agent/scripts/telegraf/telegraf.conf
@@ -0,0 +1,107 @@
+[global_tags]
+  namespace = '$NAMESPACE'
+# Configuration for telegraf agent
+[agent]
+  ## Default data collection interval for all inputs
+  interval = "$METRIC_COLLECTION_INTERVAL"
+  ## Rounds collection interval to 'interval'
+  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
+  round_interval = true
+
+  ## Telegraf will send metrics to outputs in batches of at
+  ## most metric_batch_size metrics.
+  # metric_batch_size = 1000
+  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
+  ## output, and will flush this buffer on a successful write. Oldest metrics
+  ## are dropped first when this buffer fills.
+  # metric_buffer_limit = 10000
+
+  ## Collection jitter is used to jitter the collection by a random amount.
+  ## Each plugin will sleep for a random time within jitter before collecting.
+  ## This can be used to avoid many plugins querying things like sysfs at the
+  ## same time, which can have a measurable effect on the system.
+  # collection_jitter = "0s"
+
+  ## Default flushing interval for all outputs. You shouldn't set this below
+  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
+  flush_interval = "60s"
+  ## Jitter the flush interval by a random amount. This is primarily to avoid
+  ## large write spikes for users running a large number of telegraf instances.
+  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
+  # flush_jitter = "0s"
+
+  ## Run telegraf in debug mode
+  debug = $INFLUXDB_DEBUG
+  ## Run telegraf in quiet mode
+  quiet = false
+
+  # Override default hostname, if empty use os.Hostname()
+  # hostname = "system-monitor"
+
+  ## If set to true, do no set the "host" tag in the telegraf agent.
+  # omit_hostname = false
+
+# Configuration for sending metrics to InfluxDB
+[[outputs.influxdb]]
+  ## The full HTTP or UDP URL for your InfluxDB instance.
+  ##
+  ## Multiple URLs can be specified for a single cluster, only ONE of the
+  ## urls will be written to each interval.
+  urls = ["$INFLUXDB_URL"]
+
+  ## The target database for metrics; will be created as needed.
+  database = "$INFLUXDB_DATABASE"
+
+  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using
+  ## Telegraf with a user without permissions to create databases or when the
+  ## database already exists.
+  skip_database_creation = false
+
+  ## Name of existing retention policy to write to.  Empty string writes to
+  ## the default retention policy.  Only takes effect when using HTTP.
+  # retention_policy = ""
+
+  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all".
+  ## Only takes effect when using HTTP.
+  # write_consistency = "any"
+
+  ## Timeout for HTTP messages.
+  timeout = "30s"
+
+  ## HTTP Basic Auth
+  username = "$INFLUXDB_USER"
+  password = "$INFLUXDB_PASSWORD"
+
+  ## HTTP User-Agent
+  # user_agent = "telegraf"
+
+  ## UDP payload size is the maximum packet size to send.
+  # udp_payload = 512
+
+  ## Optional TLS Config for use on HTTP connections.
+  # tls_ca = "/etc/telegraf/ca.pem"
+  # tls_cert = "/etc/telegraf/cert.pem"
+  # tls_key = "/etc/telegraf/key.pem"
+  ## Use TLS but skip chain & host verification
+  # insecure_skip_verify = false
+
+  ## HTTP Proxy override, if unset values the standard proxy environment
+  ## variables are consulted to determine which proxy, if any, should be used.
+  # http_proxy = "http://corporate.proxy:3128"
+
+  ## Additional HTTP headers
+  # http_headers = {"X-Special-Header" = "Special-Value"}
+
+  ## HTTP Content-Encoding for write request body, can be set to "gzip" to
+  ## compress body or "identity" to apply no encoding.
+  # content_encoding = "identity"
+
+  ## When true, Telegraf will output unsigned integers as unsigned values,
+  ## i.e.: "42u".  You will need a version of InfluxDB supporting unsigned
+  ## integer values.  Enabling this option will result in field type errors if
+  ## existing data has been written.
+  # influx_uint_support = false
+
+
+
+
diff --git a/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.metric.conf b/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.metric.conf
new file mode 100644
index 0000000..d899953
--- /dev/null
+++ b/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.metric.conf
@@ -0,0 +1,15 @@
+[[inputs.exec]]
+  commands = [
+    "/monitor/metric-collector.py --host=${KUBERNETES_SERVICE_HOST} --port=${KUBERNETES_SERVICE_PORT} --project=${NAMESPACE} --pgcluster=${PGCLUSTER} --loglevel=${LOGLEVEL} --shell-metrics-collector-executors=${SHELL_METRICS_COLLECTOR_EXECUTORS}",
+    "/monitor/metrics"
+  ]
+  timeout = "${TELEGRAF_PLUGIN_TIMEOUT}s"
+  data_format = "influx"
+
+  [[inputs.exec]]
+  commands = [
+    "bash /monitor/scripts/additional_metrics_wrapper.sh"
+  ]
+  timeout = "${DEV_METRICS_TIMEOUT}m"
+  interval = "${DEV_METRICS_INTERVAL}m"
+  data_format = "influx"
\ No newline at end of file
diff --git a/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.ura.conf b/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.ura.conf
new file mode 100644
index 0000000..ec654b5
--- /dev/null
+++ b/docker-monitoring-agent/scripts/telegraf/telegraf.d/exec.ura.conf
@@ -0,0 +1,15 @@
+[[inputs.exec]]
+  #name_suffix = "_ura"
+  ## Commands array
+  commands = [
+    "python /monitor/ura-collector.py"
+  ]
+
+  ## Timeout for each command to complete.
+  timeout = "${TELEGRAF_PLUGIN_TIMEOUT}s"
+
+  ## Data format to consume.
+  ## Each data format has its own unique set of configuration options, read
+  ## more about them here:
+  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
+  data_format = "influx"
diff --git a/docker-monitoring-agent/scripts/telegraf/telegraf.sh b/docker-monitoring-agent/scripts/telegraf/telegraf.sh
new file mode 100755
index 0000000..43c8693
--- /dev/null
+++ b/docker-monitoring-agent/scripts/telegraf/telegraf.sh
@@ -0,0 +1,42 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+export METRICS_PROFILE=${METRICS_PROFILE:-prod}
+
+# for DEV metrics all of the configuration vars should be specified in minutes
+export DEV_METRICS_TIMEOUT=${DEV_METRICS_TIMEOUT:-5}
+export DEV_METRICS_INTERVAL=${DEV_METRICS_INTERVAL:-5}
+
+set -e
+
+python /telegrafwd/preparation_script.py || true
+
+# substitute environment variables
+echo "Applying environment variables to Telegraf configs"
+for f in /etc/telegraf/telegraf.templates/*.conf; do envsubst < "${f}" > /etc/telegraf/telegraf.d/$(basename ${f}) ; done
+
+envsubst < "${TELEGRAF_CONFIG_TEMP}" > "${TELEGRAF_CONFIG}"
+
+echo "Telegraf config file after applying envs:"
+cat "${TELEGRAF_CONFIG}" | grep -vE '\s*#' | grep -v '^$' | grep -v 'password'
+
+METRICS_PROFILE=`echo "print('$METRICS_PROFILE'.lower())" | python`
+echo "Metrics profile is set to $METRICS_PROFILE."
+
+# showing info about timeouts and intervals for DEV metrics if profile is set to dev
+[ "$METRICS_PROFILE" == dev ] && echo "DEV_METRICS_TIMEOUT: $DEV_METRICS_TIMEOUT DEV_METRICS_INTERVAL: $DEV_METRICS_INTERVAL"
+
+exec env METRICS_PROFILE=${METRICS_PROFILE} telegraf --config-directory /etc/telegraf/telegraf.d
diff --git a/docker-monitoring-agent/scripts/ura-collector.py b/docker-monitoring-agent/scripts/ura-collector.py
new file mode 100644
index 0000000..2461820
--- /dev/null
+++ b/docker-monitoring-agent/scripts/ura-collector.py
@@ -0,0 +1,160 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from datetime import datetime
+import urllib.request, urllib.error, urllib.parse
+import json
+import os
+from collector_utils import linearizeJson, get_influxdb_value, mapStatuses
+from oc_utils import get_pods_info, get_configmaps
+from prometheus_client.parser import text_string_to_metric_families
+
+import logging
+
+
+class ConfigMapCollector(object):
+    SA_TOKEN_PATH = '/var/run/secrets/kubernetes.io/serviceaccount/token'
+
+    def __init__(self, remote_host, remote_port, namespace):
+        with open(ConfigMapCollector.SA_TOKEN_PATH) as f:
+            self.token = f.read()
+
+        self.remote_host = remote_host
+        self.remote_port = remote_port
+        self.namespace = namespace
+        self.log = logging.getLogger("ura-collector")
+
+    def get_config_maps(self):
+        return get_configmaps(self.remote_host, self.remote_port, self.namespace, self.token)
+
+    def collect(self):
+        cms = get_configmaps(self.remote_host, self.remote_port, self.namespace, self.token)
+        for config_map in cms['items']:
+            if config_map['metadata']['name'].endswith("collector-config"):
+                self.log.info(f"start to process cm: "
+                              f"{config_map['metadata']['name']}")
+                self.process_config_map(config_map['data'])
+
+    def process_config_map(self, data):
+        for key in data:
+            json_config = json.loads(data[key])
+            for module_ in json_config:
+                json_data = {}
+
+                parameters = module_.get('parameters')
+                type_ = parameters.get('type', 'url')
+                metrics_type = parameters.get('metrics_type', 'json')
+
+                if type_ == 'url':
+                    url = parameters.get('url')
+                    try:
+                        json_data = json.loads(self.process_url_collect(url)) \
+                            if metrics_type == 'json' else self.get_prometheus_metrics(url, module_)
+                    except Exception as exc:
+                        self.log.error(f"Something went wrong during collection of data via url {url}", exc)
+                        continue
+                elif type_ == 'script':
+                    path = parameters.get('path')
+                    json_data = self.process_script_collect(path)
+                    if not json_data:
+                        continue
+                self.handle_out_put_json(json_data, module_)
+
+    def get_prometheus_metrics(self, monitoring_url, module):
+        import requests
+        request = requests.get(monitoring_url)
+        request.encoding = 'utf-8'
+        data = request.text
+        res_data = {}
+        for family in text_string_to_metric_families(data):
+            for sample in family.samples:
+                key, tags, value = sample[0], sample[1], get_influxdb_value("", sample[2])
+                tags["namespace"] = self.namespace
+                tags["pod_name"] = self.get_pod_name(module)
+                tags["service_name"] = module['parameters']['service_name']
+                tag = ",".join(["{0}={1}".format(t, tags[t].replace(" ", "_")) for t in tags])
+                print(f"ma.{key},{tag} value={value}")
+        return res_data
+
+    @staticmethod
+    def process_url_collect(url):
+        contents = urllib.request.urlopen(url).read()
+        return contents.decode()
+
+    # kostyl production
+    @staticmethod
+    def process_script_collect(path):
+        import subprocess
+        proc = subprocess.Popen(['python', path], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
+        str = proc.communicate()[0].decode('UTF-8')
+        if not str:
+            return ""
+        return json.loads(str.replace("'", "\""))
+
+    def get_pod_name(self, module_):
+        pod_data = get_pods_info(
+            self.remote_host, self.remote_port, self.namespace, self.token,
+            module_['parameters']['selector']['key'],
+            module_['parameters']['selector']['value']
+        )
+        pod_name = json.dumps(pod_data['items'][0]['metadata']['name']).replace('"', '')
+        return pod_name
+
+    def handle_out_put_json(self, json_data, module_):
+        pod_name = self.get_pod_name(module_)
+        tags = {
+            "namespace": self.namespace,
+            "pod_name": pod_name,
+            "selector": module_['parameters']['output_selector'],
+            "service_name": module_['parameters']['service_name']
+        }
+
+        points = linearizeJson(json_data)
+        points = mapStatuses(points)
+
+        tag = ",".join([f"{t}={tags[t]}" for t in tags])
+
+        for key in points:
+            value = get_influxdb_value(key, points[key])
+            msg = f"ma.{key},{tag} value={value}"
+            self.log.debug("data to telegraf: {}".format(msg))
+            # output to stdout in influx-format metrics
+            print(msg)
+
+
+def main():
+    # logging setup
+    logging.basicConfig(
+        filename='/proc/1/fd/1',
+        filemode='w',
+        level=logging.INFO,
+        format='[%(asctime)s,%(msecs)03d][%(levelname)-5s][category=%(name)s]'
+               '[pid=%(process)d] %(message)s',
+        datefmt='%Y-%m-%dT%H:%M:%S'
+    )
+
+    debug = os.environ.get('DEBUG', 'false')
+    if debug == "true":
+        logging.getLogger().setLevel(logging.DEBUG)
+
+    collector = ConfigMapCollector(
+        remote_host=os.environ.get('KUBERNETES_SERVICE_HOST', 'localhost'),
+        remote_port=os.environ.get('KUBERNETES_SERVICE_PORT', '5432'),
+        namespace=os.environ.get('NAMESPACE', 'postgres-service')
+    )
+    collector.collect()
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-patroni/.gitignore b/docker-patroni/.gitignore
new file mode 100644
index 0000000..0da7671
--- /dev/null
+++ b/docker-patroni/.gitignore
@@ -0,0 +1,22 @@
+# To ignore oc config file.
+ansible/.tmp
+
+# For failed run artifacts.
+ansible/*.retry
+
+# Test results.
+ansible/tests
+
+# Pv cleaner results.
+ansible/pv_cleaner_results
+
+# Build output.
+target
+
+pg15/scripts
+pg15/docs
+pg15/maintenance
+
+pg16/scripts
+pg16/docs
+pg16/maintenance
\ No newline at end of file
diff --git a/docker-patroni/CODE-OF-CONDUCT.md b/docker-patroni/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-patroni/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-patroni/CONTRIBUTING.md b/docker-patroni/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-patroni/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-patroni/Dockerfile b/docker-patroni/Dockerfile
new file mode 100644
index 0000000..70c9a48
--- /dev/null
+++ b/docker-patroni/Dockerfile
@@ -0,0 +1,170 @@
+FROM ubuntu:22.04
+
+USER root
+
+ARG PG_VERSION=15  # Default to 15 if not provided
+
+ENV POD_IDENTITY="node1" \
+    PATRONI_TTL=60 \
+    PATRONI_LOOP_WAIT=10 \
+    PATRONI_RETRY_TIMEOUT=40 \
+    PATRONI_MAXIMUM_LAG_ON_FAILOVER=1048576 \
+    PATRONI_SYNCHRONOUS_MODE="false" \
+    PG_CLUST_NAME="common" \
+    PG_MAX_CONNECTIONS=200 \
+    PG_CONF_MAX_PREPARED_TRANSACTIONS=200 \
+    PATRONICTL_CONFIG_FILE="/patroni/pg_node.yml" \
+    PG_BIN_DIR="/usr/lib/postgresql/$PG_VERSION/bin/" \
+    POSTGRESQL_VERSION=$PG_VERSION \
+    LC_ALL=en_US.UTF-8 \
+    LANG=en_US.UTF-8 \
+    EDITOR=/usr/bin/vi \
+    PATH="/usr/lib/postgresql/$PG_VERSION/bin/:${PATH}"
+
+# Official CentOS repos contain libprotobuf-c 1.0.2, but decoderbufs require 1.1+, thus,
+# we craft a custom build of protobuf-c and publish it at this repo.
+# Remove this line after moving to the next CentOS releases.
+COPY scripts/archive_wal.sh /opt/scripts/archive_wal.sh
+ADD ./scripts/pip.conf /root/.pip/pip.conf
+COPY ./scripts/postgresql.conf /tmp/postgresql.conf
+COPY ./scripts/fix_permission.sh /usr/libexec/fix-permissions
+ADD ./scripts/* /
+
+RUN echo "deb [trusted=yes] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" >> /etc/apt/sources.list.d/pgdg.list
+RUN ls -la /etc/apt/
+RUN apt-get -y update
+RUN apt-get -o DPkg::Options::="--force-confnew" -y dist-upgrade
+RUN apt-get update && \
+    apt-get install -y --allow-downgrades gcc-12 cpp-12 gcc-12-base libgcc-12-dev libstdc++6 libgcc-s1 libnsl2
+RUN apt-get --no-install-recommends install -y python3.11 python3-pip python3-dev libpq-dev cython3 wget curl
+
+# rename 'tape' group to 'postgres' and creating postgres user - hask for ubuntu
+RUN groupmod -n postgres tape
+RUN adduser -uid 26 -gid 26 postgres
+
+# Install pgbackrest 2.55.1 from source
+RUN apt-get --no-install-recommends install -y \
+    build-essential \
+    libssl-dev \
+    libxml2-dev \
+    libpq-dev \
+    liblz4-dev \
+    libzstd-dev \
+    libbz2-dev \
+    libyaml-dev \
+    meson \
+    ninja-build \
+    pkg-config && \
+    cd /tmp && \
+    wget https://github.com/pgbackrest/pgbackrest/archive/release/2.55.1.tar.gz && \
+    tar -xzf 2.55.1.tar.gz && \
+    cd pgbackrest-release-2.55.1 && \
+    meson setup build && \
+    ninja -C build && \
+    ninja -C build install && \
+    cd / && \
+    rm -rf /tmp/pgbackrest-release-2.55.1 /tmp/2.55.1.tar.gz && \
+    apt-get purge -y --auto-remove \
+    build-essential \
+    libssl-dev \
+    libxml2-dev \
+    libpq-dev \
+    liblz4-dev \
+    libzstd-dev \
+    libbz2-dev \
+    libyaml-dev \
+    meson \
+    ninja-build \
+    pkg-config && \
+    apt-get clean && \
+    mkdir -p /var/lib/pgbackrest && \
+    mkdir -p /var/log/pgbackrest && \
+    mkdir -p /var/spool/pgbackrest
+
+ARG DEBIAN_FRONTEND=noninteractive
+RUN apt-get --no-install-recommends install -y postgresql-$PG_VERSION postgresql-contrib-$PG_VERSION postgresql-server-dev-$PG_VERSION postgresql-plpython3-$PG_VERSION postgresql-$PG_VERSION-hypopg postgresql-$PG_VERSION-powa postgresql-$PG_VERSION-orafce\
+    hostname gettext jq vim \
+    postgresql-$PG_VERSION-cron postgresql-$PG_VERSION-repack postgresql-$PG_VERSION-pgaudit postgresql-$PG_VERSION-pg-stat-kcache postgresql-$PG_VERSION-pg-qualstats postgresql-$PG_VERSION-set-user postgresql-$PG_VERSION-postgis \
+    postgresql-$PG_VERSION-pg-wait-sampling postgresql-$PG_VERSION-pg-track-settings postgresql-$PG_VERSION-pg-hint-plan postgresql-$PG_VERSION-pgnodemx postgresql-$PG_VERSION-decoderbufs postgresql-$PG_VERSION-pglogical postgresql-$PG_VERSION-pgvector
+
+
+# Install LDAP utilities including openldap-clients and necessary libraries
+RUN apt-get update && apt-get install -y \
+    ldap-utils \
+    libldap-2.5-0 \
+    libsasl2-modules-gssapi-mit \
+    libldap-common \
+    && rm -rf /var/lib/apt/lists/*
+
+
+RUN localedef -i en_US -f UTF-8 en_US.UTF-8 && \
+    localedef -i es_PE -f UTF-8 es_PE.UTF-8 && \
+    localedef -i es_ES -f UTF-8 es_ES.UTF-8
+
+RUN wget https://github.com/zubkov-andrei/pg_profile/releases/download/4.8/pg_profile--4.8.tar.gz && \
+    tar -xzf pg_profile--4.8.tar.gz --directory $(pg_config --sharedir)/extension && \
+    rm -rf pg_profile--4.8.tar.gz
+
+# Install pgsentinel and pg_dbms_stats
+RUN apt update && apt-get install -y git make gcc && \
+    git clone https://github.com/pgsentinel/pgsentinel.git && \
+    cd pgsentinel && \
+    git checkout 0218c2147daab0d2dbbf08433cb480163d321839 && \
+    cd src && make install && \
+    cd ../.. && git clone --depth 1 --branch REL14_0 https://github.com/ossc-db/pg_dbms_stats.git && \
+    cd pg_dbms_stats && sed -i 's/$(MAJORVERSION)/14/g' Makefile && \
+    make install && \
+    apt-get purge -y --auto-remove git make gcc && \
+    cd .. && rm -rf pgsentinel
+
+RUN apt-get install -y alien vmtouch openssh-server
+
+RUN cat /root/.pip/pip.conf
+RUN python3 -m pip install -U setuptools==78.1.1 wheel==0.38.0
+RUN python3 -m pip install psutil patroni[kubernetes,etcd]==3.3.5 psycopg2-binary==2.9.5 requests python-dateutil urllib3 six prettytable --no-cache
+# Explicitly install patched libaom3 version
+RUN apt-get --no-install-recommends install -y libaom3=3.3.0-1ubuntu0.1 || apt-get --no-install-recommends install -y libaom3
+RUN mv /var/lib/postgresql /var/lib/pgsql
+
+RUN sed -i "s/postgres:!/postgres:*/" /etc/shadow && \
+    sed -i "s/#PubkeyAuthentication yes/PubkeyAuthentication yes/" /etc/ssh/sshd_config && \
+    sed -i "s/#PasswordAuthentication yes/PasswordAuthentication no/" /etc/ssh/sshd_config && \
+    sed -i 's/#Port.*$/Port 3022/' /etc/ssh/sshd_config && \
+    sed -i "s/#PermitUserEnvironment no/PermitUserEnvironment yes/" /etc/ssh/sshd_config && \
+    sed -i "s/UsePAM yes/UsePAM no/" /etc/ssh/sshd_config && \
+    sed -i "s@#HostKey /etc/ssh/ssh_host_rsa_key@HostKey ~/.ssh/id_rsa@" /etc/ssh/sshd_config
+
+RUN chgrp 0 /etc &&  \
+    chmod g+w /etc && \
+    chgrp 0 /etc/passwd &&  \
+    chmod g+w /etc/passwd && \
+    chmod g+w /home && \
+    mkdir /patroni && chmod -R 777 /patroni/ && \
+    chmod +x /usr/libexec/fix-permissions && \
+    /usr/libexec/fix-permissions /var/run/postgresql && \
+    /usr/libexec/fix-permissions /var/lib/pgsql && \
+    mkdir -p /var/lib/pgsql/data/ && \
+    chown -R postgres:postgres /var/lib/pgsql && \
+    chmod +x /*.py && \
+    chmod +x /*.sh && \
+    chmod 777 /opt/scripts/archive_wal.sh && \
+    ln -s /usr/bin/python3 /usr/bin/python
+
+RUN chmod 777 /var/lib/pgbackrest && \
+    chmod 777 /var/log/pgbackrest && \
+    chmod 777 /var/spool/pgbackrest && \
+    chown postgres:0 /var/lib/pgbackrest && \
+    chown postgres:0 /var/log/pgbackrest && \
+    chown postgres:0 /var/spool/pgbackrest
+    
+# Volumes are defined to support read-only root file system
+VOLUME /etc
+VOLUME /patroni
+VOLUME /run/postgresql
+
+WORKDIR /patroni
+ENTRYPOINT ["/start.sh"]
+
+USER 26
+EXPOSE 5432
+EXPOSE 8008
diff --git a/docker-patroni/LICENSE b/docker-patroni/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-patroni/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-patroni/README.md b/docker-patroni/README.md
new file mode 100644
index 0000000..0ec68ce
--- /dev/null
+++ b/docker-patroni/README.md
@@ -0,0 +1 @@
+# pgskipper-patroni
\ No newline at end of file
diff --git a/docker-patroni/SECURITY.md b/docker-patroni/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-patroni/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-patroni/fixes.patch b/docker-patroni/fixes.patch
new file mode 100644
index 0000000..2ebb7ce
--- /dev/null
+++ b/docker-patroni/fixes.patch
@@ -0,0 +1,130 @@
+diff --git a/Dockerfile b/Dockerfile
+index 1c7a3d1..2995ba6 100644
+--- a/Dockerfile
++++ b/Dockerfile
+@@ -1,8 +1,8 @@
+-FROM ubuntu:22.04
++FROM ubuntu:24.04
+ 
+ USER root
+ 
+-ARG PG_VERSION=15  # Default to 15 if not provided
++ARG PG_VERSION=18  # Default to 15 if not provided
+ 
+ ENV POD_IDENTITY="node1" \
+     PATRONI_TTL=60 \
+@@ -30,7 +30,7 @@ COPY ./scripts/postgresql.conf /tmp/postgresql.conf
+ COPY ./scripts/fix_permission.sh /usr/libexec/fix-permissions
+ ADD ./scripts/* /
+ 
+-RUN echo "deb [trusted=yes] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" >> /etc/apt/sources.list.d/pgdg.list
++RUN echo "deb [trusted=yes] http://apt.postgresql.org/pub/repos/apt noble-pgdg main" >> /etc/apt/sources.list.d/pgdg.list
+ RUN ls -la /etc/apt/
+ RUN apt-get -y update
+ RUN apt-get -o DPkg::Options::="--force-confnew" -y dist-upgrade
+@@ -38,10 +38,6 @@ RUN apt-get update && \
+     apt-get install -y --allow-downgrades gcc-12 cpp-12 gcc-12-base libgcc-12-dev libstdc++6 libgcc-s1 libnsl2
+ RUN apt-get --no-install-recommends install -y python3.11 python3-pip python3-dev libpq-dev cython3 wget curl
+ 
+-# rename 'tape' group to 'postgres' and creating postgres user - hask for ubuntu
+-RUN groupmod -n postgres tape
+-RUN adduser -uid 26 -gid 26 postgres
+-
+ # Install pgbackrest 2.55.1 from source
+ RUN apt-get --no-install-recommends install -y \
+     build-essential \
+@@ -87,15 +83,25 @@ RUN apt-get --no-install-recommends install -y postgresql-$PG_VERSION postgresql
+     postgresql-$PG_VERSION-cron postgresql-$PG_VERSION-repack postgresql-$PG_VERSION-pgaudit postgresql-$PG_VERSION-pg-stat-kcache postgresql-$PG_VERSION-pg-qualstats postgresql-$PG_VERSION-set-user postgresql-$PG_VERSION-postgis \
+     postgresql-$PG_VERSION-pg-wait-sampling postgresql-$PG_VERSION-pg-track-settings postgresql-$PG_VERSION-pg-hint-plan postgresql-$PG_VERSION-pgnodemx postgresql-$PG_VERSION-decoderbufs postgresql-$PG_VERSION-pglogical postgresql-$PG_VERSION-pgvector
+ 
++# Ensure postgres user exists with UID 26 (postgresql packages create the user, but may not have UID 26)
++RUN POSTGRES_UID=$(id -u postgres 2>/dev/null || echo "") && \
++    if [ -z "$POSTGRES_UID" ]; then \
++        groupadd -g 26 postgres && \
++        useradd -u 26 -g 26 -d /var/lib/pgsql -s /bin/bash postgres; \
++    elif [ "$POSTGRES_UID" != "26" ]; then \
++        groupmod -g 26 postgres 2>/dev/null || groupadd -g 26 postgres; \
++        usermod -u 26 -g 26 postgres; \
++    fi
+ 
+ # Install LDAP utilities including openldap-clients and necessary libraries
+ RUN apt-get update && apt-get install -y \
+     ldap-utils \
+-    libldap-2.5-0 \
+     libsasl2-modules-gssapi-mit \
+     libldap-common \
+     && rm -rf /var/lib/apt/lists/*
+ 
++COPY pg-oidc-validator-pgdg18.deb pg-oidc-validator-pgdg18.deb
++RUN dpkg -i pg-oidc-validator-pgdg18.deb
+ 
+ RUN localedef -i en_US -f UTF-8 en_US.UTF-8 && \
+     localedef -i es_PE -f UTF-8 es_PE.UTF-8 && \
+@@ -111,19 +117,20 @@ RUN apt update && apt-get install -y git make gcc && \
+     cd pgsentinel && \
+     git checkout 0218c2147daab0d2dbbf08433cb480163d321839 && \
+     cd src && make install && \
+-    cd ../.. && git clone --depth 1 --branch REL14_0 https://github.com/ossc-db/pg_dbms_stats.git && \
+-    cd pg_dbms_stats && sed -i 's/$(MAJORVERSION)/14/g' Makefile && \
+-    make install && \
++    # cd ../.. && git clone --depth 1 --branch REL14_0 https://github.com/ossc-db/pg_dbms_stats.git && \
++    # cd pg_dbms_stats && sed -i 's/$(MAJORVERSION)/14/g' Makefile && \
++    # make install && \
+     apt-get purge -y --auto-remove git make gcc && \
+     cd .. && rm -rf pgsentinel
+ 
+ RUN apt-get install -y alien vmtouch openssh-server
+ 
+ RUN cat /root/.pip/pip.conf
+-RUN python3 -m pip install -U setuptools==78.1.1 wheel==0.38.0
+-RUN python3 -m pip install psutil patroni[kubernetes,etcd]==3.3.5 psycopg2-binary==2.9.5 requests python-dateutil urllib3 six prettytable --no-cache
++RUN python3 -m pip install -U setuptools
++RUN python3 -m pip install psutil patroni[kubernetes,etcd]==3.3.5 psycopg2-binary requests python-dateutil urllib3 six prettytable --no-cache
+ # Explicitly install patched libaom3 version
+ RUN apt-get --no-install-recommends install -y libaom3=3.3.0-1ubuntu0.1 || apt-get --no-install-recommends install -y libaom3
++RUN apt-get install -y libpq-oauth
+ RUN mv /var/lib/postgresql /var/lib/pgsql
+ 
+ RUN sed -i "s/postgres:!/postgres:*/" /etc/shadow && \
+@@ -148,7 +155,9 @@ RUN chgrp 0 /etc &&  \
+     chmod +x /*.py && \
+     chmod +x /*.sh && \
+     chmod 777 /opt/scripts/archive_wal.sh && \
+-    ln -s /usr/bin/python3 /usr/bin/python
++    ln -s /usr/bin/python3 /usr/bin/python && \
++    chmod -R 777 /usr/local/share/ca-certificates && \
++    chmod -R 777 /etc/ssl/certs
+ 
+ RUN chmod 777 /var/lib/pgbackrest && \
+     chmod 777 /var/log/pgbackrest && \
+diff --git a/scripts/utils.py b/scripts/utils.py
+index b8cb11d..d4070fd 100644
+--- a/scripts/utils.py
++++ b/scripts/utils.py
+@@ -21,7 +21,7 @@ import logging
+ 
+ import time
+ 
+-comment_pattern = re.compile("\s*#.*")
++comment_pattern = re.compile(r"\s*#.*")
+ 
+ 
+ def read_property_file(filename):
+@@ -41,7 +41,7 @@ def read_property_file(filename):
+                     param_value = param_value.strip()
+                 else:
+                     if param_value[:1] == '%':
+-                        param_value = "\{}".format(param_value)
++                        param_value = "\\{}".format(param_value)
+                     param_value = param_value.lstrip()
+                     param_value = param_value.rstrip('\n')
+                 result[param_name] = param_value
+@@ -49,7 +49,7 @@ def read_property_file(filename):
+     return result
+ 
+ def is_ipv4(host):
+-    p = re.compile("^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
++    p = re.compile(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
+     return p.match(host)
+ 
+ def get_host_ip():
diff --git a/docker-patroni/scripts/archive_wal.sh b/docker-patroni/scripts/archive_wal.sh
new file mode 100644
index 0000000..4aa75e5
--- /dev/null
+++ b/docker-patroni/scripts/archive_wal.sh
@@ -0,0 +1,26 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+p="${1}"
+f="${2}"
+
+set +x
+
+export `cat /proc/1/environ  | tr '\0' '\n' | grep PG_ROOT_PASSWORD`
+
+sha256sum -b "$p" | cut -d " " -f1 | xargs -I {} echo sha256={} | \
+python3 -c "import sys; print(chr(38) + sys.stdin.read().strip())" | \
+xargs -I SHA curl -u postgres:"${PG_ROOT_PASSWORD}" -s -S -f --connect-timeout 5 --speed-time 30 --speed-limit 100 -XPOST -F "file=@$p" postgres-backup-daemon:8082/archive/put?filename="$f"SHA
\ No newline at end of file
diff --git a/docker-patroni/scripts/daemon-recovery.sh b/docker-patroni/scripts/daemon-recovery.sh
new file mode 100644
index 0000000..8f0b327
--- /dev/null
+++ b/docker-patroni/scripts/daemon-recovery.sh
@@ -0,0 +1,62 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+restore_version=""
+
+die() {
+    printf '%s\n' "$1" >&2
+    exit 1
+}
+
+while :; do
+    case $1 in
+        --restore-version)
+            if [ "$2" ]; then
+                restore_version=$2
+                shift
+            else
+                echo "Proceed with empty restore_version"
+#                die 'ERROR: "--restore-version" requires a non-empty option argument.'
+            fi
+            ;;
+        --restore-version=?*)
+            restore_version=${1#*=} # Delete everything up to "=" and assign the remainder.
+            ;;
+        --restore-version=)
+            echo "Proceed with empty restore_version"
+#            die 'ERROR: "--restore-version" requires a non-empty option argument if specified.'
+            ;;
+        --)
+            shift
+            break
+            ;;
+        -?*)
+            printf 'WARN: Unknown option (ignored): %s\n' "$1" >&2
+            ;;
+        *)
+            break
+    esac
+    shift
+done
+
+cd /var/lib/pgsql/data/postgresql_${POD_IDENTITY}
+
+
+if [[ -z "${restore_version}" ]] ; then
+    curl -u postgres:"${PG_ROOT_PASSWORD}" postgres-backup-daemon:8081/get | tar -xzf -
+else
+    curl -u postgres:"${PG_ROOT_PASSWORD}"  postgres-backup-daemon:8081/get?id=${restore_version} | tar -xzf -
+fi
\ No newline at end of file
diff --git a/docker-patroni/scripts/fix_permission.sh b/docker-patroni/scripts/fix_permission.sh
new file mode 100644
index 0000000..d287182
--- /dev/null
+++ b/docker-patroni/scripts/fix_permission.sh
@@ -0,0 +1,21 @@
+#!/bin/sh
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# Fix permissions on the given directory to allow group read/write of
+# regular files and execute of directories.
+find "$1" -exec chown postgres {} \;
+find "$1" -exec chgrp 0 {} \;
+find "$1" -exec chmod g+rw {} \;
+find "$1" -type d -exec chmod g+x {} +
\ No newline at end of file
diff --git a/docker-patroni/scripts/pip.conf b/docker-patroni/scripts/pip.conf
new file mode 100755
index 0000000..d547ee0
--- /dev/null
+++ b/docker-patroni/scripts/pip.conf
@@ -0,0 +1,4 @@
+[global]
+index-url = https://pypi.org/simple
+break-system-packages = true
+trusted-host = pypi.org
\ No newline at end of file
diff --git a/docker-patroni/scripts/populate_patroni_config.py b/docker-patroni/scripts/populate_patroni_config.py
new file mode 100644
index 0000000..48f8416
--- /dev/null
+++ b/docker-patroni/scripts/populate_patroni_config.py
@@ -0,0 +1,89 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import logging
+
+import sys
+import yaml
+# python /patroni/populate_patroni_config.py /patroni/pg_node.yml patroni/pg_conf_active.conf
+import json
+from utils import get_log_level
+
+logging.basicConfig(
+    level=get_log_level(),
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+
+def is_number(s):
+    """ Returns True if string is a number. """
+    try:
+        float(s)
+        return True
+    except ValueError:
+        return False
+
+
+def populate_patroni_config(patroni_conf_filename, settings_conf_filename):
+    with open(patroni_conf_filename) as f:
+        patroni_conf = yaml.safe_load(f)
+
+        # todo[anin] check utils.read_property_file instead
+    config_data = ''
+    with open(settings_conf_filename) as f:
+        for line in f:
+            if "=" in line:
+                param_name = line[0:line.find("=")]
+                param_value = line[line.find("=")+1:-1]
+                config_data = \
+                    config_data + '\n' + \
+                    param_name + ": " + \
+                    ("!!str " if not is_number(param_value) else "") + \
+                    param_value
+            else:
+                config_data = config_data + '\n' + line
+    logger.debug("Result data from config file: {}\n".format(config_data))
+    conf = yaml.safe_load(config_data)
+    logger.debug(conf)
+    if conf:
+        params = patroni_conf['bootstrap']['dcs']['postgresql']['parameters']
+        for key in conf:
+            value = conf[key]
+            if key == "log_line_prefix" and value[:1] == "\\":
+                value = value[1:]
+
+            logger.debug("Apply {}={}".format(key, value))
+            params[key] = value
+
+    with open(patroni_conf_filename, mode='w') as f:
+        yaml.dump(patroni_conf, f, default_flow_style=False)
+
+
+def main():
+    logger.info("Try to apply provided settings to patroni config. {}"
+                .format(sys.argv))
+    if len(sys.argv) == 3:
+        patroni_conf_filename = sys.argv[1]
+        settings_conf_filename = sys.argv[2]
+        populate_patroni_config(patroni_conf_filename, settings_conf_filename)
+
+    else:
+        sys.exit("Usage: {0} /patroni/pg_node.yml /patroni/pg_conf_active.conf"
+                 .format(sys.argv[0]))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-patroni/scripts/postgresql.conf b/docker-patroni/scripts/postgresql.conf
new file mode 100644
index 0000000..b1002d6
--- /dev/null
+++ b/docker-patroni/scripts/postgresql.conf
@@ -0,0 +1 @@
+include_dir '/properties'
\ No newline at end of file
diff --git a/docker-patroni/scripts/prepare_settings_file.py b/docker-patroni/scripts/prepare_settings_file.py
new file mode 100644
index 0000000..a1b2c82
--- /dev/null
+++ b/docker-patroni/scripts/prepare_settings_file.py
@@ -0,0 +1,95 @@
+#!/usr/bin/env python3
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+import sys
+import os
+
+import logging
+from utils import read_property_file, get_log_level
+
+logging.basicConfig(
+    level=get_log_level(),
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+PG_USER_CONF = "/properties/postgresql.user.conf"
+POSTGRESQL_VERSION = int(os.getenv("POSTGRESQL_VERSION", "96"))
+RUN_PROPAGATE_SCRIPT = os.getenv("RUN_PROPAGATE_SCRIPT", "True").lower()
+
+
+def get_parameters_from_env():
+    result = {}
+    for key in os.environ:
+        if key.lower().startswith("pg_conf_"):
+            prop_name = (key.lower()[8:len(key)]).strip()
+            prop_value = (os.environ[key]).strip()
+            result[prop_name] = prop_value
+    return result
+
+
+def get_parameters_from_user_conf():
+    return read_property_file(PG_USER_CONF)
+
+
+def main():
+    logger.info("Try to prepare active properties configuration based on "
+                "current env and provided properties file. {}"
+                .format(sys.argv))
+    if len(sys.argv) == 2:
+        target_file = sys.argv[1]
+
+        params = {
+            "shared_preload_libraries": "pg_stat_statements, "
+                                        "pg_hint_plan, pg_cron",
+        }
+
+        logger.debug("Default parameters: {}".format(params))
+
+        env_params = get_parameters_from_env()
+        logger.info("Parameters from env: {}".format(env_params))
+        for key, value in list(env_params.items()):
+            params[key] = value
+
+        logger.info("RUN_PROPAGATE_SCRIPT is set to: {}, ".format(RUN_PROPAGATE_SCRIPT))
+        if RUN_PROPAGATE_SCRIPT == "true":
+            conf_params = get_parameters_from_user_conf()
+            logger.debug("Parameters from user config: {}".format(conf_params))
+            for key, value in list(conf_params.items()):
+                params[key] = value
+        else:
+            params.pop('shared_preload_libraries', None)
+
+        # pg_cron is required extension.
+        # check if it is present in shared_preload_libraries
+        if RUN_PROPAGATE_SCRIPT == "true":
+            libraries = [x.strip() for x in params.get("shared_preload_libraries", "").split(",")]
+            if 'pg_cron' not in libraries:
+                libraries.append("pg_cron")
+                params["shared_preload_libraries"] = ", ".join(libraries)
+
+        logger.info("Result: {}".format(params))
+        logger.debug("Target file {}".format(target_file))
+        with open(target_file, mode='w') as f:
+            for key, value in list(params.items()):
+                f.write("{}={}\n".format(key, value))
+    else:
+        sys.exit("Usage: {0} ./new.properties".format(sys.argv[0]))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-patroni/scripts/propagate_settings.sh b/docker-patroni/scripts/propagate_settings.sh
new file mode 100755
index 0000000..e651099
--- /dev/null
+++ b/docker-patroni/scripts/propagate_settings.sh
@@ -0,0 +1,23 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source /setEnv.sh
+
+echo "Prepare file with current properties"
+python3 /prepare_settings_file.py /patroni/pg_conf_propagate.conf
+
+echo "Propagate to patroni file with current properties"
+python3 /propagate_settings_file.py /patroni/pg_conf_propagate.conf
diff --git a/docker-patroni/scripts/propagate_settings_file.py b/docker-patroni/scripts/propagate_settings_file.py
new file mode 100644
index 0000000..7c2e9c1
--- /dev/null
+++ b/docker-patroni/scripts/propagate_settings_file.py
@@ -0,0 +1,127 @@
+#!/usr/bin/env python
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import json
+import re
+import subprocess
+import sys
+import os
+import time
+
+from utils import read_property_file, get_host_ip, get_log_level
+from utils_db import get_settings_data, get_context_data, is_values_diff, is_restart_pending, schedule_restart, patroni_restart_state
+import logging
+import requests
+
+logging.basicConfig(
+    level=logging.INFO,
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+
+def main():
+
+    logger.info("Try to propagate property file to cluster. {}".format(sys.argv))
+    if len(sys.argv) == 2:
+        source_file = sys.argv[1]
+        properties = read_property_file(source_file)
+
+        # find properties which requires update
+        properties4update = {}
+        restart_required = False
+        for key, value in list(properties.items()):
+            current_value = get_settings_data(key)
+            if is_values_diff(value, current_value):
+                context = get_context_data(key)
+                logger.info(context)
+                if context == "internal":
+                    logger.error("We cannot change variable of internal context: {}".format(key))
+                    sys.exit(1)
+                properties4update[key] = value
+
+        if not properties4update:
+            logger.info("No properties to update")
+            return
+        logger.info("Need to update: {}".format(properties4update))
+
+        # form patch
+        #  todo[anin] add parameters validation (int - max, min val; string; enum)
+        patch_data = {"postgresql": {"parameters": {}}}
+        for key, value in list(properties4update.items()):
+            tmp = ""
+            if key != 'log_line_prefix':
+                tmp = value.strip()
+                if "\\" in tmp:
+                    tmp = tmp.replace("\\", "\\\\")
+            else:
+                if "\\" == value[:2]:
+                    tmp = value[2:]
+                else:
+                    tmp = value
+
+            patch_data["postgresql"]["parameters"][key] = tmp  # json.dumps(value)
+
+        # send patch
+        # curl -i -XPATCH -d @/patroni/parameters_data http://$(hostname -i):8008/config
+        logger.debug("Patch prepared: {}".format(patch_data))
+        user = os.getenv('PATRONI_REST_API_USER')
+        password = os.getenv('PATRONI_REST_API_PASSWORD')
+        from requests.auth import HTTPBasicAuth
+        basic_auth = HTTPBasicAuth(user, password)
+        logger.info(requests.patch(
+            "http://{}:8008/config".format(get_host_ip()),
+            data=json.dumps(patch_data),
+            auth=basic_auth))
+
+
+        # todo[anin] replace with pg_settings.pending_restart check.
+        # There is problem - patroni updates config after restart command.
+        # So we cannot detect pending_restart flag until actual restart.
+        # for key, value in properties4update.items():
+        #     (current_value, unit, category, vartype, context) = get_setting_data(key)
+        #     if is_values_differs(value, current_value, unit, vartype):
+        #         logger.info("Schedule restart because some settings requires restart")
+        #         schedule_restart()
+        #         return
+        iterations = int(os.getenv('CHANGE_SETTINGS_RETRIES', 5))
+        sleep = int(os.getenv('CHANGE_SETTINGS_INTERVAL', 3))
+        if patroni_restart_state(basic_auth, iterations, sleep):
+            schedule_restart()
+
+            return
+
+        # # todo[anin] this code can be interrupted by callback executor
+        # # wait while value will be applied to current server
+        # applied = True
+        # for i in range(1, 60):
+        #     applied = True
+        #     for key, value in properties4update.items():
+        #         (current_value, unit, category, vartype) = get_setting_data(key)
+        #         if is_values_differs(value, current_value, unit, vartype):
+        #             applied = False
+        #             sleep(1)
+        #             break
+        #
+        # if not applied:
+        #     sys.exit("Setting were not applied")
+
+    else:
+        sys.exit("Usage: {0} ./active.properties".format(sys.argv[0]))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-patroni/scripts/setEnv.sh b/docker-patroni/scripts/setEnv.sh
new file mode 100755
index 0000000..0c094da
--- /dev/null
+++ b/docker-patroni/scripts/setEnv.sh
@@ -0,0 +1,78 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+LOG_LEVEL=${LOG_LEVEL:-info}
+
+if [[ -z "${POD_IDENTITY}" ]];
+then
+    echo "POD_IDENTITY is not defined, defaulting POD_IDENTITY"
+    export POD_IDENTITY="node"
+fi
+
+ROOT_DIR_NAME="postgresql_${POD_IDENTITY}"
+ROOT_DIR="/var/lib/pgsql/data/${ROOT_DIR_NAME}"
+
+#####################################################################################################
+## Calculate PG_CONF_MAX_CONNECTIONS parameters based on merge of old and new configurations
+PG_MAX_CONNECTIONS=${PG_MAX_CONNECTIONS:-200}
+export PG_CONF_MAX_CONNECTIONS=${PG_CONF_MAX_CONNECTIONS:-$PG_MAX_CONNECTIONS}
+echo "PG_CONF_MAX_CONNECTIONS=${PG_CONF_MAX_CONNECTIONS}"
+
+#####################################################################################################
+## Calculate max_prepared_transactions
+PG_CONF_MAX_PREPARED_TRANSACTIONS=${PG_CONF_MAX_PREPARED_TRANSACTIONS:-200}
+
+#####################################################################################################
+## Calculate memory setting based on memory limit and max_connections settings
+PG_RESOURCES_LIMIT_MEM=${PG_RESOURCES_LIMIT_MEM:-256Mi}
+echo "PG_RESOURCES_LIMIT_MEM=${PG_RESOURCES_LIMIT_MEM}"
+
+declare -A m
+# see https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-memory
+m=(["ki"]=1 ["mi"]=1024 ["gi"]=1048576 ["k"]=1 ["m"]=1000 ["g"]=1000000 )
+
+if [[ ${PG_RESOURCES_LIMIT_MEM} =~ ^([0-9]*)([A-Za-z]+) ]] ; then
+    ei=`echo ${BASH_REMATCH[2]} | tr '[:upper:]' '[:lower:]'`
+    LIM_VAL=${BASH_REMATCH[1]}
+    LIM_MULT=${m[$ei]}
+    _PG_RESOURCES_LIMIT_MEM_KIB=$(expr $LIM_VAL \* $LIM_MULT)
+elif [[ ${PG_RESOURCES_LIMIT_MEM} =~ ^([0-9]*) ]] ; then
+    _PG_RESOURCES_LIMIT_MEM_KIB=$(expr ${PG_RESOURCES_LIMIT_MEM} / 1024)
+else
+    echo "Cannot parse PG_RESOURCES_LIMIT_MEM value ${PG_RESOURCES_LIMIT_MEM}"
+    exit 1
+fi
+
+echo "_PG_RESOURCES_LIMIT_MEM_KIB=${_PG_RESOURCES_LIMIT_MEM_KIB}"
+patroni_mem=$((${_PG_RESOURCES_LIMIT_MEM_KIB}>512000?102400:51200))
+_PG_AVAILABLE_KIB=$(expr ${_PG_RESOURCES_LIMIT_MEM_KIB} - ${patroni_mem})
+PG_AVAILABLE=${_PG_AVAILABLE_KIB}kB
+#    echo "PG_AVAILABLE=${PG_AVAILABLE}"
+
+_PG_CONF_SHARED_BUFFERS_KIB=$(expr ${_PG_AVAILABLE_KIB} / 4 )
+export PG_CONF_SHARED_BUFFERS=${_PG_CONF_SHARED_BUFFERS_KIB}kB
+#    echo "PG_CONF_SHARED_BUFFERS=${PG_CONF_SHARED_BUFFERS}"
+
+export PG_CONF_EFFECTIVE_CACHE_SIZE=$(expr ${_PG_AVAILABLE_KIB} - ${_PG_CONF_SHARED_BUFFERS_KIB})kB
+#    echo "PG_CONF_EFFECTIVE_CACHE_SIZE=${PG_CONF_EFFECTIVE_CACHE_SIZE}"
+
+_PG_CONF_WORK_MEM_KIB=$(expr ${_PG_CONF_SHARED_BUFFERS_KIB} / ${PG_CONF_MAX_CONNECTIONS})
+_PG_CONF_WORK_MEM_KIB=$(($_PG_CONF_WORK_MEM_KIB>64?$_PG_CONF_WORK_MEM_KIB:64))
+export PG_CONF_WORK_MEM=${_PG_CONF_WORK_MEM_KIB}kB
+#    echo "PG_CONF_WORK_MEM=${PG_CONF_WORK_MEM}"
+
+export PG_CONF_MAINTENANCE_WORK_MEM=$(expr ${_PG_CONF_SHARED_BUFFERS_KIB} / 4)kB
+#    echo "PG_CONF_MAINTENANCE_WORK_MEM=${PG_CONF_MAINTENANCE_WORK_MEM}"
diff --git a/docker-patroni/scripts/settings_check.sh b/docker-patroni/scripts/settings_check.sh
new file mode 100644
index 0000000..1db8af2
--- /dev/null
+++ b/docker-patroni/scripts/settings_check.sh
@@ -0,0 +1,25 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source /setEnv.sh
+
+echo "Prepare file with current properties"
+python3 /prepare_settings_file.py /patroni/pg_conf_check.conf
+
+RESTART_PG=${RESTART_PG:-false}
+
+echo "Propagate to patroni file with current properties"
+python3 /validate_settings_file.py --conf-file=/patroni/pg_conf_check.conf --restart-pg=${RESTART_PG}
\ No newline at end of file
diff --git a/docker-patroni/scripts/setup_endpoint_callback.py b/docker-patroni/scripts/setup_endpoint_callback.py
new file mode 100644
index 0000000..cb680ec
--- /dev/null
+++ b/docker-patroni/scripts/setup_endpoint_callback.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import subprocess
+import sys
+import logging
+import os
+
+logging.basicConfig(
+    level=logging.INFO,
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+RUN_PROPAGATE_SCRIPT = os.getenv("RUN_PROPAGATE_SCRIPT", "True").lower()
+
+
+def main():
+    logger.info("Start callback with parameters {}".format(sys.argv))
+    if len(sys.argv) == 4:
+        action, role, cluster = sys.argv[1], sys.argv[2], sys.argv[3]
+        logger.info("Cluster name: {}, new role: {}".format(cluster, role))
+        if action not in ('on_start', 'on_role_change', 'on_restart', 'on_reload'):
+            return
+        if role == "master":
+            logger.info("We were promoted to master. "
+                        "Start configuration checks.")
+            logger.info("Triggering propagate_settings script.")
+            subprocess.check_call("/propagate_settings.sh")
+        elif role == "replica":
+            logger.info("Role is set to replica, "
+                        "will terminate active applications connections")
+            connection_properties = {
+                'host': 'localhost',
+                'user': 'postgres',
+            }
+            import psycopg2
+            with psycopg2.connect(**connection_properties) as conn:
+                with conn.cursor() as cur:
+                    def execute_silently(query_):
+                        logger.debug(
+                            "Executing next query: {}".format(query_))
+                        try:
+                            cur.execute(query_)
+                        except psycopg2.Error:
+                            logger.exception("Exception happened during "
+                                             "execution of the query")
+                    # TODO handle pg-pool case
+                    execute_silently("""
+                        select pg_terminate_backend(pid) from 
+                        pg_stat_activity where datname <> 'postgres' and
+                        pid <> pg_backend_pid()
+                    """)
+                    logger.info("Connections are terminated successfully")
+    else:
+        sys.exit("Usage: {0} action role name".format(sys.argv[0]))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docker-patroni/scripts/start.sh b/docker-patroni/scripts/start.sh
new file mode 100755
index 0000000..7712ba6
--- /dev/null
+++ b/docker-patroni/scripts/start.sh
@@ -0,0 +1,215 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+source /setEnv.sh
+
+export PASSWD_DIR=$(dirname ${ROOT_DIR})/passwd
+export HOME=/home/postgres
+
+
+# prepare datadir
+mkdir -p ${ROOT_DIR}
+chmod 700 ${ROOT_DIR}
+chown $(id -u):$(id -u) ${ROOT_DIR}
+
+
+function set_property() {
+    PROP_NAME=$1
+    PROP_VALUE=$2
+    FILE=$3
+    sed -i "/^#*${PROP_NAME}[ ]*=/{h;s/.*/${PROP_NAME} = ${PROP_VALUE}/};\${x;/^\$/{s//$PROP_NAME = '$PROP_VALUE'/;H};x}" ${FILE}
+}
+
+cur_user=$(id -u)
+if [ "$cur_user" != "26" ]
+then
+    echo "starting as not postgres user"
+    set -e
+
+    echo "Adding randomly generated uid to passwd file..."
+
+    sed -i '/postgres/d' /etc/passwd
+    export HOME=/home/pg
+    if ! whoami &> /dev/null; then
+      if [ -w /etc/passwd ]; then
+        if [ -n "$PGBACKREST_PG2_HOST" ]; then
+          echo "${USER_NAME:-postgres}:x:$(id -u):0:${USER_NAME:-postgres} user:${HOME}:/bin/sh" >> /etc/passwd
+        else
+          echo "${USER_NAME:-postgres}:x:$(id -u):0:${USER_NAME:-postgres} user:${ROOT_DIR}:/bin/nologin" >> /etc/passwd
+        fi
+      fi
+    fi
+
+fi
+
+if [ -n "$PGBACKREST_PG1_PATH" ]; then
+    echo "Create spool directory for pgbackrest..."
+    mkdir -p /var/lib/pgsql/data/pgbackrest/spool
+    chmod -R 770 /var/lib/pgsql/data/pgbackrest
+fi
+
+if [ -n "$PGBACKREST_PG2_HOST" ]; then
+    echo "Preparation for standby backup..."
+    mkdir -p ${HOME}
+    chmod 700 ${HOME}
+    mkdir -p ${HOME}/.ssh
+    chmod 700 ${HOME}/.ssh
+
+    cp /keys/id_rsa ${HOME}/.ssh/id_rsa
+    cp /keys/id_rsa.pub ${HOME}/.ssh/id_rsa.pub
+    cp /keys/id_rsa.pub ${HOME}/.ssh/authorized_keys
+    cp /keys/id_rsa.pub ${HOME}/.ssh/known_hosts
+    sed -i "s/ssh-rsa/pg-patroni ssh-rsa/" ${HOME}/.ssh/known_hosts
+
+    chmod 600 ${HOME}/.ssh/id_rsa
+
+    /usr/sbin/sshd -E /tmp/sshd.log -o PidFile=/tmp/sshd.pid
+fi
+
+# removing postmaster.pid file in case if pgsql was stoped not gracefully  
+if [ -e "$ROOT_DIR/postmaster.pid" ]
+then
+    echo "Removing postmaster.pid file"
+    rm -rf "$ROOT_DIR/postmaster.pid"
+    rm -rf "$ROOT_DIR/postmaster.opts"
+fi
+
+if [[ -z "$PG_ROOT_PASSWORD" ]] ; then
+    echo "Cannot start cluster with empty postgres password"
+    exit 1
+fi
+
+# copy file from provided template if needed
+if [[ -f /patroni-properties/patroni-config-template.yaml ]] ; then
+    echo "File /patroni-properties/patroni-config-template.yaml found. Will use as template for patroni configuration."
+    cp -f /patroni-properties/patroni-config-template.yaml /patroni/pg_template.yaml
+else
+    echo "Cannot work without /patroni-properties/patroni-config-template.yaml. Please provide template via configmap."
+    exit 1
+fi
+
+if [[ "${DR_MODE}" =~ ^[Tt]rue$ ]]; then
+  PATRONI_CLUSTER_MEMBER_ID="${POD_IDENTITY}-dr"
+else
+  PATRONI_CLUSTER_MEMBER_ID="${POD_IDENTITY}"
+  DR_MODE="false"
+fi
+
+if [[ -z "${ETCD_HOST}" ]]; then
+  ETCD_HOST="etcd"
+fi
+
+NODE_NAME="${POD_IDENTITY}"
+
+export NODE_NAME
+export PATRONI_CLUSTER_MEMBER_ID
+export DR_MODE
+export ETCD_HOST
+
+if [[ -n "${FROM_SCRATCH}" ]]; then
+  echo "Cluster from scratch. Removing everything under ${ROOT_DIR}/*"
+  rm -rf ${ROOT_DIR}/*
+fi
+
+# prepare config for patroni
+PG_BIN_DIR=${PG_BIN_DIR} \
+PG_ROOT_PASSWORD=${PG_ROOT_PASSWORD} \
+PG_REPL_PASSWORD=${PG_REPL_PASSWORD} \
+LISTEN_ADDR=`hostname -i` \
+PG_CLUST_NAME=${PG_CLUST_NAME} \
+POD_NAMESPACE=${POD_NAMESPACE} \
+envsubst < /patroni/pg_template.yaml > /patroni/pg_node.yml
+
+echo "Prepare file with initial properties"
+python3 /prepare_settings_file.py /patroni/pg_conf_initial.conf
+
+echo "Initial properties: "
+cat /patroni/pg_conf_initial.conf
+
+echo "Apply properties to bootstrap section"
+python3 /populate_patroni_config.py /patroni/pg_node.yml /patroni/pg_conf_initial.conf
+
+echo "Config result"
+cat /patroni/pg_node.yml | grep -v password
+
+echo "Check if we have datafiles from previous start and apply required settings"
+
+if [[ -d /var/lib/pgsql/data/data/${ROOT_DIR_NAME} ]] ; then
+    echo "Find an uncommon database location"
+    echo "Moving it to ROOT_DIR, might take a while"
+    mv /var/lib/pgsql/data/data/${ROOT_DIR_NAME} /var/lib/pgsql/data/
+    [[ $? != 0 ]] && echo "Something goes wrong, please check is moving correctly" || echo "Moving complete"
+fi
+
+required_array=("shared_buffers" "effective_cache_size" "work_mem" "maintenance_work_mem")
+if [[ -f ${ROOT_DIR}/postgresql.base.conf ]] ; then
+    echo "Set properties"
+    cat /patroni/pg_conf_initial.conf | while read line
+    do
+      if ! [[ ${line} =~ \s*#.* ]]; then
+        IFS='=' read -r pg_setting_name value <<< "${line}"
+        for e in "${@:required_array}"; do
+            if [[ "$e" == "${pg_setting_name}" ]] ; then
+                echo "Setting from config file: ${pg_setting_name} = ${value}"
+                set_property ${pg_setting_name} ${value} ${ROOT_DIR}/postgresql.base.conf;
+            fi
+        done
+      fi
+    done
+fi
+
+if [[ -f /certs/server.crt ]] ; then
+    cp /certs/server.crt /patroni/server.crt && chmod 600 /patroni/server.crt
+    cp /certs/server.key /patroni/server.key && chmod 600 /patroni/server.key
+    ls -ll /
+fi
+
+# Disable coredumps to keep PV clean and free.
+ulimit -c 0
+
+# Start Patroni in the background. We need background mode to be
+# able to handle TERM, which is sent by docker upon stopping
+# the container. This is our chance to stop DB gracefully.
+PATH="${PATH}:${PG_BIN_DIR}" patroni /patroni/pg_node.yml &
+
+PATRONI_PID=$!
+if [[ -z ${PATRONI_PID} ]]
+then
+    echo -e "\nERROR: could not find PID of Patroni!"
+    exit 1
+else
+    echo -e "\nPatroni PID is ${PATRONI_PID}."
+fi
+
+function exit_handler() {
+    echo "Received termination signal. Propagating it to Patroni..."
+
+    # Handle both SIGINT and SIGTERM similarly, because Patroni
+    # shuts the database down using SIGINT both on SIGINT and SIGTERM.
+    kill ${PATRONI_PID}
+
+    echo "Termination signal sent, waiting for the process to stop..."
+    wait ${PATRONI_PID}
+    echo "Patroni process terminated."
+
+    echo "Try to collect controldata"
+    pg_controldata -D ${ROOT_DIR}
+}
+
+trap exit_handler SIGINT SIGTERM
+
+wait ${PATRONI_PID}
+
diff --git a/docker-patroni/scripts/utils.py b/docker-patroni/scripts/utils.py
new file mode 100644
index 0000000..b8cb11d
--- /dev/null
+++ b/docker-patroni/scripts/utils.py
@@ -0,0 +1,122 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import os
+import socket, struct
+import re
+import subprocess
+
+import logging
+
+import time
+
+comment_pattern = re.compile("\s*#.*")
+
+
+def read_property_file(filename):
+    """
+    Reads data from filename and parse it to dictionary.
+    :param filename:
+    :return:
+    :rtype: dict
+    """
+    result = {}
+    with open(filename) as f:
+        for line in f:
+            if "=" in line and not comment_pattern.match(line):
+                param_name = (line[0:line.find("=")]).strip()
+                param_value = (line[line.find("=")+1:])
+                if param_name != 'log_line_prefix':
+                    param_value = param_value.strip()
+                else:
+                    if param_value[:1] == '%':
+                        param_value = "\{}".format(param_value)
+                    param_value = param_value.lstrip()
+                    param_value = param_value.rstrip('\n')
+                result[param_name] = param_value
+
+    return result
+
+def is_ipv4(host):
+    p = re.compile("^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
+    return p.match(host)
+
+def get_host_ip():
+    IP = os.getenv("POD_IP")
+    if not IP:
+        import fcntl
+        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+        return socket.inet_ntoa(fcntl.ioctl(s.fileno(), 0x8915, struct.pack('256s', b"eth0"[:15]))[20:24])
+    else:
+        if is_ipv4(IP):
+            return IP
+        else:
+            if IP:
+                return "[{}]".format(IP)
+
+def get_log_level():
+    # todo[anin] change default
+    # loglevel = os.getenv('LOG_LEVEL', 'info')
+    loglevel = os.getenv('LOG_LEVEL', 'debug')
+    return logging.DEBUG if loglevel == "debug" else logging.INFO
+
+def execute_shell_command(cmd):
+    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
+    (output, error) = p.communicate()
+    exit_code = p.wait()
+    return {'output': output.decode("utf-8"), 'exit_code': exit_code, 'error': error}
+
+
+def retry(exceptions=None, tries=5, delay=1, backoff=1, logger=None):
+    """
+    :param exceptions: if defined - only specified exceptions will be checked
+    :type exceptions: tuple of Exception or Exception
+    :param tries: how much to try before fail. <=0 means no limits.
+    :param delay: basic delay between tries
+    :param backoff: delay increase factor after each retry
+    :param logger:
+    :type logger: logging.Logger
+    :return:
+    """
+    def deco_retry(f):
+
+        def handle_error(e, mtries, mdelay):
+            msg = "Error occurred during execution: {}. Will retry in {} seconds.".format(str(e), delay)
+            if logger:
+                logger.exception(msg)
+            else:
+                print(msg)
+            time.sleep(mdelay)
+            mtries -= 1
+            mdelay *= backoff
+            return mtries, mdelay
+
+        def f_retry(*args, **kwargs):
+            mtries, mdelay = tries, delay
+            while tries <= 0 or mtries > 1:
+                if exceptions:
+                    try:
+                        return f(*args, **kwargs)
+                    except exceptions as e:
+                        mtries, mdelay = handle_error(e, mtries, mdelay)
+                else:
+                    try:
+                        return f(*args, **kwargs)
+                    except Exception as e:
+                        mtries, mdelay = handle_error(e, mtries, mdelay)
+            return f(*args, **kwargs)
+
+        return f_retry
+    return deco_retry
+
diff --git a/docker-patroni/scripts/utils_db.py b/docker-patroni/scripts/utils_db.py
new file mode 100644
index 0000000..6ce1eda
--- /dev/null
+++ b/docker-patroni/scripts/utils_db.py
@@ -0,0 +1,127 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import re
+import subprocess
+import time
+import logging
+import psycopg2
+import requests
+
+from utils import get_log_level, get_host_ip, execute_shell_command
+
+logging.basicConfig(
+    level=get_log_level(),
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+
+def get_context_data(setting_name):
+    conn = None
+    cursor = None
+    conn_string = "host='localhost' dbname='postgres' user='postgres' " \
+                  "connect_timeout=3 options='-c statement_timeout=3000'"
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        cursor.execute("select context from pg_settings where name=%(sname)s",
+                       {"sname": setting_name})
+        row = cursor.fetchone()
+        if row:
+            return row[0]
+        else:
+            return None
+    except psycopg2.OperationalError:
+        return None
+    finally:
+        close_connection(cursor, conn)
+    # return map(lambda x: x.strip(), subprocess.check_output(
+    #     "psql -U postgres -t -c \"select setting, unit, category, vartype from pg_settings where name='{}'\"".format(setting_name), shell=True).split("|"))
+
+
+def get_settings_data(setting_name):
+    conn = None
+    cursor = None
+    conn_string = "host='localhost' dbname='postgres' user='postgres' " \
+                  "connect_timeout=3 options='-c statement_timeout=3000'"
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        logger.info(setting_name)
+        cursor.execute("select current_setting (%(sname)s, 't')",
+                       {"sname": setting_name})
+        row = cursor.fetchone()
+        if row:
+            return row[0]
+        else:
+            return None
+    except psycopg2.OperationalError:
+        return None
+    finally:
+        close_connection(cursor, conn)
+
+
+def is_restart_pending():
+    conn = None
+    cursor = None
+    conn_string = "host='localhost' dbname='postgres' user='postgres' " \
+                  "connect_timeout=3 options='-c statement_timeout=3000'"
+    try:
+        conn = psycopg2.connect(conn_string)
+        cursor = conn.cursor()
+        cursor.execute("select count(*) from pg_settings where pending_restart = TRUE")
+        row = cursor.fetchone()
+        return int(row[0]) > 0
+    except Exception as e:
+        logger.exception("Cannot get amount of parameters which requires restart")
+        raise e
+    finally:
+        close_connection(cursor, conn)
+
+
+def schedule_restart():
+    logger.debug("Schedule restart")
+    restart_command = "patronictl -c /patroni/pg_node.yml restart $PG_CLUST_NAME $(hostname) --force"
+#    res = execute_shell_command(restart_command)
+#    logger.debug(res)
+    return execute_shell_command(restart_command)
+
+
+def close_connection(cursor, conn):
+    # see http://initd.org/psycopg/docs/cursor.html#cursor.closed
+    if cursor and not cursor.closed:
+        cursor.close()
+    # see http://initd.org/psycopg/docs/connection.html#connection.closed
+    if conn and conn.closed == 0:
+        conn.close()
+
+
+def is_values_diff(value, db_value):
+    logger.debug("Start comparison for value: {}, db_value: {}"
+                 .format(value, db_value))
+    return value != db_value
+
+
+def patroni_restart_state(basic_auth, iterations=5, sleep=3):
+    for i in range(iterations):
+        time.sleep(sleep)
+        r = requests.get("http://{}:8008".format(get_host_ip()),
+                         auth=basic_auth)
+        logger.info("Checking restart state... It is {}".format(r.json().get('pending_restart', False)))
+        restart_required = r.json().get('pending_restart', False)
+        if restart_required:
+            break
+    return restart_required
\ No newline at end of file
diff --git a/docker-patroni/scripts/validate_settings_file.py b/docker-patroni/scripts/validate_settings_file.py
new file mode 100644
index 0000000..86b6792
--- /dev/null
+++ b/docker-patroni/scripts/validate_settings_file.py
@@ -0,0 +1,71 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import sys
+import os
+import argparse
+
+from utils import read_property_file, get_log_level
+from utils_db import get_settings_data, is_values_diff, is_restart_pending, schedule_restart, patroni_restart_state
+import logging
+
+logging.basicConfig(
+    level=get_log_level(),
+    format='[%(asctime)s][%(levelname)-5s][category=%(name)s] %(message)s',
+    datefmt='%Y-%m-%dT%H:%M:%S'
+)
+logger = logging.getLogger(__name__)
+
+
+def main(conf_file, restart_pg=False):
+    logger.info("Start settings validation {}".format(sys.argv))
+    properties = read_property_file(conf_file)
+
+    # find properties which requires update
+    properties4update = {}
+    for key, value in list(properties.items()):
+        logger.debug("Try to check setting: {} with expected value {}".format(key, value))
+        (current_value) = get_settings_data(key)
+        logger.debug("Value from DB: {}".format(current_value))
+        if is_values_diff(value, current_value):
+            properties4update[key] = value
+
+    if not properties4update:
+        logger.info("No properties to update")
+        return
+    user = os.getenv('PATRONI_REST_API_USER')
+    password = os.getenv('PATRONI_REST_API_PASSWORD')
+    from requests.auth import HTTPBasicAuth
+    basic_auth = HTTPBasicAuth(user, password)
+
+    if patroni_restart_state(basic_auth):
+        logger.info("Schedule restart because some settings requires restart and restart_pg is true")
+        schedule_restart()
+        sys.exit(1)
+    else:
+        sys.exit(0)
+
+
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser(description='Validation procedure')
+    parser.add_argument('--conf-file', dest='conf_file', default=None, required=True,
+                        help='path to file with postgresql settings')
+    parser.add_argument('--restart-pg', dest='restart_pg', default='false',
+                        help='Restart postgres if there are settings which requires restart')
+
+    args = parser.parse_args()
+
+    main(args.conf_file, args.restart_pg == "true")
+
diff --git a/docker-pgbackrest-sidecar/.gitignore b/docker-pgbackrest-sidecar/.gitignore
new file mode 100644
index 0000000..7fb90fa
--- /dev/null
+++ b/docker-pgbackrest-sidecar/.gitignore
@@ -0,0 +1 @@
+/build/_output
\ No newline at end of file
diff --git a/docker-pgbackrest-sidecar/CODE-OF-CONDUCT.md b/docker-pgbackrest-sidecar/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-pgbackrest-sidecar/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-pgbackrest-sidecar/CONTRIBUTING.md b/docker-pgbackrest-sidecar/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-pgbackrest-sidecar/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-pgbackrest-sidecar/LICENSE b/docker-pgbackrest-sidecar/LICENSE
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/docker-pgbackrest-sidecar/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-pgbackrest-sidecar/Makefile b/docker-pgbackrest-sidecar/Makefile
new file mode 100644
index 0000000..0dd9be5
--- /dev/null
+++ b/docker-pgbackrest-sidecar/Makefile
@@ -0,0 +1,35 @@
+DOCKER_FILE := build/Dockerfile
+
+ifndef TAG_ENV
+override TAG_ENV = local
+endif
+
+ifndef DOCKER_NAMES
+override DOCKER_NAMES = "ghcr.io/netcracker/pgskipper-pgbackrest-sidecar:${TAG_ENV}"
+endif
+
+sandbox-build: deps compile docker-build
+
+all: sandbox-build docker-push
+
+local: deps compile docker-build docker-push
+
+local-bench: deps compile-bench
+
+deps:
+	GO111MODULE=on go mod tidy
+	@echo "Move helm charts"
+
+
+compile:
+	CGO_ENABLED=0 go build -o ./build/_output/bin/pgskipper-pgbackrest-sidecar \
+                  -gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./main/main.go
+
+docker-build:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker build --file="${DOCKER_FILE}" --pull -t $(docker_tag) ./;)
+
+docker-push:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker push $(docker_tag);)
+
+clean:
+	rm -rf build/_output
diff --git a/docker-pgbackrest-sidecar/README.md b/docker-pgbackrest-sidecar/README.md
new file mode 100644
index 0000000..adee358
--- /dev/null
+++ b/docker-pgbackrest-sidecar/README.md
@@ -0,0 +1 @@
+# pgskipper-pgbackrest-sidecar
\ No newline at end of file
diff --git a/docker-pgbackrest-sidecar/SECURITY.md b/docker-pgbackrest-sidecar/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-pgbackrest-sidecar/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-pgbackrest-sidecar/build/Dockerfile b/docker-pgbackrest-sidecar/build/Dockerfile
new file mode 100644
index 0000000..1562511
--- /dev/null
+++ b/docker-pgbackrest-sidecar/build/Dockerfile
@@ -0,0 +1,105 @@
+FROM --platform=$BUILDPLATFORM golang:1.25.3-alpine3.22 AS builder
+
+ENV GOSUMDB=off
+
+WORKDIR /workspace
+
+# Copy the Go Modules manifests
+COPY go.mod go.mod
+COPY go.sum go.sum
+
+RUN go mod download
+
+# Copy the go source
+COPY main/ main/
+COPY pkg/ pkg/
+
+RUN go mod tidy
+
+# Build
+ARG TARGETOS TARGETARCH
+RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH go build -o ./build/_output/bin/pgskipper-pgbackrest-sidecar \
+    -gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./main/main.go
+
+FROM alpine:3.22
+
+ENV USER=postgres \
+    HOME=/home/pg
+
+COPY build/build-pgbackrest.sh /tmp/build-pgbackrest.sh
+
+# Install build dependencies and runtime dependencies
+RUN apk add --update --no-cache \
+    # Runtime dependencies
+    busybox \
+    ssl_client \
+    curl \
+    openssh \
+    # Runtime libraries for pgbackrest
+    openssl \
+    libxml2 \
+    zlib \
+    bzip2 \
+    lz4 \
+    zstd \
+    postgresql-client \
+    yaml \
+    # Build dependencies for pgbackrest
+    build-base \
+    meson \
+    ninja \
+    autoconf \
+    automake \
+    libtool \
+    pkgconfig \
+    openssl-dev \
+    libxml2-dev \
+    zlib-dev \
+    bzip2-dev \
+    lz4-dev \
+    zstd-dev \
+    postgresql-dev \
+    yaml-dev \
+    wget \
+    tar && \
+    chmod +x /tmp/build-pgbackrest.sh && \
+    sh /tmp/build-pgbackrest.sh && \
+    rm /tmp/build-pgbackrest.sh && \
+    # Remove build dependencies to reduce image size
+    apk del \
+    build-base \
+    meson \
+    ninja \
+    autoconf \
+    automake \
+    libtool \
+    pkgconfig \
+    openssl-dev \
+    postgresql-dev \
+    yaml-dev \
+    wget \
+    tar
+
+COPY --from=builder /workspace/build/_output/bin/pgskipper-pgbackrest-sidecar /usr/local/bin/pgskipper-pgbackrest-sidecar
+COPY build/start.sh /opt/start.sh
+
+RUN mkdir ${HOME}
+
+RUN chgrp 0 /etc &&  \
+    chmod g+w /etc && \
+    chgrp 0 /etc/passwd &&  \
+    chmod g+w /etc/passwd && \
+    chmod 777 ${HOME} && \
+    chmod 770 /var/lib/pgbackrest && \
+    chmod 770 /var/log/pgbackrest && \
+    chmod 770 /var/spool/pgbackrest && \
+    chown postgres:0 /var/lib/pgbackrest && \
+    chown postgres:0 /var/log/pgbackrest && \
+    chown postgres:0 /var/spool/pgbackrest && \
+    chmod 777 /opt/start.sh &&  \
+    chmod +x /opt/start.sh
+
+VOLUME /etc
+
+CMD ["sh", "/opt/start.sh"]
+USER ${USER}
\ No newline at end of file
diff --git a/docker-pgbackrest-sidecar/build/build-pgbackrest.sh b/docker-pgbackrest-sidecar/build/build-pgbackrest.sh
new file mode 100644
index 0000000..2b39bd7
--- /dev/null
+++ b/docker-pgbackrest-sidecar/build/build-pgbackrest.sh
@@ -0,0 +1,50 @@
+#!/bin/bash
+set -e
+
+# pgBackRest version to build
+PGBACKREST_VERSION="2.55.1"
+PGBACKREST_URL="https://github.com/pgbackrest/pgbackrest/archive/release/${PGBACKREST_VERSION}.tar.gz"
+
+# Build directory
+BUILD_DIR="/tmp/pgbackrest-build"
+INSTALL_PREFIX="/usr"
+
+echo "Building pgBackRest ${PGBACKREST_VERSION} from source..."
+
+# Create build directory
+mkdir -p ${BUILD_DIR}
+cd ${BUILD_DIR}
+
+# Download and extract source
+echo "Downloading pgBackRest ${PGBACKREST_VERSION}..."
+wget -O pgbackrest-${PGBACKREST_VERSION}.tar.gz ${PGBACKREST_URL}
+tar -xzf pgbackrest-${PGBACKREST_VERSION}.tar.gz
+cd pgbackrest-release-${PGBACKREST_VERSION}
+
+# Configure build with Meson (pgBackRest 2.51+ uses Meson instead of autotools)
+echo "Configuring build with Meson..."
+meson setup build \
+    --prefix=${INSTALL_PREFIX} \
+    --buildtype=release
+
+# Build with Ninja
+echo "Building pgBackRest..."
+ninja -C build
+
+# Install
+echo "Installing pgBackRest..."
+ninja -C build install
+
+# Verify installation
+echo "Verifying installation..."
+pgbackrest version
+
+# Create directories for pgBackRest
+mkdir -p /var/lib/pgbackrest
+mkdir -p /var/log/pgbackrest
+mkdir -p /var/spool/pgbackrest
+
+rm -rf ${BUILD_DIR}
+
+echo "pgBackRest ${PGBACKREST_VERSION} build completed successfully!"
+
diff --git a/docker-pgbackrest-sidecar/build/start.sh b/docker-pgbackrest-sidecar/build/start.sh
new file mode 100644
index 0000000..ba8c9e5
--- /dev/null
+++ b/docker-pgbackrest-sidecar/build/start.sh
@@ -0,0 +1,56 @@
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+function check_user(){
+    cur_user=$(id -u)
+    if [ "$cur_user" != "0" ]
+    then
+        echo "Adding randomly generated uid to passwd file..."
+        sed -i '/postgres/d' /etc/passwd
+        if ! whoami &> /dev/null; then
+          if [ -w /etc/passwd ]; then
+            echo "postgres:x:$(id -u):0:postgres user:${HOME}:/sbin/nologin" >> /etc/passwd
+          fi
+        fi
+    fi
+#    fix_backrest_permissions "$cur_user"
+}
+
+function fix_backrest_permissions(){
+    chmod 750 /var/lib/pgbackrest
+    chmod 750 /var/log/pgbackrest
+    chmod 750 /var/spool/pgbackrest
+    chown $1:$1 /var/lib/pgbackrest
+    chown $1:$1 /var/log/pgbackrest
+    chown $1:$1 /var/spool/pgbackrest
+}
+
+function prepare_home_folder(){
+    mkdir -p ${HOME}/.ssh
+    chmod 700 ${HOME}/.ssh
+
+    cp /keys/id_rsa ${HOME}/.ssh/id_rsa
+    cp /keys/id_rsa.pub ${HOME}/.ssh/id_rsa.pub
+    cp /keys/id_rsa.pub ${HOME}/.ssh/authorized_keys
+    cp /keys/id_rsa.pub ${HOME}/.ssh/known_hosts
+    sed -i "s/ssh-rsa/pg-patroni ssh-rsa/" ${HOME}/.ssh/known_hosts
+
+    chmod 600 ${HOME}/.ssh/id_rsa
+}
+
+check_user
+if [ -n "$PGBACKREST_PG2_HOST" ]; then
+  prepare_home_folder
+fi
+/usr/local/bin/pgskipper-pgbackrest-sidecar
\ No newline at end of file
diff --git a/docker-pgbackrest-sidecar/go.mod b/docker-pgbackrest-sidecar/go.mod
new file mode 100644
index 0000000..e5ea802
--- /dev/null
+++ b/docker-pgbackrest-sidecar/go.mod
@@ -0,0 +1,22 @@
+module github.com/Netcracker/pgskipper-pgbackrest-sidecar
+
+go 1.25.3
+
+require (
+	github.com/gofiber/fiber/v2 v2.52.9
+	go.uber.org/zap v1.27.0
+)
+
+require (
+	github.com/andybalholm/brotli v1.2.0 // indirect
+	github.com/google/uuid v1.6.0 // indirect
+	github.com/klauspost/compress v1.18.0 // indirect
+	github.com/mattn/go-colorable v0.1.14 // indirect
+	github.com/mattn/go-isatty v0.0.20 // indirect
+	github.com/mattn/go-runewidth v0.0.16 // indirect
+	github.com/rivo/uniseg v0.4.7 // indirect
+	github.com/valyala/bytebufferpool v1.0.0 // indirect
+	github.com/valyala/fasthttp v1.65.0 // indirect
+	go.uber.org/multierr v1.11.0 // indirect
+	golang.org/x/sys v0.36.0 // indirect
+)
diff --git a/docker-pgbackrest-sidecar/go.sum b/docker-pgbackrest-sidecar/go.sum
new file mode 100644
index 0000000..9a69223
--- /dev/null
+++ b/docker-pgbackrest-sidecar/go.sum
@@ -0,0 +1,40 @@
+github.com/andybalholm/brotli v1.2.0 h1:ukwgCxwYrmACq68yiUqwIWnGY0cTPox/M94sVwToPjQ=
+github.com/andybalholm/brotli v1.2.0/go.mod h1:rzTDkvFWvIrjDXZHkuS16NPggd91W3kUSvPlQ1pLaKY=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/gofiber/fiber/v2 v2.52.9 h1:YjKl5DOiyP3j0mO61u3NTmK7or8GzzWzCFzkboyP5cw=
+github.com/gofiber/fiber/v2 v2.52.9/go.mod h1:YEcBbO/FB+5M1IZNBP9FO3J9281zgPAreiI1oqg8nDw=
+github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
+github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/klauspost/compress v1.18.0 h1:c/Cqfb0r+Yi+JtIEq73FWXVkRonBlf0CRNYc8Zttxdo=
+github.com/klauspost/compress v1.18.0/go.mod h1:2Pp+KzxcywXVXMr50+X0Q/Lsb43OQHYWRCY2AiWywWQ=
+github.com/mattn/go-colorable v0.1.14 h1:9A9LHSqF/7dyVVX6g0U9cwm9pG3kP9gSzcuIPHPsaIE=
+github.com/mattn/go-colorable v0.1.14/go.mod h1:6LmQG8QLFO4G5z1gPvYEzlUgJ2wF+stgPZH1UqBm1s8=
+github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
+github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
+github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
+github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
+github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
+github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
+github.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=
+github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
+github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=
+github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=
+github.com/valyala/fasthttp v1.65.0 h1:j/u3uzFEGFfRxw79iYzJN+TteTJwbYkru9uDp3d0Yf8=
+github.com/valyala/fasthttp v1.65.0/go.mod h1:P/93/YkKPMsKSnATEeELUCkG8a7Y+k99uxNHVbKINr4=
+github.com/xyproto/randomstring v1.0.5 h1:YtlWPoRdgMu3NZtP45drfy1GKoojuR7hmRcnhZqKjWU=
+github.com/xyproto/randomstring v1.0.5/go.mod h1:rgmS5DeNXLivK7YprL0pY+lTuhNQW3iGxZ18UQApw/E=
+go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
+go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
+go.uber.org/multierr v1.11.0 h1:blXXJkSxSSfBVBlC76pxqeO+LN3aDfLQo+309xJstO0=
+go.uber.org/multierr v1.11.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
+go.uber.org/zap v1.27.0 h1:aJMhYGrd5QSmlpLMr2MftRKl7t8J8PTZPA732ud/XR8=
+go.uber.org/zap v1.27.0/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
+golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.36.0 h1:KVRy2GtZBrk1cBYA7MKu5bEZFxQk4NIDV6RLVcC8o0k=
+golang.org/x/sys v0.36.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
diff --git a/docker-pgbackrest-sidecar/main/main.go b/docker-pgbackrest-sidecar/main/main.go
new file mode 100644
index 0000000..e62d078
--- /dev/null
+++ b/docker-pgbackrest-sidecar/main/main.go
@@ -0,0 +1,110 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"log"
+
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/backup"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/restore"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/stanza"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils"
+	fiber "github.com/gofiber/fiber/v2"
+)
+
+func main() {
+
+	_ = stanza.CreateStanza()
+	app := fiber.New()
+
+	app.Post("/backup", func(c *fiber.Ctx) error {
+		payload := struct {
+			Timestamp string `json:"timestamp"`
+		}{}
+		if err := c.BodyParser(&payload); err != nil {
+			return err
+		}
+		if err := backup.MakeFullBackup(payload.Timestamp); err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON("Backup started successfully")
+	})
+
+	app.Post("/backup/diff", func(c *fiber.Ctx) error {
+		payload := struct {
+			Timestamp string `json:"timestamp"`
+		}{}
+		if err := c.BodyParser(&payload); err != nil {
+			return err
+		}
+
+		if err := backup.MakeDiffBackup(payload.Timestamp); err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON("Diff backup has been started successfully")
+	})
+
+	app.Post("/backup/incr", func(c *fiber.Ctx) error {
+		payload := struct {
+			Timestamp string `json:"timestamp"`
+		}{}
+		if err := c.BodyParser(&payload); err != nil {
+			return err
+		}
+
+		if err := backup.MakeIncrBackup(payload.Timestamp); err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON("Delta backup has been started successfully")
+	})
+	app.Get("/status", func(c *fiber.Ctx) error {
+		err, status := backup.GetBackupStatus(c.Query("timestamp"))
+		if err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON(status)
+	})
+	app.Get("/list", func(c *fiber.Ctx) error {
+		err, status := backup.GetBackupList()
+		if err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON(status)
+	})
+
+	app.Post("/restore", func(c *fiber.Ctx) error {
+
+		payload := utils.Payload{}
+
+		if err := c.BodyParser(&payload); err != nil {
+			return err
+		}
+		err := restore.PerformRestore(payload)
+		if err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON("Restore procedure has been started successfully")
+	})
+
+	app.Post("/upgrade", func(c *fiber.Ctx) error {
+		err := stanza.UpgradeStanza()
+		if err != nil {
+			return c.Status(500).SendString(err.Error())
+		}
+		return c.Status(200).JSON("Stanza has been upgraded")
+	})
+
+	log.Fatal(app.Listen(":3000"))
+}
diff --git a/docker-pgbackrest-sidecar/pkg/backup/backup.go b/docker-pgbackrest-sidecar/pkg/backup/backup.go
new file mode 100644
index 0000000..e47e978
--- /dev/null
+++ b/docker-pgbackrest-sidecar/pkg/backup/backup.go
@@ -0,0 +1,144 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package backup
+
+import (
+	"encoding/json"
+	"fmt"
+	"os"
+
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils/constants"
+	"go.uber.org/zap"
+)
+
+var (
+	logger = utils.GetLogger()
+)
+
+type Root struct {
+	Backup []Backup `json:"backup"`
+	Status Status   `json:"status"`
+}
+type Backup struct {
+	Annotation map[string]string `json:"annotation"`
+	Error      bool              `json:"error"`
+	Info       Info              `json:"info"`
+	Label      string            `json:"label"`
+	Type       string            `json:"type"`
+}
+
+type Info struct {
+	Delta int `json:"delta"`
+	Size  int `json:"size"`
+}
+
+type LockBackup struct {
+	Held bool `json:"held"`
+}
+
+type Lock struct {
+	Backup LockBackup `json:"backup"`
+}
+type Status struct {
+	Code    int    `json:"code"`
+	Lock    Lock   `json:"lock"`
+	Message string `json:"message"`
+}
+
+func MakeFullBackup(timestamp string) error {
+	annotation := fmt.Sprintf("--annotation=timestamp=%s", timestamp)
+	args := []string{fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA")), "--repo=1", annotation}
+	args = append(args, "--type=full", "backup")
+	logger.Info("Backup has been started")
+	if err, _ := utils.ExecCommand(constants.BackrestBin, args); err != nil {
+
+		return err
+	}
+	return nil
+}
+
+func MakeDiffBackup(timestamp string) error {
+	annotation := fmt.Sprintf("--annotation=timestamp=%s", timestamp)
+	args := []string{fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA")), "--repo=1", annotation}
+	args = append(args, "--type=diff", "backup")
+	logger.Info("Diff Backup has been started")
+	if err, _ := utils.ExecCommand(constants.BackrestBin, args); err != nil {
+
+		return err
+	}
+	return nil
+}
+
+func MakeIncrBackup(timestamp string) error {
+	annotation := fmt.Sprintf("--annotation=timestamp=%s", timestamp)
+	args := []string{fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA")), "--repo=1", annotation}
+	args = append(args, "--type=incr", "backup")
+	logger.Info("Incremental Backup has been started")
+	if err, _ := utils.ExecCommand(constants.BackrestBin, args); err != nil {
+
+		return err
+	}
+	return nil
+}
+
+func GetBackupStatus(label string) (error, Backup) {
+	var root []Root
+	args := []string{"info", "--output=json", fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA"))}
+	logger.Info("Backup status requested")
+	err, output := utils.ExecCommand(constants.BackrestBin, args)
+	if err != nil {
+		return err, Backup{}
+	}
+	if err := json.Unmarshal([]byte(output), &root); err != nil {
+		logger.Error("Error while unmarshalling output", zap.Error(err))
+		return err, Backup{}
+	}
+	logger.Info(fmt.Sprintf("info %v", root))
+	status := GetBackupIdStatus(label, root)
+	return nil, status
+}
+
+func GetBackupIdStatus(label string, backupList []Root) Backup {
+	for _, item := range backupList {
+		logger.Info(fmt.Sprintf("Item %v", item))
+		for _, backup := range item.Backup {
+			logger.Info(fmt.Sprintf("Backup %v", backup))
+			if timestamp, ok := backup.Annotation["timestamp"]; ok {
+				logger.Info(fmt.Sprintf("Compare %v and %v", label, timestamp))
+				if label == timestamp {
+					return backup
+				}
+			}
+		}
+
+	}
+	return Backup{}
+}
+
+func GetBackupList() (error, []Backup) {
+	var root []Root
+	args := []string{"info", "--output=json", fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA"))}
+	logger.Info("Backup list requested")
+	err, output := utils.ExecCommand(constants.BackrestBin, args)
+	if err != nil {
+		return err, []Backup{}
+	}
+	if err := json.Unmarshal([]byte(output), &root); err != nil {
+		logger.Error("Error while unmarshalling output", zap.Error(err))
+		return err, []Backup{}
+	}
+	return nil, root[0].Backup
+}
diff --git a/docker-pgbackrest-sidecar/pkg/restore/restore.go b/docker-pgbackrest-sidecar/pkg/restore/restore.go
new file mode 100644
index 0000000..56c2591
--- /dev/null
+++ b/docker-pgbackrest-sidecar/pkg/restore/restore.go
@@ -0,0 +1,56 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package restore
+
+import (
+	"fmt"
+	"os"
+
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils/constants"
+)
+
+var (
+	logger = utils.GetLogger()
+)
+
+func setParams(payload utils.Payload) []string {
+	var args []string
+	if payload.BackupId != "" {
+		value := fmt.Sprintf("--set=%s", payload.BackupId)
+		args = append(args, value)
+	}
+	if payload.Type != "" {
+		value := fmt.Sprintf("--type=%s", payload.Type)
+		args = append(args, value)
+	}
+	if payload.Target != "" {
+		value := fmt.Sprintf("--target=%s", payload.Target)
+		args = append(args, value)
+	}
+	return args
+}
+
+func PerformRestore(payload utils.Payload) error {
+	restoreParams := setParams(payload)
+	args := []string{"restore", fmt.Sprintf("--stanza=%s", os.Getenv("PGBACKREST_STANZA"))}
+	args = append(args, restoreParams...)
+	logger.Info("Restore procedure has been started")
+	if err, _ := utils.ExecCommand(constants.BackrestBin, args); err != nil {
+
+		return err
+	}
+	return nil
+}
diff --git a/docker-pgbackrest-sidecar/pkg/stanza/stanza.go b/docker-pgbackrest-sidecar/pkg/stanza/stanza.go
new file mode 100644
index 0000000..da8f9c2
--- /dev/null
+++ b/docker-pgbackrest-sidecar/pkg/stanza/stanza.go
@@ -0,0 +1,48 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package stanza
+
+import (
+	"fmt"
+	"time"
+
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils"
+	"github.com/Netcracker/pgskipper-pgbackrest-sidecar/pkg/utils/constants"
+)
+
+var (
+	args   = []string{"stanza-create", "--log-level-console=info"}
+	logger = utils.GetLogger()
+)
+
+func CreateStanza() error {
+	time.Sleep(30 * time.Second)
+	if err, _ := utils.ExecCommand(constants.BackrestBin, args); err != nil {
+		logger.Error(fmt.Sprintf("While creating stanza en error occures %v", err))
+		return err
+	}
+	return nil
+
+}
+
+func UpgradeStanza() error {
+	cmd := []string{"stanza-upgrade", "--log-level-console=info"}
+	if err, _ := utils.ExecCommand(constants.BackrestBin, cmd); err != nil {
+		logger.Error(fmt.Sprintf("While creating stanza en error occures %v", err))
+		return err
+	}
+	return nil
+
+}
diff --git a/docker-pgbackrest-sidecar/pkg/utils/constants/constants.go b/docker-pgbackrest-sidecar/pkg/utils/constants/constants.go
new file mode 100644
index 0000000..d46a1a7
--- /dev/null
+++ b/docker-pgbackrest-sidecar/pkg/utils/constants/constants.go
@@ -0,0 +1,19 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package constants
+
+const (
+	BackrestBin = "pgbackrest"
+)
diff --git a/docker-pgbackrest-sidecar/pkg/utils/utils.go b/docker-pgbackrest-sidecar/pkg/utils/utils.go
new file mode 100644
index 0000000..00dcb72
--- /dev/null
+++ b/docker-pgbackrest-sidecar/pkg/utils/utils.go
@@ -0,0 +1,85 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"bufio"
+	"fmt"
+	"go.uber.org/zap"
+	"go.uber.org/zap/zapcore"
+	"os"
+	"os/exec"
+)
+
+var (
+	logger = GetLogger()
+)
+
+type Payload struct {
+	BackupId string `json:"backupId,omitempty"`
+	Type     string `json:"type,omitempty"`
+	Target   string `json:"target,omitempty"`
+}
+
+func ExecCommand(command string, args []string) (error, string) {
+	var output string
+
+	logger.Info(fmt.Sprintf("Executed command is %s with args %v", command, args))
+	cmd := exec.Command(command, args...)
+
+	stdout, err := cmd.StdoutPipe()
+	if err != nil {
+		logger.Error("Error obtaining StdoutPipe", zap.Error(err))
+		return err, ""
+	}
+	if err := cmd.Start(); err != nil {
+		logger.Error("Error starting command", zap.Error(err))
+		return err, ""
+	}
+
+	in := bufio.NewScanner(stdout)
+
+	for in.Scan() {
+		output = in.Text()
+		logger.Info(output)
+
+	}
+	if err = in.Err(); err != nil {
+		logger.Error("Error reading stdout", zap.Error(err))
+		return err, ""
+	}
+
+	if err := cmd.Wait(); err != nil {
+		logger.Error("Error waiting for command to finish", zap.Error(err))
+		return err, ""
+	}
+
+	return nil, output
+}
+
+func GetLogger() *zap.Logger {
+	atom := zap.NewAtomicLevel()
+	encoderCfg := zap.NewProductionEncoderConfig()
+	encoderCfg.TimeKey = "timestamp"
+	encoderCfg.EncodeTime = zapcore.ISO8601TimeEncoder
+
+	logger := zap.New(zapcore.NewCore(
+		zapcore.NewJSONEncoder(encoderCfg),
+		zapcore.Lock(os.Stdout),
+		atom,
+	))
+	defer func() { _ = logger.Sync() }()
+	return logger
+}
diff --git a/docker-query-exporter/.editorconfig b/docker-query-exporter/.editorconfig
new file mode 100644
index 0000000..007f685
--- /dev/null
+++ b/docker-query-exporter/.editorconfig
@@ -0,0 +1,35 @@
+root = true
+
+[{*,.*}]
+charset = utf-8
+indent_style = space
+insert_final_newline = true
+trim_trailing_whitespace = true
+
+[*.sh]
+end_of_line = lf
+
+[{*.bat,*.cmd}]
+end_of_line = crlf
+
+[*.go]
+# gofmt defaults to LF for all the platforms: https://github.com/golang/go/issues/16355
+end_of_line = lf
+
+[*.md]
+# Trailing whitespace is important in Markdown (they distinguish a new line from a new paragraph)
+eclint_indent_style = unset
+trim_trailing_whitespace = false
+
+[{go.mod,go.sum,*.go,.gitmodules}]
+indent_size = 4
+indent_style = tab
+
+[Dockerfile]
+indent_size = 4
+
+[*.py]
+profile = black
+
+[*.sh]
+indent_size = 4
diff --git a/docker-query-exporter/.gitattributes b/docker-query-exporter/.gitattributes
new file mode 100644
index 0000000..569df3f
--- /dev/null
+++ b/docker-query-exporter/.gitattributes
@@ -0,0 +1,126 @@
+###############################
+# Qubership common            #
+###############################
+.editorconfig text
+.flake8 text
+.gitattributes text
+.gitignore text
+.helmignore text
+.prettierignore text
+
+*.env text eol=lf
+*.json text
+*.md text
+*.mod text
+*.robot text
+*.sum text
+*.tpl text
+*.txt text
+*.yaml text
+*.yml text
+
+LICENSE text
+Dockerfile text
+
+/CHANGELOG.md merge=union
+/contributors.json merge=union
+/CODE-OF-CONDUCT.md text
+/CONTRIBUTING.md text
+/README.md text
+/SECURITY.md text
+
+###############################
+# Git Line Endings            #
+###############################
+
+# Set default behaviour to automatically normalize line endings.
+* text=auto
+
+
+# Force batch scripts to always use CRLF line endings so that if a repo is accessed
+# in Windows via a file share from Linux, the scripts will work.
+*.{cmd,[cC][mM][dD]} text eol=crlf
+*.{bat,[bB][aA][tT]} text eol=crlf
+
+# Force bash scripts to always use LF line endings so that if a repo is accessed
+# in Unix via a file share from Windows, the scripts will work.
+*.sh text eol=lf
+# gofmt defaults to LF for all the platforms: https://github.com/golang/go/issues/16355
+*.go text eol=lf
+
+##########################################
+# Basic .gitattributes for a Java repo.#
+##########################################
+
+# Java sources
+*.java          text diff=java
+*.kt            text diff=kotlin
+*.groovy        text diff=java
+*.scala         text diff=java
+*.gradle        text diff=java
+*.gradle.kts    text diff=kotlin
+
+# These files are text and should be normalized (Convert crlf => lf)
+*.css           text diff=css
+*.scss          text diff=css
+*.sass          text
+*.df            text
+*.htm           text diff=html
+*.html          text diff=html
+*.js            text
+*.mjs           text
+*.cjs           text
+*.jsp           text
+*.jspf          text
+*.jspx          text
+*.properties    text
+*.tld           text
+*.tag           text
+*.tagx          text
+*.xml           text
+
+# These files are binary and should be left untouched
+# (binary is a macro for -text -diff)
+*.class         binary
+*.dll           binary
+*.ear           binary
+*.jar           binary
+*.so            binary
+*.war           binary
+*.jks           binary
+
+# Common build-tool wrapper scripts ('.cmd' versions are handled by 'Common.gitattributes')
+mvnw            text eol=lf
+gradlew         text eol=lf
+
+##########################################
+# Basic .gitattributes for a python repo.#
+##########################################
+
+# Source files
+# ============
+*.pxd    text diff=python
+*.py     text diff=python
+*.py3    text diff=python
+*.pyw    text diff=python
+*.pyx    text diff=python
+*.pyz    text diff=python
+*.pyi    text diff=python
+
+# Binary files
+# ============
+*.db     binary
+*.p      binary
+*.pkl    binary
+*.pickle binary
+*.pyc    binary export-ignore
+*.pyo    binary export-ignore
+*.pyd    binary
+
+# Jupyter notebook
+*.ipynb  text eol=lf
+
+# Note: .db, .p, and .pkl files are associated
+# with the python modules ``pickle``, ``dbm.*``,
+# ``shelve``, ``marshal``, ``anydbm``, & ``bsddb``
+# (among others).
diff --git a/docker-query-exporter/.gitignore b/docker-query-exporter/.gitignore
new file mode 100644
index 0000000..f420e5d
--- /dev/null
+++ b/docker-query-exporter/.gitignore
@@ -0,0 +1,16 @@
+# Default ignored files
+/shelf/
+/.idea/workspace.xml
+# Datasource local storage ignored files
+/dataSources/
+/dataSources.local.xml
+# Editor-based HTTP Client requests
+/httpRequests/
+# Idea files
+.idea
+# Build artifacts
+/query_exporter
+/query_exporter.log
+/query_exporter.zip
+/local_test_config.yaml
+build/_output
\ No newline at end of file
diff --git a/docker-query-exporter/CODE-OF-CONDUCT.md b/docker-query-exporter/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-query-exporter/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-query-exporter/CONTRIBUTING.md b/docker-query-exporter/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-query-exporter/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-query-exporter/LICENSE b/docker-query-exporter/LICENSE
new file mode 100644
index 0000000..d645695
--- /dev/null
+++ b/docker-query-exporter/LICENSE
@@ -0,0 +1,202 @@
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-query-exporter/Makefile b/docker-query-exporter/Makefile
new file mode 100644
index 0000000..c36794d
--- /dev/null
+++ b/docker-query-exporter/Makefile
@@ -0,0 +1,35 @@
+DOCKER_FILE := build/Dockerfile
+
+ifndef TAG_ENV
+override TAG_ENV = local
+endif
+
+ifndef DOCKER_NAMES
+override DOCKER_NAMES = "ghcr.io/netcracker/qubership-query-exporter:${TAG_ENV}"
+endif
+
+sandbox-build: deps docker-build
+
+all: sandbox-build docker-push
+
+local: deps docker-build docker-push
+
+deps:
+	go mod tidy
+	GO111MODULE=on
+
+fmt:
+	gofmt -l -s -w .
+
+
+docker-build:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker build --file="${DOCKER_FILE}" --pull -t $(docker_tag) ./;)
+
+docker-push:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker push $(docker_tag);)
+
+clean:
+	rm -rf build/_output
+
+test:
+	go test -v ./...
diff --git a/docker-query-exporter/README.md b/docker-query-exporter/README.md
new file mode 100644
index 0000000..8f616be
--- /dev/null
+++ b/docker-query-exporter/README.md
@@ -0,0 +1 @@
+# qubership-query-exporter
\ No newline at end of file
diff --git a/docker-query-exporter/SECURITY.md b/docker-query-exporter/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-query-exporter/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-query-exporter/build/Dockerfile b/docker-query-exporter/build/Dockerfile
new file mode 100644
index 0000000..9f75854
--- /dev/null
+++ b/docker-query-exporter/build/Dockerfile
@@ -0,0 +1,27 @@
+FROM golang:1.25-alpine3.22
+
+ENV EXPORTER_FILE=/usr/local/bin/query-exporter \
+    USER_UID=1001
+
+COPY build/alpine-repositories /etc/apk/repositories
+
+RUN apk add --upgrade --no-cache gcc 
+RUN apk add --upgrade --no-cache build-base
+
+COPY . /workdir
+WORKDIR /workdir
+
+RUN go mod tidy
+
+RUN env GOOS=$TARGETOS GOARCH=$TARGETARCH CGO_ENABLED=1 go build -o /build/_output/query-exporter \
+ 				-gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH}
+
+RUN cp /build/_output/query-exporter ${EXPORTER_FILE}
+
+
+COPY build/bin /usr/local/bin
+
+
+USER ${USER_UID}
+
+ENTRYPOINT ["/usr/local/bin/entrypoint"]
\ No newline at end of file
diff --git a/docker-query-exporter/build/alpine-repositories b/docker-query-exporter/build/alpine-repositories
new file mode 100644
index 0000000..4f390a2
--- /dev/null
+++ b/docker-query-exporter/build/alpine-repositories
@@ -0,0 +1,2 @@
+https://dl-cdn.alpinelinux.org/alpine/v3.22/main/
+https://dl-cdn.alpinelinux.org/alpine/v3.22/community/
\ No newline at end of file
diff --git a/docker-query-exporter/build/bin/entrypoint b/docker-query-exporter/build/bin/entrypoint
new file mode 100755
index 0000000..837b791
--- /dev/null
+++ b/docker-query-exporter/build/bin/entrypoint
@@ -0,0 +1,3 @@
+#!/bin/sh -e
+
+exec ${EXPORTER_FILE} $@
diff --git a/docker-query-exporter/collectors/histogram.go b/docker-query-exporter/collectors/histogram.go
new file mode 100644
index 0000000..b9016f5
--- /dev/null
+++ b/docker-query-exporter/collectors/histogram.go
@@ -0,0 +1,132 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package collectors
+
+import (
+	"fmt"
+	"strings"
+	"sync"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/prometheus/client_golang/prometheus"
+	log "github.com/sirupsen/logrus"
+)
+
+type CustomHistogram struct {
+	sync.RWMutex
+	constHistogramMap map[string]*prometheus.Metric
+	stateMap          map[string]*CurrentHistogramState
+	Desc              *prometheus.Desc
+	LabelsMap         map[string][]string
+	summarize         bool
+}
+
+type CurrentHistogramState struct {
+	Sum     float64
+	Cnt     uint64
+	Buckets map[float64]uint64
+}
+
+func NewCustomHistogram(desc *prometheus.Desc, summarize bool) *CustomHistogram {
+	customHistogram := CustomHistogram{}
+	customHistogram.Desc = desc
+	customHistogram.LabelsMap = make(map[string][]string)
+	customHistogram.stateMap = make(map[string]*CurrentHistogramState)
+	customHistogram.constHistogramMap = make(map[string]*prometheus.Metric)
+	customHistogram.summarize = summarize
+	return &customHistogram
+}
+
+func (e *CustomHistogram) Describe(ch chan<- *prometheus.Desc) {
+	e.RLock()
+	defer e.RUnlock()
+	for _, constHistogram := range e.constHistogramMap {
+		ch <- (*constHistogram).Desc()
+	}
+}
+
+func (e *CustomHistogram) Collect(ch chan<- prometheus.Metric) {
+	e.RLock()
+	defer e.RUnlock()
+	for _, constHistogram := range e.constHistogramMap {
+		ch <- *constHistogram
+	}
+}
+
+func (e *CustomHistogram) Observe(sum float64, cnt uint64, buckets map[float64]uint64, labels map[string]string, labelKeys []string) {
+	e.Lock()
+	defer e.Unlock()
+	if cnt < 0 {
+		log.Errorf("Value for cnt is less than 0 : %v ; Skipping histogram update", cnt)
+		return
+	}
+	labelValues := utils.GetOrderedMapValues(labels, labelKeys)
+	histKey := utils.MapToString(labels)
+	if e.stateMap[histKey] == nil {
+		e.stateMap[histKey] = &CurrentHistogramState{
+			Sum:     0,
+			Cnt:     0,
+			Buckets: make(map[float64]uint64),
+		}
+	}
+
+	if e.summarize {
+		e.stateMap[histKey].Sum += sum
+		e.stateMap[histKey].Cnt += cnt
+		for key, value := range buckets {
+			e.stateMap[histKey].Buckets[key] += value
+		}
+	} else {
+		e.stateMap[histKey].Sum = sum
+		e.stateMap[histKey].Cnt = cnt
+		for key, value := range buckets {
+			e.stateMap[histKey].Buckets[key] = value
+		}
+	}
+
+	constHistogram := prometheus.MustNewConstHistogram(e.Desc, e.stateMap[histKey].Cnt, e.stateMap[histKey].Sum, e.stateMap[histKey].Buckets, labelValues...)
+	e.constHistogramMap[histKey] = &constHistogram
+}
+
+func (e *CustomHistogram) DeletePartialMatch(labels prometheus.Labels) {
+	e.Lock()
+	defer e.Unlock()
+
+	if len(labels) == 0 {
+		return
+	}
+
+	keysToDelete := []string{}
+	for stateKey := range e.stateMap {
+		allLabelsPresent := true
+		for lKey, lValue := range labels {
+			jStr := fmt.Sprintf("%s=\"%s\"", lKey, lValue)
+			if !strings.Contains(stateKey, jStr) {
+				allLabelsPresent = false
+			}
+		}
+
+		if allLabelsPresent {
+			keysToDelete = append(keysToDelete, stateKey)
+		}
+	}
+
+	for _, key := range keysToDelete {
+		delete(e.stateMap, key)
+		delete(e.constHistogramMap, key)
+		delete(e.LabelsMap, key)
+	}
+}
diff --git a/docker-query-exporter/config/config.go b/docker-query-exporter/config/config.go
new file mode 100644
index 0000000..d3438ed
--- /dev/null
+++ b/docker-query-exporter/config/config.go
@@ -0,0 +1,448 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package config
+
+import (
+	"bytes"
+	"crypto/rand"
+	"encoding/base64"
+	"flag"
+	"fmt"
+	"io"
+	"os"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/Netcracker/qubership-query-exporter/crypto"
+	log "github.com/sirupsen/logrus"
+	"gopkg.in/yaml.v2"
+)
+
+var allowedMetricTypes = map[string]bool{
+	"histogram": true,
+	"gauge":     true,
+	"counter":   true,
+}
+
+type Config struct {
+	DbName         string          `yaml:"-"`
+	Db             *DatabaseConfig `yaml:"-"`
+	Databases      map[string]*DatabaseConfig
+	Metrics        map[string]*MetricsConfig
+	Queries        map[string]*QueryConfig
+	SelfMonitoring SelfMonitoringConfig `yaml:"self-monitoring,omitempty"`
+	Flags          map[string]string    `yaml:"flags,omitempty"`
+	ErrorHandling  *ErrorHandlingConfig `yaml:"error-handling,omitempty"`
+}
+
+type DatabaseConfig struct {
+	Type         string `yaml:"type,omitempty"`
+	Dsn          string
+	DecryptedDsn string `yaml:"-"`
+	KeepCon      bool   `yaml:"keep-connected"`
+	Labels       map[string]string
+	InitSqls     []string `yaml:"init-sql"`
+}
+
+type MetricsConfig struct {
+	Type           string
+	Description    string
+	Labels         []string          `yaml:",flow"`
+	ConstLabels    map[string]string `yaml:"const-labels,omitempty"`
+	Sum            string
+	Summarize      *bool `yaml:"summarize,omitempty"`
+	Count          string
+	Buckets        map[string]float64
+	Parameters     map[string]string
+	ExpectedLabels []map[string][]string `yaml:"expected-labels,flow"`
+}
+
+type QueryConfig struct {
+	Master           bool
+	Databases        []string     `yaml:",flow"`
+	LogicalDatabases []string     `yaml:"logicalDatabases,omitempty"`
+	Classifiers      []Classifier `yaml:"classifiers,omitempty"`
+	Interval         string
+	Croniter         string
+	Metrics          []string `yaml:",flow"`
+	Timeout          string
+	Parameters       []map[string]string
+	Sql              string
+}
+
+type Classifier map[string]string
+
+type SelfMonitoringConfig struct {
+	QueryLatencyBuckets []float64 `yaml:"query-latency-buckets,flow"`
+}
+
+type ErrorHandlingConfig struct {
+	DbErrorRules map[string]*DbErrorRule `yaml:"db-error-rules,flow"`
+}
+
+type DbErrorRule struct {
+	Action   string
+	Contains string
+	Timeout  string
+}
+
+var (
+	keyPath = flag.String("key-path", "", "Path to the key for dsn encryption")
+)
+
+const (
+	enc_prefix = "{ENC}"
+	keySize    = 32
+)
+
+func Read(path string) (*Config, error) {
+	config := Config{}
+	configFile, err := os.Open(path)
+	if err != nil {
+		return nil, fmt.Errorf("Error opening config file %v : %+v", path, err)
+	} else {
+		defer configFile.Close()
+	}
+
+	buf := bytes.Buffer{}
+
+	length, err := io.Copy(&buf, configFile)
+	if err != nil {
+		return nil, fmt.Errorf("Error copying config file %v : %+v", path, err)
+	}
+	log.Debugf("Copied %v bytes successfully to the buffer from file %v", length, path)
+
+	err = yaml.Unmarshal(buf.Bytes(), &config)
+	if err != nil {
+		return nil, fmt.Errorf("Error unmarshalling config file %v : %+v", path, err)
+	}
+
+	err = validateConfig(&config)
+	if err != nil {
+		return nil, fmt.Errorf("Error vaildating config file %v : %+v", path, err)
+	}
+
+	err = processCrypto(path, &config)
+	if err != nil {
+		return nil, fmt.Errorf("Crypto error for config %v : %+v", path, err)
+	}
+
+	config.initDefaultDB()
+	return &config, nil
+}
+
+func validateConfig(config *Config) error {
+	startupBlockingErrors := make([]string, 0)
+	log.Infof("CONFIG VALIDATION STARTED")
+	if len(config.Databases) == 0 {
+		return fmt.Errorf("Databases are not defined")
+	}
+	for dbName, dbConfig := range config.Databases {
+		if dbConfig.Dsn == "" {
+			return fmt.Errorf("Database %v has empty Dsn", dbName)
+		}
+		for label, labelValue := range dbConfig.Labels {
+			if labelValue == "" {
+				log.Warnf("Label %v for database %v has empty value", label, dbName)
+			}
+		}
+	}
+
+	if len(config.Queries) == 0 {
+		return fmt.Errorf("Queries are not defined")
+	}
+
+	if len(config.Metrics) == 0 {
+		log.Warnf("Metrics are not defined in yaml")
+	} else {
+		for mName, mCfg := range config.Metrics {
+			if mCfg == nil {
+				log.Errorf("Metric %v has empty configuration", mName)
+				continue
+			}
+			if mCfg.Description == "" {
+				log.Warnf("Metric %v has empty description", mName)
+			}
+			if mCfg.Type == "" {
+				log.Errorf("Metric %v has empty type", mName)
+			} else if !allowedMetricTypes[mCfg.Type] {
+				log.Errorf("Metric %v has not supported type %v", mName, mCfg.Type)
+			}
+			checkHistogramFields(startupBlockingErrors, mName, mCfg)
+			if !checkExpectedLabelsFields(mName, mCfg) {
+				log.Warnf("Expected labels for metric %v were reset to nil", mName)
+				mCfg.ExpectedLabels = nil
+			}
+		}
+	}
+
+	parser := utils.GetCronParser()
+
+	for qName, qCfg := range config.Queries {
+		if len(qCfg.Databases) == 0 {
+			log.Errorf("Databases for query %v are not defined in yaml", qName)
+		} else {
+			for _, dbName := range qCfg.Databases {
+				if config.Databases[dbName] == nil {
+					log.Errorf("Database %v for query %v is not defined in yaml", dbName, qName)
+				}
+			}
+		}
+		if qCfg.Timeout != "" {
+			_, err := time.ParseDuration(qCfg.Timeout)
+			if err != nil {
+				log.Errorf("Failed to parse timeout %v for query %v : %+v", qCfg.Timeout, qName, err)
+			}
+		}
+		if len(qCfg.Metrics) == 0 {
+			log.Warnf("For query %v metrics are not defined; dynamic logic will be applied, columns will be interpreted as [MetricName][MetricValue][LabelValue1]...[LabelValueN]", qName)
+		} else {
+			for _, metricName := range qCfg.Metrics {
+				if config.Metrics[metricName] == nil {
+					errorMessage := fmt.Sprintf("Metric %v for query %v is not defined in yaml", metricName, qName)
+					startupBlockingErrors = append(startupBlockingErrors, errorMessage)
+					log.Error(errorMessage)
+				}
+			}
+		}
+		if qCfg.Croniter == "" && qCfg.Interval == "" {
+			log.Warnf("Query %v has both croniter and interval fields empty. Query will be executed on scrape event", qName)
+		} else {
+			if qCfg.Croniter != "" {
+				_, parseErr := parser.Parse(qCfg.Croniter)
+				if parseErr != nil {
+					log.Errorf("Query %v has invalid croniter expression: %v ; %+v", qName, qCfg.Croniter, parseErr)
+				}
+			}
+			if qCfg.Interval != "" {
+				_, parseErr := parser.Parse("@every " + qCfg.Interval)
+				if parseErr != nil {
+					log.Errorf("Query %v has invalid interval expression: %v ; %+v", qName, qCfg.Interval, parseErr)
+				}
+			}
+			if qCfg.Croniter != "" && qCfg.Interval != "" {
+				log.Warnf("Query %v has both croniter and interval expressions defined. Croniter expression will be used if it is valid", qName)
+			}
+		}
+	}
+
+	log.Infof("CONFIG VALIDATION FINISHED")
+
+	if len(startupBlockingErrors) != 0 {
+		return fmt.Errorf("Query exporter can not start, reasons: %+v", startupBlockingErrors)
+	}
+	return nil
+}
+
+func checkHistogramFields(startupBlockingErrors []string, mName string, mCfg *MetricsConfig) {
+	if mCfg.Type == "histogram" {
+		if len(mCfg.Buckets) == 0 {
+			errorMessage := fmt.Sprintf("Buckets are not defined for histogram metric %v", mName)
+			startupBlockingErrors = append(startupBlockingErrors, errorMessage)
+		}
+		if mCfg.Sum == "" {
+			errorMessage := fmt.Sprintf("Sum field is not defined for histogram metric %v", mName)
+			startupBlockingErrors = append(startupBlockingErrors, errorMessage)
+		}
+		if mCfg.Count == "" {
+			errorMessage := fmt.Sprintf("Count field is not defined for histogram metric %v", mName)
+			startupBlockingErrors = append(startupBlockingErrors, errorMessage)
+		}
+	} else {
+		if len(mCfg.Buckets) != 0 {
+			log.Errorf("Buckets are defined for non-histogram metric %v, the field will be ignored", mName)
+		}
+		if mCfg.Sum != "" {
+			log.Errorf("Sum is defined for non-histogram metric %v, the field will be ignored", mName)
+		}
+		if mCfg.Count != "" {
+			log.Errorf("Count is defined for non-histogram metric %v, the field will be ignored", mName)
+		}
+	}
+}
+
+func checkExpectedLabelsFields(mName string, mCfg *MetricsConfig) bool {
+	labelsCount := len(mCfg.Labels)
+	for itemNum, expectedLabelsItem := range mCfg.ExpectedLabels {
+		if len(expectedLabelsItem) != labelsCount {
+			log.Errorf("Invalid expected labels configuration for metric %v, itemNum %v : Metric has %v labels defined while in expected labels item %v labels defined", mName, itemNum, labelsCount, len(expectedLabelsItem))
+			return false
+		}
+		for _, labelName := range mCfg.Labels {
+			if len(expectedLabelsItem[labelName]) == 0 {
+				log.Errorf("Invalid expected labels configuration for metric %v, itemNum %v : Metric has label %v defined while in expected labels this label is not defined", mName, itemNum, labelName)
+				return false
+			}
+		}
+	}
+	return true
+}
+
+func (c *Config) initDefaultDB() {
+	for dbName, dbConfig := range c.Databases {
+		c.DbName = dbName
+		c.Db = dbConfig
+		break
+	}
+	log.Infof("DbName is initialized as %v", c.DbName)
+}
+
+func processCrypto(path string, config *Config) error {
+	if *keyPath == "" {
+		log.Info("Key-path is not specified, so read dsn as plain-text")
+		for _, dbConfig := range config.Databases {
+			dbConfig.DecryptedDsn = dbConfig.Dsn
+		}
+		return nil
+	}
+
+	key, isNew, err := getOrCreateKey()
+	if err != nil {
+		return fmt.Errorf("Key Error : %+v", err)
+	}
+
+	cryptoService, err := crypto.NewCrypto(key)
+	if err != nil {
+		return fmt.Errorf("Crypto Error : %+v", err)
+	}
+
+	configModified := false
+	for dbName, dbConfig := range config.Databases {
+		if !strings.HasPrefix(dbConfig.Dsn, enc_prefix) {
+			dbConfig.DecryptedDsn = dbConfig.Dsn
+			encryptedDsn, err := cryptoService.Encrypt([]byte(dbConfig.DecryptedDsn))
+			if err != nil {
+				log.Errorf("Failed to encrypt dsn for %v : %+v", dbName, err)
+			} else {
+				dbConfig.Dsn = enc_prefix + encryptedDsn
+				log.Infof("Dsn encrypted successfully for %v", dbName)
+				configModified = true
+			}
+		} else {
+			if isNew {
+				return fmt.Errorf("There is no sense trying to decipher with fresh key %v", *keyPath)
+			}
+			dbConfig.DecryptedDsn, err = cryptoService.Decrypt([]byte(dbConfig.Dsn[len(enc_prefix):]))
+			if err != nil {
+				log.Errorf("Failed to decrypt dsn for %v : %+v", dbName, err)
+			} else {
+				log.Infof("Dsn decrypted successfully for %v", dbName)
+			}
+		}
+	}
+
+	if configModified {
+		log.Infof("Modifying %v ...", path)
+		err = writeConfigToFile(path, *config)
+		if err != nil {
+			return fmt.Errorf("Error modifying %v : %+v", path, err)
+		} else {
+			log.Infof("Config %v modified successfully", path)
+		}
+	}
+
+	return nil
+}
+
+func writeConfigToFile(path string, cfg Config) error {
+	err := os.Truncate(path, 0)
+	if err != nil {
+		return fmt.Errorf("Error truncating file %v : %+v", path, err)
+	}
+	perms := os.FileMode(utils.GetOctalUintEnvironmentVariable("CONFIG_FILE_PERMS", 0600))
+	err = os.Chmod(path, perms)
+	if err != nil {
+		return fmt.Errorf("Error chmod file %v : %+v", path, err)
+	}
+	f, err := os.OpenFile(path, os.O_RDWR|os.O_APPEND, perms)
+	defer func() {
+		log.Infof("File %v closed", path)
+		_ = f.Close()
+	}()
+	if err != nil {
+		return fmt.Errorf("Error opening file %v : %+v", path, err)
+	}
+	_, err = f.Seek(0, 0)
+	if err != nil {
+		return fmt.Errorf("Error executing Seek for file %v : %+v", path, err)
+	}
+	out, err := yaml.Marshal(cfg)
+	if err != nil {
+		return fmt.Errorf("Error marshalling yaml : %+v", err)
+	}
+	_, err = f.Write(out)
+	if err != nil {
+		return fmt.Errorf("Error writing to file %v : %+v", path, err)
+	}
+	return nil
+}
+
+func getOrCreateKey() (key []byte, new bool, err error) {
+	file, err := os.Open(*keyPath)
+	defer file.Close()
+
+	if err != nil && !os.IsNotExist(err) {
+		return nil, false, err
+	}
+	buf := bytes.Buffer{}
+	length, err := io.Copy(&buf, file)
+	if err != nil {
+		log.Infof("Error copying key-file %v, probably it doesn't exist : %+v", *keyPath, err)
+	} else {
+		log.Debugf("Copied %v bytes successfully to the buffer from key-file %v", length, *keyPath)
+	}
+
+	encodedKey := buf.String()
+	if len(encodedKey) != 0 {
+		log.Infof("Key file is not empty. Using key from the file")
+		key, err := base64.StdEncoding.DecodeString(encodedKey)
+		if err != nil {
+			return nil, false, err
+		}
+		return key, false, nil
+	}
+
+	log.Infof("Key file is empty. Generating new key and writing it to %v...", *keyPath)
+	newKey, err := randStringBytes(keySize)
+	if err != nil {
+		return nil, false, fmt.Errorf("Error generating crypto key %+v", err)
+	}
+	key = []byte(base64.StdEncoding.EncodeToString(newKey))
+	file, err = os.Create(*keyPath)
+	if err != nil {
+		return nil, false, fmt.Errorf("Error creating file %v : %+v", *keyPath, err)
+	}
+	err = os.Chmod(*keyPath, os.FileMode(utils.GetOctalUintEnvironmentVariable("KEY_FILE_PERMS", 0600)))
+	if err != nil {
+		return nil, false, fmt.Errorf("Error chmod file %v : %+v", *keyPath, err)
+	}
+	_, err = file.Write(key)
+	if err != nil {
+		return nil, false, fmt.Errorf("Error writing file %v : %+v", *keyPath, err)
+	}
+	log.Infof("Key file generated successfully and written to %v", *keyPath)
+	return newKey, true, nil
+}
+
+func randStringBytes(n int) ([]byte, error) {
+	b := make([]byte, n)
+	_, err := rand.Read(b)
+	if err != nil {
+		return nil, err
+	}
+	return b, nil
+}
diff --git a/docker-query-exporter/crypto/crypto.go b/docker-query-exporter/crypto/crypto.go
new file mode 100644
index 0000000..faac97d
--- /dev/null
+++ b/docker-query-exporter/crypto/crypto.go
@@ -0,0 +1,65 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package crypto
+
+import (
+	"crypto/aes"
+	"crypto/cipher"
+	"crypto/rand"
+	"fmt"
+	"io"
+)
+
+type Crypto interface {
+	Encrypt(text []byte) (string, error)
+	Decrypt(text []byte) (string, error)
+}
+
+type aesCrypto struct {
+	gcm cipher.AEAD
+}
+
+func NewCrypto(key []byte) (Crypto, error) {
+	c, err := aes.NewCipher(key)
+	if err != nil {
+		return nil, err
+	}
+	gcm, err := cipher.NewGCM(c)
+	if err != nil {
+		return nil, err
+	}
+	return aesCrypto{gcm: gcm}, nil
+}
+
+func (ac aesCrypto) Encrypt(text []byte) (string, error) {
+	nonce := make([]byte, ac.gcm.NonceSize())
+	if _, err := io.ReadFull(rand.Reader, nonce); err != nil {
+		return "", err
+	}
+	return string(ac.gcm.Seal(nonce, nonce, text, nil)), nil
+}
+
+func (ac aesCrypto) Decrypt(text []byte) (string, error) {
+	nonceSize := ac.gcm.NonceSize()
+	if len(text) < nonceSize {
+		return "", fmt.Errorf("crypto err: text (%v) len is less than nonce len (%v)", text, nonceSize)
+	}
+	nonce, text := text[:nonceSize], text[nonceSize:]
+	plaintext, err := ac.gcm.Open(nil, nonce, text, nil)
+	if err != nil {
+		return "", err
+	}
+	return string(plaintext), nil
+}
diff --git a/docker-query-exporter/dbservice/dbservice.go b/docker-query-exporter/dbservice/dbservice.go
new file mode 100644
index 0000000..8fb2c69
--- /dev/null
+++ b/docker-query-exporter/dbservice/dbservice.go
@@ -0,0 +1,190 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package dbservice
+
+import (
+	"context"
+	"flag"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+
+	log "github.com/sirupsen/logrus"
+)
+
+const (
+	TypeOracle   = "oracle"
+	TypePostgres = "postgres"
+)
+
+var (
+	maxOpenConns = flag.Int("max-open-connections", utils.GetEnvInt("MAX_OPEN_CONNECTIONS_MASTER", 10), "Set max number of DB pool connections")
+	queryTimeout = flag.Int64("query-timeout", int64(utils.GetEnvInt("MAX_QUERY_TIMEOUT", 30)), "Query execution timeout in seconds")
+
+	maxFailedTimeouts  = flag.Int64("max-failed-timeouts", int64(utils.GetEnvInt("MAX_FAILED_TIMEOUTS", 3)), "Query timeout failed before skipped list")
+	timeoutsMap        = make(map[string]int64)
+	longRunningQueries = []string{}
+)
+
+type DBService interface {
+	Initialize(dsn string, dbName string) (err error)
+	Close()
+	ExecuteInitSqls(sqls []string)
+	ExecuteSelect(qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error)
+	DBStat() string
+	GetAllDatabases() ([]string, error)
+	GetDatabaseName() string
+	GetType() string
+	GetDSN() string
+	IsMaster() bool
+	GetClassifier() config.Classifier
+	Ping() bool
+}
+
+func GetDBServices(appConfig *config.Config) []DBService {
+	dbServices := make([]DBService, 0)
+	for dbName, database := range appConfig.Databases {
+		log.Debugf("New database with name %s and type %s was added", dbName, database.Type)
+		dbService := getServiceByType(database.Type, true)
+		err := dbService.Initialize(database.Dsn, "")
+		if err != nil {
+			log.Fatalf("Cannot connect to physical database %v : %+v", dbName, err)
+		}
+		dbService.ExecuteInitSqls(database.InitSqls)
+		dbServices = append(dbServices, dbService)
+	}
+
+	if utils.IsAutodiscoveryEnabled() {
+		for _, dbService := range dbServices {
+			logicalDatabases, err := getLogicalDbsServices(dbService, dbServices)
+			if err != nil {
+				log.Fatalf("Cannot get logical databases list : %+v", err)
+			}
+			dbServices = append(dbServices, logicalDatabases...)
+		}
+	}
+
+	return dbServices
+}
+
+func getServiceByType(dbType string, master bool) DBService {
+	var dbService DBService
+	if dbType == "" || strings.ToLower(dbType) == TypeOracle {
+		dbService = &OracleDBService{master: master}
+	} else if strings.ToLower(dbType) == TypePostgres {
+		dbService = &PostgresDBService{master: master, dbName: "postgres"}
+	}
+	return dbService
+}
+
+func GetNewDBServices(dbServices []DBService) []DBService {
+	resultDbServices := []DBService{}
+	for _, dbService := range dbServices {
+		if dbService.IsMaster() {
+			newDbServices, err := getLogicalDbsServices(dbService, dbServices)
+			if err != nil {
+				log.Fatalf("Cannot get logical databases list from %s : %+v", dbService.GetDatabaseName(), err)
+			}
+			resultDbServices = append(resultDbServices, newDbServices...)
+		}
+	}
+	return resultDbServices
+}
+
+func getLogicalDbsServices(dbService DBService, existingServices []DBService) ([]DBService, error) {
+	dbServices := []DBService{}
+	databases, err := dbService.GetAllDatabases()
+	if err != nil {
+		return nil, err
+	}
+	for _, dbName := range databases {
+		if !serviceAlreadyPresent(dbService.GetType(), dbName, existingServices) {
+			newDbService := getServiceByType(dbService.GetType(), false)
+			log.Debugf("New database with name %s and type %s was added", dbName, newDbService.GetType())
+			newDbService.Initialize(dbService.GetDSN(), dbName)
+			dbServices = append(dbServices, newDbService)
+		}
+	}
+	return dbServices, nil
+}
+
+func serviceAlreadyPresent(dbType string, name string, existingServices []DBService) bool {
+	for _, existingService := range existingServices {
+		if dbType == existingService.GetType() {
+			if name == existingService.GetDatabaseName() {
+				return true
+			}
+		}
+	}
+	return false
+}
+
+func getTimeout(qCfg *config.QueryConfig) time.Duration {
+	if qCfg == nil {
+		return time.Second * time.Duration(*queryTimeout)
+	}
+
+	var timeout time.Duration
+	if qCfg.Timeout != "" {
+		t, parseErr := time.ParseDuration(qCfg.Timeout)
+		if parseErr == nil {
+			timeout = t
+		}
+	}
+	if timeout == 0 {
+		timeout = time.Second * time.Duration(*queryTimeout)
+	}
+
+	return timeout
+}
+
+func execWithTimeout(qName string, qRand string, qCfg *config.QueryConfig, executeSelect func(ctx context.Context, qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error)) ([]map[string]string, []string, error) {
+	timeout := getTimeout(qCfg)
+	log.Debugf("[%v] Executing query %v with timeout %+v : %+v", qRand, qName, timeout, qCfg)
+
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+
+	rows, columnNames, err := executeSelect(ctx, qName, qRand, qCfg)
+	if err != nil {
+		handleTimeoutFail(ctx, qName)
+		return nil, nil, err
+	}
+	return rows, columnNames, nil
+}
+
+func GetLongRunningQueries() []string {
+	return longRunningQueries
+}
+
+// Circuit breaker mechanism
+func handleTimeoutFail(ctx context.Context, qName string) {
+	select {
+	default:
+	case <-ctx.Done():
+		timeoutCount := timeoutsMap[qName]
+		if timeoutCount >= *maxFailedTimeouts {
+			log.Warnf("Adding query %s to long running list", qName)
+			longRunningQueries = append(longRunningQueries, qName)
+		} else {
+			timeoutCount++
+			log.Warnf("Timeout for query %s has been execeeded %d time(s)", qName, timeoutCount)
+			timeoutsMap[qName] = timeoutCount
+		}
+	}
+}
diff --git a/docker-query-exporter/dbservice/oracledbservice.go b/docker-query-exporter/dbservice/oracledbservice.go
new file mode 100644
index 0000000..7f10895
--- /dev/null
+++ b/docker-query-exporter/dbservice/oracledbservice.go
@@ -0,0 +1,240 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package dbservice
+
+import (
+	"context"
+	"database/sql"
+	"fmt"
+	"reflect"
+	"strings"
+
+	"github.com/Netcracker/qubership-query-exporter/selfmonitor"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+	"github.com/godror/godror"
+	log "github.com/sirupsen/logrus"
+)
+
+type OracleDBService struct {
+	db     *sql.DB
+	dbName string
+	master bool
+	dsn    string
+}
+
+func (odbs *OracleDBService) Initialize(dsn string, dbName string) (err error) {
+	odbs.db, err = sql.Open("godror", dsn)
+	if err != nil {
+		return
+	}
+	odbs.db.SetMaxOpenConns(*maxOpenConns)
+	odbs.dsn = dsn
+	return
+}
+
+func (odbs *OracleDBService) Close() {
+	log.Debugf("DB connection pool stat: %+v", odbs.db.Stats())
+	if odbs.db != nil {
+		err := odbs.db.Close()
+		if err != nil {
+			log.Errorf("Error closing DB : %+v", err)
+		} else {
+			log.Info("DB Closed successfully")
+		}
+	} else {
+		log.Warn("Attempt to close db failed: dbservice.db == nil, nothing to close")
+	}
+}
+
+func (odbs *OracleDBService) ExecuteInitSqls(sqls []string) {
+	for _, sql := range sqls {
+		odbs.executeInitQuery(sql)
+	}
+}
+
+func (odbs *OracleDBService) executeInitQuery(sql string) {
+	timeout := getTimeout(nil)
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+	log.Infof("Executing init-sql '%v' with timeout %+v", sql, timeout)
+	res, err := odbs.db.ExecContext(ctx, sql)
+
+	if err != nil {
+		log.Errorf("Error executing init-sql '%v' : %+v", sql, err)
+	} else {
+		log.Infof("Init-sql '%v' executed successfully : %+v", sql, res)
+	}
+	odbs.printDBStatAndUpdatePrometheusSelfMetric("Init-query")
+}
+
+func (odbs *OracleDBService) ExecuteSelect(qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error) {
+	return execWithTimeout(qName, qName, qCfg, odbs.executeSelect)
+}
+
+func (odbs *OracleDBService) executeSelect(ctx context.Context, qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error) {
+	rows, err := odbs.getRows(ctx, qName, qCfg)
+
+	if err != nil {
+		odbs.printDBStatAndUpdatePrometheusSelfMetric(qRand)
+		return nil, nil, fmt.Errorf("Error executing query %v : %+v", qName, err)
+	}
+
+	defer rows.Close()
+	odbs.printDBStatAndUpdatePrometheusSelfMetric(qRand)
+
+	columnNames, err := getOracleColumnNames(rows)
+	if err != nil {
+		log.Errorf("[%v] Error getting Column Names: %+v", qRand, err)
+	}
+	log.Tracef("[%v] Query %v, columnNames : %+v", qRand, qName, columnNames)
+
+	i := 0
+	result := make([]map[string]string, 0)
+	for rows.Next() {
+		mapRow, err := processOracleRowToMap(rows, qRand)
+		i++
+		log.Tracef("[%v] Query %v, row %v : %+v", qRand, qName, i, mapRow)
+		if err != nil {
+			log.Errorf("[%v] Error processing row to map for query %v : %+v", qRand, qName, err)
+		} else {
+			result = append(result, mapRow)
+			//log.Tracef("qName: %v, address: %p, length: %d, capacity: %d, items: %+v\n", qName, result, len(result), cap(result), result)
+		}
+	}
+	return result, columnNames, nil
+}
+
+func (odbs *OracleDBService) getRows(ctx context.Context, qName string, qCfg *config.QueryConfig) (*sql.Rows, error) {
+
+	query := qCfg.Sql
+
+	if query == "" {
+		return nil, fmt.Errorf("Can not find a query")
+	}
+
+	var parameters = make([]interface{}, 0)
+	if len(qCfg.Parameters) != 0 {
+		parameters = make([]interface{}, len(qCfg.Parameters[0]))
+		i := 0
+		for pName, pValue := range qCfg.Parameters[0] {
+			parameters[i] = sql.Named(pName, pValue)
+			i++
+		}
+	}
+
+	return odbs.db.QueryContext(ctx, query, parameters...)
+}
+
+func processOracleRowToMap(rows *sql.Rows, qRand string) (map[string]string, error) {
+	columns, err := rows.ColumnTypes()
+	if err != nil {
+		return nil, fmt.Errorf("Error reading ColumnTypes: %+v", err)
+	}
+
+	values := make([]interface{}, len(columns))
+	object := map[string]interface{}{}
+	for i, column := range columns {
+		object[column.Name()] = reflect.New(column.ScanType()).Interface()
+		values[i] = object[column.Name()]
+	}
+
+	err = rows.Scan(values...)
+	if err != nil {
+		return nil, fmt.Errorf("Error reading rows %+v", err)
+	}
+
+	result := make(map[string]string)
+	for i, v := range values {
+		switch t := v.(type) {
+		case *string:
+			result[strings.ToUpper(columns[i].Name())] = *t
+		case *godror.Number:
+			result[strings.ToUpper(columns[i].Name())] = string(*t)
+		/*case float64:
+		  result[strings.ToUpper(columns[i].Name())] = strconv.FormatFloat(t, 'E', -1, 64)*/
+		default:
+			log.Errorf("[%v] Error converting for %v type %+v", qRand, columns[i].Name(), reflect.TypeOf(t))
+		}
+	}
+	return result, nil
+}
+
+/*
+func (odbs *OracleDBService) ExecuteSimpleQuery(query string) ([]map[string]string, []string, error) {
+    var qCfg config.QueryConfig
+    qCfg.Sql = query
+    return odbs.ExecuteSelect("SIMPLE SELECT", "SIMPLE SELECT", &qCfg)
+}
+
+func (odbs *OracleDBService) PrintSimpleQuery(query string) {
+    res, _, err := odbs.ExecuteSimpleQuery(query)
+    if err != nil {
+        log.Errorf("Error executing simple query %v : %+v", query, err)
+    } else {
+        log.Infof("Simple query %v executed successfully, result is : %+v", query, res)
+    }
+}
+*/
+
+func getOracleColumnNames(rows *sql.Rows) ([]string, error) {
+	columns, err := rows.ColumnTypes()
+	if err != nil {
+		return nil, fmt.Errorf("Error reading ColumnTypes: %+v", err)
+	}
+	columnNames := make([]string, len(columns))
+	for i, column := range columns {
+		columnNames[i] = column.Name()
+	}
+	return columnNames, nil
+}
+
+func (odbs *OracleDBService) DBStat() string {
+	return fmt.Sprintf("DB connection pool stat: %+v", odbs.db.Stats())
+}
+
+func (odbs *OracleDBService) printDBStatAndUpdatePrometheusSelfMetric(qRand string) {
+	dbStat := odbs.db.Stats()
+	log.Debugf("[%v] DB connection pool stat: %+v", qRand, dbStat)
+	selfmonitor.UpdateOracleDbPoolStats(dbStat)
+}
+
+func (odbs *OracleDBService) GetAllDatabases() ([]string, error) {
+	return []string{}, nil
+}
+
+func (odbs *OracleDBService) GetDatabaseName() string {
+	return odbs.dbName
+}
+
+func (odbs *OracleDBService) IsMaster() bool {
+	return odbs.master
+}
+
+func (odbs *OracleDBService) GetType() string {
+	return TypeOracle
+}
+
+func (odbs *OracleDBService) GetDSN() string {
+	return odbs.dsn
+}
+
+func (odbs *OracleDBService) Ping() bool {
+	return true
+}
+
+func (odbs *OracleDBService) GetClassifier() config.Classifier {
+	return config.Classifier{}
+}
diff --git a/docker-query-exporter/dbservice/postgresdbservice.go b/docker-query-exporter/dbservice/postgresdbservice.go
new file mode 100644
index 0000000..00a0723
--- /dev/null
+++ b/docker-query-exporter/dbservice/postgresdbservice.go
@@ -0,0 +1,425 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package dbservice
+
+import (
+	"context"
+	"database/sql"
+	"encoding/json"
+	"flag"
+	"fmt"
+	"os"
+	"reflect"
+	"regexp"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/Netcracker/qubership-query-exporter/selfmonitor"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+
+	"github.com/jackc/pgx/v5"
+	"github.com/jackc/pgx/v5/pgtype"
+	"github.com/jackc/pgx/v5/pgxpool"
+	log "github.com/sirupsen/logrus"
+)
+
+var (
+	maxOpenConnsLog = flag.Int("max-open-connections-logical", utils.GetEnvInt("MAX_OPEN_CONNECTIONS_LOGICAL", 1), "Set max number of DB pool connections per not master logical db")
+
+	pgAllDatabasesQuery = "SELECT datname FROM pg_database WHERE datistemplate = false;"
+	pgPingQuery         = "SELECT version()"
+
+	metadataTable    = "_dbaas_metadata"
+	tableExistsQuery = fmt.Sprintf("SELECT 1 FROM information_schema.tables WHERE table_schema = 'public' AND table_name = '%s';", metadataTable)
+	classifierQuery  = fmt.Sprintf("select value#>>'{classifier}' as classifier from %s where key='metadata'", metadataTable)
+
+	// Env variables regExp
+	re = regexp.MustCompile(`\$\{(.*?)\}`)
+)
+
+type PostgresDBService struct {
+	dbPool     *pgxpool.Pool
+	dbName     string
+	master     bool
+	dsn        string
+	classifier config.Classifier
+}
+
+func findMacros(dsn string) map[string]string {
+	resultMap := make(map[string]string, 0)
+	match := re.FindAllStringSubmatch(dsn, -1)
+	for _, variable := range match {
+		resultMap[variable[0]] = variable[1]
+	}
+	return resultMap
+}
+
+func prepareDSN(dsn string) string {
+	macros := findMacros(dsn)
+	for macro, envName := range macros {
+		value, ok := os.LookupEnv(envName)
+		if !ok {
+			panic(fmt.Sprintf("Environment variable %s is not found", macro))
+		}
+		dsn = strings.ReplaceAll(dsn, macro, value)
+	}
+	return dsn
+}
+
+func (pdbs *PostgresDBService) Initialize(dsn string, dbName string) (err error) {
+	if pdbs.IsMaster() {
+		dsn = prepareDSN(dsn)
+	}
+	pgxConfig, err := pgxpool.ParseConfig(dsn)
+	if err != nil {
+		log.Errorf("Unable to parse dsn for Postgres: %+v", err)
+		return err
+	}
+	if pdbs.IsMaster() {
+		pgxConfig.MaxConns = int32(*maxOpenConns)
+	} else {
+		pgxConfig.MaxConns = int32(*maxOpenConnsLog)
+	}
+
+	if dbName == "" {
+		dbName = "postgres"
+	}
+	pgxConfig.ConnConfig.Database = dbName
+	dbPool, err := pgxpool.NewWithConfig(context.Background(), pgxConfig)
+	if err != nil {
+		log.Errorf("Unable to connect to Postgres database: %+v", err)
+		return err
+	}
+	pdbs.dbPool = dbPool
+	pdbs.dbName = dbName
+	pdbs.dsn = dsn
+
+	// Set up db classifier
+	err = pdbs.initClassifier()
+	if err != nil {
+		return err
+	}
+
+	return
+}
+
+func (pdbs *PostgresDBService) Close() {
+	log.Debugf("DB connection pool stat: %+v", pdbs.dbPool.Stat())
+	if pdbs.dbPool != nil {
+		pdbs.dbPool.Close()
+		log.Info("Postgres DBPool was closed")
+	} else {
+		log.Warn("Attempt to close db failed: dbservice.db == nil, nothing to close")
+	}
+}
+
+func (pdbs *PostgresDBService) ExecuteInitSqls(sqls []string) {
+	for _, sql := range sqls {
+		pdbs.executeInitQuery(sql)
+	}
+}
+
+func (pdbs *PostgresDBService) executeInitQuery(sql string) {
+	timeout := getTimeout(nil)
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+	log.Infof("Executing init-sql '%v' with timeout %+v", sql, timeout)
+	res, err := pdbs.dbPool.Exec(ctx, sql)
+	if err != nil {
+		log.Errorf("Error executing init-sql '%v' : %+v", sql, err)
+	} else {
+		log.Infof("Init-sql '%v' executed successfully : %+v", sql, res)
+	}
+	pdbs.printDBStatAndUpdatePrometheusSelfMetric("Init-query")
+}
+
+func (odbs *PostgresDBService) ExecuteSelect(qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error) {
+	return execWithTimeout(qName, qName, qCfg, odbs.executeSelect)
+}
+
+func (pdbs *PostgresDBService) executeSelect(ctx context.Context, qName string, qRand string, qCfg *config.QueryConfig) ([]map[string]string, []string, error) {
+	timeout := getTimeout(qCfg)
+	log.Debugf("[%v] Executing query %v with timeout %+v : %+v", qRand, qName, timeout, qCfg)
+
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+
+	rows, err := pdbs.getRows(ctx, qName, qCfg)
+
+	if err != nil {
+		pdbs.printDBStatAndUpdatePrometheusSelfMetric(qRand)
+		return nil, nil, fmt.Errorf("Error executing query %v : %+v", qName, err)
+	}
+
+	defer rows.Close()
+	pdbs.printDBStatAndUpdatePrometheusSelfMetric(qRand)
+
+	columnNames := getPostgresColumnNames(rows)
+	log.Tracef("[%v] Query %v, columnNames : %+v", qRand, qName, columnNames)
+
+	i := 0
+	result := make([]map[string]string, 0)
+	for rows.Next() {
+		mapRow, err := processPostgresRowToMap(rows, qRand)
+		i++
+		log.Tracef("[%v] Query %v, row %v : %+v", qRand, qName, i, mapRow)
+		if err != nil {
+			log.Errorf("[%v] Error processing row to map for query %v : %+v", qRand, qName, err)
+		} else {
+			result = append(result, mapRow)
+			//log.Tracef("qName: %v, address: %p, length: %d, capacity: %d, items: %+v\n", qName, result, len(result), cap(result), result)
+		}
+	}
+	return result, columnNames, nil
+}
+
+func (dbservice *PostgresDBService) getRows(ctx context.Context, qName string, qCfg *config.QueryConfig) (pgx.Rows, error) {
+
+	query := qCfg.Sql
+
+	if query == "" {
+		return nil, fmt.Errorf("Can not find a query")
+	}
+
+	var parameters = make([]interface{}, 0)
+	if len(qCfg.Parameters) != 0 {
+		parameters = make([]interface{}, len(qCfg.Parameters[0]))
+		i := 0
+		for pName, pValue := range qCfg.Parameters[0] {
+			parameters[i] = sql.Named(pName, pValue)
+			i++
+		}
+	}
+
+	return dbservice.dbPool.Query(ctx, query, parameters...)
+}
+
+func processPostgresRowToMap(rows pgx.Rows, qRand string) (map[string]string, error) {
+	fieldDescriptions := rows.FieldDescriptions()
+
+	values, err := rows.Values()
+	if err != nil {
+		return nil, fmt.Errorf("Error reading rows %+v", err)
+	}
+	log.Tracef("[%v] Got values %+v", values)
+
+	result := make(map[string]string)
+	for i, v := range values {
+		switch t := v.(type) {
+		case nil:
+			log.Tracef("[%v] For column %v got nil column value type", qRand, fieldDescriptions[i].Name)
+		case *string:
+			log.Tracef("[%v] For column %v got *string column value type", qRand, fieldDescriptions[i].Name)
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = *t
+		case string:
+			log.Tracef("[%v] For column %v got string column value type", qRand, fieldDescriptions[i].Name)
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = t
+		case int32, int64, uint32:
+			if log.IsLevelEnabled(log.TraceLevel) {
+				log.Tracef("[%v] For column %v got %+v column value type", qRand, fieldDescriptions[i].Name, reflect.TypeOf(t))
+			}
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%v", t)
+		case pgtype.Numeric:
+			log.Tracef("[%v] For column %v got pgtype.Numeric column value type", qRand, fieldDescriptions[i].Name)
+			val, err := t.Float64Value()
+			if err != nil {
+				log.Errorf("[%v] for column %v got error during processing pgtype.Numeric response to Float64 : %+v", qRand, fieldDescriptions[i].Name, err)
+			} else {
+				result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%v", val.Float64)
+			}
+		case bool:
+			if log.IsLevelEnabled(log.TraceLevel) {
+				log.Tracef("[%v] For column %v got pgtype.Bool column value type", qRand, fieldDescriptions[i].Name)
+			}
+			// convert bool to int
+			valInt := 0
+			if t {
+				valInt = 1
+			}
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%v", valInt)
+		case float32, float64:
+			if log.IsLevelEnabled(log.TraceLevel) {
+				log.Tracef("[%v] For column %v float column value type", qRand, fieldDescriptions[i].Name)
+			}
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%f", t)
+		case time.Time:
+			if log.IsLevelEnabled(log.TraceLevel) {
+				log.Tracef("[%v] For column %v got %+v column value type", qRand, fieldDescriptions[i].Name, reflect.TypeOf(t))
+			}
+			val := float64(t.Unix())
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%v", val)
+		default:
+			log.Warnf("[%v] For column %v got type %+v; This is unexpected and default logic is applied for this column", qRand, fieldDescriptions[i].Name, reflect.TypeOf(t))
+			result[strings.ToUpper(fieldDescriptions[i].Name)] = fmt.Sprintf("%v", t)
+		}
+	}
+	return result, nil
+}
+
+/* func (pdbs *PostgresDBService) ExecuteSimpleQuery(query string) ([]map[string]string, []string, error) {
+    var qCfg config.QueryConfig
+    qCfg.Sql = query
+    return pdbs.ExecuteSelect("SIMPLE SELECT", "SIMPLE SELECT", &qCfg)
+}
+
+func (pdbs *PostgresDBService) PrintSimpleQuery(query string) {
+    res, _, err := pdbs.ExecuteSimpleQuery(query)
+    if err != nil {
+        log.Errorf("Error executing simple query %v : %+v", query, err)
+    } else {
+        log.Infof("Simple query %v executed successfully, result is : %+v", query, res)
+    }
+} */
+
+func getPostgresColumnNames(rows pgx.Rows) []string {
+	fieldDescriptions := rows.FieldDescriptions()
+	columnNames := make([]string, len(fieldDescriptions))
+	for i, fd := range fieldDescriptions {
+		columnNames[i] = fd.Name
+	}
+	return columnNames
+}
+
+func (pdbs *PostgresDBService) DBStat() string {
+	return fmt.Sprintf("DB connection pool stat: %+v", pdbs.dbPool.Stat())
+}
+
+func (pdbs *PostgresDBService) printDBStatAndUpdatePrometheusSelfMetric(qRand string) {
+	dbStat := pdbs.dbPool.Stat()
+	log.Debugf("[%v] Postgres DB connection pool stat: %+v", qRand, dbStat)
+	selfmonitor.UpdatePostgresDbPoolStats(*dbStat)
+}
+
+func (pdbs *PostgresDBService) GetAllDatabases() ([]string, error) {
+	timeout := getTimeout(nil)
+
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+	rows, err := pdbs.dbPool.Query(ctx, pgAllDatabasesQuery)
+	if err != nil {
+		log.Errorf("Cannot get logical databases list for %s", pdbs.dbName)
+		return nil, err
+	}
+	defer rows.Close()
+
+	databases := []string{}
+	for rows.Next() {
+		var database string
+		err = rows.Scan(&database)
+		if err != nil {
+			log.Errorf("Cannot parse logical database value for %s", pdbs.dbName)
+			return nil, err
+		}
+		databases = append(databases, database)
+	}
+	return databases, nil
+}
+
+func (pdbs *PostgresDBService) GetDatabaseName() string {
+	return pdbs.dbName
+}
+
+func (pdbs *PostgresDBService) IsMaster() bool {
+	return pdbs.master
+}
+
+func (pdbs *PostgresDBService) GetType() string {
+	return TypePostgres
+}
+
+func (pdbs *PostgresDBService) GetDSN() string {
+	return pdbs.dsn
+}
+
+func (pdbs *PostgresDBService) Ping() bool {
+	timeout := getTimeout(nil)
+
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+	_, err := pdbs.dbPool.Exec(ctx, pgPingQuery)
+	if err != nil {
+		log.Errorf("Cannot connect to database %s", pdbs.dbName)
+		return false
+	}
+	return true
+}
+
+func (pdbs *PostgresDBService) GetClassifier() config.Classifier {
+	return pdbs.classifier
+}
+
+func (pdbs *PostgresDBService) initClassifier() error {
+	timeout := getTimeout(nil)
+
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+	isMetadataExists, err := pdbs.isMetadataExists(ctx)
+	if err != nil {
+		return err
+	}
+	if !isMetadataExists {
+		log.Debugf("No classifier found in database %s", pdbs.dbName)
+		return nil
+	}
+
+	classifier, err := pdbs.obtainClassifierFromMetadata(ctx)
+	if err != nil {
+		return err
+	}
+
+	pdbs.classifier = classifier
+	return nil
+}
+
+func (pdbs *PostgresDBService) isMetadataExists(ctx context.Context) (bool, error) {
+	rows, err := pdbs.dbPool.Query(ctx, tableExistsQuery)
+	if err != nil {
+		log.Errorf("Cannot connect to database %s", pdbs.dbName)
+		return false, err
+	}
+	defer rows.Close()
+
+	return rows.Next(), nil
+}
+
+func (pdbs *PostgresDBService) obtainClassifierFromMetadata(ctx context.Context) (config.Classifier, error) {
+	rows, err := pdbs.dbPool.Query(ctx, classifierQuery)
+	if err != nil {
+		log.Errorf("Cannot connect to database %s", pdbs.dbName)
+		return nil, err
+	}
+	defer rows.Close()
+
+	classifier := config.Classifier{}
+	classifierStr := ""
+	for rows.Next() {
+		err = rows.Scan(&classifierStr)
+		if err != nil {
+			log.Warn("No Classifier found in metadata for database %s", pdbs.dbName)
+			return config.Classifier{}, nil
+		}
+
+		err = json.Unmarshal([]byte(classifierStr), &classifier)
+		if err != nil {
+			log.Error("JSON classifier parse error for database %s", pdbs.dbName)
+			return config.Classifier{}, nil
+		}
+		log.Debug(fmt.Sprintf("Classifier %s was found for database %s", classifier, pdbs.dbName))
+	}
+	return classifier, nil
+}
diff --git a/docker-query-exporter/go.mod b/docker-query-exporter/go.mod
new file mode 100644
index 0000000..8561c98
--- /dev/null
+++ b/docker-query-exporter/go.mod
@@ -0,0 +1,39 @@
+module github.com/Netcracker/qubership-query-exporter
+
+go 1.25.1
+
+require (
+	github.com/godror/godror v0.49.2
+	github.com/jackc/pgx/v5 v5.7.6
+	github.com/prometheus/client_golang v1.23.2
+	github.com/robfig/cron/v3 v3.0.1
+	github.com/sirupsen/logrus v1.9.3
+	gopkg.in/natefinch/lumberjack.v2 v2.2.1
+	gopkg.in/yaml.v2 v2.4.0
+)
+
+require (
+	github.com/VictoriaMetrics/easyproto v0.1.4 // indirect
+	github.com/beorn7/perks v1.0.1 // indirect
+	github.com/cespare/xxhash/v2 v2.3.0 // indirect
+	github.com/go-logfmt/logfmt v0.6.0 // indirect
+	github.com/godror/knownpb v0.3.0 // indirect
+	github.com/golang/protobuf v1.5.4 // indirect
+	github.com/jackc/pgpassfile v1.0.0 // indirect
+	github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 // indirect
+	github.com/jackc/puddle/v2 v2.2.2 // indirect
+	github.com/kr/text v0.2.0 // indirect
+	github.com/matttproud/golang_protobuf_extensions v1.0.4 // indirect
+	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
+	github.com/prometheus/client_model v0.6.2 // indirect
+	github.com/prometheus/common v0.66.1 // indirect
+	github.com/prometheus/procfs v0.17.0 // indirect
+	github.com/rogpeppe/go-internal v1.13.1 // indirect
+	go.yaml.in/yaml/v2 v2.4.3 // indirect
+	golang.org/x/crypto v0.42.0 // indirect
+	golang.org/x/exp v0.0.0-20250819193227-8b4c13bb791b // indirect
+	golang.org/x/sync v0.17.0 // indirect
+	golang.org/x/sys v0.36.0 // indirect
+	golang.org/x/text v0.29.0 // indirect
+	google.golang.org/protobuf v1.36.9 // indirect
+)
diff --git a/docker-query-exporter/go.sum b/docker-query-exporter/go.sum
new file mode 100644
index 0000000..acb221a
--- /dev/null
+++ b/docker-query-exporter/go.sum
@@ -0,0 +1,128 @@
+github.com/VictoriaMetrics/easyproto v0.1.4 h1:r8cNvo8o6sR4QShBXQd1bKw/VVLSQma/V2KhTBPf+Sc=
+github.com/VictoriaMetrics/easyproto v0.1.4/go.mod h1:QlGlzaJnDfFd8Lk6Ci/fuLxfTo3/GThPs2KH23mv710=
+github.com/beorn7/perks v1.0.1 h1:VlbKKnNfV8bJzeqoa4cOKqO6bYr3WgKZxO8Z16+hsOM=
+github.com/beorn7/perks v1.0.1/go.mod h1:G2ZrVWU2WbWT9wwq4/hrbKbnv/1ERSJQ0ibhJ6rlkpw=
+github.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=
+github.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
+github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
+github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
+github.com/creack/pty v1.1.9/go.mod h1:oKZEueFk5CKHvIhNR5MUki03XCEU+Q6VDXinZuGJ33E=
+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/go-logfmt/logfmt v0.6.0 h1:wGYYu3uicYdqXVgoYbvnkrPVXkuLM1p1ifugDMEdRi4=
+github.com/go-logfmt/logfmt v0.6.0/go.mod h1:WYhtIu8zTZfxdn5+rREduYbwxfcBr/Vr6KEVveWlfTs=
+github.com/godror/godror v0.39.2 h1:Hj27TRhsW3+xCNL6acx8j5Rrkvk/XS50ph6XQdU+x1U=
+github.com/godror/godror v0.39.2/go.mod h1:1zxzPh0ECu0FtlSuVF7WhB2wRyWyGoJyLHwkuULydq4=
+github.com/godror/godror v0.49.2 h1:d/ztc6YYwwHdAmJQ2cLcF9eDIwBkDCTIRl679YFJW+0=
+github.com/godror/godror v0.49.2/go.mod h1:kTMcxZzRw73RT5kn9v3JkBK4kHI6dqowHotqV72ebU8=
+github.com/godror/knownpb v0.1.1 h1:A4J7jdx7jWBhJm18NntafzSC//iZDHkDi1+juwQ5pTI=
+github.com/godror/knownpb v0.1.1/go.mod h1:4nRFbQo1dDuwKnblRXDxrfCFYeT4hjg3GjMqef58eRE=
+github.com/godror/knownpb v0.3.0 h1:+caUdy8hTtl7X05aPl3tdL540TvCcaQA6woZQroLZMw=
+github.com/godror/knownpb v0.3.0/go.mod h1:PpTyfJwiOEAzQl7NtVCM8kdPCnp3uhxsZYIzZ5PV4zU=
+github.com/golang/protobuf v1.2.0/go.mod h1:6lQm79b+lXiMfvg/cZm0SGofjICqVBUtrP5yJMmIC1U=
+github.com/golang/protobuf v1.3.5/go.mod h1:6O5/vntMXwX2lRkT1hjjk0nAC1IDOTvTlVgjlRvqsdk=
+github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
+github.com/golang/protobuf v1.5.3 h1:KhyjKVUg7Usr/dYsdSqoFveMYd5ko72D+zANwlG1mmg=
+github.com/golang/protobuf v1.5.3/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
+github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
+github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
+github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
+github.com/google/go-cmp v0.5.9 h1:O2Tfq5qg4qc4AmwVlvv0oLiVAGB7enBSJ2x2DqQFi38=
+github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
+github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
+github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
+github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
+github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a h1:bbPeKD0xmW/Y25WS6cokEszi5g+S0QxI/d45PkRi7Nk=
+github.com/jackc/pgservicefile v0.0.0-20221227161230-091c0ba34f0a/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
+github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761 h1:iCEnooe7UlwOQYpKFhBabPMi4aNAfoODPEFNiAnClxo=
+github.com/jackc/pgservicefile v0.0.0-20240606120523-5a60cdf6a761/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
+github.com/jackc/pgx/v5 v5.5.4 h1:Xp2aQS8uXButQdnCMWNmvx6UysWQQC+u1EoizjguY+8=
+github.com/jackc/pgx/v5 v5.5.4/go.mod h1:ez9gk+OAat140fv9ErkZDYFWmXLfV+++K0uAOiwgm1A=
+github.com/jackc/pgx/v5 v5.7.6 h1:rWQc5FwZSPX58r1OQmkuaNicxdmExaEz5A2DO2hUuTk=
+github.com/jackc/pgx/v5 v5.7.6/go.mod h1:aruU7o91Tc2q2cFp5h4uP3f6ztExVpyVv88Xl/8Vl8M=
+github.com/jackc/puddle/v2 v2.2.1 h1:RhxXJtFG022u4ibrCSMSiu5aOq1i77R3OHKNJj77OAk=
+github.com/jackc/puddle/v2 v2.2.1/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
+github.com/jackc/puddle/v2 v2.2.2 h1:PR8nw+E/1w0GLuRFSmiioY6UooMp6KJv0/61nB7icHo=
+github.com/jackc/puddle/v2 v2.2.2/go.mod h1:vriiEXHvEE654aYKXXjOvZM39qJ0q+azkZFrfEOc3H4=
+github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
+github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
+github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
+github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
+github.com/matttproud/golang_protobuf_extensions v1.0.4 h1:mmDVorXM7PCGKw94cs5zkfA9PSy5pEvNWRP0ET0TIVo=
+github.com/matttproud/golang_protobuf_extensions v1.0.4/go.mod h1:BSXmuO+STAnVfrANrmjBb36TMTDstsz7MSK+HVaYKv4=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
+github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
+github.com/oklog/ulid/v2 v2.0.2 h1:r4fFzBm+bv0wNKNh5eXTwU7i85y5x+uwkxCUTNVQqLc=
+github.com/oklog/ulid/v2 v2.0.2/go.mod h1:mtBL0Qe/0HAx6/a4Z30qxVIAL1eQDweXq5lxOEiwQ68=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/prometheus/client_golang v1.16.0 h1:yk/hx9hDbrGHovbci4BY+pRMfSuuat626eFsHb7tmT8=
+github.com/prometheus/client_golang v1.16.0/go.mod h1:Zsulrv/L9oM40tJ7T815tM89lFEugiJ9HzIqaAx4LKc=
+github.com/prometheus/client_golang v1.23.2 h1:Je96obch5RDVy3FDMndoUsjAhG5Edi49h0RJWRi/o0o=
+github.com/prometheus/client_golang v1.23.2/go.mod h1:Tb1a6LWHB3/SPIzCoaDXI4I8UHKeFTEQ1YCr+0Gyqmg=
+github.com/prometheus/client_model v0.3.0 h1:UBgGFHqYdG/TPFD1B1ogZywDqEkwp3fBMvqdiQ7Xew4=
+github.com/prometheus/client_model v0.3.0/go.mod h1:LDGWKZIo7rky3hgvBe+caln+Dr3dPggB5dvjtD7w9+w=
+github.com/prometheus/client_model v0.6.2 h1:oBsgwpGs7iVziMvrGhE53c/GrLUsZdHnqNwqPLxwZyk=
+github.com/prometheus/client_model v0.6.2/go.mod h1:y3m2F6Gdpfy6Ut/GBsUqTWZqCUvMVzSfMLjcu6wAwpE=
+github.com/prometheus/common v0.42.0 h1:EKsfXEYo4JpWMHH5cg+KOUWeuJSov1Id8zGR8eeI1YM=
+github.com/prometheus/common v0.42.0/go.mod h1:xBwqVerjNdUDjgODMpudtOMwlOwf2SaTr1yjz4b7Zbc=
+github.com/prometheus/common v0.66.1 h1:h5E0h5/Y8niHc5DlaLlWLArTQI7tMrsfQjHV+d9ZoGs=
+github.com/prometheus/common v0.66.1/go.mod h1:gcaUsgf3KfRSwHY4dIMXLPV0K/Wg1oZ8+SbZk/HH/dA=
+github.com/prometheus/procfs v0.10.1 h1:kYK1Va/YMlutzCGazswoHKo//tZVlFpKYh+PymziUAg=
+github.com/prometheus/procfs v0.10.1/go.mod h1:nwNm2aOCAYw8uTR/9bWRREkZFxAUcWzPHWJq+XBB/FM=
+github.com/prometheus/procfs v0.17.0 h1:FuLQ+05u4ZI+SS/w9+BWEM2TXiHKsUQ9TADiRH7DuK0=
+github.com/prometheus/procfs v0.17.0/go.mod h1:oPQLaDAMRbA+u8H5Pbfq+dl3VDAvHxMUOVhe0wYB2zw=
+github.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=
+github.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=
+github.com/rogpeppe/go-internal v1.13.1 h1:KvO1DLK/DRN07sQ1LQKScxyZJuNnedQ5/wKSR38lUII=
+github.com/rogpeppe/go-internal v1.13.1/go.mod h1:uMEvuHeurkdAXX61udpOXGD/AzZDWNMNyH2VO9fmH0o=
+github.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=
+github.com/sirupsen/logrus v1.9.3/go.mod h1:naHLuLoDiP4jHNo9R0sCBMtWGeIprob74mVsIT4qYEQ=
+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
+github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=
+github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
+github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
+go.yaml.in/yaml/v2 v2.4.3 h1:6gvOSjQoTB3vt1l+CU+tSyi/HOjfOjRLJ4YwYZGwRO0=
+go.yaml.in/yaml/v2 v2.4.3/go.mod h1:zSxWcmIDjOzPXpjlTTbAsKokqkDNAVtZO0WOMiT90s8=
+golang.org/x/crypto v0.35.0 h1:b15kiHdrGCHrP6LvwaQ3c03kgNhhiMgvlhxHQhmg2Xs=
+golang.org/x/crypto v0.35.0/go.mod h1:dy7dXNW32cAb/6/PRuTNsix8T+vJAqvuIy5Bli/x0YQ=
+golang.org/x/crypto v0.42.0 h1:chiH31gIWm57EkTXpwnqf8qeuMUi0yekh6mT2AvFlqI=
+golang.org/x/crypto v0.42.0/go.mod h1:4+rDnOTJhQCx2q7/j6rAN5XDw8kPjeaXEUR2eL94ix8=
+golang.org/x/exp v0.0.0-20230817173708-d852ddb80c63 h1:m64FZMko/V45gv0bNmrNYoDEq8U5YUhetc9cBWKS1TQ=
+golang.org/x/exp v0.0.0-20230817173708-d852ddb80c63/go.mod h1:0v4NqG35kSWCMzLaMeX+IQrlSnVE/bqGSyC2cz/9Le8=
+golang.org/x/exp v0.0.0-20250819193227-8b4c13bb791b h1:DXr+pvt3nC887026GRP39Ej11UATqWDmWuS99x26cD0=
+golang.org/x/exp v0.0.0-20250819193227-8b4c13bb791b/go.mod h1:4QTo5u+SEIbbKW1RacMZq1YEfOBqeXa19JeshGi+zc4=
+golang.org/x/sync v0.0.0-20181221193216-37e7f081c4d4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sync v0.11.0 h1:GGz8+XQP4FvTTrjZPzNKTMFtSXH80RAzG+5ghFPgK9w=
+golang.org/x/sync v0.11.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
+golang.org/x/sync v0.17.0 h1:l60nONMj9l5drqw6jlhIELNv9I0A4OFgRsG9k2oT9Ug=
+golang.org/x/sync v0.17.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
+golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.30.0 h1:QjkSwP/36a20jFYWkSue1YwXzLmsV5Gfq7Eiy72C1uc=
+golang.org/x/sys v0.30.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
+golang.org/x/sys v0.36.0 h1:KVRy2GtZBrk1cBYA7MKu5bEZFxQk4NIDV6RLVcC8o0k=
+golang.org/x/sys v0.36.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
+golang.org/x/text v0.22.0 h1:bofq7m3/HAFvbF51jz3Q9wLg3jkvSPuiZu/pD1XwgtM=
+golang.org/x/text v0.22.0/go.mod h1:YRoo4H8PVmsu+E3Ou7cqLVH8oXWIHVoX0jqUWALQhfY=
+golang.org/x/text v0.29.0 h1:1neNs90w9YzJ9BocxfsQNHKuAT4pkghyXc4nhZ6sJvk=
+golang.org/x/text v0.29.0/go.mod h1:7MhJOA9CD2qZyOKYazxdYMF85OwPdEr9jTtBpO7ydH4=
+golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
+google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
+google.golang.org/protobuf v1.33.0 h1:uNO2rsAINq/JlFpSdYEKIZ0uKD/R9cpdv0T+yoGwGmI=
+google.golang.org/protobuf v1.33.0/go.mod h1:c6P6GXX6sHbq/GpV6MGZEdwhWPcYBgnhAHhKbcUYpos=
+google.golang.org/protobuf v1.36.9 h1:w2gp2mA27hUeUzj9Ex9FBjsBm40zfaDtEWow293U7Iw=
+google.golang.org/protobuf v1.36.9/go.mod h1:fuxRtAxBytpl4zzqUh6/eyUujkJdNiuEkXntxiD/uRU=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
+gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/natefinch/lumberjack.v2 v2.2.1 h1:bBRl1b0OH9s/DuPhuXpNl+VtCaJXFZ5/uEFST95x9zc=
+gopkg.in/natefinch/lumberjack.v2 v2.2.1/go.mod h1:YD8tP3GAjkrDg1eZH7EGmyESg/lsYskCTPBJVb9jqSc=
+gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
+gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
+gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
diff --git a/docker-query-exporter/logger/log-config.go b/docker-query-exporter/logger/log-config.go
new file mode 100644
index 0000000..c1e3b4b
--- /dev/null
+++ b/docker-query-exporter/logger/log-config.go
@@ -0,0 +1,97 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package logger
+
+import (
+	"flag"
+	"fmt"
+	log "github.com/sirupsen/logrus"
+	"gopkg.in/natefinch/lumberjack.v2"
+	"os"
+	"path/filepath"
+)
+
+var (
+	logLevel       = logLevelFlag(log.DebugLevel)
+	logFile        = flag.String("log-path", "", "Redirect log output to file (stdout if empty)")
+	logJson        = flag.Bool("log-json", false, "Serialize log messages in JSON")
+	logRotation    = flag.Bool("log-rotation", true, "Enabling log rotation")
+	logMaxSize     = flag.Int("log-max-size", 100, "Set max log size in Mb which triggers rotation")
+	logMaxBackups  = flag.Int("log-max-backups", 20, "Set max number of backups")
+	logMaxAge      = flag.Int("log-max-age", 90, "Set max age of log backups in days")
+	logArchivation = flag.Bool("log-archivation", true, "Archivation for rotated logs")
+)
+
+func (level *logLevelFlag) Set(value string) error {
+	if lvl, err := log.ParseLevel(value); err != nil {
+		return err
+	} else {
+		*level = logLevelFlag(lvl)
+	}
+	return nil
+}
+
+type logLevelFlag log.Level
+
+func (level logLevelFlag) String() string {
+	return log.Level(level).String()
+}
+
+func init() {
+	flag.Var(&logLevel, "log-level", "Log level")
+}
+
+func ConfigureLog() {
+	log.SetLevel(log.Level(logLevel))
+	createDirForFile(*logFile)
+	if *logFile != "" {
+		lf, err := os.OpenFile(*logFile, os.O_RDWR|os.O_APPEND|os.O_CREATE, 0644)
+		if err != nil {
+			log.
+				WithField("logFile", *logFile).
+				Fatal("unable to create or truncate log file")
+		}
+		if !*logRotation {
+			log.SetOutput(lf)
+			fmt.Printf("Log rotation disabled: logFile=%v\n", *logFile)
+		} else {
+			log.SetOutput(&lumberjack.Logger{
+				Filename:   *logFile,
+				MaxSize:    *logMaxSize,
+				MaxBackups: *logMaxBackups,
+				MaxAge:     *logMaxAge,
+				Compress:   *logArchivation,
+			})
+			fmt.Printf("Log rotation enabled: logFile=%v, logMaxSize=%v MB, logMaxBackups=%v, logMaxAge=%v, logArchivation=%v, logLevel=%v\n", *logFile, *logMaxSize, *logMaxBackups, *logMaxAge, *logArchivation, logLevel)
+		}
+	}
+	if *logJson {
+		log.SetFormatter(&log.JSONFormatter{})
+	}
+	log.RegisterExitHandler(func() {
+		log.Error("fatal error occurred, exit query-exporter")
+	})
+}
+
+func createDirForFile(filePath string) {
+	dir := filepath.Dir(filePath)
+	if dir == "" || dir == "." {
+		return
+	}
+	err := os.MkdirAll(dir, 0755)
+	if err != nil {
+		fmt.Printf("Error creating directory %v : %+v", dir, err)
+	}
+}
diff --git a/docker-query-exporter/main.go b/docker-query-exporter/main.go
new file mode 100644
index 0000000..7abbfd5
--- /dev/null
+++ b/docker-query-exporter/main.go
@@ -0,0 +1,835 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"flag"
+	"fmt"
+	"math"
+	"net/http"
+	_ "net/http/pprof"
+	"os"
+	"os/signal"
+	"runtime"
+	"runtime/debug"
+	"strconv"
+	"strings"
+	"syscall"
+	"time"
+
+	"github.com/Netcracker/qubership-query-exporter/collectors"
+	"github.com/Netcracker/qubership-query-exporter/config"
+	"github.com/Netcracker/qubership-query-exporter/dbservice"
+	"github.com/Netcracker/qubership-query-exporter/logger"
+	"github.com/Netcracker/qubership-query-exporter/monstate"
+	"github.com/Netcracker/qubership-query-exporter/scheduler"
+	"github.com/Netcracker/qubership-query-exporter/selfmonitor"
+	"github.com/Netcracker/qubership-query-exporter/utils"
+	"github.com/prometheus/client_golang/prometheus"
+	"github.com/prometheus/client_golang/prometheus/promhttp"
+	"github.com/robfig/cron/v3"
+	log "github.com/sirupsen/logrus"
+)
+
+var (
+	printVersion             = flag.Bool("version", false, "Print the query-exporter version and exit")
+	addr                     = flag.String("listen-address", ":8080", "The address to listen (port) for HTTP requests")
+	configPath               = flag.String("config-path", utils.GetEnv("QUERY_EXPORTER_EXTEND_QUERY_PATH", "config.yaml"), "Path to the yaml configuration")
+	histSummarize            = flag.Bool("histogram-sum", utils.GetEnvBool("HISTOGRAM_SUMMARIZE", true), "If histogram values should be summarized")
+	excQueriesFromEnv        = flag.String("excluded-queries", utils.GetEnv("EXCLUDED_QUERIES", ""), "Excluded queries list")
+	appConfig                *config.Config
+	gaugeVecRepository       = utils.GaugeVecRepository{}
+	dynamicMetricsRepository = utils.DynamicMetricsRepository{}
+	counterVecs              = make(map[string]*prometheus.CounterVec)
+	histogramVecs            = make(map[string]*collectors.CustomHistogram)
+	omnipresentLabels        map[string]string
+	omnipresentLabelsKeys    []string
+	croniter                 *cron.Cron
+	monitoringState          = monstate.MonitoringState{}
+	metricDefaultValuesRepo  *monstate.MetricDefaultValuesRepository
+	dbServices               []dbservice.DBService
+	nonScheduledQueries      []string
+	osIntSignals             = make(chan os.Signal, 1)
+	osQuitSignals            = make(chan os.Signal, 1)
+	dbErrorHandlingEnabled   = false
+
+	excludedQueries []string
+	excludedMetrics []string
+)
+
+func initPrometheus() {
+	omnipresentLabels = appConfig.Db.Labels
+	omnipresentLabelsKeys = utils.GetKeys(omnipresentLabels)
+	gaugeVecRepository.Initialize()
+	dynamicMetricsRepository.Initialize()
+
+	for metricName, metricConfig := range appConfig.Metrics {
+		for _, excMetric := range excludedMetrics {
+			if metricName == excMetric {
+				continue
+			}
+		}
+
+		labels := append(metricConfig.Labels, omnipresentLabelsKeys...)
+		switch metricConfig.Type {
+		case "gauge":
+			gaugeVec := prometheus.NewGaugeVec(prometheus.GaugeOpts{
+				Name:        metricName,
+				Help:        metricConfig.Description,
+				ConstLabels: metricConfig.ConstLabels,
+			}, labels)
+			prometheus.MustRegister(gaugeVec)
+			gaugeVecRepository.Set(metricName, gaugeVec)
+			log.Infof("gaugeVec %v registered with labels %+v", metricName, labels)
+		case "counter":
+			counterVec := prometheus.NewCounterVec(prometheus.CounterOpts{
+				Name:        metricName,
+				Help:        metricConfig.Description,
+				ConstLabels: metricConfig.ConstLabels,
+			}, labels)
+			prometheus.MustRegister(counterVec)
+			counterVecs[metricName] = counterVec
+			log.Infof("counterVec %v registered with labels %+v", metricName, labels)
+			initCounterMetric(metricConfig, metricName, counterVec)
+		case "histogram":
+			customHistogram := collectors.NewCustomHistogram(
+				prometheus.NewDesc(
+					metricName,
+					metricConfig.Description,
+					labels,
+					metricConfig.ConstLabels,
+				),
+				isSummarize(metricConfig.Summarize),
+			)
+			histogramVecs[metricName] = customHistogram
+			prometheus.MustRegister(customHistogram)
+			log.Infof("customHistogram %v registered with labels %+v", metricName, labels)
+			initHistogramMetric(metricConfig, metricName, customHistogram)
+		default:
+			log.Errorf("Metric %v has not supported type %v", metricName, metricConfig.Type)
+		}
+	}
+}
+
+func initCounterMetric(metricConfig *config.MetricsConfig, metricName string, counterVec *prometheus.CounterVec) {
+	initValue := metricConfig.Parameters["init-value"]
+	if initValue == "" {
+		return
+	}
+	if len(metricConfig.Labels) == 0 {
+		if strings.ToUpper(initValue) == "NAN" {
+			counterVec.With(omnipresentLabels).Add(math.NaN())
+			log.Infof("Metric %v is initialized with value NaN", metricName)
+		} else {
+			initValueFloat, err := strconv.ParseFloat(initValue, 64)
+			if err == nil {
+				if initValueFloat >= 0 {
+					counterVec.With(omnipresentLabels).Add(initValueFloat)
+					log.Infof("Metric %v is initialized with value %v", metricName, initValue)
+				} else {
+					log.Infof("Counter metric %v can not be initialized with negative value %v", metricName, initValue)
+				}
+			} else {
+				log.Errorf("Error parsing init-value %v for metric %v : %+v", initValue, metricName, err)
+			}
+		}
+	} else if len(metricConfig.ExpectedLabels) > 0 {
+		initValueFloat, err := strconv.ParseFloat(initValue, 64)
+		if err == nil {
+			if initValueFloat < 0 {
+				log.Errorf("Counter metric %v can not be initialized with negative value %v", metricName, initValue)
+				return
+			}
+		} else {
+			log.Errorf("Error parsing init-value %v for metric %v : %+v", initValue, metricName, err)
+			return
+		}
+		for itemNum, expectedLabelsItem := range metricConfig.ExpectedLabels {
+			cartesian := utils.LabelsCartesian(expectedLabelsItem)
+			log.Infof("For metric %v (itemNum %v) expected labels cartesian generated : %+v", metricName, itemNum, cartesian)
+			for _, labels := range cartesian {
+				for labelName, labelValue := range omnipresentLabels {
+					labels[labelName] = labelValue
+				}
+				counterVec.With(labels).Add(initValueFloat)
+			}
+		}
+	} else {
+		log.Errorf("Metric %v can not be initialized because it has labels and expected labels are not defined", metricName)
+	}
+}
+
+func initHistogramMetric(metricConfig *config.MetricsConfig, metricName string, customHistogram *collectors.CustomHistogram) {
+	initValue := metricConfig.Parameters["init-value"]
+	if initValue == "" {
+		return
+	}
+	buckets := make(map[float64]uint64)
+	for _, bucketKey := range metricConfig.Buckets {
+		buckets[bucketKey] = 0
+	}
+	buckets[math.Inf(1.0)] = 0
+	if len(metricConfig.Labels) == 0 {
+		customHistogram.Observe(0, 0, buckets, omnipresentLabels, omnipresentLabelsKeys)
+	} else if len(metricConfig.ExpectedLabels) > 0 {
+		for itemNum, expectedLabelsItem := range metricConfig.ExpectedLabels {
+			cartesian := utils.LabelsCartesian(expectedLabelsItem)
+			log.Infof("For metric %v (itemNum %v) expected labels cartesian generated : %+v", metricName, itemNum, cartesian)
+			for _, labels := range cartesian {
+				for labelName, labelValue := range omnipresentLabels {
+					labels[labelName] = labelValue
+				}
+				customHistogram.Observe(0, 0, buckets, labels, append(metricConfig.Labels, omnipresentLabelsKeys...))
+			}
+		}
+	} else {
+		log.Errorf("Metric %v can not be initialized because it has labels and expected labels are not defined", metricName)
+	}
+}
+
+func initErrorHandling() {
+	if appConfig.ErrorHandling != nil && len(appConfig.ErrorHandling.DbErrorRules) > 0 {
+		dbErrorHandlingEnabled = true
+	}
+}
+
+func registerGaugeDynamically(metricName string, labels []string) {
+	allLabels := append(labels, omnipresentLabelsKeys...)
+	gaugeVec := prometheus.NewGaugeVec(prometheus.GaugeOpts{
+		Name: metricName,
+		Help: "Dynamic gauge without description",
+	}, allLabels)
+	prometheus.MustRegister(gaugeVec)
+	gaugeVecRepository.Set(metricName, gaugeVec)
+	log.Warnf("Dynamic gaugeVec %v registered with labels %+v", metricName, allLabels)
+}
+
+func initMonitoringState() {
+	monitoringState.Initialize()
+	for metricName, metricCfg := range appConfig.Metrics {
+		if metricCfg.Type != "gauge" {
+			continue
+		}
+		metricState := monstate.MetricState{}
+		metricState.Initialize()
+		for itemNum, expectedLabelsItem := range metricCfg.ExpectedLabels {
+			cartesian := utils.LabelsCartesian(expectedLabelsItem)
+			log.Infof("For metric %v (itemNum %v) expected labels cartesian generated : %+v", metricName, itemNum, cartesian)
+			for _, labels := range cartesian {
+				for labelName, labelValue := range omnipresentLabels {
+					labels[labelName] = labelValue
+				}
+				metricState.Set(utils.MapToString(labels), labels)
+			}
+		}
+		monitoringState.Set(metricName, &metricState)
+		log.Debugf("New metric %v registered in monitoringState map", metricName)
+	}
+	metricDefaultValuesRepo = monstate.CreateMetricDefaultValuesRepository(appConfig.Metrics)
+}
+
+func main() {
+	flag.Parse()
+	if *printVersion {
+		fmt.Printf("%v\n", versionString())
+		return
+	}
+	signal.Notify(osIntSignals, syscall.SIGINT, syscall.SIGTERM)
+	signal.Notify(osQuitSignals, syscall.SIGQUIT)
+	go interruptionHandler()
+	go quitHandler()
+
+	defer func() {
+		stopCroniter()
+		log.Info("QUERY EXPORTER STOPPED, no signal received")
+		fmt.Printf("\nstop query-exporter, %v\n", versionString())
+	}()
+	defer func() {
+		if rec := recover(); rec != nil {
+			log.Errorf("Panic in main : %+v ; Stacktrace of the panic : %v", rec, string(debug.Stack()))
+		}
+	}()
+
+	fmt.Printf("start query-exporter, %v\n", versionString())
+
+	logger.ConfigureLog()
+
+	log.Infof("QUERY EXPORTER STARTED; %v", versionString())
+
+	var err error
+	appConfig, err = config.Read(*configPath)
+	if err != nil {
+		log.Fatalf("Fatal: %v", err)
+		return
+	}
+	reapplyFlags()
+	initErrorHandling()
+	initExcludedQueries()
+	croniter = utils.GetCron()
+
+	initPrometheus()
+	initMonitoringState()
+	selfmonitor.InitSelfMonitoring(appConfig)
+
+	dbServices = dbservice.GetDBServices(appConfig)
+	defer func() {
+		go hardstopAfterTimeout(time.Minute)
+		for _, dbService := range dbServices {
+			dbService.Close()
+		}
+	}()
+	//dbService.PrintSimpleQuery("select sys_context( 'userenv', 'current_schema' ) from dual")
+
+	nonScheduledQueries = scheduler.Schedule(croniter, appConfig, executeQueryAndUpdateMetrics, updateDatabases)
+
+	http.HandleFunc("/metrics", httpHandlerFunc)
+	http.HandleFunc("/health", health)
+
+	log.Error(http.ListenAndServe(*addr, nil))
+}
+
+func initExcludedQueries() {
+	excludedQueriesArr := strings.Split(*excQueriesFromEnv, ",")
+	for qName, qcontent := range appConfig.Queries {
+		for _, excludedQuery := range excludedQueriesArr {
+			if qName == excludedQuery {
+				log.Infof("query %s was excluded by initial config", qName)
+				excludedQueries = append(excludedQueries, qName)
+				for _, m := range qcontent.Metrics {
+					log.Infof("metric %s was excluded by initial config", m)
+					excludedMetrics = append(excludedMetrics, m)
+				}
+			}
+		}
+	}
+}
+
+func health(w http.ResponseWriter, r *http.Request) {
+	dbService := dbServices[0]
+	if dbService != nil {
+		if dbService.Ping() {
+			w.Write([]byte("OK"))
+		}
+	} else {
+		w.WriteHeader(503)
+	}
+}
+
+func httpHandlerFunc(w http.ResponseWriter, r *http.Request) {
+	log.Debugf("HttpHandler started, %v non-scheduled queries need to be executed: %+v", len(nonScheduledQueries), nonScheduledQueries)
+	defer log.Debug("HttpHandler finished")
+
+	for _, qName := range nonScheduledQueries {
+		executeQueryAndUpdateMetrics(qName)
+	}
+
+	promhttp.HandlerFor(
+		prometheus.DefaultGatherer,
+		promhttp.HandlerOpts{},
+	).ServeHTTP(w, r)
+}
+
+func executeQueryAndUpdateMetrics(qName string) {
+	startTime := time.Now()
+	qRand := utils.GenerateRandomString(10)
+	var rows []map[string]string
+	defer func() {
+		log.Debugf("[%v] Update job for %v executed in %+v ; total rows returned : %v", qRand, qName, time.Since(startTime), len(rows))
+	}()
+	defer func() {
+		if rec := recover(); rec != nil {
+			log.Errorf("[%v] Panic during execution of query %v: %+v ; Stacktrace of the panic : %v", qRand, qName, rec, string(debug.Stack()))
+		}
+	}()
+
+	qCfg := appConfig.Queries[qName]
+
+	queryExecutedSuccessfully := true
+	totalUpdatedMetricLabelValues := make(map[string]map[string]prometheus.Labels)
+	defer func() {
+		if len(qCfg.Metrics) != 0 {
+			resetNotUpdatedGauges(totalUpdatedMetricLabelValues, qCfg.Metrics, queryExecutedSuccessfully, qRand)
+		} else {
+			metricSet := dynamicMetricsRepository.Get(qName)
+			if metricSet != nil {
+				metrics := metricSet.GetAllEntries()
+				log.Debugf("[%v] Resetting not updated gauges to NaN for dynamic query %v, metrics list to check is %+v", qRand, qName, metrics)
+				resetNotUpdatedGauges(totalUpdatedMetricLabelValues, metrics, queryExecutedSuccessfully, qRand)
+			} else {
+				log.Warnf("[%v] Metric Set for dynamic query %v is nil, resetting not updated gauges to NaN can not be performed", qRand, qName)
+			}
+		}
+	}()
+	dbSrv := dbServices
+	for _, dbService := range dbSrv {
+		if skipQuery(dbService, qName, qCfg) {
+			continue
+		}
+		timeBeforeQueryExecution := time.Now()
+		rows, columnNames, err := dbService.ExecuteSelect(qName, qRand, qCfg)
+		selfmonitor.QueryLatencyExecution(timeBeforeQueryExecution, qName, dbService.GetDatabaseName())
+		if err != nil {
+			log.Errorf("[%v] Error running query %s : %+v", qRand, qName, err)
+			selfmonitor.IncQueryStatusCount(qName, dbService.GetDatabaseName(), "failed")
+			queryExecutedSuccessfully = false
+			if dbErrorHandlingEnabled {
+				handleDbError(err)
+			}
+			continue
+		}
+		selfmonitor.IncQueryStatusCount(qName, dbService.GetDatabaseName(), "completed")
+
+		if len(qCfg.Metrics) == 0 && len(columnNames) < 2 {
+			log.Errorf("[%v] For query %v metrics are not defined, but result has %v columns. Impossible to apply dynamic behavior.", qRand, qName, len(columnNames))
+			return
+		}
+
+		var updatedMetricLabelValues map[string]map[string]prometheus.Labels
+		for _, row := range rows {
+			if len(qCfg.Metrics) != 0 {
+				updatedMetricLabelValues = updateStaticMetricByRow(row, qCfg.Metrics, qRand)
+			} else {
+				updatedMetricLabelValues = updateDynamicMetricByRow(row, columnNames, qRand)
+				for metricName := range updatedMetricLabelValues {
+					registerNewDynamicMetricInRepository(qName, qRand, metricName)
+				}
+			}
+
+			for metric, updatedLabelValues := range updatedMetricLabelValues {
+				if totalUpdatedMetricLabelValues[metric] == nil {
+					totalUpdatedMetricLabelValues[metric] = make(map[string]prometheus.Labels)
+				}
+				for labelValues, labels := range updatedLabelValues {
+					totalUpdatedMetricLabelValues[metric][labelValues] = labels
+				}
+			}
+		}
+	}
+}
+
+func updateStaticMetricByRow(row map[string]string, metrics []string, qRand string) map[string]map[string]prometheus.Labels {
+	updatedMetricsLabelsValues := make(map[string]map[string]prometheus.Labels)
+MainUpdateStaticMetricByRowLoop:
+	for _, metric := range metrics {
+		metricsConfig := appConfig.Metrics[metric]
+		if metricsConfig == nil {
+			//log.Warnf("Ignoring metric %v, because it is not defined", metric)
+			continue
+		}
+		labels := make(map[string]string)
+		for label, labelValue := range omnipresentLabels {
+			labels[label] = labelValue
+		}
+		for _, label := range metricsConfig.Labels {
+			labels[label] = row[strings.ToUpper(label)]
+		}
+		switch metricsConfig.Type {
+		case "gauge":
+			if monitoringState.Get(metric) == nil {
+				metricState := monstate.MetricState{}
+				metricState.Initialize()
+				monitoringState.Set(metric, &metricState)
+				log.Infof("[%v] New metric %v registered in runtime for monitoringState map", qRand, metric)
+			}
+
+			updatedLabelValues := make(map[string]prometheus.Labels)
+			existingLabelValues := monitoringState.Get(metric)
+			gaugeVec := gaugeVecRepository.Get(metric)
+
+			val, err := strconv.ParseFloat(row[strings.ToUpper(metric)], 64)
+			stringLabels := utils.MapToString(labels)
+			if err != nil {
+				updatedLabelValues[stringLabels] = labels
+				if existingLabelValues.Get(stringLabels) == nil {
+					existingLabelValues.Set(stringLabels, labels)
+					selfmonitor.IncMonStateEntriesCount()
+				}
+				updatedMetricsLabelsValues[metric] = updatedLabelValues
+				defaultValueQueryReturnedNaN := metricDefaultValuesRepo.GetMetricDefaultValueQueryReturnedNaN(metric)
+				gaugeVec.With(labels).Set(defaultValueQueryReturnedNaN)
+				if row[strings.ToUpper(metric)] == "" {
+					log.Debugf("[%v] Empty value received for metric %v and labels %v; metric set with the default value %v", qRand, metric, stringLabels, defaultValueQueryReturnedNaN)
+				} else {
+					log.Warnf("[%v] Error for metric %v and labels %v : Can not parse value %v : %+v ; metric set with the default value %v", qRand, metric, stringLabels, row[strings.ToUpper(metric)], err, defaultValueQueryReturnedNaN)
+				}
+			} else {
+				log.Tracef("[%v] For metric %v and labels %v exported value %v", qRand, metric, stringLabels, val)
+				updatedLabelValues[stringLabels] = labels
+				if existingLabelValues.Get(stringLabels) == nil {
+					existingLabelValues.Set(stringLabels, labels)
+					selfmonitor.IncMonStateEntriesCount()
+				}
+				updatedMetricsLabelsValues[metric] = updatedLabelValues
+				gaugeVec.With(labels).Set(val)
+			}
+		case "counter":
+			counterVec := counterVecs[metric]
+			val, err := strconv.ParseFloat(row[strings.ToUpper(metric)], 64)
+			if err != nil {
+				log.Errorf("[%v] Error parsing metric %v for value %v : %+v", qRand, metric, row[strings.ToUpper(metric)], err)
+			} else if val < 0 {
+				log.Errorf("[%v] Error processing metric %v for value %+v : Counter can not be decreased, value of the metric is not changed", qRand, metric, val)
+			} else {
+				counterVec.With(labels).Add(val)
+			}
+		case "histogram":
+			histogramVec := histogramVecs[metric]
+			sum, err := strconv.ParseFloat(row[strings.ToUpper(metricsConfig.Sum)], 64)
+			if err != nil {
+				log.Errorf("[%v] Error parsing sum of histogram metric %v for value %v : %+v", qRand, metric, row[strings.ToUpper(metric)], err)
+				continue
+			}
+			cnt, err := strconv.ParseUint(row[strings.ToUpper(metricsConfig.Count)], 10, 64)
+			if err != nil {
+				log.Errorf("[%v] Error parsing count of histogram metric %v for value %v : %+v", qRand, metric, row[strings.ToUpper(metric)], err)
+				continue
+			}
+			buckets := make(map[float64]uint64)
+			for columnName, bucketKey := range metricsConfig.Buckets {
+				bucketValue, err := strconv.ParseUint(row[strings.ToUpper(columnName)], 10, 64)
+				if err != nil {
+					log.Errorf("[%v] Error parsing column %v of histogram metric %v for value %v : %+v", qRand, columnName, metric, row[strings.ToUpper(metric)], err)
+					continue MainUpdateStaticMetricByRowLoop
+				}
+				buckets[bucketKey] = bucketValue
+			}
+			labelKeys := append(metricsConfig.Labels, omnipresentLabelsKeys...)
+			buckets[math.Inf(1.0)] = cnt
+			histogramVec.Observe(sum, cnt, buckets, labels, labelKeys)
+		default:
+			log.Errorf("[%v] Metric %v has not supported type %v", qRand, metric, metricsConfig.Type)
+		}
+	}
+
+	return updatedMetricsLabelsValues
+}
+
+func updateDynamicMetricByRow(row map[string]string, columnNames []string, qRand string) map[string]map[string]prometheus.Labels {
+	updatedMetricsLabelsValues := make(map[string]map[string]prometheus.Labels)
+	metric := row[strings.ToUpper(columnNames[0])]
+
+	if monitoringState.Get(metric) == nil {
+		metricState := monstate.MetricState{}
+		metricState.Initialize()
+		monitoringState.Set(metric, &metricState)
+		log.Infof("[%v] New dynamic metric %v registered in runtime in monitoringState map", qRand, metric)
+	}
+
+	updatedLabelValues := make(map[string]prometheus.Labels)
+	existingLabelValues := monitoringState.Get(metric)
+
+	labels := make(map[string]string)
+	for label, labelValue := range omnipresentLabels {
+		labels[label] = labelValue
+	}
+	for i := 2; i < len(columnNames); i++ {
+		labels[columnNames[i]] = row[strings.ToUpper(columnNames[i])]
+	}
+
+	if gaugeVecRepository.Get(metric) == nil {
+		registerGaugeDynamically(metric, columnNames[2:])
+	}
+
+	gaugeVec := gaugeVecRepository.Get(metric)
+	val, err := strconv.ParseFloat(row[strings.ToUpper(columnNames[1])], 64)
+	stringLabels := utils.MapToString(labels)
+	if err != nil {
+		if existingLabelValues.Get(stringLabels) == nil {
+			existingLabelValues.Set(stringLabels, labels)
+			selfmonitor.IncMonStateEntriesCount()
+		}
+		if row[strings.ToUpper(metric)] == "" {
+			log.Debugf("[%v] Empty value received for metric %v and labels %v", qRand, metric, stringLabels)
+		} else {
+			log.Warnf("[%v] Error for metric %v and labels %v : Can not parse value %v : %+v", qRand, metric, stringLabels, row[strings.ToUpper(metric)], err)
+		}
+		gaugeVec.With(labels).Set(math.NaN())
+	} else {
+		log.Tracef("[%v] For metric %v and labels %v exported value %v", qRand, metric, stringLabels, val)
+		updatedLabelValues[stringLabels] = labels
+		if existingLabelValues.Get(stringLabels) == nil {
+			existingLabelValues.Set(stringLabels, labels)
+			selfmonitor.IncMonStateEntriesCount()
+		}
+		updatedMetricsLabelsValues[metric] = updatedLabelValues
+		gaugeVec.With(labels).Set(val)
+	}
+
+	return updatedMetricsLabelsValues
+}
+
+func updateDatabases() {
+	rDbList := []string{}
+	resultServices := []dbservice.DBService{}
+
+	newDbSrv := dbservice.GetNewDBServices(dbServices)
+	dbSrvs := append(dbServices, newDbSrv...)
+	for _, dbSrv := range dbSrvs {
+		if dbSrv.GetType() != dbservice.TypePostgres {
+			continue
+		}
+		if dbSrv.Ping() {
+			resultServices = append(resultServices, dbSrv)
+		} else {
+			log.Infof("database %s has been removed from db list", dbSrv.GetDatabaseName())
+			rDbList = append(rDbList, dbSrv.GetDatabaseName())
+		}
+	}
+
+	dbServices = resultServices
+	unregisterUnusedMetrics(rDbList)
+}
+
+func unregisterUnusedMetrics(databases []string) {
+	for _, database := range databases {
+		labels := prometheus.Labels{"datname": database}
+		for metric, metricConfig := range appConfig.Metrics {
+			switch metricConfig.Type {
+			case "gauge":
+				metricState := monitoringState.Get(metric)
+				for _, key := range metricState.GetAllKeys() {
+					if strings.Contains(key, "datname=\""+database+"\"") {
+						metricState.Delete(key)
+					}
+				}
+				gaugeVec := gaugeVecRepository.Get(metric)
+				if gaugeVec != nil {
+					gaugeVec.DeletePartialMatch(labels)
+				}
+			case "counter":
+				if counterVec, ok := counterVecs[metric]; ok {
+					counterVec.DeletePartialMatch(labels)
+				}
+			case "histogram":
+				if histMetric, ok := histogramVecs[metric]; ok {
+					histMetric.DeletePartialMatch(labels)
+				}
+			default:
+				log.Errorf("Metric %s has not supported type", metric)
+			}
+		}
+	}
+}
+
+func registerNewDynamicMetricInRepository(qName string, qRand string, metric string) {
+	if dynamicMetricsRepository.Get(qName) == nil {
+		metricSet := utils.SyncStringSet{}
+		metricSet.Initialize()
+		dynamicMetricsRepository.Set(qName, &metricSet)
+	}
+	metricSet := dynamicMetricsRepository.Get(qName)
+	if metricSet.Get(metric) == false {
+		metricSet.Add(metric)
+		log.Infof("[%v] New metric %v registered in DynamicMetricRepository for query %v", qRand, metric, qName)
+	}
+}
+
+func resetNotUpdatedGauges(totalUpdatedMetricLabelValues map[string]map[string]prometheus.Labels, metrics []string, queryExecutedSuccessfully bool, qRand string) {
+	for _, metric := range metrics {
+		if totalUpdatedMetricLabelValues[metric] == nil {
+			totalUpdatedMetricLabelValues[metric] = make(map[string]prometheus.Labels)
+		}
+
+		metricState := monitoringState.Get(metric)
+		if metricState == nil {
+			if gaugeVecRepository.Get(metric) != nil {
+				log.Errorf("[%v] For gauge metric %v metricState is not defined, skipping resetNotUpdatedGauges for this metric", qRand, metric)
+			}
+			continue
+		}
+		metricStateKeys := metricState.GetAllKeys()
+
+		var resetValue float64
+		if queryExecutedSuccessfully {
+			resetValue = metricDefaultValuesRepo.GetMetricDefaultValue(metric)
+		} else {
+			resetValue = metricDefaultValuesRepo.GetMetricDefaultValueQueryFailed(metric)
+		}
+
+		for _, labelValues := range metricStateKeys {
+			labels := metricState.Get(labelValues)
+			if totalUpdatedMetricLabelValues[metric][labelValues] == nil {
+				log.Tracef("[%v] Gauge for metric %v with labelValues %v was not updated. Resetting it to %v", qRand, metric, labelValues, resetValue)
+				gaugeVec := gaugeVecRepository.Get(metric)
+				if gaugeVec != nil {
+					gaugeVec.With(labels).Set(resetValue)
+				} else {
+					log.Debugf("[%v] gaugeVecRepository.Get(%v) is nil, which is unexpected", qRand, metric)
+				}
+			}
+		}
+	}
+}
+
+func interruptionHandler() {
+	signal := <-osIntSignals
+
+	go hardstopAfterTimeout(time.Minute)
+	logRuntimeInfo()
+	for _, dbService := range dbServices {
+		dbService.Close()
+	}
+	stopCroniter()
+
+	log.Infof("STOPPING QUERY EXPORTER (received %+v signal)", signal)
+	fmt.Printf("\nstop query-exporter, %v\n", versionString())
+	os.Exit(0)
+}
+
+func hardstopAfterTimeout(d time.Duration) {
+	time.Sleep(d)
+	log.Infof("STOPPING QUERY EXPORTER (hardstop after timeout)")
+	os.Exit(0)
+}
+
+func quitHandler() {
+	for {
+		signal := <-osQuitSignals
+		log.Infof("Received %+v signal, printing thread dumps...", signal)
+		logRuntimeInfo()
+	}
+}
+
+func stopCroniter() {
+	if croniter != nil {
+		croniter.Stop()
+	} else {
+		log.Warnf("croniter is nil before exiting, nothing to stop")
+	}
+}
+
+func handleDbError(err error) {
+	for ruleName, dbErrorRule := range appConfig.ErrorHandling.DbErrorRules {
+		if !strings.Contains(err.Error(), dbErrorRule.Contains) {
+			log.Tracef("Error [ %+v ] does not contain text %v ; skipping errorHandling rule %v", err, dbErrorRule.Contains, ruleName)
+			return
+		}
+
+		var timeout time.Duration = 0
+		if dbErrorRule.Timeout != "" {
+			t, parseErr := time.ParseDuration(dbErrorRule.Timeout)
+			if parseErr == nil {
+				timeout = t
+			} else {
+				log.Errorf("Error parsing dbErrorHandling timeout %v for errorHandling rule %v : %v ; the rule will be executed without timeout", dbErrorRule.Timeout, ruleName, parseErr)
+			}
+		}
+		log.Tracef("Timeout for errorHandling rule %v is set to %+v", ruleName, timeout)
+
+		if dbErrorRule.Action == "exit" {
+			go exitAfterTimeout(timeout, ruleName)
+		} else {
+			log.Warnf("Unknown action %v for rule %v; action will be skipped", dbErrorRule.Action, ruleName)
+		}
+	}
+}
+
+func exitAfterTimeout(timeout time.Duration, ruleName string) {
+	stopCroniter()
+	log.Warnf("Croniter is stopped after triggering rule %v. Query-exporter will be stopped after %+v", ruleName, timeout)
+	time.Sleep(timeout)
+	go hardstopAfterTimeout(time.Minute)
+	for _, dbService := range dbServices {
+		dbService.Close()
+	}
+	log.Infof("STOPPING QUERY EXPORTER (triggered by rule %+v)", ruleName)
+	fmt.Printf("\nstop query-exporter, %v\n", versionString())
+	os.Exit(0)
+}
+
+func logRuntimeInfo() {
+	buf := make([]byte, 1<<20)
+	stacklen := runtime.Stack(buf, true)
+	log.Errorf("GOROUTINES DUMP: %s", buf[:stacklen])
+	for _, dbService := range dbServices {
+		log.Error(dbService.DBStat())
+	}
+}
+
+func reapplyFlags() {
+	if len(appConfig.Flags) == 0 {
+		log.Info("No flags need to be reloaded from YAML config")
+	} else {
+		for name, value := range appConfig.Flags {
+			err := flag.Set(name, value)
+			if err != nil {
+				log.Errorf("Failed to set flag %v with new value %v", name, value)
+			} else {
+				log.Infof("Flag %v successfully set to new value %v", name, value)
+			}
+		}
+	}
+}
+
+func isSummarize(valueFromMetric *bool) bool {
+	if valueFromMetric == nil {
+		return *histSummarize
+	}
+	return *valueFromMetric
+}
+
+func skipQuery(dbSrv dbservice.DBService, qName string, qcfg *config.QueryConfig) bool {
+	for _, qForExc := range excludedQueries {
+		if qName == qForExc {
+			return true
+		}
+	}
+
+	if qcfg.Master && !dbSrv.IsMaster() {
+		return true
+	}
+
+	skippedlisted := false
+	for _, v := range dbservice.GetLongRunningQueries() {
+		if qName == v {
+			skippedlisted = true
+		}
+	}
+
+	if skippedlisted {
+		log.Infof("Query %s is skipped as long-running", qName)
+		return true
+	}
+
+	if len(qcfg.LogicalDatabases) > 0 {
+		for _, d := range qcfg.LogicalDatabases {
+			if d == dbSrv.GetDatabaseName() {
+				return false
+			}
+		}
+		return true
+	}
+
+	if len(qcfg.Classifiers) > 0 {
+		currentClassifier := dbSrv.GetClassifier()
+		for _, classifier := range qcfg.Classifiers {
+			if isAcceptable(classifier, currentClassifier) {
+				return false
+			}
+		}
+		return true
+	}
+
+	return false
+}
+
+func isAcceptable(mappingClassifier, dbClassifier config.Classifier) bool {
+	for k, v := range mappingClassifier {
+		if dbClassifier[k] != v {
+			return false
+		}
+	}
+	return true
+}
diff --git a/docker-query-exporter/monstate/metricdefaultvalues.go b/docker-query-exporter/monstate/metricdefaultvalues.go
new file mode 100644
index 0000000..d5b4ab7
--- /dev/null
+++ b/docker-query-exporter/monstate/metricdefaultvalues.go
@@ -0,0 +1,131 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package monstate
+
+import (
+	"math"
+	"strconv"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+	log "github.com/sirupsen/logrus"
+)
+
+type MetricDefaultValues struct {
+	DefaultValue                 float64
+	DefaultValueQueryFailed      float64
+	DefaultValueQueryReturnedNaN float64
+}
+
+func CreateMetricDefaultValues(metricName string, defaultValue string, defaultValueQueryFailed string, defaultValueQueryReturnedNaN string) *MetricDefaultValues {
+	mdv := MetricDefaultValues{}
+
+	val, err := strconv.ParseFloat(defaultValue, 64)
+	if err != nil {
+		if defaultValue == "" {
+			log.Debugf("Empty defaultValue received for metric %v; defaultValue is set to NaN", metricName)
+		} else {
+			log.Errorf("Error parsing defaultValue %v for metric %v : %+v; defaultValue is set to NaN", defaultValue, metricName, err)
+		}
+		mdv.DefaultValue = math.NaN()
+	} else {
+		mdv.DefaultValue = val
+	}
+
+	val, err = strconv.ParseFloat(defaultValueQueryFailed, 64)
+	if err != nil {
+		if defaultValueQueryFailed == "" {
+			log.Debugf("Empty defaultValueQueryFailed received for metric %v; defaultValueQueryFailed is set to NaN", metricName)
+		} else {
+			log.Errorf("Error parsing defaultValueQueryFailed %v for metric %v : %+v; defaultValueQueryFailed is set to NaN", defaultValueQueryFailed, metricName, err)
+		}
+		mdv.DefaultValueQueryFailed = math.NaN()
+	} else {
+		mdv.DefaultValueQueryFailed = val
+	}
+
+	val, err = strconv.ParseFloat(defaultValueQueryReturnedNaN, 64)
+	if err != nil {
+		if defaultValueQueryReturnedNaN == "" {
+			log.Debugf("Empty defaultValueQueryReturnedNaN received for metric %v; defaultValueQueryReturnedNaN is set to NaN", metricName)
+		} else {
+			log.Errorf("Error parsing defaultValueQueryReturnedNaN %v for metric %v : %+v; defaultValueQueryReturnedNaN is set to NaN", defaultValueQueryReturnedNaN, metricName, err)
+		}
+		mdv.DefaultValueQueryReturnedNaN = math.NaN()
+	} else {
+		mdv.DefaultValueQueryReturnedNaN = val
+	}
+
+	return &mdv
+}
+
+type MetricDefaultValuesRepository struct {
+	m map[string]*MetricDefaultValues
+}
+
+func CreateMetricDefaultValuesRepository(metricsMap map[string]*config.MetricsConfig) *MetricDefaultValuesRepository {
+	repo := MetricDefaultValuesRepository{}
+	repo.m = make(map[string]*MetricDefaultValues)
+
+	for metricName, metricCfg := range metricsMap {
+		if len(metricCfg.Parameters) == 0 {
+			log.Debugf("DefaultValues : No parameters found for metric %v", metricName)
+			continue
+		}
+		defaultValue := metricCfg.Parameters["default-value"]
+		defaultValueQueryFailed := metricCfg.Parameters["default-value-query-failed"]
+		defaultValueQueryReturnedNaN := metricCfg.Parameters["default-value-query-returned-nan"]
+		if defaultValue == "" && defaultValueQueryFailed == "" && defaultValueQueryReturnedNaN == "" {
+			log.Debugf("DefaultValues : No default values found for metric %v", metricName)
+			continue
+		}
+		mdv := CreateMetricDefaultValues(metricName, defaultValue, defaultValueQueryFailed, defaultValueQueryReturnedNaN)
+		repo.m[metricName] = mdv
+		log.Infof("Default values successfully created for metric %v : %+v", metricName, mdv)
+	}
+
+	return &repo
+}
+
+/*
+func (repo *MetricDefaultValuesRepository) GetMetricDefaultValues(metric string) {
+    return repo.m[metric]
+} */
+
+func (repo *MetricDefaultValuesRepository) GetMetricDefaultValue(metric string) float64 {
+	if repo.m[metric] == nil {
+		log.Tracef("For metric %v DefaultValue is NaN", metric)
+		return math.NaN()
+	}
+
+	return repo.m[metric].DefaultValue
+}
+
+func (repo *MetricDefaultValuesRepository) GetMetricDefaultValueQueryFailed(metric string) float64 {
+	if repo.m[metric] == nil {
+		log.Tracef("For metric %v DefaultValueQueryFailed is NaN", metric)
+		return math.NaN()
+	}
+
+	return repo.m[metric].DefaultValueQueryFailed
+}
+
+func (repo *MetricDefaultValuesRepository) GetMetricDefaultValueQueryReturnedNaN(metric string) float64 {
+	if repo.m[metric] == nil {
+		log.Tracef("For metric %v DefaultValueQueryReturnedNaN is NaN", metric)
+		return math.NaN()
+	}
+
+	return repo.m[metric].DefaultValueQueryReturnedNaN
+}
diff --git a/docker-query-exporter/monstate/metricstate.go b/docker-query-exporter/monstate/metricstate.go
new file mode 100644
index 0000000..2cb3873
--- /dev/null
+++ b/docker-query-exporter/monstate/metricstate.go
@@ -0,0 +1,57 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package monstate
+
+import (
+	"github.com/prometheus/client_golang/prometheus"
+	"sync"
+)
+
+type MetricState struct {
+	sync.RWMutex
+	m map[string]prometheus.Labels
+}
+
+func (ms *MetricState) Initialize() {
+	ms.m = make(map[string]prometheus.Labels)
+}
+
+func (ms *MetricState) Get(key string) prometheus.Labels {
+	ms.RLock()
+	defer ms.RUnlock()
+	return ms.m[key]
+}
+
+func (ms *MetricState) GetAllKeys() []string {
+	ms.RLock()
+	defer ms.RUnlock()
+	result := make([]string, 0, len(ms.m))
+	for key := range ms.m {
+		result = append(result, key)
+	}
+	return result
+}
+
+func (ms *MetricState) Set(key string, val prometheus.Labels) {
+	ms.Lock()
+	defer ms.Unlock()
+	ms.m[key] = val
+}
+
+func (ms *MetricState) Delete(key string) {
+	ms.Lock()
+	defer ms.Unlock()
+	delete(ms.m, key)
+}
diff --git a/docker-query-exporter/monstate/monstate.go b/docker-query-exporter/monstate/monstate.go
new file mode 100644
index 0000000..ffd353d
--- /dev/null
+++ b/docker-query-exporter/monstate/monstate.go
@@ -0,0 +1,40 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package monstate
+
+import (
+	"sync"
+)
+
+type MonitoringState struct {
+	sync.RWMutex
+	m map[string]*MetricState
+}
+
+func (ms *MonitoringState) Initialize() {
+	ms.m = make(map[string]*MetricState)
+}
+
+func (ms *MonitoringState) Get(key string) *MetricState {
+	ms.RLock()
+	defer ms.RUnlock()
+	return ms.m[key]
+}
+
+func (ms *MonitoringState) Set(key string, val *MetricState) {
+	ms.Lock()
+	defer ms.Unlock()
+	ms.m[key] = val
+}
diff --git a/docker-query-exporter/scheduler/scheduler.go b/docker-query-exporter/scheduler/scheduler.go
new file mode 100644
index 0000000..6ad09a6
--- /dev/null
+++ b/docker-query-exporter/scheduler/scheduler.go
@@ -0,0 +1,77 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package scheduler
+
+import (
+	"flag"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+
+	"github.com/robfig/cron/v3"
+	log "github.com/sirupsen/logrus"
+)
+
+var autodiscoveryInterval = flag.String("autodiscovery-interval", utils.GetEnv("AUTODISCOVERY_INTERVAL", "1m"), "Specifies autodiscovery interval")
+var defaultInterval = flag.String("collection-interval", utils.GetEnv("COLLECTION_INTERVAL", ""), "Specifies autodiscovery interval")
+
+func Schedule(croniter *cron.Cron, appConfig *config.Config, executeQueryAndUpdateMetrics func(string), updateDatabases func()) []string {
+	if utils.IsAutodiscoveryEnabled() {
+		croniter.AddFunc("@every "+*autodiscoveryInterval, func() { updateDatabases() })
+	}
+	nonScheduledQueries := addQueriesExecutionsToCron(croniter, appConfig, executeQueryAndUpdateMetrics)
+	croniter.Start()
+	log.Infof("Croniter started")
+	return nonScheduledQueries
+}
+
+func addQueriesExecutionsToCron(croniter *cron.Cron, appConfig *config.Config, executeQueryAndUpdateMetrics func(string)) []string {
+	nonScheduledQueries := make([]string, 0)
+	parser := utils.GetCronParser()
+	for qName, qCfg := range appConfig.Queries {
+		qName := qName
+		qCfg := qCfg //  Get a fresh version of the variable with the same name, deliberately shadowing the loop variable locally but unique to each goroutine.
+		if qCfg.Croniter != "" {
+			_, parseErr := parser.Parse(qCfg.Croniter)
+			if parseErr != nil {
+				log.Errorf("Query %v is not scheduled, because it has invalid croniter expression: %v ; %+v", qName, qCfg.Croniter, parseErr)
+			} else {
+				go executeQueryAndUpdateMetrics(qName)
+				croniter.AddFunc(qCfg.Croniter, func() { executeQueryAndUpdateMetrics(qName) })
+				log.Infof("Croniter %v registered for query %v", qCfg.Croniter, qName)
+			}
+		} else if qCfg.Interval != "" {
+			scheduleWithInterval(qName, croniter, parser, qCfg.Interval, executeQueryAndUpdateMetrics)
+		} else if len(*defaultInterval) > 0 {
+			scheduleWithInterval(qName, croniter, parser, *defaultInterval, executeQueryAndUpdateMetrics)
+		} else {
+			log.Warnf("Query %v is not scheduled, because neither croniter nor interval are defined for it. Query will be executed on scrape event", qName)
+			nonScheduledQueries = append(nonScheduledQueries, qName)
+		}
+	}
+	return nonScheduledQueries
+}
+
+func scheduleWithInterval(qName string, croniter *cron.Cron, parser cron.Parser, interval string, executeQueryAndUpdateMetrics func(string)) {
+	_, parseErr := parser.Parse("@every " + interval)
+	if parseErr != nil {
+		log.Errorf("Query %v is not scheduled, because it has invalid interval expression: %v ; %+v", qName, interval, parseErr)
+	} else {
+		go executeQueryAndUpdateMetrics(qName)
+		croniter.AddFunc("@every "+interval, func() { executeQueryAndUpdateMetrics(qName) })
+		log.Infof("Interval @every %v registered for query %v", interval, qName)
+	}
+}
diff --git a/docker-query-exporter/selfmonitor/self-monitor.go b/docker-query-exporter/selfmonitor/self-monitor.go
new file mode 100644
index 0000000..9fd2dc2
--- /dev/null
+++ b/docker-query-exporter/selfmonitor/self-monitor.go
@@ -0,0 +1,175 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package selfmonitor
+
+import (
+	"database/sql"
+	"flag"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/qubership-query-exporter/utils"
+
+	"github.com/Netcracker/qubership-query-exporter/config"
+	"github.com/jackc/pgx/v5/pgxpool"
+	"github.com/prometheus/client_golang/prometheus"
+	log "github.com/sirupsen/logrus"
+)
+
+var (
+	disabledSelfMonitor = flag.Bool("disable-self-monitor", utils.GetEnvBool("QUERY_EXPORTER_DISABLE_SELF_MONITOR", false), "Disables self monitoring")
+
+	dbPoolStatsGaugeVec        *prometheus.GaugeVec
+	queryDurationsHistogramVec *prometheus.HistogramVec
+	queryStatusCounterVec      *prometheus.CounterVec
+	monStateEntriesCounterVec  *prometheus.CounterVec
+	labelsMapCache             map[string]map[string]string
+	dbPoolStatsLabelValues     []string
+	staticLabels               map[string]string
+	omnipresentLabelsKeys      []string
+)
+
+func InitSelfMonitoring(appConfig *config.Config) {
+	if *disabledSelfMonitor {
+		return
+	}
+	staticLabels = appConfig.Db.Labels
+	omnipresentLabelsKeys = utils.GetKeys(staticLabels)
+	if appConfig.Db.Type == "" || strings.ToLower(appConfig.Db.Type) == "oracle" {
+		dbPoolStatsLabelValues = []string{"MaxOpenConnections", "OpenConnections", "InUse",
+			"Idle", "WaitCount", "WaitDuration", "MaxIdleClosed", "MaxIdleTimeClosed", "MaxLifetimeClosed"}
+	} else if strings.ToLower(appConfig.Db.Type) == "postgres" {
+		dbPoolStatsLabelValues = []string{"AcquireCount", "AcquireDuration", "AcquiredConns", "CanceledAcquireCount",
+			"ConstructingConns", "EmptyAcquireCount", "IdleConns", "MaxConns", "MaxIdleDestroyCount",
+			"MaxLifetimeDestroyCount", "NewConnsCount", "TotalConns"}
+	}
+	initLabelsMapCache()
+	dbPoolStatsGaugeVec = prometheus.NewGaugeVec(prometheus.GaugeOpts{
+		Name: "db_connection_pool_stats",
+		Help: "DB connection pool statistic",
+	}, append([]string{"name"}, omnipresentLabelsKeys...))
+	prometheus.MustRegister(dbPoolStatsGaugeVec)
+
+	var queryLatencyBuckets []float64
+	if len(appConfig.SelfMonitoring.QueryLatencyBuckets) != 0 {
+		queryLatencyBuckets = appConfig.SelfMonitoring.QueryLatencyBuckets
+		log.Infof("Using query latency buckets from yaml for self-monitoring: %+v", queryLatencyBuckets)
+	} else {
+		queryLatencyBuckets = []float64{0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10, 30, 60}
+		log.Infof("Using default query latency buckets for self-monitoring: %+v", queryLatencyBuckets)
+	}
+	queryDurationsHistogramVec = prometheus.NewHistogramVec(prometheus.HistogramOpts{
+		Name:    "query_latency",
+		Help:    "Query execution latency in seconds",
+		Buckets: queryLatencyBuckets,
+	}, append([]string{"query", "datname"}, omnipresentLabelsKeys...))
+	prometheus.MustRegister(queryDurationsHistogramVec)
+
+	queryStatusCounterVec = prometheus.NewCounterVec(prometheus.CounterOpts{
+		Name: "query_status",
+		Help: "Query execution count by status",
+	}, append([]string{"query", "datname", "status"}, omnipresentLabelsKeys...))
+	prometheus.MustRegister(queryStatusCounterVec)
+
+	monStateEntriesCounterVec = prometheus.NewCounterVec(prometheus.CounterOpts{
+		Name: "mon_state_map_entries_count",
+		Help: "Number of entries in the monitoring state map",
+	}, omnipresentLabelsKeys)
+	prometheus.MustRegister(monStateEntriesCounterVec)
+}
+
+func UpdateOracleDbPoolStats(dbStat sql.DBStats) {
+	if *disabledSelfMonitor {
+		return
+	}
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxOpenConnections"]).Set(float64(dbStat.MaxOpenConnections))
+	dbPoolStatsGaugeVec.With(labelsMapCache["OpenConnections"]).Set(float64(dbStat.OpenConnections))
+	dbPoolStatsGaugeVec.With(labelsMapCache["InUse"]).Set(float64(dbStat.InUse))
+	dbPoolStatsGaugeVec.With(labelsMapCache["Idle"]).Set(float64(dbStat.Idle))
+	dbPoolStatsGaugeVec.With(labelsMapCache["WaitCount"]).Set(float64(dbStat.WaitCount))
+	dbPoolStatsGaugeVec.With(labelsMapCache["WaitDuration"]).Set(float64(dbStat.WaitDuration / time.Second))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxIdleClosed"]).Set(float64(dbStat.MaxIdleClosed))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxIdleTimeClosed"]).Set(float64(dbStat.MaxIdleTimeClosed))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxLifetimeClosed"]).Set(float64(dbStat.MaxLifetimeClosed))
+}
+
+func UpdatePostgresDbPoolStats(dbStat pgxpool.Stat) {
+	if *disabledSelfMonitor {
+		return
+	}
+	dbPoolStatsGaugeVec.With(labelsMapCache["AcquireCount"]).Set(float64(dbStat.AcquireCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["AcquireDuration"]).Set(float64(dbStat.AcquireDuration() / time.Second))
+	dbPoolStatsGaugeVec.With(labelsMapCache["AcquiredConns"]).Set(float64(dbStat.AcquiredConns()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["CanceledAcquireCount"]).Set(float64(dbStat.CanceledAcquireCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["ConstructingConns"]).Set(float64(dbStat.ConstructingConns()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["EmptyAcquireCount"]).Set(float64(dbStat.EmptyAcquireCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["IdleConns"]).Set(float64(dbStat.IdleConns()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxConns"]).Set(float64(dbStat.MaxConns()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxIdleDestroyCount"]).Set(float64(dbStat.MaxIdleDestroyCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["MaxLifetimeDestroyCount"]).Set(float64(dbStat.MaxLifetimeDestroyCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["NewConnsCount"]).Set(float64(dbStat.NewConnsCount()))
+	dbPoolStatsGaugeVec.With(labelsMapCache["TotalConns"]).Set(float64(dbStat.TotalConns()))
+}
+
+func initLabelsMapCache() {
+	labelsMapCache = make(map[string]map[string]string)
+	for _, nameValue := range dbPoolStatsLabelValues {
+		labels := make(map[string]string)
+		for label, labelValue := range staticLabels {
+			labels[label] = labelValue
+		}
+		labels["name"] = nameValue
+		labelsMapCache[nameValue] = labels
+		log.Debugf("LabelsMapCache value generated for %v, value is %+v", nameValue, labelsMapCache[nameValue])
+	}
+}
+
+func QueryLatencyExecution(start time.Time, qName string, dbName string) {
+	if *disabledSelfMonitor {
+		return
+	}
+
+	elapsed := time.Since(start)
+	seconds := float64(elapsed) / float64(time.Second)
+	labels := make(map[string]string)
+	for label, labelValue := range staticLabels {
+		labels[label] = labelValue
+	}
+	labels["query"] = qName
+	labels["datname"] = dbName
+	queryDurationsHistogramVec.With(labels).Observe(seconds)
+}
+
+func IncQueryStatusCount(qName string, dbName string, status string) {
+	if *disabledSelfMonitor {
+		return
+	}
+
+	labels := make(map[string]string)
+	for label, labelValue := range staticLabels {
+		labels[label] = labelValue
+	}
+	labels["query"] = qName
+	labels["status"] = status
+	labels["datname"] = dbName
+	queryStatusCounterVec.With(labels).Inc()
+}
+
+func IncMonStateEntriesCount() {
+	if *disabledSelfMonitor {
+		return
+	}
+	monStateEntriesCounterVec.With(staticLabels).Inc()
+}
diff --git a/docker-query-exporter/utils/dynamicmetricrepo.go b/docker-query-exporter/utils/dynamicmetricrepo.go
new file mode 100644
index 0000000..ee3a213
--- /dev/null
+++ b/docker-query-exporter/utils/dynamicmetricrepo.go
@@ -0,0 +1,40 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"sync"
+)
+
+type DynamicMetricsRepository struct {
+	sync.RWMutex
+	m map[string]*SyncStringSet
+}
+
+func (repo *DynamicMetricsRepository) Initialize() {
+	repo.m = make(map[string]*SyncStringSet)
+}
+
+func (repo *DynamicMetricsRepository) Get(queryName string) *SyncStringSet {
+	repo.RLock()
+	defer repo.RUnlock()
+	return repo.m[queryName]
+}
+
+func (repo *DynamicMetricsRepository) Set(queryName string, val *SyncStringSet) {
+	repo.Lock()
+	defer repo.Unlock()
+	repo.m[queryName] = val
+}
diff --git a/docker-query-exporter/utils/env.go b/docker-query-exporter/utils/env.go
new file mode 100644
index 0000000..0e6331c
--- /dev/null
+++ b/docker-query-exporter/utils/env.go
@@ -0,0 +1,77 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	log "github.com/sirupsen/logrus"
+	"os"
+	"strconv"
+)
+
+func GetOctalUintEnvironmentVariable(name string, defaultValue uint32) uint32 {
+	valueStr := os.Getenv(name)
+	if valueStr == "" {
+		log.Infof("Environment variable %v is empty. Default value %o is used", name, defaultValue)
+		return defaultValue
+	}
+	result, err := strconv.ParseUint(valueStr, 8, 32)
+
+	if err != nil {
+		log.Errorf("Error trying to parse uint octal environment variable %v with value %v, default value %o is used instead. Error : %+v", name, valueStr, defaultValue, err)
+		return defaultValue
+	}
+
+	log.Infof("Environment variable %v with value %v parsed successfully as octal uint %o", name, valueStr, result)
+
+	return uint32(result)
+
+}
+
+func GetEnv(key string, defaultVal string) string {
+	if value, exists := os.LookupEnv(key); exists {
+		log.Infof("Values for %s is %s", key, value)
+		return value
+	}
+
+	return defaultVal
+}
+
+func GetEnvBool(key string, defaultVal bool) bool {
+	if value, exists := os.LookupEnv(key); exists {
+		result, err := strconv.ParseBool(value)
+		if err != nil {
+			log.Errorf("Cannot parse %s with value %s to bool, set %t ; err : %+v", key, value, defaultVal, err)
+			return defaultVal
+		}
+		log.Infof("Env variable %s has value %t", key, result)
+		return result
+	}
+
+	return defaultVal
+}
+
+func GetEnvInt(key string, defaultVal int) int {
+	if value, exists := os.LookupEnv(key); exists {
+		result, err := strconv.ParseInt(value, 10, 32)
+		if err != nil {
+			log.Errorf("Cannot parse %s with value %s to int, set %d ; err : %+v", key, value, defaultVal, err)
+			return defaultVal
+		}
+		log.Infof("Env variable %s has value %d", key, result)
+		return int(result)
+	}
+
+	return defaultVal
+}
diff --git a/docker-query-exporter/utils/gaugevecrepo.go b/docker-query-exporter/utils/gaugevecrepo.go
new file mode 100644
index 0000000..1f1dc97
--- /dev/null
+++ b/docker-query-exporter/utils/gaugevecrepo.go
@@ -0,0 +1,41 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"github.com/prometheus/client_golang/prometheus"
+	"sync"
+)
+
+type GaugeVecRepository struct {
+	sync.RWMutex
+	m map[string]*prometheus.GaugeVec
+}
+
+func (repo *GaugeVecRepository) Initialize() {
+	repo.m = make(map[string]*prometheus.GaugeVec)
+}
+
+func (repo *GaugeVecRepository) Get(key string) *prometheus.GaugeVec {
+	repo.RLock()
+	defer repo.RUnlock()
+	return repo.m[key]
+}
+
+func (repo *GaugeVecRepository) Set(key string, val *prometheus.GaugeVec) {
+	repo.Lock()
+	defer repo.Unlock()
+	repo.m[key] = val
+}
diff --git a/docker-query-exporter/utils/syncstringset.go b/docker-query-exporter/utils/syncstringset.go
new file mode 100644
index 0000000..0db79cc
--- /dev/null
+++ b/docker-query-exporter/utils/syncstringset.go
@@ -0,0 +1,56 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"sync"
+)
+
+type SyncStringSet struct {
+	sync.RWMutex
+	m map[string]bool
+}
+
+func (set *SyncStringSet) Initialize() {
+	set.m = make(map[string]bool)
+}
+
+func (set *SyncStringSet) Get(key string) bool {
+	set.RLock()
+	defer set.RUnlock()
+	return set.m[key]
+}
+
+func (set *SyncStringSet) Add(key string) {
+	set.Lock()
+	defer set.Unlock()
+	set.m[key] = true
+}
+
+func (set *SyncStringSet) Remove(key string) {
+	set.Lock()
+	defer set.Unlock()
+	delete(set.m, key)
+}
+
+func (set *SyncStringSet) GetAllEntries() []string {
+	set.RLock()
+	defer set.RUnlock()
+	result := make([]string, 0, len(set.m))
+	for key := range set.m {
+		result = append(result, key)
+	}
+	return result
+}
diff --git a/docker-query-exporter/utils/utils.go b/docker-query-exporter/utils/utils.go
new file mode 100644
index 0000000..f2784d3
--- /dev/null
+++ b/docker-query-exporter/utils/utils.go
@@ -0,0 +1,174 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"bytes"
+	"flag"
+	"fmt"
+	"math/rand"
+	"sort"
+	"strings"
+
+	"github.com/robfig/cron/v3"
+	log "github.com/sirupsen/logrus"
+)
+
+var (
+	croniterPrecision = flag.String("croniter-precision", "second", "Croniter precision, possible values: second, minute")
+	autodiscovery     = flag.String("autodiscovery", GetEnv("AUTODISCOVERY", "false"), "If autodiscovery enabled")
+)
+
+func MapToString(m map[string]string) string {
+	keys := make([]string, len(m))
+	i := 0
+	for k := range m {
+		keys[i] = k
+		i++
+	}
+	sort.Strings(keys)
+
+	b := new(bytes.Buffer)
+
+	for _, key := range keys {
+		fmt.Fprintf(b, "%s=\"%s\",", key, m[key])
+	}
+
+	return b.String()
+}
+
+func GetKeys(m map[string]string) []string {
+	result := make([]string, len(m))
+	i := 0
+	for key := range m {
+		result[i] = key
+		i++
+	}
+	return result
+}
+
+func GetOrderedMapValues(m map[string]string, keys []string) []string {
+	values := make([]string, len(keys))
+	for i, key := range keys {
+		values[i] = m[key]
+	}
+	return values
+}
+
+const letters = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
+
+func GenerateRandomString(n int) string {
+	b := make([]byte, n)
+	for i := range b {
+		b[i] = letters[rand.Int63()%int64(len(letters))]
+	}
+	return string(b)
+}
+
+func GetCronParser() cron.Parser {
+	switch *croniterPrecision {
+	case "second":
+		log.Info("Parser with second precision created")
+		return cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)
+	case "minute":
+		log.Info("Parser with minute precision created")
+		return cron.NewParser(cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)
+	default:
+		log.Errorf("croniter-precision property has invalid value: %v . Default second precision will be applied for Parser", *croniterPrecision)
+		return cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)
+	}
+}
+
+func GetCron() *cron.Cron {
+	switch *croniterPrecision {
+	case "second":
+		log.Info("Cron with second precision created")
+		return cron.New(cron.WithParser(cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)))
+	case "minute":
+		log.Info("Cron with minute precision created")
+		return cron.New()
+	default:
+		log.Errorf("croniter-precision property has invalid value: %v . Default second precision will be applied for Cron", *croniterPrecision)
+		return cron.New(cron.WithParser(cron.NewParser(cron.SecondOptional | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor)))
+	}
+}
+
+func IsAutodiscoveryEnabled() bool {
+	return strings.ToLower(*autodiscovery) == "true"
+}
+
+func LabelsCartesian(expectedLabels map[string][]string) []map[string]string {
+	sortedLabelNames := getSortedLabelNames(expectedLabels)
+	sortedLabelSizes := getSortedLabelSizes(expectedLabels, sortedLabelNames)
+	indexes := make([]int64, len(sortedLabelSizes))
+	result := make([]map[string]string, 0, multiplyArrayItems(sortedLabelSizes))
+	for {
+		newLabels := make(map[string]string)
+
+		for i, index := range indexes {
+			labelName := sortedLabelNames[i]
+			newLabels[labelName] = expectedLabels[labelName][index]
+		}
+		result = append(result, newLabels)
+		if incrementIndexes(indexes, sortedLabelSizes) {
+			return result
+		}
+	}
+}
+
+func getSortedLabelNames(expectedLabels map[string][]string) []string {
+	labelNames := make([]string, len(expectedLabels))
+	i := 0
+	for k := range expectedLabels {
+		labelNames[i] = k
+		i++
+	}
+	sort.Strings(labelNames)
+	return labelNames
+}
+
+func getSortedLabelSizes(expectedLabels map[string][]string, sortedLabelNames []string) []int64 {
+	labelSizes := make([]int64, len(expectedLabels))
+	i := 0
+	for _, labelName := range sortedLabelNames {
+		labelSizes[i] = int64(len(expectedLabels[labelName]))
+		i++
+	}
+	return labelSizes
+}
+
+func multiplyArrayItems(arr []int64) int64 {
+	var result int64 = 1
+	for _, a := range arr {
+		result = result * a
+	}
+	return result
+}
+
+func incrementIndexes(indexes []int64, sortedLabelSizes []int64) bool {
+	i := 0
+	labelsCount := len(sortedLabelSizes)
+	for {
+		indexes[i]++
+		if indexes[i] < sortedLabelSizes[i] {
+			return false
+		} else if i == labelsCount-1 {
+			return true
+		} else {
+			indexes[i] = 0
+			i++
+		}
+	}
+}
diff --git a/docker-query-exporter/version.go b/docker-query-exporter/version.go
new file mode 100644
index 0000000..3285028
--- /dev/null
+++ b/docker-query-exporter/version.go
@@ -0,0 +1,34 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"fmt"
+	"github.com/godror/godror"
+	"runtime"
+)
+
+var (
+	Version   string
+	BuildDate string
+	Branch    string
+	Revision  string
+	GoVersion = runtime.Version()
+	Platform  = runtime.GOOS + "-" + runtime.GOARCH
+)
+
+func versionString() string {
+	return fmt.Sprintf("query-exporter version: %v (build date: %v, branch: %v, revision: %v, go version: %v, platform: %v, godror: %v)", Version, BuildDate, Branch, Revision, GoVersion, Platform, godror.Version)
+}
diff --git a/docker-replication-controller/.dockerignore b/docker-replication-controller/.dockerignore
new file mode 100644
index 0000000..258238f
--- /dev/null
+++ b/docker-replication-controller/.dockerignore
@@ -0,0 +1,4 @@
+**
+
+!build/_output/bin/pgskipper-replication-controller
+!build/bin
\ No newline at end of file
diff --git a/docker-replication-controller/.gitignore b/docker-replication-controller/.gitignore
new file mode 100644
index 0000000..63e2c39
--- /dev/null
+++ b/docker-replication-controller/.gitignore
@@ -0,0 +1,7 @@
+.idea/
+.history
+target/
+
+# Temporary Build Files
+build/_output
+build/_test
\ No newline at end of file
diff --git a/docker-replication-controller/CODE-OF-CONDUCT.md b/docker-replication-controller/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-replication-controller/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-replication-controller/CONTRIBUTING.md b/docker-replication-controller/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-replication-controller/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-replication-controller/LICENSE b/docker-replication-controller/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-replication-controller/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-replication-controller/Makefile b/docker-replication-controller/Makefile
new file mode 100644
index 0000000..c58ce6c
--- /dev/null
+++ b/docker-replication-controller/Makefile
@@ -0,0 +1,43 @@
+DOCKER_FILE := build/Dockerfile
+NAMESPACE := 
+
+ifndef DOCKER_NAMES
+override DOCKER_NAMES = "${IMAGE_NAME}"
+endif
+
+sandbox-build: deps compile docker-build
+
+all: sandbox-build docker-push
+
+local: fmt deps compile docker-build docker-push
+
+deps:
+	go mod tidy
+	GO111MODULE=on
+
+update:
+	go get -u ./...
+
+fmt:
+	gofmt -l -s -w .
+
+compile:
+	CGO_ENABLED=0 go build -o ./build/_output/bin/pgskipper-replication-controller \
+				-gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./cmd/pgskipper-replication-controller
+
+
+docker-build:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker build --file="${DOCKER_FILE}" --pull -t $(docker_tag) ./;)
+
+docker-push:
+	$(foreach docker_tag,$(DOCKER_NAMES),docker push $(docker_tag);)
+
+clean:
+	rm -rf build/_output
+
+test:
+	go test -v ./...
+
+replace-image: local
+	$(foreach docker_tag,$(DOCKER_NAMES),kubectl patch deployment pgskipper-replication-controller -n $(NAMESPACE) --type "json" -p '[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":'$(docker_tag)'},{"op":"replace","path":"/spec/template/spec/containers/0/imagePullPolicy","value":"Always"}, {"op":"replace","path":"/spec/replicas","value":0}]';)
+	$(foreach docker_tag,$(DOCKER_NAMES),kubectl patch deployment pgskipper-replication-controller -n $(NAMESPACE) --type "json" -p '[{"op":"replace","path":"/spec/replicas","value":1}]';)
\ No newline at end of file
diff --git a/docker-replication-controller/README.md b/docker-replication-controller/README.md
new file mode 100644
index 0000000..bc7e17b
--- /dev/null
+++ b/docker-replication-controller/README.md
@@ -0,0 +1,3 @@
+# pgskipper-replication-controller
+
+Component provides REST API for Postgres Publication management.
\ No newline at end of file
diff --git a/docker-replication-controller/SECURITY.md b/docker-replication-controller/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-replication-controller/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-replication-controller/build/Dockerfile b/docker-replication-controller/build/Dockerfile
new file mode 100644
index 0000000..6dab416
--- /dev/null
+++ b/docker-replication-controller/build/Dockerfile
@@ -0,0 +1,32 @@
+FROM --platform=$BUILDPLATFORM golang:1.25.3-alpine3.22 AS builder
+
+ENV GOSUMDB=off
+
+WORKDIR /workspace
+
+# Copy the Go Modules manifests
+COPY go.mod go.mod
+COPY go.sum go.sum
+
+RUN go mod download
+
+COPY pkg/ pkg/
+COPY cmd/ cmd/
+
+RUN go mod tidy
+
+# Build
+ARG TARGETOS TARGETARCH
+RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH CGO_ENABLED=0 go build -o ./build/_output/bin/pgskipper-replication-controller \
+				-gcflags all=-trimpath=${GOPATH} -asmflags all=-trimpath=${GOPATH} ./cmd/pgskipper-replication-controller
+
+FROM alpine:3.20
+
+ENV CONTROLLER=/usr/local/bin/pgskipper-replication-controller \
+    USER_UID=1001
+
+COPY --from=builder /workspace/build/_output/bin/pgskipper-replication-controller ${CONTROLLER}
+
+USER ${USER_UID}
+
+CMD ["sh", "-c", "${CONTROLLER}"]
diff --git a/docker-replication-controller/cmd/pgskipper-replication-controller/main.go b/docker-replication-controller/cmd/pgskipper-replication-controller/main.go
new file mode 100644
index 0000000..0a10ad5
--- /dev/null
+++ b/docker-replication-controller/cmd/pgskipper-replication-controller/main.go
@@ -0,0 +1,139 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package main
+
+import (
+	"flag"
+	"fmt"
+	"runtime/debug"
+	"strconv"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/postgres"
+	publication "github.com/Netcracker/pgskipper-replication-controller/pkg/publicaion"
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/users"
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"github.com/gofiber/fiber/v2"
+	"github.com/gofiber/fiber/v2/middleware/basicauth"
+	"github.com/gofiber/fiber/v2/middleware/recover"
+	fiberUtils "github.com/gofiber/fiber/v2/utils"
+	"go.uber.org/zap"
+)
+
+const (
+	pgDB            = "postgres"
+	publicationPath = "/publications"
+	usersPath       = "/users"
+
+	httpsPort = 8443
+)
+
+var (
+	pgHost = flag.String("pg_host", utils.GetEnv("POSTGRES_HOST", "127.0.0.1"), "Host of PostgreSQL cluster, env: POSTGRES_HOST")
+	pgPort = flag.Int("pg_port", utils.GetEnvInt("POSTGRES_PORT", 5432), "Port of PostgreSQL cluster, env: POSTGRES_PORT")
+	pgUser = flag.String("pg_user", utils.GetEnv("POSTGRES_ADMIN_USER", "postgres"), "Username of controller user in PostgreSQL, env: POSTGRES_ADMIN_USER")
+	pgPass = flag.String("pg_pass", utils.GetEnv("POSTGRES_ADMIN_PASSWORD", ""), "Password of controller user in PostgreSQL, env: POSTGRES_ADMIN_PASSWORD")
+	pgSsl  = flag.String("pg_ssl", utils.GetEnv("PG_SSL", "off"), "Enable ssl connection to postgreSQL, env: PG_SSL")
+
+	servePort = flag.Int("serve_port", 8080, "Port to serve requests incoming to controller")
+	serveUser = flag.String(
+		"server_user",
+		utils.GetEnv("API_USER", "logical-repl-user"),
+		"Username to authorize incoming requests, env: API_USER",
+	)
+	servePass = flag.String(
+		"server_pass",
+		utils.GetEnv("API_PASSWORD", "logical-repl-password"),
+		"Password to authorize incoming requests, env: API_PASSWORD",
+	)
+
+	log      = utils.GetLogger()
+	pgClient *postgres.Client
+)
+
+func main() {
+	flag.Parse()
+	log.Debug("Controller started")
+
+	app := fiber.New(fiber.Config{Network: "tcp"})
+
+	app.Get("/health", HealthHandler)
+	setAuth(app)
+
+	setRecovery(app)
+
+	pgClient = postgres.NewClient(*pgHost, *pgPort, *pgUser, *pgPass, pgDB, *pgSsl)
+
+	pubController := publication.NewPublicationController(pgClient)
+	pubGroup := app.Group(publicationPath, func(c *fiber.Ctx) error {
+		//Common API Handler
+		return c.Next()
+	})
+	pubGroup.Get("/:database/:publication", pubController.PublicationGetHandler)
+	pubGroup.Post("/create", pubController.PublicationCreateHandler)
+	pubGroup.Post("/alter/add", pubController.PublicationAlterAddHandler)
+	pubGroup.Post("/alter/set", pubController.PublicationAlterSetHandler)
+	pubGroup.Delete("/drop", pubController.PublicationDropHandler)
+
+	userController := users.NewUsersController(pgClient)
+	usersGroup := app.Group(usersPath, func(c *fiber.Ctx) error {
+		//Common API Handler
+		return c.Next()
+	})
+	usersGroup.Post("/grant", userController.GrantUserHandler)
+
+	log.Fatal("Controller has been stopped", zap.Error(RunFiberServer(app)))
+}
+
+func setRecovery(app *fiber.App) {
+	recoverConfig := recover.ConfigDefault
+	recoverConfig.EnableStackTrace = true
+	recoverConfig.StackTraceHandler = func(c *fiber.Ctx, e interface{}) {
+		log.Error(fmt.Sprintf("Panic: %+v\nStacktrace:\n%s", e, string(debug.Stack())))
+	}
+	app.Use(recover.New(recoverConfig))
+	app.Use(func(c *fiber.Ctx) error {
+		// Setting defaults for existed handlers
+		c.Request().Header.SetContentType(fiberUtils.GetMIME("json"))
+		log.Debug(fmt.Sprintf("%s %s", c.Request().Header.Method(), c.Path()))
+		return c.Next()
+	})
+}
+
+func setAuth(app *fiber.App) {
+	app.Use(basicauth.New(basicauth.Config{
+		Users: map[string]string{
+			*serveUser: *servePass,
+		},
+	}))
+}
+
+func HealthHandler(c *fiber.Ctx) error {
+	pgClient.RequestHealth()
+	return nil
+}
+
+func RunFiberServer(app *fiber.App) error {
+	if utils.IsHttpsEnabled() {
+		go runServerTLS(app)
+	}
+	return app.Listen(":" + strconv.Itoa(*servePort))
+}
+
+func runServerTLS(app *fiber.App) {
+	err := app.ListenTLS(":"+strconv.Itoa(httpsPort), "/certs/tls.crt", "/certs/tls.key")
+	if err != nil {
+		log.Fatal("error during server execution")
+	}
+}
diff --git a/docker-replication-controller/go.mod b/docker-replication-controller/go.mod
new file mode 100644
index 0000000..353f4bb
--- /dev/null
+++ b/docker-replication-controller/go.mod
@@ -0,0 +1,36 @@
+module github.com/Netcracker/pgskipper-replication-controller
+
+go 1.25.3
+
+require (
+	github.com/gofiber/fiber/v2 v2.52.9
+	github.com/google/uuid v1.6.0
+	github.com/jackc/pgx/v4 v4.18.3
+)
+
+require (
+	github.com/andybalholm/brotli v1.1.0 // indirect
+	github.com/klauspost/compress v1.17.9 // indirect
+	github.com/mattn/go-colorable v0.1.13 // indirect
+	github.com/mattn/go-isatty v0.0.20 // indirect
+	github.com/mattn/go-runewidth v0.0.16 // indirect
+	github.com/rivo/uniseg v0.4.7 // indirect
+	github.com/valyala/bytebufferpool v1.0.0 // indirect
+	github.com/valyala/fasthttp v1.55.0 // indirect
+	github.com/valyala/tcplisten v1.0.0 // indirect
+	go.uber.org/multierr v1.11.0 // indirect
+	golang.org/x/sys v0.37.0 // indirect
+)
+
+require (
+	github.com/jackc/chunkreader/v2 v2.0.1 // indirect
+	github.com/jackc/pgconn v1.14.3
+	github.com/jackc/pgio v1.0.0 // indirect
+	github.com/jackc/pgpassfile v1.0.0 // indirect
+	github.com/jackc/pgproto3/v2 v2.3.3 // indirect
+	github.com/jackc/pgservicefile v0.0.0-20231201235250-de7065d80cb9 // indirect
+	github.com/jackc/pgtype v1.14.1 // indirect
+	go.uber.org/zap v1.27.0
+	golang.org/x/crypto v0.43.0 // indirect
+	golang.org/x/text v0.30.0 // indirect
+)
diff --git a/docker-replication-controller/go.sum b/docker-replication-controller/go.sum
new file mode 100644
index 0000000..a8053da
--- /dev/null
+++ b/docker-replication-controller/go.sum
@@ -0,0 +1,213 @@
+github.com/BurntSushi/toml v0.3.1/go.mod h1:xHWCNGjB5oqiDr8zfno3MHue2Ht5sIBksp03qcyfWMU=
+github.com/Masterminds/semver/v3 v3.1.1/go.mod h1:VPu/7SZ7ePZ3QOrcuXROw5FAcLl4a0cBrbBpGY/8hQs=
+github.com/andybalholm/brotli v1.1.0 h1:eLKJA0d02Lf0mVpIDgYnqXcUn0GqVmEFny3VuID1U3M=
+github.com/andybalholm/brotli v1.1.0/go.mod h1:sms7XGricyQI9K10gOSf56VKKWS4oLer58Q+mhRPtnY=
+github.com/cockroachdb/apd v1.1.0 h1:3LFP3629v+1aKXU5Q37mxmRxX/pIu1nijXydLShEq5I=
+github.com/cockroachdb/apd v1.1.0/go.mod h1:8Sl8LxpKi29FqWXR16WEFZRNSz3SoPzUzeMeY4+DwBQ=
+github.com/coreos/go-systemd v0.0.0-20190321100706-95778dfbb74e/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
+github.com/coreos/go-systemd v0.0.0-20190719114852-fd7a80b32e1f/go.mod h1:F5haX7vjVVG0kc13fIWeqUViNPyEJxv/OmvnBo0Yme4=
+github.com/creack/pty v1.1.7/go.mod h1:lj5s0c3V2DBrqTV7llrYr5NG6My20zk30Fl46Y7DoTY=
+github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
+github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
+github.com/go-kit/log v0.1.0/go.mod h1:zbhenjAZHb184qTLMA9ZjW7ThYL0H2mk7Q6pNt4vbaY=
+github.com/go-logfmt/logfmt v0.5.0/go.mod h1:wCYkCAKZfumFQihp8CzCvQ3paCTfi41vtzG1KdI/P7A=
+github.com/go-stack/stack v1.8.0/go.mod h1:v0f6uXyyMGvRgIKkXu+yp6POWl0qKG85gN/melR3HDY=
+github.com/gofiber/fiber/v2 v2.52.9 h1:YjKl5DOiyP3j0mO61u3NTmK7or8GzzWzCFzkboyP5cw=
+github.com/gofiber/fiber/v2 v2.52.9/go.mod h1:YEcBbO/FB+5M1IZNBP9FO3J9281zgPAreiI1oqg8nDw=
+github.com/gofrs/uuid v4.0.0+incompatible h1:1SD/1F5pU8p29ybwgQSwpQk+mwdRrXCYuPhW6m+TnJw=
+github.com/gofrs/uuid v4.0.0+incompatible/go.mod h1:b2aQJv3Z4Fp6yNu3cdSllBxTCLRxnplIgP/c0N/04lM=
+github.com/google/renameio v0.1.0/go.mod h1:KWCgfxg9yswjAJkECMjeO8J8rahYeXnNhOm40UhjYkI=
+github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
+github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/jackc/chunkreader v1.0.0/go.mod h1:RT6O25fNZIuasFJRyZ4R/Y2BbhasbmZXF9QQ7T3kePo=
+github.com/jackc/chunkreader/v2 v2.0.0/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=
+github.com/jackc/chunkreader/v2 v2.0.1 h1:i+RDz65UE+mmpjTfyz0MoVTnzeYxroil2G82ki7MGG8=
+github.com/jackc/chunkreader/v2 v2.0.1/go.mod h1:odVSm741yZoC3dpHEUXIqA9tQRhFrgOHwnPIn9lDKlk=
+github.com/jackc/pgconn v0.0.0-20190420214824-7e0022ef6ba3/go.mod h1:jkELnwuX+w9qN5YIfX0fl88Ehu4XC3keFuOJJk9pcnA=
+github.com/jackc/pgconn v0.0.0-20190824142844-760dd75542eb/go.mod h1:lLjNuW/+OfW9/pnVKPazfWOgNfH2aPem8YQ7ilXGvJE=
+github.com/jackc/pgconn v0.0.0-20190831204454-2fabfa3c18b7/go.mod h1:ZJKsE/KZfsUgOEh9hBm+xYTstcNHg7UPMVJqRfQxq4s=
+github.com/jackc/pgconn v1.8.0/go.mod h1:1C2Pb36bGIP9QHGBYCjnyhqu7Rv3sGshaQUvmfGIB/o=
+github.com/jackc/pgconn v1.9.0/go.mod h1:YctiPyvzfU11JFxoXokUOOKQXQmDMoJL9vJzHH8/2JY=
+github.com/jackc/pgconn v1.9.1-0.20210724152538-d89c8390a530/go.mod h1:4z2w8XhRbP1hYxkpTuBjTS3ne3J48K83+u0zoyvg2pI=
+github.com/jackc/pgconn v1.14.3 h1:bVoTr12EGANZz66nZPkMInAV/KHD2TxH9npjXXgiB3w=
+github.com/jackc/pgconn v1.14.3/go.mod h1:RZbme4uasqzybK2RK5c65VsHxoyaml09lx3tXOcO/VM=
+github.com/jackc/pgio v1.0.0 h1:g12B9UwVnzGhueNavwioyEEpAmqMe1E/BN9ES+8ovkE=
+github.com/jackc/pgio v1.0.0/go.mod h1:oP+2QK2wFfUWgr+gxjoBH9KGBb31Eio69xUb0w5bYf8=
+github.com/jackc/pgmock v0.0.0-20190831213851-13a1b77aafa2/go.mod h1:fGZlG77KXmcq05nJLRkk0+p82V8B8Dw8KN2/V9c/OAE=
+github.com/jackc/pgmock v0.0.0-20201204152224-4fe30f7445fd/go.mod h1:hrBW0Enj2AZTNpt/7Y5rr2xe/9Mn757Wtb2xeBzPv2c=
+github.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65 h1:DadwsjnMwFjfWc9y5Wi/+Zz7xoE5ALHsRQlOctkOiHc=
+github.com/jackc/pgmock v0.0.0-20210724152146-4ad1a8207f65/go.mod h1:5R2h2EEX+qri8jOWMbJCtaPWkrrNc7OHwsp2TCqp7ak=
+github.com/jackc/pgpassfile v1.0.0 h1:/6Hmqy13Ss2zCq62VdNG8tM1wchn8zjSGOBJ6icpsIM=
+github.com/jackc/pgpassfile v1.0.0/go.mod h1:CEx0iS5ambNFdcRtxPj5JhEz+xB6uRky5eyVu/W2HEg=
+github.com/jackc/pgproto3 v1.1.0/go.mod h1:eR5FA3leWg7p9aeAqi37XOTgTIbkABlvcPB3E5rlc78=
+github.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190420180111-c116219b62db/go.mod h1:bhq50y+xrl9n5mRYyCBFKkpRVTLYJVWeCc+mEAI3yXA=
+github.com/jackc/pgproto3/v2 v2.0.0-alpha1.0.20190609003834-432c2951c711/go.mod h1:uH0AWtUmuShn0bcesswc4aBTWGvw0cAxIJp+6OB//Wg=
+github.com/jackc/pgproto3/v2 v2.0.0-rc3/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=
+github.com/jackc/pgproto3/v2 v2.0.0-rc3.0.20190831210041-4c03ce451f29/go.mod h1:ryONWYqW6dqSg1Lw6vXNMXoBJhpzvWKnT95C46ckYeM=
+github.com/jackc/pgproto3/v2 v2.0.6/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgproto3/v2 v2.1.1/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgproto3/v2 v2.3.3 h1:1HLSx5H+tXR9pW3in3zaztoEwQYRC9SQaYUHjTSUOag=
+github.com/jackc/pgproto3/v2 v2.3.3/go.mod h1:WfJCnwN3HIg9Ish/j3sgWXnAfK8A9Y0bwXYU5xKaEdA=
+github.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b/go.mod h1:vsD4gTJCa9TptPL8sPkXrLZ+hDuNrZCnj29CQpr4X1E=
+github.com/jackc/pgservicefile v0.0.0-20231201235250-de7065d80cb9 h1:L0QtFUgDarD7Fpv9jeVMgy/+Ec0mtnmYuImjTz6dtDA=
+github.com/jackc/pgservicefile v0.0.0-20231201235250-de7065d80cb9/go.mod h1:5TJZWKEWniPve33vlWYSoGYefn3gLQRzjfDlhSJ9ZKM=
+github.com/jackc/pgtype v0.0.0-20190421001408-4ed0de4755e0/go.mod h1:hdSHsc1V01CGwFsrv11mJRHWJ6aifDLfdV3aVjFF0zg=
+github.com/jackc/pgtype v0.0.0-20190824184912-ab885b375b90/go.mod h1:KcahbBH1nCMSo2DXpzsoWOAfFkdEtEJpPbVLq8eE+mc=
+github.com/jackc/pgtype v0.0.0-20190828014616-a8802b16cc59/go.mod h1:MWlu30kVJrUS8lot6TQqcg7mtthZ9T0EoIBFiJcmcyw=
+github.com/jackc/pgtype v1.8.1-0.20210724151600-32e20a603178/go.mod h1:C516IlIV9NKqfsMCXTdChteoXmwgUceqaLfjg2e3NlM=
+github.com/jackc/pgtype v1.14.1 h1:LyDar7M2K0tShCWqzJ/ctzF1QC3Wzc9c8a6cHE0PFdc=
+github.com/jackc/pgtype v1.14.1/go.mod h1:LUMuVrfsFfdKGLw+AFFVv6KtHOFMwRgDDzBt76IqCA4=
+github.com/jackc/pgx/v4 v4.0.0-20190420224344-cc3461e65d96/go.mod h1:mdxmSJJuR08CZQyj1PVQBHy9XOp5p8/SHH6a0psbY9Y=
+github.com/jackc/pgx/v4 v4.0.0-20190421002000-1b8f0016e912/go.mod h1:no/Y67Jkk/9WuGR0JG/JseM9irFbnEPbuWV2EELPNuM=
+github.com/jackc/pgx/v4 v4.0.0-pre1.0.20190824185557-6972a5742186/go.mod h1:X+GQnOEnf1dqHGpw7JmHqHc1NxDoalibchSk9/RWuDc=
+github.com/jackc/pgx/v4 v4.12.1-0.20210724153913-640aa07df17c/go.mod h1:1QD0+tgSXP7iUjYm9C1NxKhny7lq6ee99u/z+IHFcgs=
+github.com/jackc/pgx/v4 v4.18.3 h1:dE2/TrEsGX3RBprb3qryqSV9Y60iZN1C6i8IrmW9/BA=
+github.com/jackc/pgx/v4 v4.18.3/go.mod h1:Ey4Oru5tH5sB6tV7hDmfWFahwF15Eb7DNXlRKx2CkVw=
+github.com/jackc/puddle v0.0.0-20190413234325-e4ced69a3a2b/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jackc/puddle v0.0.0-20190608224051-11cab39313c9/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/jackc/puddle v1.1.3/go.mod h1:m4B5Dj62Y0fbyuIc15OsIqK0+JU8nkqQjsgx7dvjSWk=
+github.com/kisielk/gotool v1.0.0/go.mod h1:XhKaO+MFFWcvkIS/tQcRk01m1F5IRFswLeQ+oQHNcck=
+github.com/klauspost/compress v1.17.9 h1:6KIumPrER1LHsvBVuDa0r5xaG0Es51mhhB9BQB2qeMA=
+github.com/klauspost/compress v1.17.9/go.mod h1:Di0epgTjJY877eYKx5yC51cX2A2Vl2ibi7bDH9ttBbw=
+github.com/konsorten/go-windows-terminal-sequences v1.0.1/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
+github.com/konsorten/go-windows-terminal-sequences v1.0.2/go.mod h1:T0+1ngSBFLxvqU3pZ+m/2kptfBszLMUkC4ZK/EgS/cQ=
+github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
+github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
+github.com/kr/pty v1.1.8/go.mod h1:O1sed60cT9XZ5uDucP5qwvh+TE3NnUj51EiZO/lmSfw=
+github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
+github.com/lib/pq v1.0.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.1.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.2.0/go.mod h1:5WUZQaWbwv1U+lTReE5YruASi9Al49XbQIvNi/34Woo=
+github.com/lib/pq v1.10.2 h1:AqzbZs4ZoCBp+GtejcpCpcxM3zlSMx29dXbUSeVtJb8=
+github.com/lib/pq v1.10.2/go.mod h1:AlVN5x4E4T544tWzH6hKfbfQvm3HdbOxrmggDNAPY9o=
+github.com/mattn/go-colorable v0.1.1/go.mod h1:FuOcm+DKB9mbwrcAfNl7/TZVBZ6rcnceauSikq3lYCQ=
+github.com/mattn/go-colorable v0.1.6/go.mod h1:u6P/XSegPjTcexA+o6vUJrdnUu04hMope9wVRipJSqc=
+github.com/mattn/go-colorable v0.1.13 h1:fFA4WZxdEF4tXPZVKMLwD8oUnCTTo08duU7wxecdEvA=
+github.com/mattn/go-colorable v0.1.13/go.mod h1:7S9/ev0klgBDR4GtXTXX8a3vIGJpMovkB8vQcUbaXHg=
+github.com/mattn/go-isatty v0.0.5/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
+github.com/mattn/go-isatty v0.0.7/go.mod h1:Iq45c/XA43vh69/j3iqttzPXn0bhXyGjM0Hdxcsrc5s=
+github.com/mattn/go-isatty v0.0.12/go.mod h1:cbi8OIDigv2wuxKPP5vlRcQ1OAZbq2CE4Kysco4FUpU=
+github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=
+github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
+github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
+github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
+github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
+github.com/pkg/errors v0.8.1 h1:iURUrRGxPUNPdy5/HRSm+Yj6okJ6UtLINN0Q9M4+h3I=
+github.com/pkg/errors v0.8.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
+github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
+github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
+github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
+github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
+github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
+github.com/rogpeppe/go-internal v1.3.0/go.mod h1:M8bDsm7K2OlrFYOpmOWEs/qY81heoFRclV5y23lUDJ4=
+github.com/rs/xid v1.2.1/go.mod h1:+uKXf+4Djp6Md1KODXJxgGQPKngRmWyn10oCKFzNHOQ=
+github.com/rs/zerolog v1.13.0/go.mod h1:YbFCdg8HfsridGWAh22vktObvhZbQsZXe4/zB0OKkWU=
+github.com/rs/zerolog v1.15.0/go.mod h1:xYTKnLHcpfU2225ny5qZjxnj9NvkumZYjJHlAThCjNc=
+github.com/satori/go.uuid v1.2.0/go.mod h1:dA0hQrYB0VpLJoorglMZABFdXlWrHn1NEOzdhQKdks0=
+github.com/shopspring/decimal v0.0.0-20180709203117-cd690d0c9e24/go.mod h1:M+9NzErvs504Cn4c5DxATwIqPbtswREoFCre64PpcG4=
+github.com/shopspring/decimal v1.2.0 h1:abSATXmQEYyShuxI4/vyW3tV1MrKAJzCZ/0zLUXYbsQ=
+github.com/shopspring/decimal v1.2.0/go.mod h1:DKyhrW/HYNuLGql+MJL6WCR6knT2jwCFRcu2hWCYk4o=
+github.com/sirupsen/logrus v1.4.1/go.mod h1:ni0Sbl8bgC9z8RoU9G6nDWqqs/fq4eDPysMBDgk/93Q=
+github.com/sirupsen/logrus v1.4.2/go.mod h1:tLMulIdttU9McNUspp0xgXVQah82FyeX6MwdIuYE2rE=
+github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/objx v0.1.1/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
+github.com/stretchr/objx v0.2.0/go.mod h1:qt09Ya8vawLte6SNmTgCsAVtYtaKzEcn8ATUoHMkEqE=
+github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=
+github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
+github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=
+github.com/stretchr/testify v1.5.1/go.mod h1:5W2xD1RspED5o8YsWQXVCued0rvSQ+mT+I5cxcmMvtA=
+github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
+github.com/stretchr/testify v1.8.1 h1:w7B6lhMri9wdJUVmEZPGGhZzrYTPvgJArz7wNPgYKsk=
+github.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=
+github.com/valyala/bytebufferpool v1.0.0 h1:GqA5TC/0021Y/b9FG4Oi9Mr3q7XYx6KllzawFIhcdPw=
+github.com/valyala/bytebufferpool v1.0.0/go.mod h1:6bBcMArwyJ5K/AmCkWv1jt77kVWyCJ6HpOuEn7z0Csc=
+github.com/valyala/fasthttp v1.55.0 h1:Zkefzgt6a7+bVKHnu/YaYSOPfNYNisSVBo/unVCf8k8=
+github.com/valyala/fasthttp v1.55.0/go.mod h1:NkY9JtkrpPKmgwV3HTaS2HWaJss9RSIsRVfcxxoHiOM=
+github.com/valyala/tcplisten v1.0.0 h1:rBHj/Xf+E1tRGZyWIWwJDiRY0zc1Js+CV5DqwacVSA8=
+github.com/valyala/tcplisten v1.0.0/go.mod h1:T0xQ8SeCZGxckz9qRXTfG43PvQ/mcWh7FwZEA7Ioqkc=
+github.com/zenazn/goji v0.9.0/go.mod h1:7S9M489iMyHBNxwZnk9/EHS098H4/F6TATF2mIxtB1Q=
+go.uber.org/atomic v1.3.2/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
+go.uber.org/atomic v1.4.0/go.mod h1:gD2HeocX3+yG+ygLZcrzQJaqmWj9AIm7n08wl/qW/PE=
+go.uber.org/atomic v1.5.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=
+go.uber.org/atomic v1.6.0/go.mod h1:sABNBOSYdrvTF6hTgEIbc7YasKWGhgEQZyfxyTvoXHQ=
+go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
+go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
+go.uber.org/multierr v1.1.0/go.mod h1:wR5kodmAFQ0UK8QlbwjlSNy0Z68gJhDJUG5sjR94q/0=
+go.uber.org/multierr v1.3.0/go.mod h1:VgVr7evmIr6uPjLBxg28wmKNXyqE9akIJ5XnfpiKl+4=
+go.uber.org/multierr v1.5.0/go.mod h1:FeouvMocqHpRaaGuG9EjoKcStLC43Zu/fmqdUMPcKYU=
+go.uber.org/multierr v1.11.0 h1:blXXJkSxSSfBVBlC76pxqeO+LN3aDfLQo+309xJstO0=
+go.uber.org/multierr v1.11.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN80Y=
+go.uber.org/tools v0.0.0-20190618225709-2cfd321de3ee/go.mod h1:vJERXedbb3MVM5f9Ejo0C68/HhF8uaILCdgjnY+goOA=
+go.uber.org/zap v1.9.1/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
+go.uber.org/zap v1.10.0/go.mod h1:vwi/ZaCAaUcBkycHslxD9B2zi4UTXhF60s6SWpuDF0Q=
+go.uber.org/zap v1.13.0/go.mod h1:zwrFLgMcdUuIBviXEYEH1YKNaOBnKXsx2IPda5bBwHM=
+go.uber.org/zap v1.27.0 h1:aJMhYGrd5QSmlpLMr2MftRKl7t8J8PTZPA732ud/XR8=
+go.uber.org/zap v1.27.0/go.mod h1:GB2qFLM7cTU87MWRP2mPIjqfIDnGu+VIO4V/SdhGo2E=
+golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
+golang.org/x/crypto v0.0.0-20190411191339-88737f569e3a/go.mod h1:WFFai1msRO1wXaEeE5yQxYXgSfI8pQAWXbQop6sCtWE=
+golang.org/x/crypto v0.0.0-20190510104115-cbcb75029529/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20190820162420-60c769a6c586/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
+golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
+golang.org/x/crypto v0.0.0-20201203163018-be400aefbc4c/go.mod h1:jdWPYTVW3xRLrWPugEBEK3UY2ZEsg3UU495nc5E+M+I=
+golang.org/x/crypto v0.0.0-20210616213533-5ff15b29337e/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
+golang.org/x/crypto v0.0.0-20210711020723-a769d52b0f97/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
+golang.org/x/crypto v0.43.0 h1:dduJYIi3A3KOfdGOHX8AVZ/jGiyPa3IbBozJ5kNuE04=
+golang.org/x/crypto v0.43.0/go.mod h1:BFbav4mRNlXJL4wNeejLpWxB7wMbc79PdRGhWKncxR0=
+golang.org/x/lint v0.0.0-20190930215403-16217165b5de/go.mod h1:6SW0HCj/g11FgYtHlgUYUwCkIfeOF89ocIRzGO/8vkc=
+golang.org/x/mod v0.0.0-20190513183733-4bf6d317e70e/go.mod h1:mXi4GBBbnImb6dmsKGUJ2LatrhH/nqhxcFungHvyanc=
+golang.org/x/mod v0.1.1-0.20191105210325-c90efee705ee/go.mod h1:QqPTAvyqsEbceGzBzNggFXnrqF1CaUcvgkdR5Ot7KZg=
+golang.org/x/net v0.0.0-20190311183353-d8887717615a/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
+golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20190813141303-74dc4d7220e7/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
+golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
+golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
+golang.org/x/sys v0.0.0-20180905080454-ebe1bf3edb33/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190222072716-a9d3bda3a223/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
+golang.org/x/sys v0.0.0-20190403152447-81d4e9dc473e/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190422165155-953cdadca894/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20190813064441-fde4db37ae7a/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200116001909-b77594299b42/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20200223170610-d5e6a3e2c0ae/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
+golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
+golang.org/x/sys v0.37.0 h1:fdNQudmxPjkdUTPnLn5mdQv7Zwvbvpaxqs831goi9kQ=
+golang.org/x/sys v0.37.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
+golang.org/x/term v0.0.0-20201117132131-f5c789dd3221/go.mod h1:Nr5EML6q2oocZ2LXRh80K7BxOlk5/8JxuGnuhpl+muw=
+golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
+golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
+golang.org/x/text v0.3.2/go.mod h1:bEr9sfX3Q8Zfm5fL9x+3itogRgK3+ptLWKqgva+5dAk=
+golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.3.4/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
+golang.org/x/text v0.30.0 h1:yznKA/E9zq54KzlzBEAWn1NXSQ8DIp/NYMy88xJjl4k=
+golang.org/x/text v0.30.0/go.mod h1:yDdHFIX9t+tORqspjENWgzaCVXgk0yYnYuSZ8UzzBVM=
+golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
+golang.org/x/tools v0.0.0-20190311212946-11955173bddd/go.mod h1:LCzVGOaR6xXOjkQ3onu1FJEFr0SW1gC7cKk1uF8kGRs=
+golang.org/x/tools v0.0.0-20190425163242-31fd60d6bfdc/go.mod h1:RgjU9mgBXZiqYHBnxXauZ1Gv1EHHAz9KjViQ78xBX0Q=
+golang.org/x/tools v0.0.0-20190621195816-6e04913cbbac/go.mod h1:/rFqwRUd4F7ZHNgwSSTFct+R/Kf4OFW1sUzUTQQTgfc=
+golang.org/x/tools v0.0.0-20190823170909-c4a336ef6a2f/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191029041327-9cc4af7d6b2c/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20191029190741-b9c20aec41a5/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
+golang.org/x/tools v0.0.0-20200103221440-774c71fcf114/go.mod h1:TB2adYChydJhpapKDTa4BR/hXlZSLoq2Wpct/0txZ28=
+golang.org/x/xerrors v0.0.0-20190410155217-1f06c39b4373/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20190513163551-3ee3066db522/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/check.v1 v1.0.0-20180628173108-788fd7840127/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/errgo.v2 v2.1.0/go.mod h1:hNsd1EY+bozCKY1Ytp96fpM3vjJbqLJn88ws8XvfDNI=
+gopkg.in/inconshreveable/log15.v2 v2.0.0-20180818164646-67afb5ed74ec/go.mod h1:aPpfJ7XW+gOuirDoZ8gHhLh3kZ1B08FtV2bbmy7Jv3s=
+gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
+gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
+honnef.co/go/tools v0.0.1-2019.2.3/go.mod h1:a3bituU0lyd329TUQxRnasdCoJDkEUEAqEt0JzvZhAg=
diff --git a/docker-replication-controller/pkg/postgres/client.go b/docker-replication-controller/pkg/postgres/client.go
new file mode 100644
index 0000000..9a9017c
--- /dev/null
+++ b/docker-replication-controller/pkg/postgres/client.go
@@ -0,0 +1,183 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package postgres
+
+import (
+	"context"
+	"fmt"
+	"net/url"
+	"strings"
+	"time"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"github.com/jackc/pgx/v4"
+	"go.uber.org/zap"
+
+	"github.com/jackc/pgconn"
+)
+
+const (
+	HealthUP  = "UP"
+	HealthOOS = "OUT_OF_SERVICE"
+
+	healthQuery = "SELECT 1 FROM pg_catalog.pg_tables"
+)
+
+var (
+	log         = utils.GetLogger()
+	connTimeout = time.Duration(utils.GetEnvInt("PG_CONN_TIMEOUT_SEC", 20))
+)
+
+type Conn interface {
+	Query(ctx context.Context, sql string, args ...interface{}) (pgx.Rows, error)
+	Exec(ctx context.Context, sql string, arguments ...interface{}) (pgconn.CommandTag, error)
+	Close(ctx context.Context) error
+	QueryRow(ctx context.Context, sql string, args ...interface{}) pgx.Row
+}
+
+type ClusterAdapter interface {
+	GetConnection(ctx context.Context) (Conn, error)
+	GetConnectionToDb(ctx context.Context, database string) (Conn, error)
+	GetConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error)
+	GetUser() string
+	GetPassword() string
+	GetHost() string
+	GetPort() int
+}
+
+type Client struct {
+	Host      string
+	Port      int
+	SSl       string
+	User      string
+	Password  string
+	DefaultDB string
+	Health    string
+}
+
+func NewClient(host string, port int, username, password string, database string, ssl string) *Client {
+	username = url.PathEscape(username)
+	password = url.PathEscape(password)
+
+	c := &Client{
+		Host:      host,
+		Port:      port,
+		User:      username,
+		Password:  password,
+		SSl:       ssl,
+		Health:    HealthUP,
+		DefaultDB: database,
+	}
+	log.Debug(fmt.Sprintf("Checking connection for host=%s port=%d with database %s", host, port, database))
+	c.RequestHealth()
+	log.Info("PG client has been initialized")
+	return c
+}
+
+func (ca Client) RequestHealth() string {
+	ch := make(chan string, 1)
+	go func() {
+		ch <- ca.getHealth()
+	}()
+
+	select {
+	case healthStatus := <-ch:
+		if healthStatus == HealthOOS {
+			panic(fmt.Errorf("postgres is unavailable"))
+		}
+		ca.Health = healthStatus
+	case <-time.After(connTimeout * time.Second):
+		panic("postgres connection timeout expired")
+	}
+
+	return ca.Health
+}
+
+func (ca Client) GetPort() int {
+	return ca.Port
+}
+
+func (ca Client) GetUser() string {
+	return ca.User
+}
+
+func (ca Client) GetPassword() string {
+	return ca.Password
+}
+
+func (ca Client) GetHost() string {
+	return ca.Host
+}
+
+func (ca Client) GetConnection(ctx context.Context) (Conn, error) {
+	return ca.GetConnectionToDb(ctx, ca.DefaultDB)
+}
+
+func (ca Client) GetConnectionToDb(ctx context.Context, database string) (Conn, error) {
+	if database == "" {
+		database = ca.DefaultDB
+	}
+	return ca.GetConnectionToDbWithUser(ctx, database, ca.GetUser(), ca.GetPassword())
+}
+
+func (ca Client) GetConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error) {
+	return ca.getConnectionToDbWithUser(ctx, database, username, password)
+}
+
+func (ca Client) getConnectionToDbWithUser(ctx context.Context, database string, username string, password string) (Conn, error) {
+	conn, err := pgx.Connect(ctx, ca.getConnectionUrl(username, password, database))
+	if err != nil {
+		log.Error("Error occurred during connect to DB", zap.Error(err))
+		return nil, err
+	}
+	return conn, nil
+}
+
+func (ca Client) getConnectionUrl(username string, password string, database string) string {
+	if ca.SSl == "on" {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s?%s", username, password, ca.Host, ca.GetPort(), database, "sslmode=require")
+	} else {
+		return fmt.Sprintf("postgres://%s:%s@%s:%d/%s", username, password, ca.Host, ca.GetPort(), database)
+	}
+}
+
+func (ca Client) getHealth() string {
+	err := ca.executeHealthQuery()
+	if err != nil {
+		log.Error("Postgres is unavailable", zap.Error(err))
+		return HealthOOS
+	} else {
+		return HealthUP
+	}
+}
+
+func (ca Client) executeHealthQuery() error {
+	ctx, cancel := context.WithTimeout(context.Background(), connTimeout*time.Second)
+	defer cancel()
+
+	conn, err := ca.getConnectionToDbWithUser(ctx, ca.DefaultDB, ca.User, ca.Password)
+	if err != nil {
+		return err
+	}
+	defer conn.Close(ctx)
+
+	_, err = conn.Exec(ctx, healthQuery)
+	return err
+}
+
+func EscapeInputValue(value string) string {
+	singleQuote := strings.ReplaceAll(value, "'", "''")
+	return strings.ReplaceAll(singleQuote, "\"", "\"\"")
+}
diff --git a/docker-replication-controller/pkg/publicaion/controller.go b/docker-replication-controller/pkg/publicaion/controller.go
new file mode 100644
index 0000000..88d961d
--- /dev/null
+++ b/docker-replication-controller/pkg/publicaion/controller.go
@@ -0,0 +1,330 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package publication
+
+import (
+	"context"
+	"fmt"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/postgres"
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"github.com/jackc/pgx/v4"
+	"go.uber.org/zap"
+)
+
+var (
+	isNotFoundErr = pgx.ErrNoRows
+)
+
+type PublicationController struct {
+	pgClient *postgres.Client
+}
+
+type PublicationInfo struct {
+	Name     string             `json:"name"`
+	Owner    string             `json:"owner"`
+	Database string             `json:"database"`
+	Tables   map[string][]Table `json:"tables,omitempty"`
+}
+
+type Table struct {
+	Name      string   `json:"name"`
+	Attr      []string `json:"attrNames"`
+	RowFilter string   `json:"rowfilter,omitempty"`
+}
+
+func NewPublicationController(pgClient *postgres.Client) *PublicationController {
+	return &PublicationController{pgClient: pgClient}
+}
+
+func (pc *PublicationController) getPublication(ctx context.Context, request CommonRequest, withTables bool) (PublicationInfo, error) {
+	log := utils.ContextLogger(ctx)
+
+	database := request.Database
+	publication := request.PubName
+	err := validatePublication(publication, database)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return PublicationInfo{}, err
+	}
+	return pc.getPublicationInternal(ctx, publication, database, withTables)
+}
+
+// Error is only isNotFoundErr
+func (pc *PublicationController) getPublicationInternal(ctx context.Context, publication, database string, withTables bool) (PublicationInfo, error) {
+	log := utils.ContextLogger(ctx)
+
+	log.Info(fmt.Sprintf("Get publication %s for database %s", publication, database))
+	conn, err := pc.pgClient.GetConnectionToDb(ctx, database)
+	if err != nil {
+		if strings.Contains(err.Error(), "(SQLSTATE 3D000)") {
+			return PublicationInfo{}, isNotFoundErr
+		}
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	pubInfo := PublicationInfo{}
+	rows, err := conn.Query(ctx, getPubGetQuery(), publication)
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot get publication %s for database %s", publication, database))
+		panic(err)
+	}
+	defer rows.Close()
+
+	if rows.Next() {
+		err = rows.Scan(&pubInfo.Name, &pubInfo.Owner)
+		if err != nil {
+			log.Error(fmt.Sprintf("cannot scan publication %s for database %s", publication, database))
+			panic(err)
+		}
+	} else {
+		return PublicationInfo{}, isNotFoundErr
+	}
+	pubInfo.Database = database
+
+	// Fill tables info
+	if withTables {
+		rows.Close()
+		rows, err := conn.Query(ctx, getPubGetTablesQuery(), publication)
+		if err != nil {
+			log.Error(fmt.Sprintf("cannot get publication %s tables info for database %s", publication, database))
+			panic(err)
+		}
+
+		tables, err := processTableRows(rows)
+		if err != nil {
+			log.Error(fmt.Sprintf("cannot scan publication %s tables info for database %s", publication, database))
+			panic(err)
+		}
+		pubInfo.Tables = tables
+	}
+
+	log.Info(fmt.Sprintf("Publication %s has been get for database %s", publication, database))
+	return pubInfo, nil
+}
+
+func (pc *PublicationController) createPublication(ctx context.Context, request CommonRequest) error {
+	log := utils.ContextLogger(ctx)
+
+	database := request.Database
+	publication := request.PubName
+	err := validatePublication(publication, database)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return err
+	}
+	log.Info(fmt.Sprintf("Publication %s creation started for database %s", publication, database))
+	if pc.isPublicationExists(ctx, publication, database) {
+		log.Info(fmt.Sprintf("Publication %s already exists in database %s", publication, database))
+		return nil
+	}
+
+	conn, err := pc.pgClient.GetConnectionToDb(ctx, database)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	tables := request.Tables
+	schemas := request.Schemas
+	if len(tables) == 0 && len(schemas) == 0 {
+		log.Debug(getPubCreateQuery(publication, tables, schemas))
+		_, err = conn.Exec(ctx, getPubCreateAllTablesQuery(publication))
+		if err != nil {
+			log.Error(fmt.Sprintf("cannot create publication %s for database %s", publication, database))
+			panic(err)
+		}
+	} else {
+		log.Debug(getPubCreateQuery(publication, tables, schemas))
+		_, err = conn.Exec(ctx, getPubCreateQuery(publication, tables, schemas))
+		if err != nil {
+			log.Error(fmt.Sprintf("cannot create publication %s for database %s for tables %s", publication, database, tables))
+			panic(err)
+		}
+	}
+
+	log.Info(fmt.Sprintf("Publication %s has been created for database %s", publication, database))
+	return nil
+}
+
+func (pc *PublicationController) alterAddPublication(ctx context.Context, request CommonRequest) error {
+	log := utils.ContextLogger(ctx)
+
+	database := request.Database
+	publication := request.PubName
+	err := validatePublication(publication, database)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return err
+	}
+	log.Info(fmt.Sprintf("Publication %s alter add started for database %s", publication, database))
+	if !pc.isPublicationExists(ctx, publication, database) {
+		log.Info(fmt.Sprintf("Publication %s doesn't exist in database %s", publication, database))
+		return isNotFoundErr
+	}
+
+	tables := request.Tables
+	schemas := request.Schemas
+	if len(tables) == 0 && len(schemas) == 0 {
+		errMsg := fmt.Sprintf("Nothing to add to publication %s in database %s", publication, database)
+		log.Error(errMsg)
+		return fmt.Errorf("%s", errMsg)
+	}
+
+	conn, err := pc.pgClient.GetConnectionToDb(ctx, database)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	log.Debug(getPubAlterAddQuery(publication, tables, schemas))
+	_, err = conn.Exec(ctx, getPubAlterAddQuery(publication, tables, schemas))
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot alter add publication %s for database %s", publication, database), zap.Error(err))
+		if strings.Contains(err.Error(), "(SQLSTATE 42710)") {
+			return err
+		}
+		panic(err)
+	}
+
+	log.Info(fmt.Sprintf("Publication %s has been altered for database %s", publication, database))
+	return nil
+}
+
+func (pc *PublicationController) alterSetPublication(ctx context.Context, request CommonRequest) error {
+	log := utils.ContextLogger(ctx)
+
+	database := request.Database
+	publication := request.PubName
+	err := validatePublication(publication, database)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return err
+	}
+	log.Info(fmt.Sprintf("Publication %s alter set started for database %s", publication, database))
+	if !pc.isPublicationExists(ctx, publication, database) {
+		log.Info(fmt.Sprintf("Publication %s doesn't exist in database %s", publication, database))
+		return isNotFoundErr
+	}
+
+	tables := request.Tables
+	schemas := request.Schemas
+	if len(tables) == 0 && len(schemas) == 0 {
+		errMsg := fmt.Sprintf("Nothing to add to publication %s in database %s", publication, database)
+		log.Error(errMsg)
+		return fmt.Errorf("%s", errMsg)
+	}
+
+	conn, err := pc.pgClient.GetConnectionToDb(ctx, database)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	log.Debug(getPubAlterSetQuery(publication, tables, schemas))
+	_, err = conn.Exec(ctx, getPubAlterSetQuery(publication, tables, schemas))
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot alter set publication %s for database %s", publication, database), zap.Error(err))
+		if strings.Contains(err.Error(), "(SQLSTATE 42710)") {
+			return err
+		}
+		panic(err)
+	}
+
+	log.Info(fmt.Sprintf("Publication %s has been altered for database %s", publication, database))
+	return nil
+}
+
+func (pc *PublicationController) dropPublication(ctx context.Context, request CommonRequest) error {
+	log := utils.ContextLogger(ctx)
+
+	publication := request.PubName
+	database := request.Database
+	err := validatePublication(publication, database)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return err
+	}
+
+	log.Info(fmt.Sprintf("Publication %s drop started for database %s", publication, database))
+	if !pc.isPublicationExists(ctx, publication, database) {
+		log.Info(fmt.Sprintf("Publication %s doesn't exist in database %s", publication, database))
+		return nil
+	}
+
+	conn, err := pc.pgClient.GetConnectionToDb(ctx, database)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	log.Debug(getPubDropQuery(publication))
+	_, err = conn.Exec(ctx, getPubDropQuery(publication))
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot drop publication %s for database %s", publication, database))
+		panic(err)
+	}
+	log.Info(fmt.Sprintf("Publication %s has been dropped for database %s", publication, database))
+	return nil
+}
+
+func processTableRows(rows pgx.Rows) (map[string][]Table, error) {
+	defer rows.Close()
+
+	tablesInfo := make(map[string][]Table)
+	var schema string
+	var table Table
+	var attrStr string
+
+	for rows.Next() {
+		err := rows.Scan(&schema, &table.Name, &attrStr, &table.RowFilter)
+		if err != nil {
+			return nil, err
+		}
+		defer rows.Close()
+		table.Attr = convAttrStrToSlice(attrStr)
+		if tables, ok := tablesInfo[schema]; ok {
+			tables = append(tables, table)
+			tablesInfo[schema] = tables
+		} else {
+			tables = []Table{table}
+			tablesInfo[schema] = tables
+		}
+	}
+	return tablesInfo, nil
+}
+
+func (pc *PublicationController) isPublicationExists(ctx context.Context, publication, database string) bool {
+	_, err := pc.getPublicationInternal(ctx, publication, database, false)
+	return err != isNotFoundErr
+}
+
+func validatePublication(publication, database string) error {
+	if len(database) == 0 {
+		return fmt.Errorf("database must not be empty")
+	}
+	if len(publication) == 0 {
+		return fmt.Errorf("database must not be empty")
+	}
+	return nil
+}
+
+func convAttrStrToSlice(attrs string) []string {
+	attrs = strings.Trim(attrs, "{")
+	attrs = strings.Trim(attrs, "}")
+	return strings.Split(attrs, ",")
+}
diff --git a/docker-replication-controller/pkg/publicaion/handlers.go b/docker-replication-controller/pkg/publicaion/handlers.go
new file mode 100644
index 0000000..cc3e197
--- /dev/null
+++ b/docker-replication-controller/pkg/publicaion/handlers.go
@@ -0,0 +1,114 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package publication
+
+import (
+	"context"
+	"fmt"
+	"strconv"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"github.com/gofiber/fiber/v2"
+	"github.com/gofiber/fiber/v2/log"
+	"go.uber.org/zap"
+)
+
+type CommonRequest struct {
+	PubName  string   `json:"publicationName"`
+	Database string   `json:"database"`
+	Tables   []string `json:"tables,omitempty"`
+	Schemas  []string `json:"schemas,omitempty"`
+}
+
+func (pc *PublicationController) PublicationCreateHandler(c *fiber.Ctx) error {
+	return handleCommonFunc(c, pc.createPublication)
+}
+
+func (pc *PublicationController) PublicationAlterAddHandler(c *fiber.Ctx) error {
+	return handleCommonFunc(c, pc.alterAddPublication)
+}
+
+func (pc *PublicationController) PublicationAlterSetHandler(c *fiber.Ctx) error {
+	return handleCommonFunc(c, pc.alterSetPublication)
+}
+
+func (pc *PublicationController) PublicationDropHandler(c *fiber.Ctx) error {
+	return handleCommonFunc(c, pc.dropPublication)
+}
+
+func (pc *PublicationController) PublicationGetHandler(c *fiber.Ctx) error {
+	request := CommonRequest{
+		Database: c.Params("database"),
+		PubName:  c.Params("publication"),
+	}
+
+	withTables, err := getQueryBoolParam(c, "withTables")
+	if err != nil {
+		return badReq(c, err)
+	}
+
+	ctx := utils.GetRequestContext(c)
+	pubInfo, err := pc.getPublication(ctx, request, withTables)
+	if err != nil {
+		if err == isNotFoundErr {
+			return c.SendStatus(fiber.StatusNotFound)
+		}
+		return badReq(c, err)
+	}
+
+	return c.Status(fiber.StatusOK).JSON(pubInfo)
+}
+
+func handleCommonFunc(c *fiber.Ctx, handleFunc func(context.Context, CommonRequest) error) error {
+	request, err := getCommonReq(c)
+	if err != nil {
+		return err
+	}
+	ctx := utils.GetRequestContext(c)
+	err = handleFunc(ctx, request)
+	if err != nil {
+		return badReq(c, err)
+	}
+	return ok(c)
+}
+
+func getCommonReq(c *fiber.Ctx) (CommonRequest, error) {
+	var request CommonRequest
+	if len(c.Body()) > 0 {
+		err := c.BodyParser(&request)
+		if err != nil {
+			return request, err
+		}
+	}
+	return request, nil
+}
+
+func badReq(c *fiber.Ctx, err error) error {
+	return c.Status(fiber.StatusBadRequest).SendString(err.Error())
+}
+
+func ok(c *fiber.Ctx) error {
+	return c.Status(fiber.StatusOK).SendString("OK")
+}
+
+func getQueryBoolParam(c *fiber.Ctx, param string) (bool, error) {
+	paramStr := c.Query(param, "false")
+	boolVal, err := strconv.ParseBool(paramStr)
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot parse bool value for param %s", param), zap.Error(err))
+		return false, err
+	}
+	return boolVal, nil
+}
diff --git a/docker-replication-controller/pkg/publicaion/sql_helper.go b/docker-replication-controller/pkg/publicaion/sql_helper.go
new file mode 100644
index 0000000..4091278
--- /dev/null
+++ b/docker-replication-controller/pkg/publicaion/sql_helper.go
@@ -0,0 +1,125 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package publication
+
+import (
+	"fmt"
+	"strings"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/postgres"
+)
+
+const (
+	pubGetQuery                 = "select pubname as name, pubowner::regrole as owner from pg_publication where pubname=$1"
+	pubGetTablesQuery           = "select schemaname, tablename, attnames::TEXT, Coalesce(rowfilter,'') from pg_publication_tables where pubname=$1"
+	pubCreateAllTablesQuery     = "CREATE publication \"%s\" FOR ALL TABLES;"
+	pubCreateWithTablesQuery    = "CREATE publication \"%s\" FOR TABLE %s"
+	pubCreateWithSchemasQuery   = "CREATE publication \"%s\" FOR TABLES IN SCHEMA %s"
+	pubAlterAddWithTablesQuery  = "ALTER PUBLICATION \"%s\" ADD TABLE %s"
+	pubAlterAddWithSchemasQuery = "ALTER PUBLICATION \"%s\" ADD TABLES IN SCHEMA %s"
+	pubAlterSetWithTablesQuery  = "ALTER PUBLICATION \"%s\" SET TABLE %s"
+	pubDropQuery                = "DROP publication \"%s\";"
+
+	schemasAppend = "TABLES IN SCHEMA"
+)
+
+func getPubGetQuery() string {
+	return pubGetQuery
+}
+
+func getPubGetTablesQuery() string {
+	return pubGetTablesQuery
+}
+
+func getPubCreateAllTablesQuery(publication string) string {
+	return fmt.Sprintf(pubCreateAllTablesQuery, postgres.EscapeInputValue(publication))
+}
+
+func getPubCreateQuery(publication string, tables, schemas []string) string {
+	return formQueryWithTablesAndSchemas(publication, tables, schemas, pubCreateWithTablesQuery, pubCreateWithSchemasQuery)
+}
+
+func getPubAlterAddQuery(publication string, tables, schemas []string) string {
+	return formQueryWithTablesAndSchemas(publication, tables, schemas, pubAlterAddWithTablesQuery, pubAlterAddWithSchemasQuery)
+}
+
+func getPubAlterSetQuery(publication string, tables []string, schemas []string) string {
+	return formQueryWithTablesAndSchemas(publication, tables, schemas, pubAlterSetWithTablesQuery, pubAlterAddWithSchemasQuery)
+}
+
+func getPubDropQuery(publication string) string {
+	return fmt.Sprintf(pubDropQuery, postgres.EscapeInputValue(publication))
+}
+
+func formQueryWithTablesAndSchemas(publication string, tables, schemas []string, queryForTables, queryForSchemas string) string {
+	var query string
+	areTablesPresent := len(tables) > 0
+	areSchemasPresent := len(schemas) > 0
+	if areTablesPresent {
+		prepTables := prepareTables(tables)
+		query = fmt.Sprintf(queryForTables, postgres.EscapeInputValue(publication), strings.Join(prepTables, ","))
+	}
+	if areSchemasPresent {
+		prepSchemas := prepareSchemas(schemas)
+		if areTablesPresent {
+			query = fmt.Sprintf("%s, %s %s;", query, schemasAppend, strings.Join(prepSchemas, ","))
+		} else {
+			query = fmt.Sprintf(queryForSchemas, postgres.EscapeInputValue(publication), strings.Join(prepSchemas, ","))
+		}
+	}
+	return query
+}
+
+func prepareTables(tables []string) []string {
+	preparedTables := make([]string, 0, len(tables))
+	for _, origTable := range tables {
+		table := postgres.EscapeInputValue(origTable)
+		if strings.Contains(table, ".") {
+			table = prepareTableWithSchema(table)
+		} else {
+			table = prepareTableWithArgs(table)
+		}
+		preparedTables = append(preparedTables, table)
+	}
+	return preparedTables
+}
+
+func prepareTableWithSchema(table string) string {
+	tableArr := strings.Split(table, ".")
+	schema := fmt.Sprintf("\"%s\"", tableArr[0])
+	tableWithArgs := prepareTableWithArgs(tableArr[1])
+	table = fmt.Sprintf("%s.%s", schema, tableWithArgs)
+	return table
+}
+
+func prepareTableWithArgs(table string) string {
+	tableWithArgs := strings.Split(table, "(")
+	if len(tableWithArgs) > 1 {
+		table = fmt.Sprintf("\"%s\"(%s", tableWithArgs[0], tableWithArgs[1])
+	} else {
+		table = fmt.Sprintf("\"%s\"", table)
+	}
+	return table
+}
+
+func prepareSchemas(schemas []string) []string {
+	preparedSchemas := make([]string, 0, len(schemas))
+	for _, origSchema := range schemas {
+		schema := postgres.EscapeInputValue(origSchema)
+		schema = fmt.Sprintf("\"%s\"", schema)
+		preparedSchemas = append(preparedSchemas, schema)
+	}
+	return preparedSchemas
+}
diff --git a/docker-replication-controller/pkg/users/controller.go b/docker-replication-controller/pkg/users/controller.go
new file mode 100644
index 0000000..bc6c36e
--- /dev/null
+++ b/docker-replication-controller/pkg/users/controller.go
@@ -0,0 +1,63 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package users
+
+import (
+	"context"
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/postgres"
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"go.uber.org/zap"
+)
+
+type UsersController struct {
+	pgClient *postgres.Client
+}
+
+func NewUsersController(pgClient *postgres.Client) *UsersController {
+	return &UsersController{pgClient: pgClient}
+}
+
+func (pc *UsersController) grantUserToReplication(ctx context.Context, request UserRequest) error {
+	log := utils.ContextLogger(ctx)
+	username := request.Username
+	err := validateGrantRequest(username)
+	if err != nil {
+		log.Error(err.Error(), zap.Error(err))
+		return err
+	}
+
+	conn, err := pc.pgClient.GetConnection(ctx)
+	if err != nil {
+		panic(err)
+	}
+	defer conn.Close(ctx)
+
+	_, err = conn.Exec(ctx, getGrantReplicationQuery(username))
+	if err != nil {
+		log.Error(fmt.Sprintf("cannot grant user %s for Replication", username))
+		panic(err)
+	}
+	log.Info(fmt.Sprintf("User %s has been granted for Replication", username))
+	return nil
+}
+
+func validateGrantRequest(username string) error {
+	if len(username) == 0 {
+		return fmt.Errorf("username must not be empty")
+	}
+	return nil
+}
diff --git a/docker-replication-controller/pkg/users/handlers.go b/docker-replication-controller/pkg/users/handlers.go
new file mode 100644
index 0000000..478b1e4
--- /dev/null
+++ b/docker-replication-controller/pkg/users/handlers.go
@@ -0,0 +1,56 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package users
+
+import (
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/utils"
+	"github.com/gofiber/fiber/v2"
+)
+
+type UserRequest struct {
+	Username string `json:"username"`
+}
+
+func (pc *UsersController) GrantUserHandler(c *fiber.Ctx) error {
+	request, err := getUserReq(c)
+	if err != nil {
+		return err
+	}
+	ctx := utils.GetRequestContext(c)
+	err = pc.grantUserToReplication(ctx, request)
+	if err != nil {
+		return badReq(c, err)
+	}
+	return ok(c)
+}
+
+func getUserReq(c *fiber.Ctx) (UserRequest, error) {
+	var request UserRequest
+	if len(c.Body()) > 0 {
+		err := c.BodyParser(&request)
+		if err != nil {
+			return request, err
+		}
+	}
+	return request, nil
+}
+
+func badReq(c *fiber.Ctx, err error) error {
+	return c.Status(fiber.StatusBadRequest).SendString(err.Error())
+}
+
+func ok(c *fiber.Ctx) error {
+	return c.Status(fiber.StatusOK).SendString("OK")
+}
diff --git a/docker-replication-controller/pkg/users/sql_helper.go b/docker-replication-controller/pkg/users/sql_helper.go
new file mode 100644
index 0000000..63b592b
--- /dev/null
+++ b/docker-replication-controller/pkg/users/sql_helper.go
@@ -0,0 +1,25 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package users
+
+import (
+	"fmt"
+
+	"github.com/Netcracker/pgskipper-replication-controller/pkg/postgres"
+)
+
+func getGrantReplicationQuery(username string) string {
+	return fmt.Sprintf("ALTER ROLE %s WITH REPLICATION;", postgres.EscapeInputValue(username))
+}
diff --git a/docker-replication-controller/pkg/utils/utils.go b/docker-replication-controller/pkg/utils/utils.go
new file mode 100644
index 0000000..c87cb70
--- /dev/null
+++ b/docker-replication-controller/pkg/utils/utils.go
@@ -0,0 +1,110 @@
+// Copyright 2024-2025 NetCracker Technology Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package utils
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"os"
+	"strconv"
+
+	"github.com/gofiber/fiber/v2"
+	"github.com/google/uuid"
+	"go.uber.org/zap"
+	"go.uber.org/zap/zapcore"
+)
+
+var (
+	isDebugEnabled = *flag.Bool("log_debug", GetEnvBool("LOG_DEBUG", false), "If debug logs is enabled, env: LOG_DEBUG")
+	log            *zap.Logger
+)
+
+type RequestId string
+
+func init() {
+	log = GetLogger()
+}
+
+func GetLogger() *zap.Logger {
+	atom := zap.NewAtomicLevel()
+	if isDebugEnabled {
+		atom = zap.NewAtomicLevelAt(zap.DebugLevel)
+	}
+
+	encoderCfg := zap.NewProductionEncoderConfig()
+	encoderCfg.TimeKey = "timestamp"
+	encoderCfg.EncodeTime = zapcore.ISO8601TimeEncoder
+
+	logger := zap.New(zapcore.NewCore(
+		zapcore.NewJSONEncoder(encoderCfg),
+		zapcore.Lock(os.Stdout),
+		atom,
+	))
+	defer func() {
+		_ = logger.Sync()
+	}()
+	return logger
+}
+
+func GetEnv(key, fallback string) string {
+	if value, ok := os.LookupEnv(key); ok {
+		return value
+	}
+	return fallback
+}
+
+func GetEnvInt(key string, fallback int) int {
+	if value, ok := os.LookupEnv(key); ok {
+		if ivalue, err := strconv.Atoi(value); err == nil {
+			return ivalue
+		}
+	}
+	return fallback
+}
+
+func GetEnvBool(key string, fallback bool) bool {
+	if value, ok := os.LookupEnv(key); ok {
+		bvalue, err := strconv.ParseBool(value)
+		if err != nil {
+			log.Error(fmt.Sprintf("Can't parse %s boolean variable", key), zap.Error(err))
+			panic(err)
+		}
+		return bvalue
+	}
+	return fallback
+}
+
+func ContextLogger(ctx context.Context) *zap.Logger {
+	logger := GetLogger()
+	return logger.With(zap.ByteString("request_id", []byte(fmt.Sprintf("%s", ctx.Value(RequestId("request_id"))))))
+}
+
+func GetRequestContext(c *fiber.Ctx) context.Context {
+	requestId := c.Request().Header.Peek("X-Request-ID")
+	if len(requestId) == 0 {
+		id := uuid.New().String()
+		c.Set("X-Request-ID", id)
+		requestId = []byte(id)
+	}
+
+	bg := context.Background()
+	ctx := context.WithValue(bg, RequestId("request_id"), requestId)
+	return ctx
+}
+
+func IsHttpsEnabled() bool {
+	return GetEnv("TLS_ENABLED", "false") == "true"
+}
diff --git a/docker-transfer/Dockerfile b/docker-transfer/Dockerfile
index a5f00d7..610946a 100644
--- a/docker-transfer/Dockerfile
+++ b/docker-transfer/Dockerfile
@@ -2,4 +2,4 @@
 FROM scratch
 
 # Transfer
-COPY charts /charts
\ No newline at end of file
+COPY operator/charts /charts
\ No newline at end of file
diff --git a/docker-upgrade/CODE-OF-CONDUCT.md b/docker-upgrade/CODE-OF-CONDUCT.md
new file mode 100644
index 0000000..f5b511b
--- /dev/null
+++ b/docker-upgrade/CODE-OF-CONDUCT.md
@@ -0,0 +1,73 @@
+# Code of Conduct
+
+This repository is governed by following code of conduct guidelines.
+
+We put collaboration, trust, respect and transparency as core values for our community.
+Our community welcomes participants from all over the world with different experience,
+opinion and ideas to share.
+
+We have adopted this code of conduct and require all contributors to agree with that to build a healthy,
+safe and productive community for all.
+
+The guideline is aimed to support a community where all people should feel safe to participate,
+introduce new ideas and inspire others, regardless of:
+
+* Age
+* Gender
+* Gender identity or expression
+* Family status
+* Marital status
+* Ability
+* Ethnicity
+* Race
+* Sex characteristics
+* Sexual identity and orientation
+* Education
+* Native language
+* Background
+* Caste
+* Religion
+* Geographic location
+* Socioeconomic status
+* Personal appearance
+* Any other dimension of diversity
+
+## Our Standards
+
+We are welcoming the following behavior:
+
+* Be respectful for different ideas, opinions and points of view
+* Be constructive and professional
+* Use inclusive language
+* Be collaborative and show the empathy
+* Focus on the best results for the community
+
+The following behavior is unacceptable:
+
+* Violence, threats of violence, or inciting others to commit self-harm
+* Personal attacks, trolling, intentionally spreading misinformation, insulting/derogatory comments
+* Public or private harassment
+* Publishing others' private information, such as a physical or electronic address, without explicit permission
+* Derogatory language
+* Encouraging unacceptable behavior
+* Other conduct which could reasonably be considered inappropriate in a professional community
+
+## Our Responsibilities
+
+Project maintainers are responsible for clarifying the standards of the Code of Conduct
+and are expected to take appropriate actions in response to any instances of unacceptable behavior.
+
+Project maintainers have the right and responsibility to remove, edit, or reject comments,
+commits, code, wiki edits, issues, and other contributions that are not aligned
+to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors
+that they deem inappropriate, threatening, offensive, or harmful.
+
+## Reporting
+
+If you believe you’re experiencing unacceptable behavior that will not be tolerated as outlined above,
+please report to `opensourcegroup@netcracker.com`. All complaints will be reviewed and investigated and will result in a response
+that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality
+with regard to the reporter of an incident.
+
+Please also report if you observe a potentially dangerous situation, someone in distress, or violations of these guidelines,
+even if the situation is not happening to you.
diff --git a/docker-upgrade/CONTRIBUTING.md b/docker-upgrade/CONTRIBUTING.md
new file mode 100644
index 0000000..292ce26
--- /dev/null
+++ b/docker-upgrade/CONTRIBUTING.md
@@ -0,0 +1,12 @@
+# Contribution Guide
+
+We'd love to accept patches and contributions to this project.
+Please, follow these guidelines to make the contribution process easy and effective for everyone involved.
+
+## Contributor License Agreement
+
+You must sign the [Contributor License Agreement](https://pages.netcracker.com/cla-main.html) in order to contribute.
+
+## Code of Conduct
+
+Please make sure to read and follow the [Code of Conduct](CODE-OF-CONDUCT.md).
diff --git a/docker-upgrade/Dockerfile b/docker-upgrade/Dockerfile
new file mode 100644
index 0000000..4085882
--- /dev/null
+++ b/docker-upgrade/Dockerfile
@@ -0,0 +1,78 @@
+FROM ubuntu:22.04
+
+USER root
+
+ENV LC_ALL=en_US.UTF-8 \
+    LANG=en_US.UTF-8
+ARG DEBIAN_FRONTEND=noninteractive
+
+RUN echo start
+RUN mkdir -p /var/lib/pgsql/data/ && mkdir -p /var/lib/pgsql/tmp_data/
+
+# fix for non-ha
+RUN mkdir /properties && touch /properties/empty-conf.conf
+
+RUN echo "deb [trusted=yes] http://apt.postgresql.org/pub/repos/apt jammy-pgdg main" >> /etc/apt/sources.list.d/pgdg.list
+RUN ls -la /etc/apt/
+RUN apt-get -y update
+RUN apt-get -o DPkg::Options::="--force-confnew" -y dist-upgrade
+
+RUN groupmod -n postgres tape
+RUN adduser -uid 26 -gid 26 postgres
+
+# Install like base image
+RUN apt-get --no-install-recommends install -y gcc-12 python3.11 python3-pip python3-dev wget
+
+RUN python3 -m pip install --no-cache-dir --upgrade wheel==0.38.0 setuptools==78.1.1
+
+# Explicitly install patched libaom3 version
+RUN apt-get --no-install-recommends install -y libaom3=3.3.0-1ubuntu0.1 || apt-get --no-install-recommends install -y libaom3
+
+RUN apt-get -y update
+RUN apt-get -o DPkg::Options::="--force-confnew" -y dist-upgrade
+RUN apt-get --no-install-recommends install -y \
+     postgresql-12 postgresql-server-dev-12 \
+     postgresql-13 postgresql-server-dev-13 \
+     postgresql-14 postgresql-server-dev-14 \
+     postgresql-15 postgresql-server-dev-15 \
+     postgresql-16 postgresql-server-dev-16 \
+     postgresql-17 postgresql-server-dev-17 \
+     postgresql-12-pg-track-settings postgresql-12-pg-wait-sampling postgresql-12-cron postgresql-12-set-user postgresql-12-pg-stat-kcache postgresql-12-pgaudit postgresql-12-pg-qualstats postgresql-12-hypopg postgresql-12-powa \
+     postgresql-13-pg-track-settings postgresql-13-pg-wait-sampling postgresql-13-cron postgresql-13-set-user postgresql-13-pg-stat-kcache postgresql-13-pgaudit postgresql-13-pg-qualstats postgresql-13-hypopg postgresql-13-powa \
+     postgresql-14-pg-track-settings postgresql-14-pg-wait-sampling postgresql-14-cron postgresql-14-set-user postgresql-14-postgis postgresql-14-pg-stat-kcache postgresql-14-pgaudit postgresql-14-pg-qualstats postgresql-14-hypopg postgresql-14-powa postgresql-14-pg-hint-plan postgresql-14-pgnodemx postgresql-14-decoderbufs \
+     postgresql-15-pg-track-settings postgresql-15-pg-wait-sampling postgresql-15-cron postgresql-15-set-user postgresql-15-postgis postgresql-15-pg-stat-kcache postgresql-15-pgaudit postgresql-15-pg-qualstats postgresql-15-hypopg postgresql-15-powa postgresql-15-pg-hint-plan postgresql-15-pgnodemx postgresql-15-decoderbufs \
+     postgresql-16-pg-track-settings postgresql-16-pg-wait-sampling postgresql-16-cron postgresql-16-set-user postgresql-16-postgis postgresql-16-pg-stat-kcache postgresql-16-pgaudit postgresql-16-pg-qualstats postgresql-16-hypopg postgresql-16-powa postgresql-16-pg-hint-plan postgresql-16-pgnodemx postgresql-16-decoderbufs \
+     postgresql-17-pg-track-settings postgresql-17-pg-wait-sampling postgresql-17-cron postgresql-17-set-user postgresql-17-postgis postgresql-17-pg-stat-kcache postgresql-17-pgaudit postgresql-17-pg-qualstats postgresql-17-hypopg postgresql-17-powa postgresql-17-pg-hint-plan postgresql-17-pgnodemx postgresql-17-decoderbufs
+RUN apt-get --no-install-recommends install -y libproj-dev libgdal30 libgeos3.10.2 libgeotiff5 libsfcgal1
+
+RUN localedef -i en_US -f UTF-8 en_US.UTF-8 && \
+    localedef -i es_PE -f UTF-8 es_PE.UTF-8 && \
+    localedef -i es_ES -f UTF-8 es_ES.UTF-8
+# Migrate .rpm to .deb
+RUN apt-get install -y alien
+
+RUN apt-get install -y protobuf-compiler
+
+WORKDIR /tmp
+
+COPY ./docker/start.sh /start.sh
+
+RUN chgrp 0 /etc &&  \
+    chmod g+w /etc && \
+    chgrp 0 /etc/passwd &&  \
+    chmod g+w /etc/passwd
+
+RUN chgrp 0 /var/lib/pgsql/ && chmod g+w /var/lib/pgsql/ && chmod 777 /var/lib/pgsql && \
+    chgrp 0 /var/run/postgresql/ && chmod g+w /var/run/postgresql/ && chmod 777 /var/run/postgresql && \
+    chmod -R 777 /etc/passwd && \
+    mv /usr/bin/python3.11 /usr/bin/python && ln -fs /usr/bin/python /usr/bin/python3 && \
+    chmod +x /start.sh
+
+
+RUN pg_config --version | grep -o "[0-9]*" | head -n 1
+VOLUME /var/lib/pgsql
+VOLUME /var/run/postgresql
+
+CMD ["/bin/bash", "/start.sh"]
+
+USER 26
diff --git a/docker-upgrade/LICENSE b/docker-upgrade/LICENSE
new file mode 100644
index 0000000..261eeb9
--- /dev/null
+++ b/docker-upgrade/LICENSE
@@ -0,0 +1,201 @@
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
diff --git a/docker-upgrade/README.md b/docker-upgrade/README.md
new file mode 100644
index 0000000..eb2e624
--- /dev/null
+++ b/docker-upgrade/README.md
@@ -0,0 +1 @@
+# pgskipper-upgrade
\ No newline at end of file
diff --git a/docker-upgrade/SECURITY.md b/docker-upgrade/SECURITY.md
new file mode 100644
index 0000000..8162261
--- /dev/null
+++ b/docker-upgrade/SECURITY.md
@@ -0,0 +1,15 @@
+# Security Reporting Process
+
+Please, report any security issue to `opensourcegroup@netcracker.com` where the issue will be triaged appropriately.
+
+If you know of a publicly disclosed security vulnerability please IMMEDIATELY email `opensourcegroup@netcracker.com`
+to inform the team about the vulnerability, so we may start the patch, release, and communication process.
+
+# Security Release Process
+
+If the vulnerability is found in the latest stable release, then it would be fixed in patch version for that release.
+E.g., issue is found in 2.5.0 release, then 2.5.1 version with a fix will be released.
+By default, older versions will not have security releases.
+
+If the issue doesn't affect any existing public releases, the fix for medium and high issues is performed
+in a main branch before releasing a new version. For low priority issues the fix can be planned for future releases.
diff --git a/docker-upgrade/docker/start.sh b/docker-upgrade/docker/start.sh
new file mode 100755
index 0000000..5929e30
--- /dev/null
+++ b/docker-upgrade/docker/start.sh
@@ -0,0 +1,251 @@
+#!/usr/bin/env bash
+# Copyright 2024-2025 NetCracker Technology Corporation
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+RETRIES=100
+SLEEP_BETWEEN_ITERATIONS=5
+
+[[ "${DEBUG}" == 'true' ]] && set -x
+
+function handle_master_upgrade() {
+    cd /var/lib/pgsql/data/
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] cur path: `pwd`"
+
+
+    DB_SIZE_GB_FLOAT=$(du -sk /var/lib/pgsql/data/${DATA_DIR} | awk '{ print $1 / 1024 / 1024 }')
+    DB_SIZE_GB=`printf "%.0f\n" ${DB_SIZE_GB_FLOAT}`
+    PV_SIZE_GB=$(echo "${PV_SIZE}" | tr -dc '0-9')
+
+    echo
+    echo '##########################'
+    echo "MIGRATION_PV_USED: $MIGRATION_PV_USED"
+    echo "CLEAN_MIGRATION_PV: $CLEAN_MIGRATION_PV"
+    echo "MIGRATION_PATH: $MIGRATION_PATH"
+    echo "INITDB_PARAMS: $INITDB_PARAMS"
+    echo "SIZE OF VOLUME: $PV_SIZE_GB Gb"
+    echo "SOURCE DB SIZE: $DB_SIZE_GB Gb"
+    echo "DATA_DIR: $DATA_DIR"
+    echo '##########################'
+    echo
+
+    # check if there is enough space in case when migration pv is used
+    if [[ "${MIGRATION_PV_USED}" =~ ^[Tt]rue$ ]]; then
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Migration PV is used, check if there is enough space for migration in migration PV"
+        if [[ $DB_SIZE_GB -gt $PV_SIZE_GB ]]; then
+            echo "[$(date +%Y-%m-%dT%H:%M:%S)] DB size is more than PV size, exiting ..."
+            exit 1
+        fi
+        if [[ "${CLEAN_MIGRATION_PV}" =~ ^[Tt]rue$ ]]; then
+            echo "[$(date +%Y-%m-%dT%H:%M:%S)] CLEAN_MIGRATION_PV set to True, All data from MIGRATION PV will be deleted! "
+            rm -rf /var/lib/pgsql/tmp_data/*
+        fi
+    else
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Migration PV is NOT used, check if there is enough space for migration in master PV"
+        DOUBLE_DB_SIZE="$((${DB_SIZE_GB} * 2))"
+        if [[ ${DOUBLE_DB_SIZE} -gt ${PV_SIZE_GB} ]]; then
+            echo "[$(date +%Y-%m-%dT%H:%M:%S)] DB size is more than PV size, exiting ..."
+            exit 1
+        fi
+    fi
+    export PATH="/usr/lib/postgresql/${PG_VERSION_TARGET}/bin:${PATH}"
+
+    [ -d "$MIGRATION_PATH/tmp/pg" ] && echo "[$(date +%Y-%m-%dT%H:%M:%S)] Prev upgrade dir exists, removing .." && rm -rf "$MIGRATION_PATH/tmp/pg"
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] initializing target db with parameters $INITDB_PARAMS"
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/initdb ${INITDB_PARAMS} --pgdata="$MIGRATION_PATH/tmp/pg"
+
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] initialize complete, copying configs"
+    mkdir "/tmp/configs/"
+    cp /var/lib/pgsql/data/${DATA_DIR}/*.conf "/tmp/configs/"
+
+    echo "turning off wal archiving"
+    sed -e '/archive_command/ s/^#*/#/' -i "$MIGRATION_PATH/tmp/pg/postgresql.conf"
+    sed -e '/archive_mode/ s/^#*/#/' -i "$MIGRATION_PATH/tmp/pg/postgresql.conf"
+    sed -e '/ssl/ s/^#*/#/' -i "$MIGRATION_PATH/tmp/pg/postgresql.conf"
+    sed -e '/archive_command/ s/^#*/#/' -i "/var/lib/pgsql/data/${DATA_DIR}/postgresql.conf"
+    sed -e '/archive_mode/ s/^#*/#/' -i "/var/lib/pgsql/data/${DATA_DIR}/postgresql.conf"
+    sed -e '/ssl/ s/^#*/#/' -i "/var/lib/pgsql/data/${DATA_DIR}/postgresql.conf"
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] proceed with upgrade"
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Content of /tmp/configs:"
+    ls -la /tmp/configs/
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Content of /tmp/configs/postgresql.conf file:"
+    cat /tmp/configs/postgresql.conf
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] making chmod 750 to datadir"
+
+    chmod 750 $MIGRATION_PATH/${DATA_DIR}
+
+    SHARED_PRELOAD_LIBRARIES=$(grep "shared_preload_libraries" /var/lib/pgsql/data/${DATA_DIR}/postgresql.conf)
+
+    if [[ -z ${SHARED_PRELOAD_LIBRARIES} ]]; then
+        echo "shared_preload_libraries is not found in PostgreSQL config, please check PostgreSQL params, exiting..."
+        exit 1
+    fi
+
+    echo ${SHARED_PRELOAD_LIBRARIES} >> $MIGRATION_PATH/tmp/pg/postgresql.conf
+
+    ls -la $MIGRATION_PATH
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Check cluster before upgrade"
+      /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/pg_upgrade \
+      --old-datadir "/var/lib/pgsql/data/${DATA_DIR}" \
+      --new-datadir "$MIGRATION_PATH/tmp/pg" \
+      --old-bindir  "/usr/lib/postgresql/${PG_VERSION}/bin" \
+      --new-bindir  "/usr/lib/postgresql/${PG_VERSION_TARGET}/bin" \
+      --check \
+      > /var/lib/pgsql/data/check_result
+
+    CHECK_CODE=$?
+
+    if [[ "$CHECK_CODE" -ne 0 ]]; then
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Check cluster before upgrade - Failed."
+        echo "check exit code: ${CHECK_CODE}"
+        cat /var/lib/pgsql/data/check_result
+        exit 13
+    fi
+
+    if [[ "$OPERATOR" =~ ^[Tt]rue$ ]]; then
+      echo "[$(date +%Y-%m-%dT%H:%M:%S)] using link parameter"
+      /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/pg_upgrade \
+      --link \
+      --old-datadir "/var/lib/pgsql/data/${DATA_DIR}" \
+      --new-datadir "$MIGRATION_PATH/tmp/pg" \
+      --old-bindir  "/usr/lib/postgresql/${PG_VERSION}/bin" \
+      --new-bindir  "/usr/lib/postgresql/${PG_VERSION_TARGET}/bin"
+    else
+      /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/pg_upgrade \
+      --old-datadir "/var/lib/pgsql/data/${DATA_DIR}" \
+      --new-datadir "$MIGRATION_PATH/tmp/pg" \
+      --old-bindir  "/usr/lib/postgresql/${PG_VERSION}/bin" \
+      --new-bindir  "/usr/lib/postgresql/${PG_VERSION_TARGET}/bin"
+    fi
+
+    EXIT_CODE=$?
+    if [[ "$EXIT_CODE" -ne 0 ]]; then
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Error! Can not proceed, because upgrade failed, exiting"
+        ls -al /var/lib/pgsql/data
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Printing all of the logs:"
+        awk '{print}' /var/lib/pgsql/data/*.log
+#        cat pg_upgrade_server.log
+        exit 1
+    fi
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Upgrade Successful"
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Sizing After Upgrade"
+    du -sh /var/lib/pgsql/data/
+
+    rm -rf "/var/lib/pgsql/data/${DATA_DIR}"
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] moving new data to directory"
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] from -> $MIGRATION_PATH/tmp/pg"
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] to -> /var/lib/pgsql/data/${DATA_DIR}"
+    mv "$MIGRATION_PATH/tmp/pg" "/var/lib/pgsql/data/${DATA_DIR}"
+
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/pg_ctl start -D "/var/lib/pgsql/data/${DATA_DIR}"
+
+    until psql -c "select 1" > /dev/null 2>&1 || [ $RETRIES -eq 0 ]; do
+      echo "[$(date +%Y-%m-%dT%H:%M:%S)] Waiting for postgres server, $((RETRIES--)) remaining attempts..."
+      sleep ${SLEEP_BETWEEN_ITERATIONS}
+    done
+
+    EXIT_CODE=$?
+    if [[ "$EXIT_CODE" -ne 0 ]]; then
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Error! Can not proceed, because select failed, exiting"
+        cat pg_upgrade_server.log
+        exit 1
+    fi
+
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/psql -c "DROP ROLE IF EXISTS replicator; CREATE ROLE replicator with replication login password '$PGREPLPASSWORD';"
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/psql -c "ALTER ROLE postgres with password '$PGPASSWORD';"
+
+    # update extensions
+    [ -f /var/lib/pgsql/data/update_extensions.sql ] && psql --username=postgres --file=/var/lib/pgsql/data/update_extensions.sql postgres
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Running vacuumdb --all --analyze-in-stages"
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/vacuumdb --username=postgres --all --analyze-in-stages
+
+    /usr/lib/postgresql/"${PG_VERSION_TARGET}"/bin/pg_ctl stop -D "/var/lib/pgsql/data/${DATA_DIR}"
+
+    [ -f /var/lib/pgsql/data/target_version ] && rm -rf /var/lib/pgsql/data/target_version
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] moving configs to directory"
+    mv /tmp/configs/* "/var/lib/pgsql/data/${DATA_DIR}/"
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] exiting"
+}
+
+function handle_replica_upgrade() {
+    rm -rf "/var/lib/pgsql/data/${DATA_DIR}/*"
+}
+
+# restricted scc
+function check_user(){
+    cur_user=$(id -u)
+    if [[ "$cur_user" != "26" ]]
+    then
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] starting as not postgres user"
+        set -e
+
+        echo "[$(date +%Y-%m-%dT%H:%M:%S)] Adding randomly generated uid to passwd file..."
+
+        sed -i '/postgres/d' /etc/passwd
+
+        if ! whoami &> /dev/null; then
+          if [[ -w /etc/passwd ]]; then
+            export USER_ID=$(id -u)
+            export GROUP_ID=$(id -g)
+            echo "postgres:x:${USER_ID}:${GROUP_ID}:PostgreSQL Server:${PGDATA}:/bin/bash" >> /etc/passwd
+            echo "UID added ..."
+          fi
+        fi
+
+    fi
+}
+
+function check_pgsql_version(){
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] => Check Source And Target PGSQL version"
+    # get version of data files
+    PG_VERSION=$(head -n 1 "/var/lib/pgsql/data/${DATA_DIR}/PG_VERSION")
+
+    if python -c "import sys; sys.exit(0 if 11.0 <= float("${PG_VERSION}") < 12.0 else 1)"; then
+        PG_VERSION_TARGET="12"
+    elif python -c "import sys; sys.exit(0 if 10.0 <= float("${PG_VERSION}") < 11.0 else 1)"; then
+        PG_VERSION_TARGET="11"
+    else
+        PG_VERSION_TARGET="10"
+    fi
+
+    for i in {1..10}; do
+      echo "[$(date +%Y-%m-%dT%H:%M:%S)] Will try to find target_version file"
+      [ -f /var/lib/pgsql/data/target_version ] && echo "Target file found, will use version from this file" && \
+        PG_VERSION_TARGET=`cat /var/lib/pgsql/data/target_version` && break
+      sleep 1
+    done
+
+    echo "[$(date +%Y-%m-%dT%H:%M:%S)] Target version of PostgreSQL is $PG_VERSION_TARGET"
+}
+
+check_user
+
+check_pgsql_version
+
+if [[ "${TYPE}" == "master" ]]; then
+    handle_master_upgrade
+elif [[ "${TYPE}" == "replica" ]]; then
+    handle_replica_upgrade
+fi
+
+exit 0
diff --git a/Makefile b/operator/Makefile
similarity index 100%
rename from Makefile
rename to operator/Makefile
diff --git a/api/apps/v1/groupversion_info.go b/operator/api/apps/v1/groupversion_info.go
similarity index 100%
rename from api/apps/v1/groupversion_info.go
rename to operator/api/apps/v1/groupversion_info.go
diff --git a/api/apps/v1/postgresservice_types.go b/operator/api/apps/v1/postgresservice_types.go
similarity index 100%
rename from api/apps/v1/postgresservice_types.go
rename to operator/api/apps/v1/postgresservice_types.go
diff --git a/api/apps/v1/zz_generated.deepcopy.go b/operator/api/apps/v1/zz_generated.deepcopy.go
similarity index 100%
rename from api/apps/v1/zz_generated.deepcopy.go
rename to operator/api/apps/v1/zz_generated.deepcopy.go
diff --git a/api/apps/v1/zz_generated.openapi.go b/operator/api/apps/v1/zz_generated.openapi.go
similarity index 100%
rename from api/apps/v1/zz_generated.openapi.go
rename to operator/api/apps/v1/zz_generated.openapi.go
diff --git a/api/common/v1/common_types.go b/operator/api/common/v1/common_types.go
similarity index 100%
rename from api/common/v1/common_types.go
rename to operator/api/common/v1/common_types.go
diff --git a/api/common/v1/zz_generated.deepcopy.go b/operator/api/common/v1/zz_generated.deepcopy.go
similarity index 100%
rename from api/common/v1/zz_generated.deepcopy.go
rename to operator/api/common/v1/zz_generated.deepcopy.go
diff --git a/api/patroni/v1/groupversion_info.go b/operator/api/patroni/v1/groupversion_info.go
similarity index 100%
rename from api/patroni/v1/groupversion_info.go
rename to operator/api/patroni/v1/groupversion_info.go
diff --git a/api/patroni/v1/patronicore_types.go b/operator/api/patroni/v1/patronicore_types.go
similarity index 100%
rename from api/patroni/v1/patronicore_types.go
rename to operator/api/patroni/v1/patronicore_types.go
diff --git a/api/patroni/v1/zz_generated.deepcopy.go b/operator/api/patroni/v1/zz_generated.deepcopy.go
similarity index 100%
rename from api/patroni/v1/zz_generated.deepcopy.go
rename to operator/api/patroni/v1/zz_generated.deepcopy.go
diff --git a/api/patroni/v1/zz_generated.openapi.go b/operator/api/patroni/v1/zz_generated.openapi.go
similarity index 100%
rename from api/patroni/v1/zz_generated.openapi.go
rename to operator/api/patroni/v1/zz_generated.openapi.go
diff --git a/build/Dockerfile b/operator/build/Dockerfile
similarity index 100%
rename from build/Dockerfile
rename to operator/build/Dockerfile
diff --git a/build/bin/entrypoint b/operator/build/bin/entrypoint
similarity index 100%
rename from build/bin/entrypoint
rename to operator/build/bin/entrypoint
diff --git a/build/bin/user_setup b/operator/build/bin/user_setup
similarity index 100%
rename from build/bin/user_setup
rename to operator/build/bin/user_setup
diff --git a/build/configs/influxdb-telegraf-configmap b/operator/build/configs/influxdb-telegraf-configmap
similarity index 100%
rename from build/configs/influxdb-telegraf-configmap
rename to operator/build/configs/influxdb-telegraf-configmap
diff --git a/build/configs/patroni.config.yaml b/operator/build/configs/patroni.config.yaml
similarity index 100%
rename from build/configs/patroni.config.yaml
rename to operator/build/configs/patroni.config.yaml
diff --git a/build/configs/postgres b/operator/build/configs/postgres
similarity index 100%
rename from build/configs/postgres
rename to operator/build/configs/postgres
diff --git a/build/configs/postgres-backup-daemon.collector-config b/operator/build/configs/postgres-backup-daemon.collector-config
similarity index 100%
rename from build/configs/postgres-backup-daemon.collector-config
rename to operator/build/configs/postgres-backup-daemon.collector-config
diff --git a/build/configs/postgres-granular-backup-daemon.collector-config b/operator/build/configs/postgres-granular-backup-daemon.collector-config
similarity index 100%
rename from build/configs/postgres-granular-backup-daemon.collector-config
rename to operator/build/configs/postgres-granular-backup-daemon.collector-config
diff --git a/build/configs/postgresql/postgresql_prepare.sql b/operator/build/configs/postgresql/postgresql_prepare.sql
similarity index 100%
rename from build/configs/postgresql/postgresql_prepare.sql
rename to operator/build/configs/postgresql/postgresql_prepare.sql
diff --git a/build/configs/telegraf-configmap b/operator/build/configs/telegraf-configmap
similarity index 100%
rename from build/configs/telegraf-configmap
rename to operator/build/configs/telegraf-configmap
diff --git a/charts/patroni-core/.helmignore b/operator/charts/patroni-core/.helmignore
similarity index 100%
rename from charts/patroni-core/.helmignore
rename to operator/charts/patroni-core/.helmignore
diff --git a/charts/patroni-core/Chart.yaml b/operator/charts/patroni-core/Chart.yaml
similarity index 100%
rename from charts/patroni-core/Chart.yaml
rename to operator/charts/patroni-core/Chart.yaml
diff --git a/charts/patroni-core/crds/netcracker.com_patronicores.yaml b/operator/charts/patroni-core/crds/netcracker.com_patronicores.yaml
similarity index 100%
rename from charts/patroni-core/crds/netcracker.com_patronicores.yaml
rename to operator/charts/patroni-core/crds/netcracker.com_patronicores.yaml
diff --git a/charts/patroni-core/patroni-core-quickstart-sample.yaml b/operator/charts/patroni-core/patroni-core-quickstart-sample.yaml
similarity index 100%
rename from charts/patroni-core/patroni-core-quickstart-sample.yaml
rename to operator/charts/patroni-core/patroni-core-quickstart-sample.yaml
diff --git a/charts/patroni-core/templates/_helpers.tpl b/operator/charts/patroni-core/templates/_helpers.tpl
similarity index 100%
rename from charts/patroni-core/templates/_helpers.tpl
rename to operator/charts/patroni-core/templates/_helpers.tpl
diff --git a/charts/patroni-core/templates/cr.yaml b/operator/charts/patroni-core/templates/cr.yaml
similarity index 100%
rename from charts/patroni-core/templates/cr.yaml
rename to operator/charts/patroni-core/templates/cr.yaml
diff --git a/charts/patroni-core/templates/deployment.yaml b/operator/charts/patroni-core/templates/deployment.yaml
similarity index 100%
rename from charts/patroni-core/templates/deployment.yaml
rename to operator/charts/patroni-core/templates/deployment.yaml
diff --git a/charts/patroni-core/templates/hooks/creds-hook.yaml b/operator/charts/patroni-core/templates/hooks/creds-hook.yaml
similarity index 100%
rename from charts/patroni-core/templates/hooks/creds-hook.yaml
rename to operator/charts/patroni-core/templates/hooks/creds-hook.yaml
diff --git a/charts/patroni-core/templates/hooks/role.yaml b/operator/charts/patroni-core/templates/hooks/role.yaml
similarity index 100%
rename from charts/patroni-core/templates/hooks/role.yaml
rename to operator/charts/patroni-core/templates/hooks/role.yaml
diff --git a/charts/patroni-core/templates/hooks/role_binding.yaml b/operator/charts/patroni-core/templates/hooks/role_binding.yaml
similarity index 100%
rename from charts/patroni-core/templates/hooks/role_binding.yaml
rename to operator/charts/patroni-core/templates/hooks/role_binding.yaml
diff --git a/charts/patroni-core/templates/hooks/serviceaccount.yaml b/operator/charts/patroni-core/templates/hooks/serviceaccount.yaml
similarity index 100%
rename from charts/patroni-core/templates/hooks/serviceaccount.yaml
rename to operator/charts/patroni-core/templates/hooks/serviceaccount.yaml
diff --git a/charts/patroni-core/templates/patroni-pdb.yaml b/operator/charts/patroni-core/templates/patroni-pdb.yaml
similarity index 100%
rename from charts/patroni-core/templates/patroni-pdb.yaml
rename to operator/charts/patroni-core/templates/patroni-pdb.yaml
diff --git a/charts/patroni-core/templates/role.yaml b/operator/charts/patroni-core/templates/role.yaml
similarity index 100%
rename from charts/patroni-core/templates/role.yaml
rename to operator/charts/patroni-core/templates/role.yaml
diff --git a/charts/patroni-core/templates/role_binding.yaml b/operator/charts/patroni-core/templates/role_binding.yaml
similarity index 100%
rename from charts/patroni-core/templates/role_binding.yaml
rename to operator/charts/patroni-core/templates/role_binding.yaml
diff --git a/charts/patroni-core/templates/secrets/postgres-secret.yaml b/operator/charts/patroni-core/templates/secrets/postgres-secret.yaml
similarity index 100%
rename from charts/patroni-core/templates/secrets/postgres-secret.yaml
rename to operator/charts/patroni-core/templates/secrets/postgres-secret.yaml
diff --git a/charts/patroni-core/templates/secrets/powa-secret.yaml b/operator/charts/patroni-core/templates/secrets/powa-secret.yaml
similarity index 100%
rename from charts/patroni-core/templates/secrets/powa-secret.yaml
rename to operator/charts/patroni-core/templates/secrets/powa-secret.yaml
diff --git a/charts/patroni-core/templates/secrets/registry-secret.yaml b/operator/charts/patroni-core/templates/secrets/registry-secret.yaml
similarity index 100%
rename from charts/patroni-core/templates/secrets/registry-secret.yaml
rename to operator/charts/patroni-core/templates/secrets/registry-secret.yaml
diff --git a/charts/patroni-core/templates/secrets/replicator-secret.yaml b/operator/charts/patroni-core/templates/secrets/replicator-secret.yaml
similarity index 100%
rename from charts/patroni-core/templates/secrets/replicator-secret.yaml
rename to operator/charts/patroni-core/templates/secrets/replicator-secret.yaml
diff --git a/charts/patroni-core/templates/serviceaccount.yaml b/operator/charts/patroni-core/templates/serviceaccount.yaml
similarity index 100%
rename from charts/patroni-core/templates/serviceaccount.yaml
rename to operator/charts/patroni-core/templates/serviceaccount.yaml
diff --git a/charts/patroni-core/templates/tests/tests-config.yaml b/operator/charts/patroni-core/templates/tests/tests-config.yaml
similarity index 100%
rename from charts/patroni-core/templates/tests/tests-config.yaml
rename to operator/charts/patroni-core/templates/tests/tests-config.yaml
diff --git a/charts/patroni-core/templates/tls/postgres-certificate-secret.yaml b/operator/charts/patroni-core/templates/tls/postgres-certificate-secret.yaml
similarity index 96%
rename from charts/patroni-core/templates/tls/postgres-certificate-secret.yaml
rename to operator/charts/patroni-core/templates/tls/postgres-certificate-secret.yaml
index b37e2e6..332d0ab 100644
--- a/charts/patroni-core/templates/tls/postgres-certificate-secret.yaml
+++ b/operator/charts/patroni-core/templates/tls/postgres-certificate-secret.yaml
@@ -1,21 +1,21 @@
-{{- if not .Values.externalDataBase }}
-{{- if and (not .Values.tls.generateCerts.enabled ) (.Values.tls.enabled)}}
-kind: Secret
-apiVersion: v1
-metadata:
-    name: {{ .Values.tls.certificateSecretName }}
-    labels:
-        name: patroni-core
-            {{ include "kubernetes.labels" . | nindent 4 }}
-    namespace: {{ .Release.Namespace }}
-data:
-    tls.crt: {{ .Values.tls.certificates.tls_crt | quote }}
-
-    tls.key: {{ .Values.tls.certificates.tls_key | quote }}
-
-    ca.crt: {{ .Values.tls.certificates.ca_crt | quote }}
-
-type: Opaque
-{{ end }}
-{{ end }}
-
+{{- if not .Values.externalDataBase }}
+{{- if and (not .Values.tls.generateCerts.enabled ) (.Values.tls.enabled)}}
+kind: Secret
+apiVersion: v1
+metadata:
+    name: {{ .Values.tls.certificateSecretName }}
+    labels:
+        name: patroni-core
+            {{ include "kubernetes.labels" . | nindent 4 }}
+    namespace: {{ .Release.Namespace }}
+data:
+    tls.crt: {{ .Values.tls.certificates.tls_crt | quote }}
+
+    tls.key: {{ .Values.tls.certificates.tls_key | quote }}
+
+    ca.crt: {{ .Values.tls.certificates.ca_crt | quote }}
+
+type: Opaque
+{{ end }}
+{{ end }}
+
diff --git a/charts/patroni-core/templates/tls/postgres-tls-certificate.yaml b/operator/charts/patroni-core/templates/tls/postgres-tls-certificate.yaml
similarity index 100%
rename from charts/patroni-core/templates/tls/postgres-tls-certificate.yaml
rename to operator/charts/patroni-core/templates/tls/postgres-tls-certificate.yaml
diff --git a/charts/patroni-core/templates/tls/postgres-tls-issuer.yaml b/operator/charts/patroni-core/templates/tls/postgres-tls-issuer.yaml
similarity index 100%
rename from charts/patroni-core/templates/tls/postgres-tls-issuer.yaml
rename to operator/charts/patroni-core/templates/tls/postgres-tls-issuer.yaml
diff --git a/charts/patroni-core/values.yaml b/operator/charts/patroni-core/values.yaml
similarity index 100%
rename from charts/patroni-core/values.yaml
rename to operator/charts/patroni-core/values.yaml
diff --git a/charts/patroni-services/.helmignore b/operator/charts/patroni-services/.helmignore
similarity index 100%
rename from charts/patroni-services/.helmignore
rename to operator/charts/patroni-services/.helmignore
diff --git a/charts/patroni-services/Chart.yaml b/operator/charts/patroni-services/Chart.yaml
similarity index 100%
rename from charts/patroni-services/Chart.yaml
rename to operator/charts/patroni-services/Chart.yaml
diff --git a/charts/patroni-services/crds/netcracker.com_patroniservices.yaml b/operator/charts/patroni-services/crds/netcracker.com_patroniservices.yaml
similarity index 100%
rename from charts/patroni-services/crds/netcracker.com_patroniservices.yaml
rename to operator/charts/patroni-services/crds/netcracker.com_patroniservices.yaml
diff --git a/charts/patroni-services/monitoring/aws-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/aws-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/aws-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/aws-grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/azure-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/azure-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/azure-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/azure-grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/cloudsql-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/cloudsql-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/cloudsql-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/cloudsql-grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/dbaas-postgres-adapter.collector-config b/operator/charts/patroni-services/monitoring/dbaas-postgres-adapter.collector-config
similarity index 100%
rename from charts/patroni-services/monitoring/dbaas-postgres-adapter.collector-config
rename to operator/charts/patroni-services/monitoring/dbaas-postgres-adapter.collector-config
diff --git a/charts/patroni-services/monitoring/grafana-dashboard.json b/operator/charts/patroni-services/monitoring/grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/pgbackrest-exporter-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/pgbackrest-exporter-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/pgbackrest-exporter-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/pgbackrest-exporter-grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/postgres-exporter-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/postgres-exporter-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/postgres-exporter-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/postgres-exporter-grafana-dashboard.json
diff --git a/charts/patroni-services/monitoring/query-exporter-grafana-dashboard.json b/operator/charts/patroni-services/monitoring/query-exporter-grafana-dashboard.json
similarity index 100%
rename from charts/patroni-services/monitoring/query-exporter-grafana-dashboard.json
rename to operator/charts/patroni-services/monitoring/query-exporter-grafana-dashboard.json
diff --git a/charts/patroni-services/patroni-services-quickstart-sample.yaml b/operator/charts/patroni-services/patroni-services-quickstart-sample.yaml
similarity index 100%
rename from charts/patroni-services/patroni-services-quickstart-sample.yaml
rename to operator/charts/patroni-services/patroni-services-quickstart-sample.yaml
diff --git a/charts/patroni-services/query-exporter/query-exporter-queries.yaml b/operator/charts/patroni-services/query-exporter/query-exporter-queries.yaml
similarity index 100%
rename from charts/patroni-services/query-exporter/query-exporter-queries.yaml
rename to operator/charts/patroni-services/query-exporter/query-exporter-queries.yaml
diff --git a/charts/patroni-services/templates/_helpers.tpl b/operator/charts/patroni-services/templates/_helpers.tpl
similarity index 100%
rename from charts/patroni-services/templates/_helpers.tpl
rename to operator/charts/patroni-services/templates/_helpers.tpl
diff --git a/charts/patroni-services/templates/cr.yaml b/operator/charts/patroni-services/templates/cr.yaml
similarity index 100%
rename from charts/patroni-services/templates/cr.yaml
rename to operator/charts/patroni-services/templates/cr.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-adapter-credentials.yml b/operator/charts/patroni-services/templates/dbaas/dbaas-adapter-credentials.yml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-adapter-credentials.yml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-adapter-credentials.yml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-adapter-deployment.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-adapter-deployment.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-adapter-deployment.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-adapter-deployment.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-aggregator-registration-credentials.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-aggregator-registration-credentials.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-aggregator-registration-credentials.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-aggregator-registration-credentials.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-physical-databases-labels.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-physical-databases-labels.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-physical-databases-labels.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-physical-databases-labels.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter-extensions-config.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter-extensions-config.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-postgres-adapter-extensions-config.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter-extensions-config.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter.collector-config.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter.collector-config.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-postgres-adapter.collector-config.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-postgres-adapter.collector-config.yaml
diff --git a/charts/patroni-services/templates/dbaas/dbaas-service.yaml b/operator/charts/patroni-services/templates/dbaas/dbaas-service.yaml
similarity index 100%
rename from charts/patroni-services/templates/dbaas/dbaas-service.yaml
rename to operator/charts/patroni-services/templates/dbaas/dbaas-service.yaml
diff --git a/charts/patroni-services/templates/deployment.yaml b/operator/charts/patroni-services/templates/deployment.yaml
similarity index 100%
rename from charts/patroni-services/templates/deployment.yaml
rename to operator/charts/patroni-services/templates/deployment.yaml
diff --git a/charts/patroni-services/templates/external-database/cloud-sql-cm.yaml b/operator/charts/patroni-services/templates/external-database/cloud-sql-cm.yaml
similarity index 100%
rename from charts/patroni-services/templates/external-database/cloud-sql-cm.yaml
rename to operator/charts/patroni-services/templates/external-database/cloud-sql-cm.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/backrest-exporter-monitor.yaml b/operator/charts/patroni-services/templates/monitoring-templates/backrest-exporter-monitor.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/backrest-exporter-monitor.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/backrest-exporter-monitor.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/dashboards/aws-grafana-dashboard.yaml b/operator/charts/patroni-services/templates/monitoring-templates/dashboards/aws-grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/dashboards/aws-grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/dashboards/aws-grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/dashboards/azure-grafana-dashboard.yaml b/operator/charts/patroni-services/templates/monitoring-templates/dashboards/azure-grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/dashboards/azure-grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/dashboards/azure-grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/dashboards/cloud-sql-grafana-dashboard.yaml b/operator/charts/patroni-services/templates/monitoring-templates/dashboards/cloud-sql-grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/dashboards/cloud-sql-grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/dashboards/cloud-sql-grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/dashboards/grafana-dashboard.yaml b/operator/charts/patroni-services/templates/monitoring-templates/dashboards/grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/dashboards/grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/dashboards/grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/dashboards/pgbackrest-exporter-grafana-dashboard.yaml b/operator/charts/patroni-services/templates/monitoring-templates/dashboards/pgbackrest-exporter-grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/dashboards/pgbackrest-exporter-grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/dashboards/pgbackrest-exporter-grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/postgres-tls-status-metric.yaml b/operator/charts/patroni-services/templates/monitoring-templates/postgres-tls-status-metric.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/postgres-tls-status-metric.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/postgres-tls-status-metric.yaml
diff --git a/charts/patroni-services/templates/monitoring-templates/prometheus-rule.yml b/operator/charts/patroni-services/templates/monitoring-templates/prometheus-rule.yml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/prometheus-rule.yml
rename to operator/charts/patroni-services/templates/monitoring-templates/prometheus-rule.yml
diff --git a/charts/patroni-services/templates/monitoring-templates/service-monitor.yaml b/operator/charts/patroni-services/templates/monitoring-templates/service-monitor.yaml
similarity index 100%
rename from charts/patroni-services/templates/monitoring-templates/service-monitor.yaml
rename to operator/charts/patroni-services/templates/monitoring-templates/service-monitor.yaml
diff --git a/charts/patroni-services/templates/patroni-pdb.yaml b/operator/charts/patroni-services/templates/patroni-pdb.yaml
similarity index 100%
rename from charts/patroni-services/templates/patroni-pdb.yaml
rename to operator/charts/patroni-services/templates/patroni-pdb.yaml
diff --git a/charts/patroni-services/templates/powa-ingress-deployment.yaml b/operator/charts/patroni-services/templates/powa-ingress-deployment.yaml
similarity index 100%
rename from charts/patroni-services/templates/powa-ingress-deployment.yaml
rename to operator/charts/patroni-services/templates/powa-ingress-deployment.yaml
diff --git a/charts/patroni-services/templates/query-exporter/query-exporter-config-map.yaml b/operator/charts/patroni-services/templates/query-exporter/query-exporter-config-map.yaml
similarity index 100%
rename from charts/patroni-services/templates/query-exporter/query-exporter-config-map.yaml
rename to operator/charts/patroni-services/templates/query-exporter/query-exporter-config-map.yaml
diff --git a/charts/patroni-services/templates/query-exporter/query-exporter-grafana-dashboard.yaml b/operator/charts/patroni-services/templates/query-exporter/query-exporter-grafana-dashboard.yaml
similarity index 100%
rename from charts/patroni-services/templates/query-exporter/query-exporter-grafana-dashboard.yaml
rename to operator/charts/patroni-services/templates/query-exporter/query-exporter-grafana-dashboard.yaml
diff --git a/charts/patroni-services/templates/query-exporter/query-exporter-service-monitor.yaml b/operator/charts/patroni-services/templates/query-exporter/query-exporter-service-monitor.yaml
similarity index 100%
rename from charts/patroni-services/templates/query-exporter/query-exporter-service-monitor.yaml
rename to operator/charts/patroni-services/templates/query-exporter/query-exporter-service-monitor.yaml
diff --git a/charts/patroni-services/templates/query-exporter/query-exporter-user-credentials.yaml b/operator/charts/patroni-services/templates/query-exporter/query-exporter-user-credentials.yaml
similarity index 100%
rename from charts/patroni-services/templates/query-exporter/query-exporter-user-credentials.yaml
rename to operator/charts/patroni-services/templates/query-exporter/query-exporter-user-credentials.yaml
diff --git a/charts/patroni-services/templates/role.yaml b/operator/charts/patroni-services/templates/role.yaml
similarity index 100%
rename from charts/patroni-services/templates/role.yaml
rename to operator/charts/patroni-services/templates/role.yaml
diff --git a/charts/patroni-services/templates/role_binding.yaml b/operator/charts/patroni-services/templates/role_binding.yaml
similarity index 100%
rename from charts/patroni-services/templates/role_binding.yaml
rename to operator/charts/patroni-services/templates/role_binding.yaml
diff --git a/charts/patroni-services/templates/secrets/aws-credentials-secret.yaml b/operator/charts/patroni-services/templates/secrets/aws-credentials-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/aws-credentials-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/aws-credentials-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/external-database-secret.yaml b/operator/charts/patroni-services/templates/secrets/external-database-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/external-database-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/external-database-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/influx-db-secret.yaml b/operator/charts/patroni-services/templates/secrets/influx-db-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/influx-db-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/influx-db-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/logical-repliction-controller.yaml b/operator/charts/patroni-services/templates/secrets/logical-repliction-controller.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/logical-repliction-controller.yaml
rename to operator/charts/patroni-services/templates/secrets/logical-repliction-controller.yaml
diff --git a/charts/patroni-services/templates/secrets/monitoring-secret.yaml b/operator/charts/patroni-services/templates/secrets/monitoring-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/monitoring-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/monitoring-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/pgbouncer-secret.yaml b/operator/charts/patroni-services/templates/secrets/pgbouncer-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/pgbouncer-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/pgbouncer-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/postgres-secret.yaml b/operator/charts/patroni-services/templates/secrets/postgres-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/postgres-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/postgres-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/registry-secret.yaml b/operator/charts/patroni-services/templates/secrets/registry-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/registry-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/registry-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/replicator-secret.yaml b/operator/charts/patroni-services/templates/secrets/replicator-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/replicator-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/replicator-secret.yaml
diff --git a/charts/patroni-services/templates/secrets/s3-storage-secret.yaml b/operator/charts/patroni-services/templates/secrets/s3-storage-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/secrets/s3-storage-secret.yaml
rename to operator/charts/patroni-services/templates/secrets/s3-storage-secret.yaml
diff --git a/charts/patroni-services/templates/service_account.yaml b/operator/charts/patroni-services/templates/service_account.yaml
similarity index 100%
rename from charts/patroni-services/templates/service_account.yaml
rename to operator/charts/patroni-services/templates/service_account.yaml
diff --git a/charts/patroni-services/templates/tests/tests-config.yaml b/operator/charts/patroni-services/templates/tests/tests-config.yaml
similarity index 100%
rename from charts/patroni-services/templates/tests/tests-config.yaml
rename to operator/charts/patroni-services/templates/tests/tests-config.yaml
diff --git a/charts/patroni-services/templates/tls/postgres-services-certificate-secret.yaml b/operator/charts/patroni-services/templates/tls/postgres-services-certificate-secret.yaml
similarity index 100%
rename from charts/patroni-services/templates/tls/postgres-services-certificate-secret.yaml
rename to operator/charts/patroni-services/templates/tls/postgres-services-certificate-secret.yaml
diff --git a/charts/patroni-services/templates/tls/postgres-services-tls-certificate.yaml b/operator/charts/patroni-services/templates/tls/postgres-services-tls-certificate.yaml
similarity index 100%
rename from charts/patroni-services/templates/tls/postgres-services-tls-certificate.yaml
rename to operator/charts/patroni-services/templates/tls/postgres-services-tls-certificate.yaml
diff --git a/charts/patroni-services/templates/tls/postgres-services-tls-issuer.yaml b/operator/charts/patroni-services/templates/tls/postgres-services-tls-issuer.yaml
similarity index 100%
rename from charts/patroni-services/templates/tls/postgres-services-tls-issuer.yaml
rename to operator/charts/patroni-services/templates/tls/postgres-services-tls-issuer.yaml
diff --git a/charts/patroni-services/values.yaml b/operator/charts/patroni-services/values.yaml
similarity index 100%
rename from charts/patroni-services/values.yaml
rename to operator/charts/patroni-services/values.yaml
diff --git a/cmd/pgskipper-operator/main.go b/operator/cmd/pgskipper-operator/main.go
similarity index 100%
rename from cmd/pgskipper-operator/main.go
rename to operator/cmd/pgskipper-operator/main.go
diff --git a/controllers/conditions.go b/operator/controllers/conditions.go
similarity index 100%
rename from controllers/conditions.go
rename to operator/controllers/conditions.go
diff --git a/controllers/patroni_core_controller.go b/operator/controllers/patroni_core_controller.go
similarity index 100%
rename from controllers/patroni_core_controller.go
rename to operator/controllers/patroni_core_controller.go
diff --git a/controllers/postgresservice_controller.go b/operator/controllers/postgresservice_controller.go
similarity index 100%
rename from controllers/postgresservice_controller.go
rename to operator/controllers/postgresservice_controller.go
diff --git a/generator/boilerplate.go.txt b/operator/generator/boilerplate.go.txt
similarity index 100%
rename from generator/boilerplate.go.txt
rename to operator/generator/boilerplate.go.txt
diff --git a/go.mod b/operator/go.mod
similarity index 100%
rename from go.mod
rename to operator/go.mod
diff --git a/go.sum b/operator/go.sum
similarity index 100%
rename from go.sum
rename to operator/go.sum
diff --git a/pkg/client/client.go b/operator/pkg/client/client.go
similarity index 100%
rename from pkg/client/client.go
rename to operator/pkg/client/client.go
diff --git a/pkg/consul/consul.go b/operator/pkg/consul/consul.go
similarity index 100%
rename from pkg/consul/consul.go
rename to operator/pkg/consul/consul.go
diff --git a/pkg/credentials/manager.go b/operator/pkg/credentials/manager.go
similarity index 100%
rename from pkg/credentials/manager.go
rename to operator/pkg/credentials/manager.go
diff --git a/pkg/deployerrors/deployerrors.go b/operator/pkg/deployerrors/deployerrors.go
similarity index 100%
rename from pkg/deployerrors/deployerrors.go
rename to operator/pkg/deployerrors/deployerrors.go
diff --git a/pkg/deployment/backup.go b/operator/pkg/deployment/backup.go
similarity index 100%
rename from pkg/deployment/backup.go
rename to operator/pkg/deployment/backup.go
diff --git a/pkg/deployment/monitoring.go b/operator/pkg/deployment/monitoring.go
similarity index 100%
rename from pkg/deployment/monitoring.go
rename to operator/pkg/deployment/monitoring.go
diff --git a/pkg/deployment/patroni.go b/operator/pkg/deployment/patroni.go
similarity index 100%
rename from pkg/deployment/patroni.go
rename to operator/pkg/deployment/patroni.go
diff --git a/pkg/deployment/pgbackrest.go b/operator/pkg/deployment/pgbackrest.go
similarity index 100%
rename from pkg/deployment/pgbackrest.go
rename to operator/pkg/deployment/pgbackrest.go
diff --git a/pkg/deployment/tests.go b/operator/pkg/deployment/tests.go
similarity index 100%
rename from pkg/deployment/tests.go
rename to operator/pkg/deployment/tests.go
diff --git a/pkg/disasterrecovery/cloud_sql.go b/operator/pkg/disasterrecovery/cloud_sql.go
similarity index 100%
rename from pkg/disasterrecovery/cloud_sql.go
rename to operator/pkg/disasterrecovery/cloud_sql.go
diff --git a/pkg/disasterrecovery/patroni.go b/operator/pkg/disasterrecovery/patroni.go
similarity index 100%
rename from pkg/disasterrecovery/patroni.go
rename to operator/pkg/disasterrecovery/patroni.go
diff --git a/pkg/disasterrecovery/server.go b/operator/pkg/disasterrecovery/server.go
similarity index 100%
rename from pkg/disasterrecovery/server.go
rename to operator/pkg/disasterrecovery/server.go
diff --git a/pkg/helper/helper.go b/operator/pkg/helper/helper.go
similarity index 100%
rename from pkg/helper/helper.go
rename to operator/pkg/helper/helper.go
diff --git a/pkg/helper/patroni_core_helper.go b/operator/pkg/helper/patroni_core_helper.go
similarity index 100%
rename from pkg/helper/patroni_core_helper.go
rename to operator/pkg/helper/patroni_core_helper.go
diff --git a/pkg/helper/resource_management.go b/operator/pkg/helper/resource_management.go
similarity index 100%
rename from pkg/helper/resource_management.go
rename to operator/pkg/helper/resource_management.go
diff --git a/pkg/patroni/patroni.go b/operator/pkg/patroni/patroni.go
similarity index 100%
rename from pkg/patroni/patroni.go
rename to operator/pkg/patroni/patroni.go
diff --git a/pkg/pgbackrestexporter/pgbackrest-exporter.go b/operator/pkg/pgbackrestexporter/pgbackrest-exporter.go
similarity index 100%
rename from pkg/pgbackrestexporter/pgbackrest-exporter.go
rename to operator/pkg/pgbackrestexporter/pgbackrest-exporter.go
diff --git a/pkg/pooler/pooler.go b/operator/pkg/pooler/pooler.go
similarity index 100%
rename from pkg/pooler/pooler.go
rename to operator/pkg/pooler/pooler.go
diff --git a/pkg/postgresexporter/exporter.go b/operator/pkg/postgresexporter/exporter.go
similarity index 100%
rename from pkg/postgresexporter/exporter.go
rename to operator/pkg/postgresexporter/exporter.go
diff --git a/pkg/postgresexporter/watcher.go b/operator/pkg/postgresexporter/watcher.go
similarity index 100%
rename from pkg/postgresexporter/watcher.go
rename to operator/pkg/postgresexporter/watcher.go
diff --git a/pkg/powa/powa.go b/operator/pkg/powa/powa.go
similarity index 100%
rename from pkg/powa/powa.go
rename to operator/pkg/powa/powa.go
diff --git a/pkg/powa/powa_ui.go b/operator/pkg/powa/powa_ui.go
similarity index 100%
rename from pkg/powa/powa_ui.go
rename to operator/pkg/powa/powa_ui.go
diff --git a/pkg/queryexporter/query_exporter.go b/operator/pkg/queryexporter/query_exporter.go
similarity index 100%
rename from pkg/queryexporter/query_exporter.go
rename to operator/pkg/queryexporter/query_exporter.go
diff --git a/pkg/queryexporter/watcher.go b/operator/pkg/queryexporter/watcher.go
similarity index 100%
rename from pkg/queryexporter/watcher.go
rename to operator/pkg/queryexporter/watcher.go
diff --git a/pkg/reconciler/backup_daemon.go b/operator/pkg/reconciler/backup_daemon.go
similarity index 100%
rename from pkg/reconciler/backup_daemon.go
rename to operator/pkg/reconciler/backup_daemon.go
diff --git a/pkg/reconciler/common.go b/operator/pkg/reconciler/common.go
similarity index 100%
rename from pkg/reconciler/common.go
rename to operator/pkg/reconciler/common.go
diff --git a/pkg/reconciler/metric_collector.go b/operator/pkg/reconciler/metric_collector.go
similarity index 100%
rename from pkg/reconciler/metric_collector.go
rename to operator/pkg/reconciler/metric_collector.go
diff --git a/pkg/reconciler/patroni.go b/operator/pkg/reconciler/patroni.go
similarity index 100%
rename from pkg/reconciler/patroni.go
rename to operator/pkg/reconciler/patroni.go
diff --git a/pkg/reconciler/pgbackrest_exporter.go b/operator/pkg/reconciler/pgbackrest_exporter.go
similarity index 100%
rename from pkg/reconciler/pgbackrest_exporter.go
rename to operator/pkg/reconciler/pgbackrest_exporter.go
diff --git a/pkg/reconciler/pooler.go b/operator/pkg/reconciler/pooler.go
similarity index 100%
rename from pkg/reconciler/pooler.go
rename to operator/pkg/reconciler/pooler.go
diff --git a/pkg/reconciler/powa_ui.go b/operator/pkg/reconciler/powa_ui.go
similarity index 100%
rename from pkg/reconciler/powa_ui.go
rename to operator/pkg/reconciler/powa_ui.go
diff --git a/pkg/reconciler/query_exporter.go b/operator/pkg/reconciler/query_exporter.go
similarity index 100%
rename from pkg/reconciler/query_exporter.go
rename to operator/pkg/reconciler/query_exporter.go
diff --git a/pkg/reconciler/replication_controller.go b/operator/pkg/reconciler/replication_controller.go
similarity index 100%
rename from pkg/reconciler/replication_controller.go
rename to operator/pkg/reconciler/replication_controller.go
diff --git a/pkg/reconciler/site_manager.go b/operator/pkg/reconciler/site_manager.go
similarity index 100%
rename from pkg/reconciler/site_manager.go
rename to operator/pkg/reconciler/site_manager.go
diff --git a/pkg/replicationcontroller/replication_controller.go b/operator/pkg/replicationcontroller/replication_controller.go
similarity index 100%
rename from pkg/replicationcontroller/replication_controller.go
rename to operator/pkg/replicationcontroller/replication_controller.go
diff --git a/pkg/scheduler/scheduler.go b/operator/pkg/scheduler/scheduler.go
similarity index 100%
rename from pkg/scheduler/scheduler.go
rename to operator/pkg/scheduler/scheduler.go
diff --git a/pkg/scheduler/slots.go b/operator/pkg/scheduler/slots.go
similarity index 100%
rename from pkg/scheduler/slots.go
rename to operator/pkg/scheduler/slots.go
diff --git a/pkg/storage/storage.go b/operator/pkg/storage/storage.go
similarity index 100%
rename from pkg/storage/storage.go
rename to operator/pkg/storage/storage.go
diff --git a/pkg/tests/tests.go b/operator/pkg/tests/tests.go
similarity index 100%
rename from pkg/tests/tests.go
rename to operator/pkg/tests/tests.go
diff --git a/pkg/upgrade/upgrade.go b/operator/pkg/upgrade/upgrade.go
similarity index 100%
rename from pkg/upgrade/upgrade.go
rename to operator/pkg/upgrade/upgrade.go
diff --git a/pkg/util/constants/constants.go b/operator/pkg/util/constants/constants.go
similarity index 100%
rename from pkg/util/constants/constants.go
rename to operator/pkg/util/constants/constants.go
diff --git a/pkg/util/util.go b/operator/pkg/util/util.go
similarity index 100%
rename from pkg/util/util.go
rename to operator/pkg/util/util.go
diff --git a/pkg/util/wait_util.go b/operator/pkg/util/wait_util.go
similarity index 100%
rename from pkg/util/wait_util.go
rename to operator/pkg/util/wait_util.go
diff --git a/tools.go b/operator/tools.go
similarity index 100%
rename from tools.go
rename to operator/tools.go
diff --git a/version/version.go b/operator/version/version.go
similarity index 100%
rename from version/version.go
rename to operator/version/version.go
